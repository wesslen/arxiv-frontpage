{"text":"We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection.","meta":{"url":"http://arxiv.org/abs/2401.02009v1","data_type":"evaluation"},"label":"production","_input_hash":2139210307,"_task_hash":-424042898,"_view_id":"classification","answer":"reject","_timestamp":1704741761,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.","meta":{"url":"http://arxiv.org/abs/2401.02009v1","data_type":"evaluation"},"label":"production","_input_hash":-99377387,"_task_hash":1582772575,"_view_id":"classification","answer":"reject","_timestamp":1704741771,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes.","meta":{"url":"http://arxiv.org/abs/2401.01256v1","data_type":"evaluation"},"label":"production","_input_hash":2076350902,"_task_hash":-2023435089,"_view_id":"classification","answer":"reject","_timestamp":1704741773,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets.","meta":{"url":"http://arxiv.org/abs/2401.01339v1","data_type":"evaluation"},"label":"production","_input_hash":170335898,"_task_hash":-391485401,"_view_id":"classification","answer":"reject","_timestamp":1704741775,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.","meta":{"url":"http://arxiv.org/abs/2401.01283v1","data_type":"evaluation"},"label":"production","_input_hash":-1828913495,"_task_hash":546153392,"_view_id":"classification","answer":"reject","_timestamp":1704741780,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations.","meta":{"url":"http://arxiv.org/abs/2401.01179v1","data_type":"evaluation"},"label":"production","_input_hash":674764544,"_task_hash":1724107901,"_view_id":"classification","answer":"reject","_timestamp":1704741785,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"While large language models (LLMs) have exhibited impressive instruction-following capabilities, it is still unclear whether and to what extent they can respond to explicit constraints that might be entailed in various instructions.","meta":{"url":"http://arxiv.org/abs/2401.00690v1","data_type":"evaluation"},"label":"production","_input_hash":-1284166565,"_task_hash":1385087072,"_view_id":"classification","answer":"reject","_timestamp":1704741788,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"Finally, we automate the entire evaluation process to facilitate further developments.","meta":{"url":"http://arxiv.org/abs/2401.00690v1","data_type":"evaluation"},"label":"production","_input_hash":470436298,"_task_hash":-1642531777,"_view_id":"classification","answer":"reject","_timestamp":1704741798,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"We believe this benchmark will facilitate research into improving the controllability of LLMs' responses to instructions.","meta":{"url":"http://arxiv.org/abs/2401.00690v1","data_type":"evaluation"},"label":"production","_input_hash":468385443,"_task_hash":2090114279,"_view_id":"classification","answer":"accept","_timestamp":1704741823,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"We open-source our data curation tools and the checkpoints of GeoGalactica during the first 3/4 of pre-training.","meta":{"url":"http://arxiv.org/abs/2401.00434v1","data_type":"evaluation"},"label":"production","_input_hash":-2102583718,"_task_hash":-1709139520,"_view_id":"classification","answer":"reject","_timestamp":1704741827,"_annotator_id":"2024-01-08_14-22-30","_session_id":"2024-01-08_14-22-30"}
{"text":"Training large language models (LLMs) is a costly endeavour in terms of time and computational resources.","meta":{"distance":0.2931442261,"data_type":"evaluation"},"label":"production","_input_hash":93766018,"_task_hash":715086296,"_view_id":"classification","answer":"ignore","_timestamp":1704744498,"_annotator_id":"2024-01-08_15-08-03","_session_id":"2024-01-08_15-08-03"}
{"text":"However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios.","meta":{"distance":0.3189074397,"data_type":"evaluation"},"label":"production","_input_hash":1001053476,"_task_hash":1248649241,"_view_id":"classification","answer":"ignore","_timestamp":1704744520,"_annotator_id":"2024-01-08_15-08-03","_session_id":"2024-01-08_15-08-03"}
{"text":"Large language models (LLMs) have recently demonstrated surprising performance for a range of computing tasks, including generating and explaining code.","meta":{"distance":0.3209130764,"data_type":"evaluation"},"label":"production","_input_hash":946895674,"_task_hash":-1288893295,"_view_id":"classification","answer":"ignore","_timestamp":1704744525,"_annotator_id":"2024-01-08_15-08-03","_session_id":"2024-01-08_15-08-03"}
{"text":"Our Mixed Distillation framework offers a promising approach to enhance the capabilities of smaller models, bridging the gap with LLMs, and demonstrating better performance across various tasks.","meta":{"distance":0.4019542933,"data_type":"evaluation"},"label":"production","_input_hash":1021339903,"_task_hash":-1696365133,"_view_id":"classification","answer":"accept","_timestamp":1704744697,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"Specifically, on the SVAMP dataset, employing a 7 billion parameter Llama2 and CodeLlama in a mixed distillation framework not only boosts distillation capabilities beyond single-path distillation methods but also outperforms the LLM (GPT-3.5-turbo) in terms of reasoning accuracy.","meta":{"distance":0.4485834241,"data_type":"evaluation"},"label":"production","_input_hash":-493701934,"_task_hash":536305364,"_view_id":"classification","answer":"accept","_timestamp":1704744707,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"In this work, we introduce the \\textbf{Mixed Distillation} framework, which capitalizes on the strengths of Program-of-Thought (PoT) and Chain-of-Thought (CoT) capabilities within LLMs and distills these capabilities to smaller models.","meta":{"distance":0.4833549857,"data_type":"evaluation"},"label":"production","_input_hash":262395838,"_task_hash":1496434696,"_view_id":"classification","answer":"accept","_timestamp":1704744727,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"Additionally, existing dataset distillation methods often struggle to generalize to new architectures.","meta":{"distance":0.4996235967,"data_type":"evaluation"},"label":"production","_input_hash":-1082144952,"_task_hash":79494934,"_view_id":"classification","answer":"ignore","_timestamp":1704744733,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements.","meta":{"distance":0.5003793836,"data_type":"evaluation"},"label":"production","_input_hash":-809688768,"_task_hash":2026560447,"_view_id":"classification","answer":"accept","_timestamp":1704744739,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"Through sampling in multiple-path reasoning, the models achieve impressive accuracy performances of 85% and 85.5%, respectively, signifying advancements over previous distillation methods.","meta":{"distance":0.5125718713,"data_type":"evaluation"},"label":"production","_input_hash":-915445313,"_task_hash":-368467035,"_view_id":"classification","answer":"ignore","_timestamp":1704744748,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"We call such a process self score distillation (SSD).","meta":{"distance":0.5525942445,"data_type":"evaluation"},"label":"production","_input_hash":-486135703,"_task_hash":947592244,"_view_id":"classification","answer":"ignore","_timestamp":1704744791,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
{"text":"Recent research has concentrated on improving open-source smaller models through knowledge distillation from LLMs to reduce computational resource costs with promising outcomes.","meta":{"distance":0.567035675,"data_type":"evaluation"},"label":"production","_input_hash":-1731962989,"_task_hash":-1716950032,"_view_id":"classification","answer":"accept","_timestamp":1704744819,"_annotator_id":"2024-01-08_15-11-32","_session_id":"2024-01-08_15-11-32"}
