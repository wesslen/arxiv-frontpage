{"text":"The reflection capacity of Large Language Model (LLM) has garnered extensive attention.","meta":{"url":"http://arxiv.org/abs/2401.02009v1","data_type":"evaluation"},"label":"security","_input_hash":-1706781899,"_task_hash":1137629186,"_view_id":"classification","answer":"reject","_timestamp":1704741415,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts.","meta":{"url":"http://arxiv.org/abs/2401.01256v1","data_type":"evaluation"},"label":"security","_input_hash":303626635,"_task_hash":-1066208059,"_view_id":"classification","answer":"reject","_timestamp":1704741422,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"The resultant entity description is then fed into a text-to-image model to generate a reference image for each entity.","meta":{"url":"http://arxiv.org/abs/2401.01256v1","data_type":"evaluation"},"label":"security","_input_hash":-1906694962,"_task_hash":-221434083,"_view_id":"classification","answer":"reject","_timestamp":1704741424,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"The minimization is solved by exploiting the Wolfe duality theorem, reducing the problem to the solution of a Non-Negative Least Square (NNLS) problem.","meta":{"url":"http://arxiv.org/abs/2401.01199v1","data_type":"evaluation"},"label":"security","_input_hash":296306155,"_task_hash":1290852263,"_view_id":"classification","answer":"reject","_timestamp":1704741426,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world.","meta":{"url":"http://arxiv.org/abs/2401.00741v1","data_type":"evaluation"},"label":"security","_input_hash":-1129349273,"_task_hash":-545926037,"_view_id":"classification","answer":"reject","_timestamp":1704741432,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"These findings offer instructive insights aimed at advancing the field of tool learning.","meta":{"url":"http://arxiv.org/abs/2401.00741v1","data_type":"evaluation"},"label":"security","_input_hash":-1244745259,"_task_hash":2010781080,"_view_id":"classification","answer":"reject","_timestamp":1704741435,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"We provide extensive evaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval, revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs.","meta":{"url":"http://arxiv.org/abs/2401.00690v1","data_type":"evaluation"},"label":"security","_input_hash":31443406,"_task_hash":735191164,"_view_id":"classification","answer":"reject","_timestamp":1704741442,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP).","meta":{"url":"http://arxiv.org/abs/2401.00434v1","data_type":"evaluation"},"label":"security","_input_hash":-923062652,"_task_hash":1187594965,"_view_id":"classification","answer":"reject","_timestamp":1704741445,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"For the projects we study, we are able to globally increase expertise during reviews, +3%, reduce workload concentration, -12%, and reduce the files at risk, -28%.","meta":{"url":"http://arxiv.org/abs/2312.17236v1","data_type":"evaluation"},"label":"security","_input_hash":1044671975,"_task_hash":-1653110389,"_view_id":"classification","answer":"reject","_timestamp":1704741448,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"We make our scripts and data available in our replication package.","meta":{"url":"http://arxiv.org/abs/2312.17236v1","data_type":"evaluation"},"label":"security","_input_hash":-206796601,"_task_hash":921832022,"_view_id":"classification","answer":"reject","_timestamp":1704741450,"_annotator_id":"2024-01-08_14-16-51","_session_id":"2024-01-08_14-16-51"}
{"text":"Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.","meta":{"distance":0.2895673513,"data_type":"evaluation"},"label":"security","_input_hash":1484663133,"_task_hash":-1658558047,"_view_id":"classification","answer":"accept","_timestamp":1704742673,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model.","meta":{"distance":0.3071869016,"data_type":"evaluation"},"label":"security","_input_hash":-865625140,"_task_hash":1217474055,"_view_id":"classification","answer":"accept","_timestamp":1704742676,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters.","meta":{"distance":0.3465656638,"data_type":"evaluation"},"label":"security","_input_hash":-876503934,"_task_hash":-413965764,"_view_id":"classification","answer":"accept","_timestamp":1704742684,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"Using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 LLMs and 7 tasks.","meta":{"distance":0.3499628305,"data_type":"evaluation"},"label":"security","_input_hash":-1563920595,"_task_hash":-769495008,"_view_id":"classification","answer":"accept","_timestamp":1704742690,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires.","meta":{"distance":0.390227139,"data_type":"evaluation"},"label":"security","_input_hash":720841061,"_task_hash":-339832096,"_view_id":"classification","answer":"accept","_timestamp":1704742692,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks.","meta":{"distance":0.4207305908,"data_type":"evaluation"},"label":"security","_input_hash":906048471,"_task_hash":-55357392,"_view_id":"classification","answer":"ignore","_timestamp":1704742698,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"Moreover, we also propose a framework to systematize defenses against prompt injection attacks.","meta":{"distance":0.4247927666,"data_type":"evaluation"},"label":"security","_input_hash":310832134,"_task_hash":1187807168,"_view_id":"classification","answer":"ignore","_timestamp":1704742701,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"We also customize \\texttt{AutoDAN}'s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature.","meta":{"distance":0.4258551598,"data_type":"evaluation"},"label":"security","_input_hash":-1030701070,"_task_hash":-362815690,"_view_id":"classification","answer":"accept","_timestamp":1704742713,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses.","meta":{"distance":0.4301240444,"data_type":"evaluation"},"label":"security","_input_hash":-1821505878,"_task_hash":-440876344,"_view_id":"classification","answer":"ignore","_timestamp":1704742722,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"In particular, we propose a general framework to formalize prompt injection attacks.","meta":{"distance":0.4304734468,"data_type":"evaluation"},"label":"security","_input_hash":-1118087859,"_task_hash":-1419075751,"_view_id":"classification","answer":"accept","_timestamp":1704742725,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
{"text":"Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.","meta":{"distance":0.4562258124,"data_type":"evaluation"},"label":"security","_input_hash":-469678601,"_task_hash":-1647863985,"_view_id":"classification","answer":"accept","_timestamp":1704742739,"_annotator_id":"2024-01-08_14-37-45","_session_id":"2024-01-08_14-37-45"}
