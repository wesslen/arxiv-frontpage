<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Ryan's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-09-18.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by running a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <p class="text-sm text-gray-500 pt-2">This project was originally created by <a href="https://koaning.io/">Vincent Warmerdam</a>, modifying his original <a href="https://koaning.github.io/arxiv-frontpage/">frontpage</a> for different paper categories.</p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Prompt Engineering in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ArticulatePro: A Comparative Study on a Proactive and Non-Proactive Assistant in a Climate Data Exploration Task
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in Natural Language Interfaces (NLIs) and Large Language Models (LLMs) have transformed our approach to NLP tasks, allowing us to focus more on a Pragmatics-based approach.This shift enables more natural interactions between humans and voice assistants, which have been challenging to achieve.<span class='px-1 mx-1 bg-yellow-200'>Pragmatics describes how users often talk out of turn, interrupt each other, or provide relevant information without being explicitly asked (maxim of quantity). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>To explore this, we developed a digital assistant that constantly listens to conversations and proactively generates relevant visualizations during data exploration tasks.In a within-subject study, participants interacted with both proactive and non-proactive versions of a voice assistant while exploring the Hawaii Climate Data Portal (HCDP).<span class='px-1 mx-1 bg-yellow-200'>Results suggest that the proactive assistant enhanced user engagement and facilitated quicker insights. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>Our study highlights the potential of Pragmatic, proactive AI in NLIs and identifies key challenges in its implementation, offering insights for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10797v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10797v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Model (LLM)-based recommendation systems provide more comprehensive recommendations than traditional systems by deeply analyzing content and user behavior.However, these systems often exhibit biases, favoring mainstream content while marginalizing non-traditional options due to skewed training data.This study investigates the intricate relationship between bias and LLM-based recommendation systems, with a focus on music, song, and book recommendations across diverse demographic and cultural groups.Through a comprehensive analysis conducted over different LLM-models, this paper evaluates the impact of bias on recommendation outcomes.<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that bias is so deeply ingrained within these systems that even a simpler intervention like prompt engineering can significantly reduce bias, underscoring the pervasive nature of the issue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.776</span></span>Moreover, factors like intersecting identities and contextual information, such as socioeconomic status, further amplify these biases, demonstrating the complexity and depth of the challenges faced in creating fair recommendations across different groups.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10825v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10825v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have spurred interest in automatic evaluation methods for summarization, offering a faster, more cost-effective alternative to human evaluation.However, existing methods often fall short when applied to complex tasks like long-context summarizations and dialogue-based meeting summarizations.In this paper, we introduce CREAM (Comparison-Based Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a novel framework that addresses the unique challenges of evaluating meeting summaries.CREAM leverages a combination of chain-of-thought reasoning and key facts alignment to assess conciseness and completeness of model-generated summaries without requiring reference.<span class='px-1 mx-1 bg-yellow-200'>By employing an ELO ranking system, our approach provides a robust mechanism for comparing the quality of different models or prompt configurations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10883v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10883v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Obfuscation for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>System prompts that include detailed instructions to describe the task performed by the underlying large language model (LLM) can easily transform foundation models into tools and services with minimal overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product.<span class='px-1 mx-1 bg-yellow-200'>However, extracting system prompts is easily possible by using prompt injection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.786</span></span><span class='px-1 mx-1 bg-yellow-200'>As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded with carefully crafted prompt injections that bypass all protection mechanisms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we propose an alternative to conventional system prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.867</span></span><span class='px-1 mx-1 bg-yellow-200'>We introduce prompt obfuscation to prevent the extraction of the system prompt while maintaining the utility of the system itself with only little overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span><span class='px-1 mx-1 bg-yellow-200'>The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span><span class='px-1 mx-1 bg-yellow-200'>We implement an optimization-based method to find an obfuscated prompt representation while maintaining the functionality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>To evaluate our approach, we investigate eight different metrics to compare the performance of a system using the original and the obfuscated system prompts, and we show that the obfuscated version is constantly on par with the original one.We further perform three different deobfuscation attacks and show that with access to the obfuscated prompt and the LLM itself, we are not able to consistently extract meaningful information.<span class='px-1 mx-1 bg-yellow-200'>Overall, we showed that prompt obfuscation can be an effective method to protect intellectual property while maintaining the same utility as the original system prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11026v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11026v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible.Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one.<span class='px-1 mx-1 bg-yellow-200'>To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.908</span></span><span class='px-1 mx-1 bg-yellow-200'>Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances.Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11056v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11056v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models(LLMs) have exhibited remarkable few-shot learning capabilities and unified the paradigm of NLP tasks through the in-context learning(ICL) technique.Despite the success of ICL, the quality of the exemplar demonstrations can significantly influence the LLM's performance.Existing exemplar selection methods mainly focus on the semantic similarity between queries and candidate exemplars.<span class='px-1 mx-1 bg-yellow-200'>On the other hand, the logical connections between reasoning steps can be beneficial to depict the problem-solving process as well. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>In this paper, we proposes a novel method named Reasoning Graph-enhanced Exemplar Retrieval(RGER).RGER first quires LLM to generate an initial response, then expresses intermediate problem-solving steps to a graph structure.After that, it employs graph kernel to select exemplars with semantic and structural similarity.Extensive experiments demonstrate the structural relationship is helpful to the alignment of queries and candidate exemplars.The efficacy of RGER on math and logit reasoning tasks showcases its superiority over state-of-the-art retrieval-based approaches.Our code is released at https://github.com/Yukang-Lin/RGER.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present SuperCoder2.0, an advanced autonomous system designed to enhance software development through artificial intelligence.The system combines an AI-native development approach with intelligent agents to enable fully autonomous coding.Key focus areas include a retry mechanism with error output traceback, comprehensive code rewriting and replacement using Abstract Syntax Tree (ast) parsing to minimize linting issues, code embedding technique for retrieval-augmented generation, and a focus on localizing methods for problem-solving rather than identifying specific line numbers.The methodology employs a three-step hierarchical search space reduction approach for code base navigation and bug localization:utilizing Retrieval Augmented Generation (RAG) and a Repository File Level Map to identify candidate files, (2) narrowing down to the most relevant files using a File Level Schematic Map, and (3) extracting 'relevant locations' within these files.Code editing is performed through a two-part module comprising CodeGeneration and CodeEditing, which generates multiple solutions at different temperature values and replaces entire methods or classes to maintain code integrity.<span class='px-1 mx-1 bg-yellow-200'>A feedback loop executes repository-level test cases to validate and refine solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>Experiments conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's effectiveness, achieving correct file localization in 84.33% of cases within the top 5 candidates and successfully resolving 34% of test instances.This performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.The system's ability to handle diverse repositories and problem types highlights its potential as a versatile tool for autonomous software development.Future work will focus on refining the code editing process and exploring advanced embedding models for improved natural language to code mapping.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11190v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11190v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLMs are an integral part of retrieval-augmented generation (RAG) systems.While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task.Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework.<span class='px-1 mx-1 bg-yellow-200'>We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score.LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9).We release our code at: https://github.com/declare-lab/trust-align.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11242v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11242v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the Diagram of Thought
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We introduce Diagram of Thought (DoT), a framework that models iterative reasoning in large language models (LLMs) as the construction of a directed acyclic graph (DAG) within a single model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span>Unlike traditional approaches that represent reasoning as linear chains or trees, DoT organizes propositions, critiques, refinements, and verifications into a cohesive DAG structure, allowing the model to explore complex reasoning pathways while maintaining logical consistency.Each node in the diagram corresponds to a proposition that has been proposed, critiqued, refined, or verified, enabling the LLM to iteratively improve its reasoning through natural language feedback.By leveraging auto-regressive next-token prediction with role-specific tokens, DoT facilitates seamless transitions between proposing ideas and critically evaluating them, providing richer feedback than binary signals.Furthermore, we formalize the DoT framework using Topos Theory, providing a mathematical foundation that ensures logical consistency and soundness in the reasoning process.This approach enhances both the training and inference processes within a single LLM, eliminating the need for multiple models or external control mechanisms.DoT offers a conceptual framework for designing next-generation reasoning-specialized models, emphasizing training efficiency, robust reasoning capabilities, and theoretical grounding.The code is available at https://github.com/diagram-of-thought/diagram-of-thought.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10038v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10038v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking Large Language Model Uncertainty for Prompt Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompt optimization algorithms for Large Language Models (LLMs) excel in multi-step reasoning but still lack effective uncertainty estimation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.857</span></span>This paper introduces a benchmark dataset to evaluate uncertainty metrics, focusing on Answer, Correctness, Aleatoric, and Epistemic Uncertainty.Through analysis of models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that current metrics align more with Answer Uncertainty, which reflects output confidence and diversity, rather than Correctness Uncertainty, highlighting the need for improved metrics that are optimization-objective-aware to better guide prompt optimization.Our code and dataset are available at https://github.com/0Frett/PO-Uncertainty-Benchmarking.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10044v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10044v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MotionCom: Automatic and Motion-Aware Image Composition with LLM and Video Diffusion Prior
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This work presents MotionCom, a training-free motion-aware diffusion based image composition, enabling automatic and seamless integration of target objects into new scenes with dynamically coherent results without finetuning or optimization.Traditional approaches in this area suffer from two significant limitations: they require manual planning for object placement and often generate static compositions lacking motion realism.MotionCom addresses these issues by utilizing a Large Vision Language Model (LVLM) for intelligent planning, and a Video Diffusion prior for motion-infused image synthesis, streamlining the composition process.<span class='px-1 mx-1 bg-yellow-200'>Our multi-modal Chain-of-Thought (CoT) prompting with LVLM automates the strategic placement planning of foreground objects, considering their potential motion and interaction within the scenes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>Complementing this, we propose a novel method MotionPaint to distill motion-aware information from pretrained video diffusion models in the generation phase, ensuring that these objects are not only seamlessly integrated but also endowed with realistic motion.Extensive quantitative and qualitative results highlight MotionCom's superiority, showcasing its efficiency in streamlining the planning process and its capability to produce compositions that authentically depict motion and interaction.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10090v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10090v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As the modern tool of choice for question answering, large language models (LLMs) are expected to deliver answers with up-to-date knowledge.To achieve such ideal question-answering systems, locating and then editing outdated knowledge in the natural language outputs is a general target of popular knowledge editing methods.However, this target is challenging, as both identifying which tokens to edit in the reasoning steps and ensuring the coherence of the revised reasoning chain are difficult tasks.We argue that these challenges stem from the unstructured nature of natural language outputs.To address the above challenges, we propose $\textbf{Stru}$ctural $\textbf{Edit}$ing ($\textbf{StruEdit}$), an improved baseline for knowledge editing.<span class='px-1 mx-1 bg-yellow-200'>We first prompt LLMs to produce structured outputs consisting of reasoning triplets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span>Then, StruEdit removes any potentially outdated knowledge and efficiently refills the structured outputs with up-to-date information in a single step.Experimental results show that StruEdit consistently delivers the highest accuracy with lowest latency compared with other knowledge editing methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10132v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10132v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The 20 questions game to distinguish large language models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In a parallel with the 20 questions game, we present a method to determine whether two large language models (LLMs), placed in a black-box context, are the same or not.The goal is to use a small set of (benign) binary questions, typically under 20.We formalize the problem and first establish a baseline using a random selection of questions from known benchmark datasets, achieving an accuracy of nearly 100% within 20 questions.<span class='px-1 mx-1 bg-yellow-200'>After showing optimal bounds for this problem, we introduce two effective questioning heuristics able to discriminate 22 LLMs by using half as many questions for the same task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>These methods offer significant advantages in terms of stealth and are thus of interest to auditors or copyright owners facing suspicions of model leaks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10338v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10338v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities.Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM.Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures.We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span>User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs.Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10695v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10695v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking VLMs' Reasoning About Persuasive Atypical Images
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Vision language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties.For example, Fig. 1 (e) shows a beer with a feather-like texture.This requires advanced reasoning to deduce that this atypical representation signifies the beer's lightness.We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Aypical Object Recognition, to benchmark VLMs' understanding of atypicality in persuasive images.We evaluate how well VLMs use atypicality to infer an ad's message and test their reasoning abilities by employing semantically challenging negatives.Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements.<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive advertisement understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span>Code and data will be made available.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10719v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10719v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GPT takes the SAT: Tracing changes in Test Difficulty and Math Performance of Students
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Scholastic Aptitude Test (SAT) is crucial for college admissions but its effectiveness and relevance are increasingly questioned. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span>This paper enhances Synthetic Control methods by introducing "Transformed Control", a novel method that employs Large Language Models (LLMs) powered by Artificial Intelligence to generate control groups.We utilize OpenAI's API to generate a control group where GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023.This control group helps analyze shifts in SAT math difficulty over time, starting from the baseline year of 2008.Using parallel trends, we calculate the Average Difference in Scores (ADS) to assess changes in high school students' math performance.Our results indicate a significant decrease in the difficulty of the SAT math section over time, alongside a decline in students' math performance.The analysis shows a 71-point drop in the rigor of SAT math from 2008 to 2023, with student performance decreasing by 36 points, resulting in a 107-point total divergence in average student math performance.We investigate possible mechanisms for this decline in math proficiency, such as changing university selection criteria, increased screen time, grade inflation, and worsening adolescent mental health.Disparities among demographic groups show a 104-point drop for White students, 84 points for Black students, and 53 points for Asian students.Male students saw a 117-point reduction, while female students had a 100-point decrease.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10750v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10750v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown promise in tasks like code translation, prompting interest in their potential for automating software vulnerability detection (SVD) and patching (SVP). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>To further research in this area, establishing a benchmark is essential for evaluating the strengths and limitations of LLMs in these tasks.Despite their capabilities, questions remain regarding whether LLMs can accurately analyze complex vulnerabilities and generate appropriate patches.This paper introduces VulnLLMEval, a framework designed to assess the performance of LLMs in identifying and patching vulnerabilities in C code.Our study includes 307 real-world vulnerabilities extracted from the Linux kernel, creating a well-curated dataset that includes both vulnerable and patched code.This dataset, based on real-world code, provides a diverse and representative testbed for evaluating LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous assessment.Our results reveal that LLMs often struggle with distinguishing between vulnerable and patched code.Furthermore, in SVP tasks, these models tend to oversimplify the code, producing solutions that may not be directly usable without further refinement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10756v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10756v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantics Preserving Emoji Recommendation with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emojis have become an integral part of digital communication, enriching text by conveying emotions, tone, and intent.Existing emoji recommendation methods are primarily evaluated based on their ability to match the exact emoji a user chooses in the original text.However, they ignore the essence of users' behavior on social media in that each text can correspond to multiple reasonable emojis.To better assess a model's ability to align with such real-world emoji usage, we propose a new semantics preserving evaluation framework for emoji recommendation, which measures a model's ability to recommend emojis that maintain the semantic consistency with the user's text.To evaluate how well a model preserves semantics, we assess whether the predicted affective state, demographic profile, and attitudinal stance of the user remain unchanged.If these attributes are preserved, we consider the recommended emojis to have maintained the original semantics.The advanced abilities of Large Language Models (LLMs) in understanding and generating nuanced, contextually relevant output make them well-suited for handling the complexities of semantics preserving emoji recommendation.<span class='px-1 mx-1 bg-yellow-200'>To this end, we construct a comprehensive benchmark to systematically assess the performance of six proprietary and open-source LLMs using different prompting techniques on our task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>Our experiments demonstrate that GPT-4o outperforms other LLMs, achieving a semantics preservation score of 79.23%.Additionally, we conduct case studies to analyze model biases in downstream classification tasks and evaluate the diversity of the recommended emojis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10760v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10760v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have demonstrated remarkable performance across various real-world tasks.<span class='px-1 mx-1 bg-yellow-200'>However, they often struggle to fully comprehend and effectively utilize their input contexts, resulting in responses that are unfaithful or hallucinated. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>This difficulty increases for contexts that are long or contain distracting information, which can divert LLMs from fully capturing essential evidence.To address this issue, many works use prompting to help LLMs utilize contextual information more faithfully.<span class='px-1 mx-1 bg-yellow-200'>For instance, iterative prompting highlights key information in two steps that first ask the LLM to identify important pieces of context and then derive answers accordingly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.8</span></span><span class='px-1 mx-1 bg-yellow-200'>However, prompting methods are constrained to highlighting key information implicitly in token space, which is often insufficient to fully steer the model's attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.791</span></span>To improve model faithfulness more reliably, we propose AutoPASTA, a method that automatically identifies key contextual information and explicitly highlights it by steering an LLM's attention scores.Like prompting, AutoPASTA is applied at inference time and does not require changing any model parameters.Our experiments on open-book QA demonstrate that AutoPASTA effectively enables models to grasp essential contextual information, leading to substantially improved model faithfulness and performance, e.g., an average improvement of 7.95% for LLAMA3-70B-Instruct.Code will be publicly available at https://github.com/QingruZhang/AutoPASTA .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10790v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10790v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLM agents enhanced by tree search algorithms have yielded notable performances in code generation.However, current search algorithms in this domain suffer from low search quality due to several reasons: 1) Ineffective design of the search space for the high-reasoning demands of code generation tasks, 2) Inadequate integration of code feedback with the search algorithm, and 3) Poor handling of negative feedback during the search, leading to reduced search efficiency and quality.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose to search for the reasoning process of the code and use the detailed feedback of code execution to refine erroneous thoughts during the search. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>In this paper, we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS) algorithm to conduct thought-level searches before generating code, thereby exploring a wider range of strategies.<span class='px-1 mx-1 bg-yellow-200'>More importantly, we construct verbal feedback from fine-grained code execution feedback to refine erroneous thoughts during the search. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>This ensures that the search progresses along the correct reasoning paths, thus improving the overall search quality of the tree by leveraging execution feedback.Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-based and feedback-based code generation baselines.On the HumanEval dataset, it improves the pass@1 of GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51.It effectively conducts more thorough exploration through thought-level searches and enhances the search quality of the entire tree by incorporating rethink operation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09584v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09584v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Text Annotation through Rationale-Driven Collaborative Few-Shot Prompting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The traditional data annotation process is often labor-intensive, time-consuming, and susceptible to human bias, which complicates the management of increasingly complex datasets.This study explores the potential of large language models (LLMs) as automated data annotators to improve efficiency and consistency in annotation tasks.<span class='px-1 mx-1 bg-yellow-200'>By employing rationale-driven collaborative few-shot prompting techniques, we aim to improve the performance of LLMs in text annotation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>We conduct a rigorous evaluation of six LLMs across four benchmark datasets, comparing seven distinct methodologies.Our results demonstrate that collaborative methods consistently outperform traditional few-shot techniques and other baseline approaches, particularly in complex annotation tasks.Our work provides valuable insights and a robust framework for leveraging collaborative learning methods to tackle challenging text annotation tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09615v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09615v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions.<span class='px-1 mx-1 bg-yellow-200'>Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span>Current systems often limit users' flexibility to direct their reflections.We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey.ExploreSelf allows users to receive adaptive support through dynamically generated questions.Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf.Our findings demonstrate that participants valued the balance between guided support and freedom to control their reflective journey, leading to deeper engagement and insight.Building on our findings, we discuss implications for designing LLM-driven tools that promote user empowerment through effective reflective practices.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09662v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09662v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VernaCopter: Disambiguated Natural-Language-Driven Robot via Formal Specifications
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>It has been an ambition of many to control a robot for a complex task using natural language (NL).The rise of large language models (LLMs) makes it closer to coming true.However, an LLM-powered system still suffers from the ambiguity inherent in an NL and the uncertainty brought up by LLMs.This paper proposes a novel LLM-based robot motion planner, named \textit{VernaCopter}, with signal temporal logic (STL) specifications serving as a bridge between NL commands and specific task objectives.The rigorous and abstract nature of formal specifications allows the planner to generate high-quality and highly consistent paths to guide the motion control of a robot.<span class='px-1 mx-1 bg-yellow-200'>Compared to a conventional NL-prompting-based planner, the proposed VernaCopter planner is more stable and reliable due to less ambiguous uncertainty. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>Its efficacy and advantage have been validated by two small but challenging experimental scenarios, implying its potential in designing NL-driven robots.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09536v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09536v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood.This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort.Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results.<span class='px-1 mx-1 bg-yellow-200'>Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span>ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents.<span class='px-1 mx-1 bg-yellow-200'>This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.728</span></span>Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT.This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07732v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07732v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Full-text Error Correction for Chinese Speech Recognition with Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR).However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training.This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings.First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor.This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive.Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.853</span></span><span class='px-1 mx-1 bg-yellow-200'>Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>This establishes a promising baseline for further research.The dataset is available on the website.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07790v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07790v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Objection Overruled! Lay People can Distinguish Large Language Models from Lawyers, but still Favour Advice from an LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception.In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice.In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown.<span class='px-1 mx-1 bg-yellow-200'>When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>This result was replicated in Experiment 2.Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3.Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07871v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07871v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students.<span class='px-1 mx-1 bg-yellow-200'>In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span><span class='px-1 mx-1 bg-yellow-200'>iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation).We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses.Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation.<span class='px-1 mx-1 bg-yellow-200'>We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08027v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08027v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>However, it is often too time-consuming for widespread use in educational practice.<span class='px-1 mx-1 bg-yellow-200'>While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback.<span class='px-1 mx-1 bg-yellow-200'>CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span><span class='px-1 mx-1 bg-yellow-200'>Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span><span class='px-1 mx-1 bg-yellow-200'>Students can further engage with the feedback to refine their understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span><span class='px-1 mx-1 bg-yellow-200'>A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.777</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07453v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07453v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Robustness Tools in LLM Safety</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Obfuscation for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>System prompts that include detailed instructions to describe the task performed by the underlying large language model (LLM) can easily transform foundation models into tools and services with minimal overhead.Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product.However, extracting system prompts is easily possible by using prompt injection.<span class='px-1 mx-1 bg-yellow-200'>As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded with carefully crafted prompt injections that bypass all protection mechanisms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>In this work, we propose an alternative to conventional system prompts.<span class='px-1 mx-1 bg-yellow-200'>We introduce prompt obfuscation to prevent the extraction of the system prompt while maintaining the utility of the system itself with only little overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>We implement an optimization-based method to find an obfuscated prompt representation while maintaining the functionality.<span class='px-1 mx-1 bg-yellow-200'>To evaluate our approach, we investigate eight different metrics to compare the performance of a system using the original and the obfuscated system prompts, and we show that the obfuscated version is constantly on par with the original one. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span><span class='px-1 mx-1 bg-yellow-200'>We further perform three different deobfuscation attacks and show that with access to the obfuscated prompt and the LLM itself, we are not able to consistently extract meaningful information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span><span class='px-1 mx-1 bg-yellow-200'>Overall, we showed that prompt obfuscation can be an effective method to protect intellectual property while maintaining the same utility as the original system prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11026v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11026v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-as-a-Judge & Reward Model: What They Can and Cannot Do
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation.Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning.However, despite their popularity, their effectiveness outside of English remains largely unexplored.In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment.First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages.<span class='px-1 mx-1 bg-yellow-200'>Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative.<span class='px-1 mx-1 bg-yellow-200'>Traditional security defenses are becoming less effective against the advanced tactics of today's cyberattacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis.DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network.This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples.Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets.The embeddings are then used alongside few labeled samples of an unseen malware type.This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes.Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data.Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments.Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11254v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11254v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>LLMs obtain remarkable performance but suffer from hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.943</span></span><span class='px-1 mx-1 bg-yellow-200'>Most research on detecting hallucination focuses on the questions with short and concrete correct answers that are easy to check the faithfulness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span><span class='px-1 mx-1 bg-yellow-200'>Hallucination detections for text generation with open-ended answers are more challenging. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.798</span></span><span class='px-1 mx-1 bg-yellow-200'>Some researchers use external knowledge to detect hallucinations in generated texts, but external resources for specific scenarios are hard to access. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.844</span></span><span class='px-1 mx-1 bg-yellow-200'>Recent studies on detecting hallucinations in long text without external resources conduct consistency comparison among multiple sampled outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.879</span></span>To handle long texts, researchers split long texts into multiple facts and individually compare the consistency of each pairs of facts.However, these methods (1) hardly achieve alignment among multiple facts; (2) overlook dependencies between multiple contextual facts.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose a graph-based context-aware (GCA) hallucination detection for text generations, which aligns knowledge facts and considers the dependencies between contextual knowledge triples in consistency comparison. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span>Particularly, to align multiple facts, we conduct a triple-oriented response segmentation to extract multiple knowledge triples.To model dependencies among contextual knowledge triple (facts), we construct contextual triple into a graph and enhance triples' interactions via message passing and aggregating via RGCN.To avoid the omission of knowledge triples in long text, we conduct a LLM-based reverse verification via reconstructing the knowledge triples.<span class='px-1 mx-1 bg-yellow-200'>Experiments show that our model enhances hallucination detection and excels all baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11283v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11283v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.803</span></span>Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline.<span class='px-1 mx-1 bg-yellow-200'>This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.793</span></span><span class='px-1 mx-1 bg-yellow-200'>THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.941</span></span>It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation.<span class='px-1 mx-1 bg-yellow-200'>THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL.Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11353v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11353v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability.To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification.CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions.A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative.In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71\% of responses verified by seven experts.<span class='px-1 mx-1 bg-yellow-200'>Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52\% of medical answers as accurate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span>As the knowledge base expanded with expert corrections, system performance improved by 19.02\%, reducing expert workload.These insights guide the design of future LLM-powered chatbots.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10354v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10354v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) show promise as a writing aid for professionals performing legal analyses.However, LLMs can often hallucinate in this setting, in ways difficult to recognize by non-professionals and existing text evaluation metrics.In this work, we pose the question: when can machine-generated legal analysis be evaluated as acceptable?<span class='px-1 mx-1 bg-yellow-200'>We introduce the neutral notion of gaps, as opposed to hallucinations in a strict erroneous sense, to refer to the difference between human-written and machine-generated legal analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span>Gaps do not always equate to invalid generation.Working with legal experts, we consider the CLERC generation task proposed in Hou et al. (2024b), leading to a taxonomy, a fine-grained detector for predicting gap categories, and an annotated dataset for automatic evaluation.Our best detector achieves 67% F1 score and 80% precision on the test set.<span class='px-1 mx-1 bg-yellow-200'>Employing this detector as an automated metric on legal analysis generated by SOTA LLMs, we find around 80% contain hallucinations of different kinds. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.857</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09947v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09947v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have significantly advanced natural language processing tasks, yet they are susceptible to generating inaccurate or unreliable responses, a phenomenon known as hallucination. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span><span class='px-1 mx-1 bg-yellow-200'>In critical domains such as health and medicine, these hallucinations can pose serious risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.931</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper introduces HALO, a novel framework designed to enhance the accuracy and reliability of medical question-answering (QA) systems by focusing on the detection and mitigation of hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>Our approach generates multiple variations of a given query using LLMs and retrieves relevant information from external open knowledge bases to enrich the context.We utilize maximum marginal relevance scoring to prioritize the retrieved context, which is then provided to LLMs for answer generation, thereby reducing the risk of hallucinations.The integration of LangChain further streamlines this process, resulting in a notable and robust increase in the accuracy of both open-source and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56% to 70%).<span class='px-1 mx-1 bg-yellow-200'>This framework underscores the critical importance of addressing hallucinations in medical QA systems, ultimately improving clinical decision-making and patient care. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.894</span></span>The open-source HALO is available at: https://github.com/ResponsibleAILab/HALO.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10011v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10011v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Mental health disorders are among the most prevalent diseases worldwide, affecting nearly one in four people.Despite their widespread impact, the intervention rate remains below 25%, largely due to the significant cooperation required from patients for both diagnosis and intervention.The core issue behind this low treatment rate is stigma, which discourages over half of those affected from seeking help.This paper presents MindGuard, an accessible, stigma-free, and professional mobile mental healthcare system designed to provide mental health first aid.The heart of MindGuard is an innovative edge LLM, equipped with professional mental health knowledge, that seamlessly integrates objective mobile sensor data with subjective Ecological Momentary Assessment records to deliver personalized screening and intervention conversations.We conduct a broad evaluation of MindGuard using open datasets spanning four years and real-world deployment across various mobile devices involving 20 subjects for two weeks.<span class='px-1 mx-1 bg-yellow-200'>Remarkably, MindGuard achieves results comparable to GPT-4 and outperforms its counterpart with more than 10 times the model size. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span><span class='px-1 mx-1 bg-yellow-200'>We believe that MindGuard paves the way for mobile LLM applications, potentially revolutionizing mental healthcare practices by substituting self-reporting and intervention conversations with passive, integrated monitoring within daily life, thus ensuring accessible and stigma-free mental health support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10064v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10064v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving Systems Assisted by Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Autonomous driving systems (ADS) are safety-critical and require comprehensive testing before their deployment on public roads.<span class='px-1 mx-1 bg-yellow-200'>While existing testing approaches primarily aim at the criticality of scenarios, they often overlook the diversity of the generated scenarios that is also important to reflect system defects in different aspects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>To bridge the gap, we propose LeGEND, that features a top-down fashion of scenario generation: it starts with abstract functional scenarios, and then steps downwards to logical and concrete scenarios, such that scenario diversity can be controlled at the functional level.However, unlike logical scenarios that can be formally described, functional scenarios are often documented in natural languages (e.g., accident reports) and thus cannot be precisely parsed and processed by computers.To tackle that issue, LeGEND leverages the recent advances of large language models (LLMs) to transform textual functional scenarios to formal logical scenarios.To mitigate the distraction of useless information in functional scenario description, we devise a two-phase transformation that features the use of an intermediate language; consequently, we adopt two LLMs in LeGEND, one for extracting information from functional scenarios, the other for converting the extracted information to formal logical scenarios.We experimentally evaluate LeGEND on Apollo, an industry-grade ADS from Baidu.Evaluation results show that LeGEND can effectively identify critical scenarios, and compared to baseline approaches, LeGEND exhibits evident superiority in diversity of generated scenarios.Moreover, we also demonstrate the advantages of our two-phase transformation framework, and the accuracy of the adopted LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10066v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10066v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Trustworthiness in Retrieval-Augmented Generation Systems: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs).While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration.<span class='px-1 mx-1 bg-yellow-200'>From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized.To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy.Within this framework, we thoroughly review the existing literature on each dimension.Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models.Finally, we identify the potential challenges for future research based on our investigation results.Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10102v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10102v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The 20 questions game to distinguish large language models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In a parallel with the 20 questions game, we present a method to determine whether two large language models (LLMs), placed in a black-box context, are the same or not.The goal is to use a small set of (benign) binary questions, typically under 20.We formalize the problem and first establish a baseline using a random selection of questions from known benchmark datasets, achieving an accuracy of nearly 100% within 20 questions.After showing optimal bounds for this problem, we introduce two effective questioning heuristics able to discriminate 22 LLMs by using half as many questions for the same task.<span class='px-1 mx-1 bg-yellow-200'>These methods offer significant advantages in terms of stealth and are thus of interest to auditors or copyright owners facing suspicions of model leaks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10338v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10338v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability.To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification.CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions.A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative.In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71% of responses verified by seven experts.<span class='px-1 mx-1 bg-yellow-200'>Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52% of medical answers as accurate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>As the knowledge base expanded with expert corrections, system performance improved by 19.02%, reducing expert workload.These insights guide the design of future LLM-powered chatbots.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10354v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10354v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Schrodinger's Memory: Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life.With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans.But do LLMs have memory?<span class='px-1 mx-1 bg-yellow-200'>Based on current performance, LLMs do appear to exhibit memory. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>So, what is the underlying mechanism of this memory?Previous research has lacked a deep exploration of LLMs' memory capabilities and the underlying theory.In this paper, we use Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs.We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on these memory ability.We argue that LLM memory operates like Schr\"odinger's memory, meaning that it only becomes observable when a specific memory is queried.We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate.Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10482v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10482v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Code Vulnerability Detection: A Comparative Analysis of Emerging Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The growing trend of vulnerability issues in software development as a result of a large dependence on open-source projects has received considerable attention recently.<span class='px-1 mx-1 bg-yellow-200'>This paper investigates the effectiveness of Large Language Models (LLMs) in identifying vulnerabilities within codebases, with a focus on the latest advancements in LLM technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Through a comparative analysis, we assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma, and CodeGemma, alongside established state-of-the-art models such as BERT, RoBERTa, and GPT-3.<span class='px-1 mx-1 bg-yellow-200'>Our study aims to shed light on the capabilities of LLMs in vulnerability detection, contributing to the enhancement of software security practices across diverse open-source repositories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span><span class='px-1 mx-1 bg-yellow-200'>We observe that CodeGemma achieves the highest F1-score of 58\ and a Recall of 87\, amongst the recent additions of large language models to detect software security vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10490v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10490v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span><span class='px-1 mx-1 bg-yellow-200'>While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code.Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts.Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested.In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10506v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10506v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in automatic code generation using large language models (LLMs) have brought us closer to fully automated secure software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>However, existing approaches often rely on a single agent for code generation, which struggles to produce secure, vulnerability-free code.<span class='px-1 mx-1 bg-yellow-200'>Traditional program synthesis with LLMs has primarily focused on functional correctness, often neglecting critical dynamic security implications that happen during runtime. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>To address these challenges, we propose AutoSafeCoder, a multi-agent framework that leverages LLM-driven agents for code generation, vulnerability analysis, and security enhancement through continuous collaboration.The framework consists of three agents: a Coding Agent responsible for code generation, a Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent performing dynamic testing using a mutation-based fuzzing approach to detect runtime errors.<span class='px-1 mx-1 bg-yellow-200'>Our contribution focuses on ensuring the safety of multi-agent code generation by integrating dynamic and static testing in an iterative process during code generation by LLM that improves security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>Experiments using the SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities compared to baseline LLMs, with no compromise in functionality.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10737v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10737v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown promise in tasks like code translation, prompting interest in their potential for automating software vulnerability detection (SVD) and patching (SVP).To further research in this area, establishing a benchmark is essential for evaluating the strengths and limitations of LLMs in these tasks.<span class='px-1 mx-1 bg-yellow-200'>Despite their capabilities, questions remain regarding whether LLMs can accurately analyze complex vulnerabilities and generate appropriate patches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper introduces VulnLLMEval, a framework designed to assess the performance of LLMs in identifying and patching vulnerabilities in C code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span>Our study includes 307 real-world vulnerabilities extracted from the Linux kernel, creating a well-curated dataset that includes both vulnerable and patched code.This dataset, based on real-world code, provides a diverse and representative testbed for evaluating LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous assessment.<span class='px-1 mx-1 bg-yellow-200'>Our results reveal that LLMs often struggle with distinguishing between vulnerable and patched code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>Furthermore, in SVP tasks, these models tend to oversimplify the code, producing solutions that may not be directly usable without further refinement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10756v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10756v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span><span class='px-1 mx-1 bg-yellow-200'>To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>Nevertheless, repairing the identified vulnerabilities is still complex and labor-intensive, particularly for developers lacking security expertise.Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding.To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair.The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into sub-tasks.<span class='px-1 mx-1 bg-yellow-200'>Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>We evaluate ContractTinker on 48 high-risk vulnerabilities.<span class='px-1 mx-1 bg-yellow-200'>The experimental results show that among the patches generated by ContractTinker, 23 (48%) are valid patches that fix the vulnerabilities, while 10 (21%) require only minor modifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span>A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09661v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09661v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns.<span class='px-1 mx-1 bg-yellow-200'>This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span><span class='px-1 mx-1 bg-yellow-200'>Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises.A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements.The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances.Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt.<span class='px-1 mx-1 bg-yellow-200'>This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Security Challenges in LLM Development</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Obfuscation for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>System prompts that include detailed instructions to describe the task performed by the underlying large language model (LLM) can easily transform foundation models into tools and services with minimal overhead.Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product.However, extracting system prompts is easily possible by using prompt injection.<span class='px-1 mx-1 bg-yellow-200'>As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded with carefully crafted prompt injections that bypass all protection mechanisms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.843</span></span>In this work, we propose an alternative to conventional system prompts.<span class='px-1 mx-1 bg-yellow-200'>We introduce prompt obfuscation to prevent the extraction of the system prompt while maintaining the utility of the system itself with only little overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span><span class='px-1 mx-1 bg-yellow-200'>The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span><span class='px-1 mx-1 bg-yellow-200'>We implement an optimization-based method to find an obfuscated prompt representation while maintaining the functionality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span><span class='px-1 mx-1 bg-yellow-200'>To evaluate our approach, we investigate eight different metrics to compare the performance of a system using the original and the obfuscated system prompts, and we show that the obfuscated version is constantly on par with the original one. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span><span class='px-1 mx-1 bg-yellow-200'>We further perform three different deobfuscation attacks and show that with access to the obfuscated prompt and the LLM itself, we are not able to consistently extract meaningful information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span><span class='px-1 mx-1 bg-yellow-200'>Overall, we showed that prompt obfuscation can be an effective method to protect intellectual property while maintaining the same utility as the original system prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11026v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11026v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative.<span class='px-1 mx-1 bg-yellow-200'>Traditional security defenses are becoming less effective against the advanced tactics of today's cyberattacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis.DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network.This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples.Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets.The embeddings are then used alongside few labeled samples of an unseen malware type.<span class='px-1 mx-1 bg-yellow-200'>This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data.Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments.<span class='px-1 mx-1 bg-yellow-200'>Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11254v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11254v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity.Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4.Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios.To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities.Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11276v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11276v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The deployment of multimodal large language models (MLLMs) has demonstrated remarkable success in engaging in conversations involving visual inputs, thanks to the superior power of large language models (LLMs).Those MLLMs are typically built based on the LLMs, with an image encoder to process images into the token embedding space of the LLMs.<span class='px-1 mx-1 bg-yellow-200'>However, the integration of visual modality has introduced a unique vulnerability: the MLLM becomes susceptible to malicious visual inputs and prone to generating sensitive or harmful responses, even though the LLM has been trained on textual dataset to align with human value. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we first raise the question: ``Do the MLLMs possess safety-awareness against malicious image inputs?". <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.679</span></span>We find that after adding a principle that specifies the safety requirement into the input of the MLLM, the model's safety awareness becomes boosted.This phenomenon verifies the existence of MLLM's safety-awareness against image inputs, it is only weakened by the modality gap.We then introduce a simple yet effective technique termed CoCA, which amplifies the safety-awareness of the MLLM by calibrating its output distribution.Our proposed strategy helps the model reclaim its original safety awareness without losing its original capabilities.We verify the effectiveness of our approach on both multimodal safety and understanding benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11365v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11365v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing RL Safety with Counterfactual LLM Reasoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard to explain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>We use counterfactual large language model reasoning to enhance RL policy safety post-training.We show that our approach improves and helps to explain the RL policy safety.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10188v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10188v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Code Vulnerability Detection: A Comparative Analysis of Emerging Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The growing trend of vulnerability issues in software development as a result of a large dependence on open-source projects has received considerable attention recently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper investigates the effectiveness of Large Language Models (LLMs) in identifying vulnerabilities within codebases, with a focus on the latest advancements in LLM technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span>Through a comparative analysis, we assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma, and CodeGemma, alongside established state-of-the-art models such as BERT, RoBERTa, and GPT-3.<span class='px-1 mx-1 bg-yellow-200'>Our study aims to shed light on the capabilities of LLMs in vulnerability detection, contributing to the enhancement of software security practices across diverse open-source repositories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span><span class='px-1 mx-1 bg-yellow-200'>We observe that CodeGemma achieves the highest F1-score of 58\ and a Recall of 87\, amongst the recent additions of large language models to detect software security vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.854</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10490v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10490v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language.<span class='px-1 mx-1 bg-yellow-200'>While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span>We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code.Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts.Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested.In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10506v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10506v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in automatic code generation using large language models (LLMs) have brought us closer to fully automated secure software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing approaches often rely on a single agent for code generation, which struggles to produce secure, vulnerability-free code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>Traditional program synthesis with LLMs has primarily focused on functional correctness, often neglecting critical dynamic security implications that happen during runtime. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose AutoSafeCoder, a multi-agent framework that leverages LLM-driven agents for code generation, vulnerability analysis, and security enhancement through continuous collaboration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>The framework consists of three agents: a Coding Agent responsible for code generation, a Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent performing dynamic testing using a mutation-based fuzzing approach to detect runtime errors.<span class='px-1 mx-1 bg-yellow-200'>Our contribution focuses on ensuring the safety of multi-agent code generation by integrating dynamic and static testing in an iterative process during code generation by LLM that improves security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.801</span></span><span class='px-1 mx-1 bg-yellow-200'>Experiments using the SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities compared to baseline LLMs, with no compromise in functionality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10737v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10737v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown promise in tasks like code translation, prompting interest in their potential for automating software vulnerability detection (SVD) and patching (SVP). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>To further research in this area, establishing a benchmark is essential for evaluating the strengths and limitations of LLMs in these tasks.<span class='px-1 mx-1 bg-yellow-200'>Despite their capabilities, questions remain regarding whether LLMs can accurately analyze complex vulnerabilities and generate appropriate patches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper introduces VulnLLMEval, a framework designed to assess the performance of LLMs in identifying and patching vulnerabilities in C code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span><span class='px-1 mx-1 bg-yellow-200'>Our study includes 307 real-world vulnerabilities extracted from the Linux kernel, creating a well-curated dataset that includes both vulnerable and patched code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>This dataset, based on real-world code, provides a diverse and representative testbed for evaluating LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous assessment.<span class='px-1 mx-1 bg-yellow-200'>Our results reveal that LLMs often struggle with distinguishing between vulnerable and patched code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.805</span></span>Furthermore, in SVP tasks, these models tend to oversimplify the code, producing solutions that may not be directly usable without further refinement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10756v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10756v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.794</span></span><span class='px-1 mx-1 bg-yellow-200'>To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.803</span></span><span class='px-1 mx-1 bg-yellow-200'>Nevertheless, repairing the identified vulnerabilities is still complex and labor-intensive, particularly for developers lacking security expertise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span>Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding.<span class='px-1 mx-1 bg-yellow-200'>To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into sub-tasks.Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM.<span class='px-1 mx-1 bg-yellow-200'>We evaluate ContractTinker on 48 high-risk vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span><span class='px-1 mx-1 bg-yellow-200'>The experimental results show that among the patches generated by ContractTinker, 23 (48%) are valid patches that fix the vulnerabilities, while 10 (21%) require only minor modifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09661v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09661v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Alignment with Preference Optimization Is All You Need for LLM Safety
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We demonstrate that preference optimization methods can effectively enhance LLM safety. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>Applying various alignment techniques to the Falcon 11B model using safety datasets, we achieve a significant boost in global safety score (from $57.64\%$ to $99.90\%$) as measured by LlamaGuard 3 8B, competing with state-of-the-art models.<span class='px-1 mx-1 bg-yellow-200'>On toxicity benchmarks, average scores in adversarial settings dropped from over $0.6$ to less than $0.07$. However, this safety improvement comes at the cost of reduced general capabilities, particularly in math, suggesting a trade-off. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>We identify noise contrastive alignment (Safe-NCA) as an optimal method for balancing safety and performance.Our study ultimately shows that alignment techniques can be sufficient for building safe and robust models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07772v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07772v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span><span class='px-1 mx-1 bg-yellow-200'>This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.776</span></span>Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability.Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises.A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements.The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances.<span class='px-1 mx-1 bg-yellow-200'>Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span><span class='px-1 mx-1 bg-yellow-200'>This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.862</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span><span class='px-1 mx-1 bg-yellow-200'>Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span>In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance.Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses.<span class='px-1 mx-1 bg-yellow-200'>The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08234v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08234v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.845</span></span><span class='px-1 mx-1 bg-yellow-200'>SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.753</span></span><span class='px-1 mx-1 bg-yellow-200'>We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.846</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>SGCode has only a marginal cost compared with prompting LLMs.SGCode is available at: http://3.131.141.63:8501/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07368v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07368v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown great promise in vulnerability identification. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span><span class='px-1 mx-1 bg-yellow-200'>As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges.In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner.We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs.Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details.Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols.We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits.The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs.<span class='px-1 mx-1 bg-yellow-200'>Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07407v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07407v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">HCI in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ArticulatePro: A Comparative Study on a Proactive and Non-Proactive Assistant in a Climate Data Exploration Task
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in Natural Language Interfaces (NLIs) and Large Language Models (LLMs) have transformed our approach to NLP tasks, allowing us to focus more on a Pragmatics-based approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>This shift enables more natural interactions between humans and voice assistants, which have been challenging to achieve.Pragmatics describes how users often talk out of turn, interrupt each other, or provide relevant information without being explicitly asked (maxim of quantity).To explore this, we developed a digital assistant that constantly listens to conversations and proactively generates relevant visualizations during data exploration tasks.<span class='px-1 mx-1 bg-yellow-200'>In a within-subject study, participants interacted with both proactive and non-proactive versions of a voice assistant while exploring the Hawaii Climate Data Portal (HCDP). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span>Results suggest that the proactive assistant enhanced user engagement and facilitated quicker insights.Our study highlights the potential of Pragmatic, proactive AI in NLIs and identifies key challenges in its implementation, offering insights for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10797v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10797v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training.This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India.<span class='px-1 mx-1 bg-yellow-200'>Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors.CHWs trusted the information they received on ASHABot and treated it as an authoritative resource.CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability.We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10913v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10913v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences.However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns.To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework.We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics.<span class='px-1 mx-1 bg-yellow-200'>Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11032v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11032v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>Traditional learning-based approaches face two challenges.Firstly, natural language instructions often lack explicit task planning.Secondly, extensive training is required to equip models with knowledge of the task environment.Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples.To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth.Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database.In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction.Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences.Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11279v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11279v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants.As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values.<span class='px-1 mx-1 bg-yellow-200'>This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span><span class='px-1 mx-1 bg-yellow-200'>We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians.Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is writtenbut also how it is written.These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11360v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11360v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The deployment of multimodal large language models (MLLMs) has demonstrated remarkable success in engaging in conversations involving visual inputs, thanks to the superior power of large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>Those MLLMs are typically built based on the LLMs, with an image encoder to process images into the token embedding space of the LLMs.However, the integration of visual modality has introduced a unique vulnerability: the MLLM becomes susceptible to malicious visual inputs and prone to generating sensitive or harmful responses, even though the LLM has been trained on textual dataset to align with human value.In this paper, we first raise the question: ``Do the MLLMs possess safety-awareness against malicious image inputs?".We find that after adding a principle that specifies the safety requirement into the input of the MLLM, the model's safety awareness becomes boosted.This phenomenon verifies the existence of MLLM's safety-awareness against image inputs, it is only weakened by the modality gap.We then introduce a simple yet effective technique termed CoCA, which amplifies the safety-awareness of the MLLM by calibrating its output distribution.Our proposed strategy helps the model reclaim its original safety awareness without losing its original capabilities.We verify the effectiveness of our approach on both multimodal safety and understanding benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11365v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11365v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities.However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity.Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent.<span class='px-1 mx-1 bg-yellow-200'>This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works.Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types.Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects.Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems.This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11393v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11393v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability.To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification.CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions.A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative.In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71\% of responses verified by seven experts.Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52\% of medical answers as accurate.As the knowledge base expanded with expert corrections, system performance improved by 19.02\%, reducing expert workload.<span class='px-1 mx-1 bg-yellow-200'>These insights guide the design of future LLM-powered chatbots. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10354v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10354v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Assessing personality traits using large language models (LLMs) has emerged as an interesting and challenging area of research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.914</span></span><span class='px-1 mx-1 bg-yellow-200'>While previous methods employ explicit questionnaires, often derived from the Big Five model of personality, we hypothesize that LLMs implicitly encode notions of personality when modeling next-token responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.878</span></span><span class='px-1 mx-1 bg-yellow-200'>To demonstrate this, we introduce a novel approach that uncovers latent personality dimensions in LLMs by applying singular value de-composition (SVD) to the log-probabilities of trait-descriptive adjectives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments show that LLMs "rediscover" core personality traits such as extraversion, agreeableness, conscientiousness, neuroticism, and openness without relying on direct questionnaire inputs, with the top-5 factors corresponding to Big Five traits explaining 74.3% of the variance in the latent space. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>Moreover, we can use the derived principal components to assess personality along the Big Five dimensions, and achieve improvements in average personality prediction accuracy of up to 5% over fine-tuned models, and up to 21% over direct LLM-based scoring techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09905v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09905v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper provides a comprehensive survey of sentiment analysis within the context of artificial intelligence (AI) and large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>Sentiment analysis, a critical aspect of natural language processing (NLP), has evolved significantly from traditional rule-based methods to advanced deep learning techniques.This study examines the historical development of sentiment analysis, highlighting the transition from lexicon-based and pattern-based approaches to more sophisticated machine learning and deep learning models.Key challenges are discussed, including handling bilingual texts, detecting sarcasm, and addressing biases.The paper reviews state-of-the-art approaches, identifies emerging trends, and outlines future research directions to advance the field.<span class='px-1 mx-1 bg-yellow-200'>By synthesizing current methodologies and exploring future opportunities, this survey aims to understand sentiment analysis in the AI and LLM context thoroughly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09989v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09989v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation.However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans.Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios.<span class='px-1 mx-1 bg-yellow-200'>To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.837</span></span>The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences.Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches.Code and supplementary materials are available at https://e2map.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10027v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10027v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Mental health disorders are among the most prevalent diseases worldwide, affecting nearly one in four people. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>Despite their widespread impact, the intervention rate remains below 25%, largely due to the significant cooperation required from patients for both diagnosis and intervention.The core issue behind this low treatment rate is stigma, which discourages over half of those affected from seeking help.This paper presents MindGuard, an accessible, stigma-free, and professional mobile mental healthcare system designed to provide mental health first aid.The heart of MindGuard is an innovative edge LLM, equipped with professional mental health knowledge, that seamlessly integrates objective mobile sensor data with subjective Ecological Momentary Assessment records to deliver personalized screening and intervention conversations.We conduct a broad evaluation of MindGuard using open datasets spanning four years and real-world deployment across various mobile devices involving 20 subjects for two weeks.Remarkably, MindGuard achieves results comparable to GPT-4 and outperforms its counterpart with more than 10 times the model size.We believe that MindGuard paves the way for mobile LLM applications, potentially revolutionizing mental healthcare practices by substituting self-reporting and intervention conversations with passive, integrated monitoring within daily life, thus ensuring accessible and stigma-free mental health support.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10064v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10064v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper outlines the LLMs4OL 2024, the first edition of the Large Language Models for Ontology Learning Challenge.<span class='px-1 mx-1 bg-yellow-200'>LLMs4OL is a community development initiative collocated with the 23rd International Semantic Web Conference (ISWC) to explore the potential of Large Language Models (LLMs) in Ontology Learning (OL), a vital process for enhancing the web with structured knowledge to improve interoperability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>By leveraging LLMs, the challenge aims to advance understanding and innovation in OL, aligning with the goals of the Semantic Web to create a more intelligent and user-friendly web.In this paper, we give an overview of the 2024 edition of the LLMs4OL challenge and summarize the contributions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10146v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10146v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Emo-DPO: Controllable Emotional Speech Synthesis through Direct Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Current emotional text-to-speech (TTS) models predominantly conduct supervised training to learn the conversion from text and desired emotion to its emotional speech, focusing on a single emotion per text-speech pair.These models only learn the correct emotional outputs without fully comprehending other emotion characteristics, which limits their capabilities of capturing the nuances between different emotions.We propose a controllable Emo-DPO approach, which employs direct preference optimization to differentiate subtle emotional nuances between emotions through optimizing towards preferred emotions over less preferred emotional ones.<span class='px-1 mx-1 bg-yellow-200'>Instead of relying on traditional neural architectures used in existing emotional TTS models, we propose utilizing the emotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning and instruction-following capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>Comprehensive experiments confirm that our proposed method outperforms the existing baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10157v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10157v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the demand for human-like interactions with LLMs continues to grow, so does the interest in manipulating their personality traits, which has emerged as a key area of research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.911</span></span>Methods like prompt-based In-Context Knowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have been explored but show irregularity and variability.IKE depends on the prompt, leading to variability and sensitivity, while MEND yields inconsistent and gibberish outputs.To address this, we employed Opinion QA Based Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank Adaptation (QLORA), to manipulate the Big Five personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.After PEFT, models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis, despite their absence in the PEFT data.For instance, Llama-2-7B-chat generated emojis in 99.5% of extraversion-related test instances, while Mistral-8B-Instruct did so in 92.5% of openness-related test instances.<span class='px-1 mx-1 bg-yellow-200'>Explainability analysis indicated that the LLMs used emojis intentionally to express these traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span>This paper provides a number of novel contributions.First, introducing an Opinion QA dataset for PEFT-driven personality manipulation; second, developing metric models to benchmark LLM personality traits; third, demonstrating PEFT's superiority over IKE in personality manipulation; and finally, analyzing and validating emoji usage through explainability methods such as mechanistic interpretability and in-context learning explainability methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10245v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10245v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation.<span class='px-1 mx-1 bg-yellow-200'>This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion.By harnessing an intent twice reflect the mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition.<span class='px-1 mx-1 bg-yellow-200'>Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10289v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10289v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability.To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification.CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions.A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative.In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71% of responses verified by seven experts.Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52% of medical answers as accurate.As the knowledge base expanded with expert corrections, system performance improved by 19.02%, reducing expert workload.<span class='px-1 mx-1 bg-yellow-200'>These insights guide the design of future LLM-powered chatbots. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10354v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10354v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Instigating Cooperation among LLM Agents Using Adaptive Information Modulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.87</span></span>Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior.Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates.<span class='px-1 mx-1 bg-yellow-200'>This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.845</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10372v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10372v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Schrodinger's Memory: Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life.<span class='px-1 mx-1 bg-yellow-200'>With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>But do LLMs have memory?Based on current performance, LLMs do appear to exhibit memory.So, what is the underlying mechanism of this memory?Previous research has lacked a deep exploration of LLMs' memory capabilities and the underlying theory.In this paper, we use Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs.We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on these memory ability.We argue that LLM memory operates like Schr\"odinger's memory, meaning that it only becomes observable when a specific memory is queried.We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate.Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10482v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10482v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities.Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM.Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures.We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance.Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering.<span class='px-1 mx-1 bg-yellow-200'>User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10695v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10695v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs as information warriors? Auditing how LLM-powered chatbots tackle disinformation about Russia's war in Ukraine
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rise of large language models (LLMs) has a significant impact on information warfare.By facilitating the production of content related to disinformation and propaganda campaigns, LLMs can amplify different types of information operations and mislead online users.<span class='px-1 mx-1 bg-yellow-200'>In our study, we empirically investigate how LLM-powered chatbots, developed by Google, Microsoft, and Perplexity, handle disinformation about Russia's war in Ukraine and whether the chatbots' ability to provide accurate information on the topic varies across languages and over time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>Our findings indicate that while for some chatbots (Perplexity), there is a significant improvement in performance over time in several languages, for others (Gemini), the performance improves only in English but deteriorates in low-resource languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10697v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10697v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking VLMs' Reasoning About Persuasive Atypical Images
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Vision language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties.For example, Fig. 1 (e) shows a beer with a feather-like texture.This requires advanced reasoning to deduce that this atypical representation signifies the beer's lightness.We introduce three novel tasks, Multi-label Atypicality Classification, Atypicality Statement Retrieval, and Aypical Object Recognition, to benchmark VLMs' understanding of atypicality in persuasive images.We evaluate how well VLMs use atypicality to infer an ad's message and test their reasoning abilities by employing semantically challenging negatives.Finally, we pioneer atypicality-aware verbalization by extracting comprehensive image descriptions sensitive to atypical elements.Our findings reveal that: (1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple, effective strategies can extract atypicality-aware information, leading to comprehensive image verbalization; (3) atypicality aids persuasive advertisement understanding.Code and data will be made available.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10719v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10719v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantics Preserving Emoji Recommendation with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emojis have become an integral part of digital communication, enriching text by conveying emotions, tone, and intent.Existing emoji recommendation methods are primarily evaluated based on their ability to match the exact emoji a user chooses in the original text.However, they ignore the essence of users' behavior on social media in that each text can correspond to multiple reasonable emojis.To better assess a model's ability to align with such real-world emoji usage, we propose a new semantics preserving evaluation framework for emoji recommendation, which measures a model's ability to recommend emojis that maintain the semantic consistency with the user's text.<span class='px-1 mx-1 bg-yellow-200'>To evaluate how well a model preserves semantics, we assess whether the predicted affective state, demographic profile, and attitudinal stance of the user remain unchanged. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>If these attributes are preserved, we consider the recommended emojis to have maintained the original semantics.The advanced abilities of Large Language Models (LLMs) in understanding and generating nuanced, contextually relevant output make them well-suited for handling the complexities of semantics preserving emoji recommendation.To this end, we construct a comprehensive benchmark to systematically assess the performance of six proprietary and open-source LLMs using different prompting techniques on our task.Our experiments demonstrate that GPT-4o outperforms other LLMs, achieving a semantics preservation score of 79.23%.Additionally, we conduct case studies to analyze model biases in downstream classification tasks and evaluate the diversity of the recommended emojis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10760v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10760v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Mental health concerns are prevalent among college students, highlighting the need for effective interventions that promote self-awareness and holistic well-being.<span class='px-1 mx-1 bg-yellow-200'>MindScape pioneers a novel approach to AI-powered journaling by integrating passively collected behavioral patterns such as conversational engagement, sleep, and location with Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span>This integration creates a highly personalized and context-aware journaling experience, enhancing self-awareness and well-being by embedding behavioral intelligence into AI.We present an 8-week exploratory study with 20 college students, demonstrating the MindScape app's efficacy in enhancing positive affect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and depression, with a significant week-over-week decrease in PHQ-4 scores (-0.25 coefficient), alongside improvements in mindfulness (7%) and self-reflection (6%).<span class='px-1 mx-1 bg-yellow-200'>The study highlights the advantages of contextual AI journaling, with participants particularly appreciating the tailored prompts and insights provided by the MindScape app. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>Our analysis also includes a comparison of responses to AI-driven contextual versus generic prompts, participant feedback insights, and proposed strategies for leveraging contextual AI journaling to improve well-being on college campuses.By showcasing the potential of contextual AI journaling to support mental health, we provide a foundation for further investigation into the effects of contextual AI journaling on mental health and well-being.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09570v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09570v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in event-based zero-shot object recognition have demonstrated promising results.However, these methods heavily depend on extensive training and are inherently constrained by the characteristics of CLIP.<span class='px-1 mx-1 bg-yellow-200'>To the best of our knowledge, this research is the first study to explore the understanding capabilities of large language models (LLMs) for event-based visual content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>We demonstrate that LLMs can achieve event-based object recognition without additional training or fine-tuning in conjunction with CLIP, effectively enabling pure zero-shot event-based recognition.Particularly, we evaluate the ability of GPT-4o / 4turbo and two other open-source LLMs to directly recognize event-based visual content.Extensive experiments are conducted across three benchmark datasets, systematically assessing the recognition accuracy of these models.The results show that LLMs, especially when enhanced with well-designed prompts, significantly improve event-based zero-shot recognition performance.Notably, GPT-4o outperforms the compared models and exceeds the recognition accuracy of state-of-the-art event-based zero-shot methods on N-ImageNet by five orders of magnitude.The implementation of this paper is available at \url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09628v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09628v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates gender bias in Large Language Model (LLM)-generated teacher evaluations in higher education setting, focusing on evaluations produced by GPT-4 across six academic subjects.<span class='px-1 mx-1 bg-yellow-200'>By applying a comprehensive analytical framework that includes Odds Ratio (OR) analysis, Word Embedding Association Test (WEAT), sentiment analysis, and contextual analysis, this paper identified patterns of gender-associated language reflecting societal stereotypes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, words related to approachability and support were used more frequently for female instructors, while words related to entertainment were predominantly used for male instructors, aligning with the concepts of communal and agentic behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span><span class='px-1 mx-1 bg-yellow-200'>The study also found moderate to strong associations between male salient adjectives and male names, though career and family words did not distinctly capture gender biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>These findings align with prior research on societal norms and stereotypes, reinforcing the notion that LLM-generated text reflects existing biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09652v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09652v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities.To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment.Nevertheless, repairing the identified vulnerabilities is still complex and labor-intensive, particularly for developers lacking security expertise.Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding.To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair.<span class='px-1 mx-1 bg-yellow-200'>The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into sub-tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM.We evaluate ContractTinker on 48 high-risk vulnerabilities.The experimental results show that among the patches generated by ContractTinker, 23 (48%) are valid patches that fix the vulnerabilities, while 10 (21%) require only minor modifications.A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09661v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09661v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span>Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance.Current systems often limit users' flexibility to direct their reflections.<span class='px-1 mx-1 bg-yellow-200'>We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>ExploreSelf allows users to receive adaptive support through dynamically generated questions.<span class='px-1 mx-1 bg-yellow-200'>Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span>Our findings demonstrate that participants valued the balance between guided support and freedom to control their reflective journey, leading to deeper engagement and insight.<span class='px-1 mx-1 bg-yellow-200'>Building on our findings, we discuss implications for designing LLM-driven tools that promote user empowerment through effective reflective practices. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09662v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09662v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood.This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort.<span class='px-1 mx-1 bg-yellow-200'>Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.872</span></span>Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts.ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents.This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks.Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT.This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07732v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07732v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Objection Overruled! Lay People can Distinguish Large Language Models from Lawyers, but still Favour Advice from an LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception.In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice.In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown.<span class='px-1 mx-1 bg-yellow-200'>When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>This result was replicated in Experiment 2.Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3.Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07871v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07871v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored.This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes.<span class='px-1 mx-1 bg-yellow-200'>We propose a framework that analyzes candidates' "Policies, Persona, and Perspective" (3P) and how they resonate with the "Interests, Ideologies, and Identity" (3I) of four key audience groups: voters, businesses, donors, and politicians. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances.<span class='px-1 mx-1 bg-yellow-200'>Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span>This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts.In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Large Language Models in Social Sciences</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ArticulatePro: A Comparative Study on a Proactive and Non-Proactive Assistant in a Climate Data Exploration Task
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in Natural Language Interfaces (NLIs) and Large Language Models (LLMs) have transformed our approach to NLP tasks, allowing us to focus more on a Pragmatics-based approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>This shift enables more natural interactions between humans and voice assistants, which have been challenging to achieve.Pragmatics describes how users often talk out of turn, interrupt each other, or provide relevant information without being explicitly asked (maxim of quantity).To explore this, we developed a digital assistant that constantly listens to conversations and proactively generates relevant visualizations during data exploration tasks.<span class='px-1 mx-1 bg-yellow-200'>In a within-subject study, participants interacted with both proactive and non-proactive versions of a voice assistant while exploring the Hawaii Climate Data Portal (HCDP). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>Results suggest that the proactive assistant enhanced user engagement and facilitated quicker insights.<span class='px-1 mx-1 bg-yellow-200'>Our study highlights the potential of Pragmatic, proactive AI in NLIs and identifies key challenges in its implementation, offering insights for future research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10797v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10797v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Written texts reflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences.However, conventional text mining techniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns.To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework.<span class='px-1 mx-1 bg-yellow-200'>We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span>Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11032v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11032v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) and automatic speech translation (AST).<span class='px-1 mx-1 bg-yellow-200'>However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically.By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations.Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language.Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard speech encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11214v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11214v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-as-a-Judge & Reward Model: What They Can and Cannot Do
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span>Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning.However, despite their popularity, their effectiveness outside of English remains largely unexplored.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span><span class='px-1 mx-1 bg-yellow-200'>First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language.<span class='px-1 mx-1 bg-yellow-200'>Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.789</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span>Traditional learning-based approaches face two challenges.Firstly, natural language instructions often lack explicit task planning.Secondly, extensive training is required to equip models with knowledge of the task environment.Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples.To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth.Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database.In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction.Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences.Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11279v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11279v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants.As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values.<span class='px-1 mx-1 bg-yellow-200'>This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span><span class='px-1 mx-1 bg-yellow-200'>We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span>Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians.<span class='px-1 mx-1 bg-yellow-200'>Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span>but also how it is written.<span class='px-1 mx-1 bg-yellow-200'>These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.857</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11360v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11360v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Assessing personality traits using large language models (LLMs) has emerged as an interesting and challenging area of research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span><span class='px-1 mx-1 bg-yellow-200'>While previous methods employ explicit questionnaires, often derived from the Big Five model of personality, we hypothesize that LLMs implicitly encode notions of personality when modeling next-token responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.873</span></span><span class='px-1 mx-1 bg-yellow-200'>To demonstrate this, we introduce a novel approach that uncovers latent personality dimensions in LLMs by applying singular value de-composition (SVD) to the log-probabilities of trait-descriptive adjectives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.855</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments show that LLMs "rediscover" core personality traits such as extraversion, agreeableness, conscientiousness, neuroticism, and openness without relying on direct questionnaire inputs, with the top-5 factors corresponding to Big Five traits explaining 74.3% of the variance in the latent space. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, we can use the derived principal components to assess personality along the Big Five dimensions, and achieve improvements in average personality prediction accuracy of up to 5% over fine-tuned models, and up to 21% over direct LLM-based scoring techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09905v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09905v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper provides a comprehensive survey of sentiment analysis within the context of artificial intelligence (AI) and large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span><span class='px-1 mx-1 bg-yellow-200'>Sentiment analysis, a critical aspect of natural language processing (NLP), has evolved significantly from traditional rule-based methods to advanced deep learning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.717</span></span><span class='px-1 mx-1 bg-yellow-200'>This study examines the historical development of sentiment analysis, highlighting the transition from lexicon-based and pattern-based approaches to more sophisticated machine learning and deep learning models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span>Key challenges are discussed, including handling bilingual texts, detecting sarcasm, and addressing biases.The paper reviews state-of-the-art approaches, identifies emerging trends, and outlines future research directions to advance the field.<span class='px-1 mx-1 bg-yellow-200'>By synthesizing current methodologies and exploring future opportunities, this survey aims to understand sentiment analysis in the AI and LLM context thoroughly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09989v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09989v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Mental health disorders are among the most prevalent diseases worldwide, affecting nearly one in four people. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>Despite their widespread impact, the intervention rate remains below 25%, largely due to the significant cooperation required from patients for both diagnosis and intervention.<span class='px-1 mx-1 bg-yellow-200'>The core issue behind this low treatment rate is stigma, which discourages over half of those affected from seeking help. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>This paper presents MindGuard, an accessible, stigma-free, and professional mobile mental healthcare system designed to provide mental health first aid.The heart of MindGuard is an innovative edge LLM, equipped with professional mental health knowledge, that seamlessly integrates objective mobile sensor data with subjective Ecological Momentary Assessment records to deliver personalized screening and intervention conversations.We conduct a broad evaluation of MindGuard using open datasets spanning four years and real-world deployment across various mobile devices involving 20 subjects for two weeks.Remarkably, MindGuard achieves results comparable to GPT-4 and outperforms its counterpart with more than 10 times the model size.We believe that MindGuard paves the way for mobile LLM applications, potentially revolutionizing mental healthcare practices by substituting self-reporting and intervention conversations with passive, integrated monitoring within daily life, thus ensuring accessible and stigma-free mental health support.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10064v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10064v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Messy Code Makes Managing ML Pipelines Difficult? Just Let LLMs Rewrite the Code!
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Machine learning (ML) applications that learn from data are increasingly used to automate impactful decisions.Unfortunately, these applications often fall short of adequately managing critical data and complying with upcoming regulations.A technical reason for the persistence of these issues is that the data pipelines in common ML libraries and cloud services lack fundamental declarative, data-centric abstractions.Recent research has shown how such abstractions enable techniques like provenance tracking and automatic inspection to help manage ML pipelines.Unfortunately, these approaches lack adoption in the real world because they require clean ML pipeline code written with declarative APIs, instead of the messy imperative Python code that data scientists typically write for data preparation.   <span class='px-1 mx-1 bg-yellow-200'>We argue that it is unrealistic to expect data scientists to change their established development practices. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>Instead, we propose to circumvent this "code abstraction gap" by leveraging the code generation capabilities of large language models (LLMs).Our idea is to rewrite messy data science code to a custom-tailored declarative pipeline abstraction, which we implement as a proof-of-concept in our prototype Lester.We detail its application for a challenging compliance management example involving "incremental view maintenance" of deployed ML pipelines.The code rewrites for our running example show the potential of LLMs to make messy data science code declarative, e.g., by identifying hand-coded joins in Python and turning them into joins on dataframes, or by generating declarative feature encoders from NumPy code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10081v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10081v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Emo-DPO: Controllable Emotional Speech Synthesis through Direct Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Current emotional text-to-speech (TTS) models predominantly conduct supervised training to learn the conversion from text and desired emotion to its emotional speech, focusing on a single emotion per text-speech pair.These models only learn the correct emotional outputs without fully comprehending other emotion characteristics, which limits their capabilities of capturing the nuances between different emotions.<span class='px-1 mx-1 bg-yellow-200'>We propose a controllable Emo-DPO approach, which employs direct preference optimization to differentiate subtle emotional nuances between emotions through optimizing towards preferred emotions over less preferred emotional ones. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>Instead of relying on traditional neural architectures used in existing emotional TTS models, we propose utilizing the emotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning and instruction-following capabilities.Comprehensive experiments confirm that our proposed method outperforms the existing baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10157v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10157v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Quantile Regression for Distributional Reward Models in RLHF
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Reinforcement learning from human feedback (RLHF) has become a key method for aligning large language models (LLMs) with human preferences through the use of reward models.<span class='px-1 mx-1 bg-yellow-200'>However, traditional reward models typically generate point estimates, which oversimplify the diversity and complexity of human values and preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>In this paper, we introduce Quantile Reward Models (QRMs), a novel approach to reward modeling that learns a distribution over rewards instead of a single scalar value.Our method uses quantile regression to estimate a full, potentially multimodal distribution over preferences, providing a more powerful and nuanced representation of preferences.This distributional approach can better capture the diversity of human values, addresses label noise, and accommodates conflicting preferences by modeling them as distinct modes in the distribution.Our experimental results show that QRM outperforms comparable traditional point-estimate models on RewardBench.Furthermore, we demonstrate that the additional information provided by the distributional estimates can be utilized in downstream applications, such as risk-aware reinforcement learning, resulting in LLM policies that generate fewer extremely negative responses.Our code and model are released at https://github.com/Nicolinho/QRM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10164v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10164v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs for clinical risk prediction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This study compares the efficacy of GPT-4 and clinalytix Medical AI in predicting the clinical risk of delirium development.<span class='px-1 mx-1 bg-yellow-200'>Findings indicate that GPT-4 exhibited significant deficiencies in identifying positive cases and struggled to provide reliable probability estimates for delirium risk, while clinalytix Medical AI demonstrated superior accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span><span class='px-1 mx-1 bg-yellow-200'>A thorough analysis of the large language model's (LLM) outputs elucidated potential causes for these discrepancies, consistent with limitations reported in extant literature. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span><span class='px-1 mx-1 bg-yellow-200'>These results underscore the challenges LLMs face in accurately diagnosing conditions and interpreting complex clinical data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>While LLMs hold substantial potential in healthcare, they are currently unsuitable for independent clinical decision-making.Instead, they should be employed in assistive roles, complementing clinical expertise.Continued human oversight remains essential to ensure optimal outcomes for both patients and healthcare providers.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10191v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10191v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the demand for human-like interactions with LLMs continues to grow, so does the interest in manipulating their personality traits, which has emerged as a key area of research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span>Methods like prompt-based In-Context Knowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have been explored but show irregularity and variability.IKE depends on the prompt, leading to variability and sensitivity, while MEND yields inconsistent and gibberish outputs.To address this, we employed Opinion QA Based Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank Adaptation (QLORA), to manipulate the Big Five personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.After PEFT, models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis, despite their absence in the PEFT data.For instance, Llama-2-7B-chat generated emojis in 99.5% of extraversion-related test instances, while Mistral-8B-Instruct did so in 92.5% of openness-related test instances.<span class='px-1 mx-1 bg-yellow-200'>Explainability analysis indicated that the LLMs used emojis intentionally to express these traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span>This paper provides a number of novel contributions.First, introducing an Opinion QA dataset for PEFT-driven personality manipulation; second, developing metric models to benchmark LLM personality traits; third, demonstrating PEFT's superiority over IKE in personality manipulation; and finally, analyzing and validating emoji usage through explainability methods such as mechanistic interpretability and in-context learning explainability methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10245v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10245v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span>In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation.<span class='px-1 mx-1 bg-yellow-200'>This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion.<span class='px-1 mx-1 bg-yellow-200'>By harnessing an intent twice reflect the mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10289v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10289v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Instigating Cooperation among LLM Agents Using Adaptive Information Modulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments.Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior.Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates.<span class='px-1 mx-1 bg-yellow-200'>This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.859</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10372v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10372v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The growing demand for AI training data has transformed data annotation into a global industry, but traditional approaches relying on human annotators are often time-consuming, labor-intensive, and prone to inconsistent quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>We propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models into the annotation process.<span class='px-1 mx-1 bg-yellow-200'>Our research introduces a collaborative paradigm that leverages the strengths of both professional human annotators and large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>By employing LLMs as pre-annotation and real-time assistants, and judges on annotator responses, MILO enables effective interaction patterns between human annotators and LLMs.Three empirical studies on multimodal data annotation demonstrate MILO's efficacy in reducing handling time, improving data quality, and enhancing annotator experiences.We also introduce quality rubrics for flexible evaluation and fine-grained feedback on open-ended annotations.The MILO framework has implications for accelerating AI/ML development, reducing reliance on human annotation alone, and promoting better alignment between human and machine values.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10702v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10702v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GPT takes the SAT: Tracing changes in Test Difficulty and Math Performance of Students
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scholastic Aptitude Test (SAT) is crucial for college admissions but its effectiveness and relevance are increasingly questioned.This paper enhances Synthetic Control methods by introducing "Transformed Control", a novel method that employs Large Language Models (LLMs) powered by Artificial Intelligence to generate control groups.We utilize OpenAI's API to generate a control group where GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023.This control group helps analyze shifts in SAT math difficulty over time, starting from the baseline year of 2008.Using parallel trends, we calculate the Average Difference in Scores (ADS) to assess changes in high school students' math performance.Our results indicate a significant decrease in the difficulty of the SAT math section over time, alongside a decline in students' math performance.The analysis shows a 71-point drop in the rigor of SAT math from 2008 to 2023, with student performance decreasing by 36 points, resulting in a 107-point total divergence in average student math performance.<span class='px-1 mx-1 bg-yellow-200'>We investigate possible mechanisms for this decline in math proficiency, such as changing university selection criteria, increased screen time, grade inflation, and worsening adolescent mental health. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.857</span></span><span class='px-1 mx-1 bg-yellow-200'>Disparities among demographic groups show a 104-point drop for White students, 84 points for Black students, and 53 points for Asian students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span><span class='px-1 mx-1 bg-yellow-200'>Male students saw a 117-point reduction, while female students had a 100-point decrease. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.769</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10750v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10750v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantics Preserving Emoji Recommendation with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emojis have become an integral part of digital communication, enriching text by conveying emotions, tone, and intent.Existing emoji recommendation methods are primarily evaluated based on their ability to match the exact emoji a user chooses in the original text.<span class='px-1 mx-1 bg-yellow-200'>However, they ignore the essence of users' behavior on social media in that each text can correspond to multiple reasonable emojis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span>To better assess a model's ability to align with such real-world emoji usage, we propose a new semantics preserving evaluation framework for emoji recommendation, which measures a model's ability to recommend emojis that maintain the semantic consistency with the user's text.<span class='px-1 mx-1 bg-yellow-200'>To evaluate how well a model preserves semantics, we assess whether the predicted affective state, demographic profile, and attitudinal stance of the user remain unchanged. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>If these attributes are preserved, we consider the recommended emojis to have maintained the original semantics.The advanced abilities of Large Language Models (LLMs) in understanding and generating nuanced, contextually relevant output make them well-suited for handling the complexities of semantics preserving emoji recommendation.To this end, we construct a comprehensive benchmark to systematically assess the performance of six proprietary and open-source LLMs using different prompting techniques on our task.Our experiments demonstrate that GPT-4o outperforms other LLMs, achieving a semantics preservation score of 79.23%.Additionally, we conduct case studies to analyze model biases in downstream classification tasks and evaluate the diversity of the recommended emojis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10760v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10760v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper investigates gender bias in Large Language Model (LLM)-generated teacher evaluations in higher education setting, focusing on evaluations produced by GPT-4 across six academic subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.88</span></span><span class='px-1 mx-1 bg-yellow-200'>By applying a comprehensive analytical framework that includes Odds Ratio (OR) analysis, Word Embedding Association Test (WEAT), sentiment analysis, and contextual analysis, this paper identified patterns of gender-associated language reflecting societal stereotypes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.925</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, words related to approachability and support were used more frequently for female instructors, while words related to entertainment were predominantly used for male instructors, aligning with the concepts of communal and agentic behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span><span class='px-1 mx-1 bg-yellow-200'>The study also found moderate to strong associations between male salient adjectives and male names, though career and family words did not distinctly capture gender biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.853</span></span><span class='px-1 mx-1 bg-yellow-200'>These findings align with prior research on societal norms and stereotypes, reinforcing the notion that LLM-generated text reflects existing biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.894</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09652v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09652v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.88</span></span>Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance.Current systems often limit users' flexibility to direct their reflections.We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey.ExploreSelf allows users to receive adaptive support through dynamically generated questions.Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf.Our findings demonstrate that participants valued the balance between guided support and freedom to control their reflective journey, leading to deeper engagement and insight.Building on our findings, we discuss implications for designing LLM-driven tools that promote user empowerment through effective reflective practices.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09662v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09662v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This article benchmarked the ability of OpenAI's GPTs and a number of open-source LLMs to perform annotation tasks on political content.<span class='px-1 mx-1 bg-yellow-200'>We used a novel protest event dataset comprising more than three million digital interactions and created a gold standard that includes ground-truth labels annotated by human coders about toxicity and incivility on social media. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span>We included in our benchmark Google's Perspective algorithm, which, along with GPTs, was employed throughout their respective APIs while the open-source LLMs were deployed locally.The findings show that Perspective API using a laxer threshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot classification annotations.In addition, Nous Hermes 2 and Mistral OpenOrca, with a smaller number of parameters, are able to perform the task with high performance, being attractive options that could offer good trade-offs between performance, implementing costs and computing time.Ancillary findings using experiments setting different temperature levels show that although GPTs tend to show not only excellent computing time but also overall good levels of reliability, only open-source LLMs ensure full reproducibility in the annotation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09741v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09741v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Causal Inference with Large Language Model: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities.<span class='px-1 mx-1 bg-yellow-200'>Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation.We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios.Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09822v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09822v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs in Education Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training.This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India.<span class='px-1 mx-1 bg-yellow-200'>Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors.CHWs trusted the information they received on ASHABot and treated it as an authoritative resource.CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability.We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10913v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10913v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process.However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored.In this study, we investigate the impact of memory strength and evidence presentation on LLMs' receptiveness to external evidence.We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works.We also generate evidence in various styles to evaluate the effects of evidence in different styles.Two datasets are used for evaluation: Natural Questions (NQ) with popular questions and popQA featuring long-tail questions.<span class='px-1 mx-1 bg-yellow-200'>Our results show that for questions with high memory strength, LLMs are more likely to rely on internal memory, particularly for larger LLMs such as GPT-4. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.523</span></span>On the other hand, presenting paraphrased evidence significantly increases LLMs' receptiveness compared to simple repetition or adding details.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10955v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10955v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as speech recognition and audio captioning, where the instructions are usually textual prompts.Audio language models are mostly initialized from pre-trained audio encoders and large language models (LLMs).Although these pre-trained components were developed to support multiple languages, audio-language models are trained predominantly on English data, which may limit their usability to only English instructions or English speech inputs.First, this paper examines the performance of existing audio language models in an underserved language using Thai as an example.This paper demonstrates that, despite being built on multilingual backbones, audio language models do not exhibit cross-lingual emergent abilities to low-resource languages.Second, this paper studies data mixture for developing audio language models that are optimized for a target language as well as English.In addition.<span class='px-1 mx-1 bg-yellow-200'>this paper integrates audio comprehension and speech instruction-following capabilities into a single unified model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.554</span></span>Our experiments provide insights into data mixture for enhancing instruction-following capabilities in both a low-resource language and English.Our model, Typhoon-Audio, outperforms existing open-source audio language models by a considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10999v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10999v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation.As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios.In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language.We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code.With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples.<span class='px-1 mx-1 bg-yellow-200'>Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.519</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11041v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11041v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets.Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined.<span class='px-1 mx-1 bg-yellow-200'>This paper evaluates the performance of instruction-tuned LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q\&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11055v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11055v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity.Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent.This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.   Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works.Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types.Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects.Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems.This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11393v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11393v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations.We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing.We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation.<span class='px-1 mx-1 bg-yellow-200'>We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.551</span></span>Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation.Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation.This work contributes ~45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts.We will release the dialectal translation models and benchmarks curated in this study.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11404v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11404v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SFR-RAG: Towards Contextually Faithful LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Retrieval Augmented Generation (RAG), a paradigm that integrates external contextual information with large language models (LLMs) to enhance factual accuracy and relevance, has emerged as a pivotal area in generative AI.<span class='px-1 mx-1 bg-yellow-200'>The LLMs used in RAG applications are required to faithfully and completely comprehend the provided context and users' questions, avoid hallucination, handle unanswerable, counterfactual or otherwise low-quality and irrelevant contexts, perform complex multi-hop reasoning and produce reliable citations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.516</span></span>In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with an emphasis on context-grounded generation and hallucination minimization.We also present ContextualBench, a new evaluation framework compiling multiple popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with consistent RAG settings to ensure reproducibility and consistency in model assessments.Experimental results demonstrate that our SFR-RAG-9B model outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with significantly fewer parameters.The model is also shown to be resilient to alteration in the contextual information and behave appropriately when relevant context is removed.Additionally, the SFR-RAG model maintains competitive performance in general instruction-following tasks and function-calling capabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09916v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09916v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper outlines the LLMs4OL 2024, the first edition of the Large Language Models for Ontology Learning Challenge.<span class='px-1 mx-1 bg-yellow-200'>LLMs4OL is a community development initiative collocated with the 23rd International Semantic Web Conference (ISWC) to explore the potential of Large Language Models (LLMs) in Ontology Learning (OL), a vital process for enhancing the web with structured knowledge to improve interoperability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.519</span></span>By leveraging LLMs, the challenge aims to advance understanding and innovation in OL, aligning with the goals of the Semantic Web to create a more intelligent and user-friendly web.In this paper, we give an overview of the 2024 edition of the LLMs4OL challenge and summarize the contributions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10146v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10146v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GPT takes the SAT: Tracing changes in Test Difficulty and Math Performance of Students
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Scholastic Aptitude Test (SAT) is crucial for college admissions but its effectiveness and relevance are increasingly questioned. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.574</span></span>This paper enhances Synthetic Control methods by introducing "Transformed Control", a novel method that employs Large Language Models (LLMs) powered by Artificial Intelligence to generate control groups.We utilize OpenAI's API to generate a control group where GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023.This control group helps analyze shifts in SAT math difficulty over time, starting from the baseline year of 2008.<span class='px-1 mx-1 bg-yellow-200'>Using parallel trends, we calculate the Average Difference in Scores (ADS) to assess changes in high school students' math performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.587</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results indicate a significant decrease in the difficulty of the SAT math section over time, alongside a decline in students' math performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.567</span></span><span class='px-1 mx-1 bg-yellow-200'>The analysis shows a 71-point drop in the rigor of SAT math from 2008 to 2023, with student performance decreasing by 36 points, resulting in a 107-point total divergence in average student math performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.562</span></span><span class='px-1 mx-1 bg-yellow-200'>We investigate possible mechanisms for this decline in math proficiency, such as changing university selection criteria, increased screen time, grade inflation, and worsening adolescent mental health. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span><span class='px-1 mx-1 bg-yellow-200'>Disparities among demographic groups show a 104-point drop for White students, 84 points for Black students, and 53 points for Asian students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.561</span></span><span class='px-1 mx-1 bg-yellow-200'>Male students saw a 117-point reduction, while female students had a 100-point decrease. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10750v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10750v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper investigates gender bias in Large Language Model (LLM)-generated teacher evaluations in higher education setting, focusing on evaluations produced by GPT-4 across six academic subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.51</span></span>By applying a comprehensive analytical framework that includes Odds Ratio (OR) analysis, Word Embedding Association Test (WEAT), sentiment analysis, and contextual analysis, this paper identified patterns of gender-associated language reflecting societal stereotypes.Specifically, words related to approachability and support were used more frequently for female instructors, while words related to entertainment were predominantly used for male instructors, aligning with the concepts of communal and agentic behaviors.The study also found moderate to strong associations between male salient adjectives and male names, though career and family words did not distinctly capture gender biases.These findings align with prior research on societal norms and stereotypes, reinforcing the notion that LLM-generated text reflects existing biases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09652v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09652v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) offer numerous applications, the full extent of which is not yet understood.This paper investigates if LLMs can be applied for editing structured and semi-structured documents with minimal effort.<span class='px-1 mx-1 bg-yellow-200'>Using a qualitative research approach, we conduct two case studies with ChatGPT and thoroughly analyze the results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.55</span></span>Our experiments indicate that LLMs can effectively edit structured and semi-structured documents when provided with basic, straightforward prompts.ChatGPT demonstrates a strong ability to recognize and process the structure of annotated documents.<span class='px-1 mx-1 bg-yellow-200'>This suggests that explicitly structuring tasks and data in prompts might enhance an LLM's ability to understand and solve tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.567</span></span>Furthermore, the experiments also reveal impressive pattern matching skills in ChatGPT.This observation deserves further investigation, as it may contribute to understanding the processes leading to hallucinations in LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07732v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07732v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation.<span class='px-1 mx-1 bg-yellow-200'>iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.51</span></span>iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation).We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses.Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation.<span class='px-1 mx-1 bg-yellow-200'>We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.58</span></span><span class='px-1 mx-1 bg-yellow-200'>Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08027v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08027v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.566</span></span>Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources.Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability.<span class='px-1 mx-1 bg-yellow-200'>We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.565</span></span>Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SimulBench: Evaluating Language Models with Creative Simulation Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users.While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks.A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI.To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks.Then, challenging dialogue scripts are extracted for evaluating different target LLMs.<span class='px-1 mx-1 bg-yellow-200'>To facilitate automatic assessment on \DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span>Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs.For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\% more cases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07641v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07641v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the availability of various instruction datasets, a pivotal challenge is how to effectively select and integrate these instructions to fine-tune large language models (LLMs).Previous research mainly focuses on selecting individual high-quality instructions.However, these works overlooked the joint interactions and dependencies between different categories of instructions, leading to suboptimal selection strategies.Moreover, the nature of these interaction patterns remains largely unexplored, let alone optimize the instruction set with regard to them.<span class='px-1 mx-1 bg-yellow-200'>To fill these gaps, in this paper, we: (1) systemically investigate interaction and dependency patterns between different categories of instructions, (2) manage to optimize the instruction set concerning the interaction patterns using a linear programming-based method, and optimize the learning schema of SFT using an instruction dependency taxonomy guided curriculum learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>Experimental results across different LLMs demonstrate improved performance over strong baselines on widely adopted benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07045v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07045v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and Education
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed to enhance user interaction for educational and research purposes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>Leveraging cutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as a sophisticated AI assistant, exploiting the capabilities of traditional models like ChatGPT.Central to Bio-Eng-LMM is its implementation of Retrieval Augmented Generation (RAG) through three primary methods: integration of preprocessed documents, real-time processing of user-uploaded files, and information retrieval from any specified website.Additionally, the chatbot incorporates image generation via a Stable Diffusion Model (SDM), image understanding and response generation through LLAVA, and search functionality on the internet powered by secure search engine such as DuckDuckGo.To provide comprehensive support, Bio-Eng-LMM offers text summarization, website content summarization, and both text and voice interaction.The chatbot maintains session memory to ensure contextually relevant and coherent responses.This integrated platform builds upon the strengths of RAG-GPT and Web-Based RAG Query (WBRQ) where the system fetches relevant information directly from the web to enhance the LLMs response generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07110v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07110v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Identify Design Problems Through Questioning: Exploring Role-playing Interactions with Large Language Models to Foster Design Questioning Skills
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Identifying design problems is a crucial step for creating plausible solutions, but it is challenging for design novices due to their limited knowledge and experience.<span class='px-1 mx-1 bg-yellow-200'>Questioning is a promising skill that enables students to independently identify design problems without being passive or relying on instructors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span><span class='px-1 mx-1 bg-yellow-200'>This study explores role-playing interactions with Large Language Model (LLM)-powered Conversational Agents (CAs) to foster the questioning skills of novice design students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span><span class='px-1 mx-1 bg-yellow-200'>We proposed an LLM-powered CA prototype and conducted a preliminary study with 16 novice design students engaged in a real-world design class to observe the interactions between students and the LLM-powered CAs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>Our findings indicate that while the CAs stimulated questioning and reduced pressure to ask questions, it also inadvertently led to over-reliance on LLM responses.<span class='px-1 mx-1 bg-yellow-200'>We proposed design considerations and future works for LLM-powered CA to foster questioning skills. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07178v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07178v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown great promise in vulnerability identification.As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential.However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges.In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner.<span class='px-1 mx-1 bg-yellow-200'>We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details.Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols.We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits.The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs.Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07407v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07407v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>However, it is often too time-consuming for widespread use in educational practice.While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting.This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback.<span class='px-1 mx-1 bg-yellow-200'>CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span><span class='px-1 mx-1 bg-yellow-200'>Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span><span class='px-1 mx-1 bg-yellow-200'>Students can further engage with the feedback to refine their understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span><span class='px-1 mx-1 bg-yellow-200'>A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.538</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.545</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07453v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07453v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Unlock Novel Scientific Research Ideas?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>"An idea is nothing more nor less than a new combination of old elements" (Young, J.W.).The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives.This study explores the capability of LLMs in generating novel research ideas based on information from research papers.<span class='px-1 mx-1 bg-yellow-200'>We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0.We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas.This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations.Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas.We make our datasets and codes publicly available.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06185v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06185v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NOVI : Chatbot System for University Novice with BERT and LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>To mitigate the difficulties of university freshmen in adapting to university life, we developed NOVI, a chatbot system based on GPT-4o. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span>This system utilizes post and comment data from SKKU 'Everytime', a university community site.Developed using LangChain, NOVI's performance has been evaluated with a BLEU score, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR score.<span class='px-1 mx-1 bg-yellow-200'>This approach is not only limited to help university freshmen but is also expected to help various people adapting to new environments with different data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.532</span></span><span class='px-1 mx-1 bg-yellow-200'>This research explores the development and potential application of new educational technology tools, contributing to easier social adaptation for beginners and settling a foundation for future advancement in LLM studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06192v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06192v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces text-to-shape-display, a novel approach to generating dynamic shape changes in pin-based shape displays through natural language commands.By leveraging large language models (LLMs) and AI-chaining, our approach allows users to author shape-changing behaviors on demand through text prompts without programming.We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes.<span class='px-1 mx-1 bg-yellow-200'>Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display, which translates the user's textual command into executable code and allows for quick exploration through a web-based control interface. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.554</span></span>We evaluate the effectiveness of SHAPE-IT in two ways: 1) performance evaluation and 2) user evaluation (N= 10).The study conclusions highlight the ability to facilitate rapid ideation of a wide range of shape-changing behaviors with AI.However, the findings also expose accuracy-related challenges and limitations, prompting further exploration into refining the framework for leveraging AI to better suit the unique requirements of shape-changing systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06205v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06205v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MAGDA: Multi-agent guideline-driven diagnostic assistance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In emergency departments, rural hospitals, or clinics in less developed regions, clinicians often lack fast image analysis by trained radiologists, which can have a detrimental effect on patients' healthcare.Large Language Models (LLMs) have the potential to alleviate some pressure from these clinicians by providing insights that can help them in their decision-making.<span class='px-1 mx-1 bg-yellow-200'>While these LLMs achieve high test results on medical exams showcasing their great theoretical medical knowledge, they tend not to follow medical guidelines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span>In this work, we introduce a new approach for zero-shot guideline-driven decision support.We model a system of multiple LLM agents augmented with a contrastive vision-language model that collaborate to reach a patient diagnosis.After providing the agents with simple diagnostic guidelines, they will synthesize prompts and screen the image for findings following these guidelines.Finally, they provide understandable chain-of-thought reasoning for their diagnosis, which is then self-refined to consider inter-dependencies between diseases.As our method is zero-shot, it is adaptable to settings with rare diseases, where training data is limited, but expert-crafted disease descriptions are available.We evaluate our method on two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing performance improvement over existing zero-shot methods and generalizability to rare diseases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06351v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06351v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Think-on-Process: Dynamic Process Generation for Collaborative Development of Multi-Agent System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software development is a collaborative endeavor that requires individuals from different departments to work together in order to collectively develop a high-quality software system.<span class='px-1 mx-1 bg-yellow-200'>In this context, people have begun to explore a method that leverages multi-agent systems based on LLMs to carry out software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.577</span></span>However, existing research tends to rigidly fix the software development process in a framework in code form, thus failing to dynamically adjust the software development process in real-time to meet the more flexible and variable software environment.In this paper, we propose a dynamic process generation framework, named ToP (Think-on-Process).The core idea of ToP is to leverage experiential knowledge (i.e., process models) to guide LLMs in generating software development processes (i.e., instances).<span class='px-1 mx-1 bg-yellow-200'>These instances will guide multi-agent in software development and employ a compiler to provide feedback on the development outcomes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.532</span></span>Subsequently, we utilize heuristic algorithms to filter the instances and apply process mining algorithms to derive process model.Finally, the process model will be converted into text, formatted as prompts, to enhance the ability of LLMs to generate other instances.Experiments demonstrate that our framework ToP significantly enhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for five categories of software development tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06568v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06568v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human Perception of LLM-generated Text Content in Social Media Environments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emerging technologies, particularly artificial intelligence (AI), and more specifically Large Language Models (LLMs) have provided malicious actors with powerful tools for manipulating digital discourse.<span class='px-1 mx-1 bg-yellow-200'>LLMs have the potential to affect traditional forms of democratic engagements, such as voter choice, government surveys, or even online communication with regulators; since bots are capable of producing large quantities of credible text. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.578</span></span>To investigate the human perception of LLM-generated content, we recruited over 1,000 participants who then tried to differentiate bot from human posts in social media discussion threads.We found that humans perform poorly at identifying the true nature of user posts on social media.We also found patterns in how humans identify LLM-generated text content in social media discourse.Finally, we observed the Uncanny Valley effect in text dialogue in both user perception and identification.This indicates that despite humans being poor at the identification process, they can still sense discomfort when reading LLM-generated content.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06653v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06653v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI for Mathematics Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean4
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Using computerized verifiable formal languages like Lean 4 to prove mathematical theorems has a significant impact on mathematical formalization.<span class='px-1 mx-1 bg-yellow-200'>Lean 4 offers prominent potential for advancing mathematical reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.519</span></span>However, existing efforts are limited to mathematical formalization languages in substantial online corpora and are dedicated to keeping pace with rapidly evolving languages.To bridge the gap between the traditional and computerized proof, my approach to formalizing theorem proving involves generating formal steps and complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs.<span class='px-1 mx-1 bg-yellow-200'>The method is to introduce the basic structure and tactics in general, determine how AI can assist the mathematical formalization process to improve its performance, and give examples of solving problems in Lean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract algebra. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.516</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05977v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05977v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs as Recommender Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Model (LLM)-based recommendation systems provide more comprehensive recommendations than traditional systems by deeply analyzing content and user behavior. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span>However, these systems often exhibit biases, favoring mainstream content while marginalizing non-traditional options due to skewed training data.<span class='px-1 mx-1 bg-yellow-200'>This study investigates the intricate relationship between bias and LLM-based recommendation systems, with a focus on music, song, and book recommendations across diverse demographic and cultural groups. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span><span class='px-1 mx-1 bg-yellow-200'>Through a comprehensive analysis conducted over different LLM-models, this paper evaluates the impact of bias on recommendation outcomes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.764</span></span>Our findings reveal that bias is so deeply ingrained within these systems that even a simpler intervention like prompt engineering can significantly reduce bias, underscoring the pervasive nature of the issue.<span class='px-1 mx-1 bg-yellow-200'>Moreover, factors like intersecting identities and contextual information, such as socioeconomic status, further amplify these biases, demonstrating the complexity and depth of the challenges faced in creating fair recommendations across different groups. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10825v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10825v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Implicit feedback, often used to build recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias.Previous studies have attempted to alleviate this by identifying noisy samples based on their diverged patterns, such as higher loss values, and mitigating the noise through sample dropping or reweighting.Despite the progress, we observe existing approaches struggle to distinguish hard samples and noise samples, as they often exhibit similar patterns, thereby limiting their effectiveness in denoising recommendations.To address this challenge, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we construct an LLM-based scorer to evaluate the semantic consistency of items with the user preference, which is quantified based on summarized historical user interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>The resulting scores are used to assess the hardness of samples for the pointwise or pairwise training objectives.To ensure efficiency, we introduce a variance-based sample pruning strategy to filter potential hard samples before scoring.Besides, we propose an iterative preference update module designed to continuously refine summarized user preference, which may be biased due to false-positive user-item interactions.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.831</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10343v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10343v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TravelAgent: An AI Assistant for Personalized Travel Planning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As global tourism expands and artificial intelligence technology advances, intelligent travel planning services have emerged as a significant research focus.Within dynamic real-world travel scenarios with multi-dimensional constraints, services that support users in automatically creating practical and customized travel itineraries must address three key objectives: Rationality, Comprehensiveness, and Personalization.However, existing systems with rule-based combinations or LLM-based planning methods struggle to fully satisfy these criteria.To overcome the challenges, we introduce TravelAgent, a travel planning system powered by large language models (LLMs) designed to provide reasonable, comprehensive, and personalized travel itineraries grounded in dynamic scenarios.TravelAgent comprises four modules: Tool-usage, Recommendation, Planning, and Memory Module.<span class='px-1 mx-1 bg-yellow-200'>We evaluate TravelAgent's performance with human and simulated users, demonstrating its overall effectiveness in three criteria and confirming the accuracy of personalized recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.08069v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.08069v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multilingual Prompts in LLM-Based Recommenders: Performance Across Languages
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are increasingly used in natural language processing tasks.<span class='px-1 mx-1 bg-yellow-200'>Recommender systems traditionally use methods such as collaborative filtering and matrix factorization, as well as advanced techniques like deep learning and reinforcement learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span><span class='px-1 mx-1 bg-yellow-200'>Although language models have been applied in recommendation, the recent trend have focused on leveraging the generative capabilities of LLMs for more personalized suggestions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.794</span></span><span class='px-1 mx-1 bg-yellow-200'>While current research focuses on English due to its resource richness, this work explores the impact of non-English prompts on recommendation performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.817</span></span><span class='px-1 mx-1 bg-yellow-200'>Using OpenP5, a platform for developing and evaluating LLM-based recommendations, we expanded its English prompt templates to include Spanish and Turkish. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.796</span></span>Evaluation on three real-world datasets, namely ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts generally reduce performance, especially in less-resourced languages like Turkish.<span class='px-1 mx-1 bg-yellow-200'>We also retrained an LLM-based recommender model with multilingual prompts to analyze performance variations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>Retraining with multilingual prompts resulted in more balanced performance across languages, but slightly reduced English performance.<span class='px-1 mx-1 bg-yellow-200'>This work highlights the need for diverse language support in LLM-based recommenders and suggests future research on creating evaluation datasets, using newer models and additional languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07604v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07604v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tail or cold-start items. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.744</span></span>Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens.In this way, it preserves the item's semantics within these tokens and ensures that semantically similar items are represented by similar tokens.<span class='px-1 mx-1 bg-yellow-200'>These semantic tokens have become fundamental in training generative recommendation models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing generative recommendation methods typically involve multiple sub-models for embedding, quantization, and recommendation, leading to an overly complex system. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose to streamline the semantic tokenization and generative recommendation process with a unified framework, dubbed STORE, which leverages a single large language model (LLM) for both tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we formulate semantic tokenization as a text-to-token task and generative recommendation as a token-to-token task, supplemented by a token-to-text reconstruction task and a text-to-token auxiliary task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>All these tasks are framed in a generative manner and trained using a single LLM backbone.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments have been conducted to validate the effectiveness of our STORE framework across various recommendation tasks and datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>We will release the source code and configurations for reproducible research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07276v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07276v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HierLLM: Hierarchical Large Language Model for Question Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Question recommendation is a task that sequentially recommends questions for students to enhance their learning efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span><span class='px-1 mx-1 bg-yellow-200'>That is, given the learning history and learning target of a student, a question recommender is supposed to select the question that will bring the most improvement for students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span><span class='px-1 mx-1 bg-yellow-200'>Previous methods typically model the question recommendation as a sequential decision-making problem, estimating students' learning state with the learning history, and feeding the learning state with the learning target to a neural network to select the recommended question from a question set. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span><span class='px-1 mx-1 bg-yellow-200'>However, previous methods are faced with two challenges: (1) learning history is unavailable in the cold start scenario, which makes the recommender generate inappropriate recommendations; (2) the size of the question set is much large, which makes it difficult for the recommender to select the best question precisely. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span><span class='px-1 mx-1 bg-yellow-200'>To address the challenges, we propose a method called hierarchical large language model for question recommendation (HierLLM), which is a LLM-based hierarchical structure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.786</span></span>The LLM-based structure enables HierLLM to tackle the cold start issue with the strong reasoning abilities of LLM.The hierarchical structure takes advantage of the fact that the number of concepts is significantly smaller than the number of questions, narrowing the range of selectable questions by first identifying the relevant concept for the to-recommend question, and then selecting the recommended question based on that concept.<span class='px-1 mx-1 bg-yellow-200'>This hierarchical structure reduces the difficulty of the recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>To investigate the performance of HierLLM, we conduct extensive experiments, and the results demonstrate the outstanding performance of HierLLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06177v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06177v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                User Preferences for Large Language Model versus Template-Based Explanations of Movie Recommendations: A Pilot Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recommender systems have become integral to our digital experiences, from online shopping to streaming platforms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>Still, the rationale behind their suggestions often remains opaque to users.While some systems employ a graph-based approach, offering inherent explainability through paths associating recommended items and seed items, non-experts could not easily understand these explanations.A popular alternative is to convert graph-based explanations into textual ones using a template and an algorithm, which we denote here as ''template-based'' explanations.Yet, these can sometimes come across as impersonal or uninspiring.A novel method would be to employ large language models (LLMs) for this purpose, which we denote as ''LLM-based''.To assess the effectiveness of LLMs in generating more resonant explanations, we conducted a pilot study with 25 participants.They were presented with three explanations: (1) traditional template-based, (2) LLM-based rephrasing of the template output, and (3) purely LLM-based explanations derived from the graph-based explanations.Although subject to high variance, preliminary findings suggest that LLM-based explanations may provide a richer and more engaging user experience, further aligning with user expectations.This study sheds light on the potential limitations of current explanation methods and offers promising directions for leveraging large language models to improve user satisfaction and trust in recommender systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06297v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06297v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Sequential Recommendations through Multi-Perspective Reflections and Iteration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Sequence recommendation (SeqRec) aims to predict the next item a user will interact with by understanding user intentions and leveraging collaborative filtering information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown great promise in recommendation tasks through prompt-based, fixed reflection libraries, and fine-tuning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span>However, these methods face challenges, including lack of supervision, inability to optimize reflection sources, inflexibility to diverse user needs, and high computational costs.Despite promising results, current studies primarily focus on reflections of users' explicit preferences (e.g., item titles) while neglecting implicit preferences (e.g., brands) and collaborative filtering information.This oversight hinders the capture of preference shifts and dynamic user behaviors.Additionally, existing approaches lack mechanisms for reflection evaluation and iteration, often leading to suboptimal recommendations.To address these issues, we propose the Mixture of REflectors (MoRE) framework, designed to model and learn dynamic user preferences in SeqRec.Specifically, MoRE introduces three reflectors for generating LLM-based reflections on explicit preferences, implicit preferences, and collaborative signals.Each reflector incorporates a self-improving strategy, termed refining-and-iteration, to evaluate and iteratively update reflections.Furthermore, a meta-reflector employs a contextual bandit algorithm to select the most suitable expert and corresponding reflections for each user's recommendation, effectively capturing dynamic preferences.Extensive experiments on three real-world datasets demonstrate that MoRE consistently outperforms state-of-the-art methods, requiring less training time and GPU memory compared to other LLM-based approaches in SeqRec.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06377v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06377v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Socially Responsible Data for Large Multilingual Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have rapidly increased in size and apparent capabilities in the last three years, but their training data is largely English text.There is growing interest in multilingual LLMs, and various efforts are striving for models to accommodate languages of communities outside of the Global North, which include many languages that have been historically underrepresented in digital realms.These languages have been coined as "low resource languages" or "long-tail languages", and LLMs performance on these languages is generally poor.While expanding the use of LLMs to more languages may bring many potential benefits, such as assisting cross-community communication and language preservation, great care must be taken to ensure that data collection on these languages is not extractive and that it does not reproduce exploitative practices of the past.Collecting data from languages spoken by previously colonized people, indigenous people, and non-Western languages raises many complex sociopolitical and ethical questions, e.g., around consent, cultural safety, and data sovereignty.Furthermore, linguistic complexity and cultural nuances are often lost in LLMs.This position paper builds on recent scholarship, and our own work, and outlines several relevant social, cultural, and ethical considerations and potential ways to mitigate them through qualitative research, community partnerships, and participatory design approaches.<span class='px-1 mx-1 bg-yellow-200'>We provide twelve recommendations for consideration when collecting language data on underrepresented language communities outside of the Global North. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05247v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05247v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-03</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation with Collaborative Information
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Sequential recommender systems are essential for discerning user preferences from historical interactions and facilitating targeted recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span>Recent innovations employing Large Language Models (LLMs) have advanced the field by encoding item semantics, yet they often necessitate substantial parameter tuning and are resource-demanding.<span class='px-1 mx-1 bg-yellow-200'>Moreover, these works fails to consider the diverse characteristics of different types of users and thus diminishes the recommendation accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose a parameter-efficient Large Language Model Bi-Tuning framework for sequential recommendation with collaborative information (Laser). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.79</span></span>Specifically, Bi-Tuning works by inserting trainable virtual tokens at both the prefix and suffix of the input sequence and freezing the LLM parameters, thus optimizing the LLM for the sequential recommendation.In our Laser, the prefix is utilized to incorporate user-item collaborative information and adapt the LLM to the recommendation task, while the suffix converts the output embeddings of the LLM from the language space to the recommendation space for the follow-up item recommendation.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, to capture the characteristics of different types of users when integrating the collaborative information via the prefix, we introduce M-Former, a lightweight MoE-based querying transformer that uses a set of query experts to integrate diverse user-specific collaborative information encoded by frozen ID-based sequential recommender systems, significantly improving the accuracy of recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on real-world datasets demonstrate that Laser can parameter-efficiently adapt LLMs to effective recommender systems, significantly outperforming state-of-the-art methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.01605v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.01605v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-27</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                X-Reflect: Cross-Reflection Prompting for Multimodal Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) and Large Multimodal Models (LMMs) have been shown to enhance the effectiveness of enriching item descriptions, thereby improving the accuracy of recommendation systems.However, most existing approaches either rely on text-only prompting or employ basic multimodal strategies that do not fully exploit the complementary information available from both textual and visual modalities.This paper introduces a novel framework, Cross-Reflection Prompting, termed X-Reflect, designed to address these limitations by prompting LMMs to explicitly identify and reconcile supportive and conflicting information between text and images.By capturing nuanced insights from both modalities, this approach generates more comprehensive and contextually richer item representations.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments conducted on two widely used benchmarks demonstrate that our method outperforms existing prompting baselines in downstream recommendation accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>Additionally, we evaluate the generalizability of our framework across different LMM backbones and the robustness of the prompting strategies, offering insights for optimization.This work underscores the importance of integrating multimodal information and presents a novel solution for improving item understanding in multimodal recommendation systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.15172v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.15172v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Say Your Reason: Extract Contextual Rules In Situ for Context-aware Service Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces SayRea, an interactive system that facilitates the extraction of contextual rules for personalized context-aware service recommendations in mobile scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>The system monitors a user's execution of registered services on their smartphones (via accessibility service) and proactively requests a single-sentence reason from the user.By utilizing a Large Language Model (LLM), SayRea parses the reason and predicts contextual relationships between the observed service and potential contexts (such as setting the alarm clock deep in the evening).In this way, SayRea can significantly reduce the cognitive load on users in anticipating future needs and selecting contextual attributes.A 10-day field study involving 20 participants showed that SayRea accumulated an average of 62.4 rules per user and successfully recommended 45% of service usage.The participants provided positive feedback on the system's usability, interpretability, and controllability.<span class='px-1 mx-1 bg-yellow-200'>The findings highlight SayRea's effectiveness in personalized service recommendations and its potential to enhance user experience in mobile scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.13977v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.13977v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-26</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Are LLM-based Recommenders Already the Best? Simple Scaled Cross-entropy Unleashes the Potential of Traditional Sequential Recommenders
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have been garnering increasing attention in the recommendation community. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.766</span></span><span class='px-1 mx-1 bg-yellow-200'>Some studies have observed that LLMs, when fine-tuned by the cross-entropy (CE) loss with a full softmax, could achieve `state-of-the-art' performance in sequential recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.799</span></span>However, most of the baselines used for comparison are trained using a pointwise/pairwise loss function.This inconsistent experimental setting leads to the underestimation of traditional methods and further fosters over-confidence in the ranking capability of LLMs.   In this study, we provide theoretical justification for the superiority of the cross-entropy loss by demonstrating its two desirable properties: tightness and coverage.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, this study sheds light on additional novel insights: 1) Taking into account only the recommendation performance, CE is not yet optimal as it is not a quite tight bound in terms of some ranking metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>2) In scenarios that full softmax cannot be performed, an effective alternative is to scale up the sampled normalizing term.<span class='px-1 mx-1 bg-yellow-200'>These findings then help unleash the potential of traditional recommendation models, allowing them to surpass LLM-based counterparts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the substantial computational burden, existing LLM-based methods are not as effective as claimed for sequential recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span><span class='px-1 mx-1 bg-yellow-200'>We hope that these theoretical understandings in conjunction with the empirical results will facilitate an objective evaluation of LLM-based recommendation in the future. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.14238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.14238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-22</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of Large Language Models (LLMs) into recommender systems has led to substantial performance improvements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.799</span></span>However, this often comes at the cost of diminished recommendation diversity, which can negatively impact user satisfaction.<span class='px-1 mx-1 bg-yellow-200'>To address this issue, controllable recommendation has emerged as a promising approach, allowing users to specify their preferences and receive recommendations that meet their diverse needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span><span class='px-1 mx-1 bg-yellow-200'>Despite its potential, existing controllable recommender systems frequently rely on simplistic mechanisms, such as a single prompt, to regulate diversity-an approach that falls short of capturing the full complexity of user preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span><span class='px-1 mx-1 bg-yellow-200'>In response to these limitations, we propose DLCRec, a novel framework designed to enable fine-grained control over diversity in LLM-based recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span><span class='px-1 mx-1 bg-yellow-200'>Unlike traditional methods, DLCRec adopts a fine-grained task decomposition strategy, breaking down the recommendation process into three sequential sub-tasks: genre prediction, genre filling, and item prediction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>These sub-tasks are trained independently and inferred sequentially according to user-defined control numbers, ensuring more precise control over diversity.Furthermore, the scarcity and uneven distribution of diversity-related user behavior data pose significant challenges for fine-tuning.To overcome these obstacles, we introduce two data augmentation techniques that enhance the model's robustness to noisy and out-of-distribution data.These techniques expose the model to a broader range of patterns, improving its adaptability in generating recommendations with varying levels of diversity.<span class='px-1 mx-1 bg-yellow-200'>Our extensive empirical evaluation demonstrates that DLCRec not only provides precise control over diversity but also outperforms state-of-the-art baselines across multiple recommendation scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.12470v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.12470v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and Personalization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the development of the modern social economy, tourism has become an important way to meet people's spiritual needs, bringing development opportunities to the tourism industry.<span class='px-1 mx-1 bg-yellow-200'>However, existing large language models (LLMs) face challenges in personalized recommendation capabilities and the generation of content that can sometimes produce hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span>This study proposes an optimization scheme for Tibet tourism LLMs based on retrieval-augmented generation (RAG) technology.By constructing a database of tourist viewpoints and processing the data using vectorization techniques, we have significantly improved retrieval accuracy.The application of RAG technology effectively addresses the hallucination problem in content generation.The optimized model shows significant improvements in fluency, accuracy, and relevance of content generation.This research demonstrates the potential of RAG technology in the standardization of cultural tourism information and data analysis, providing theoretical and technical support for the development of intelligent cultural tourism service systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.12003v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.12003v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Production workflows for LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) and automatic speech translation (AST). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.359</span></span>However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences.To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically.By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations.Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard speech encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.347</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11214v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11214v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards specific aspect terms in a sentence and allows us to uncover nuanced perspectives and attitudes on particular aspects of a product, service, or topic.<span class='px-1 mx-1 bg-yellow-200'>However, the scarcity of labeled data poses a significant challenge to training high-quality models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.353</span></span>To address this issue, we explore the potential of data augmentation using ChatGPT, a well-performing large language model (LLM), to enhance the sentiment classification performance towards aspect terms.Specifically, we explore three data augmentation strategies based on ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation techniques.Context-focused data augmentation focuses on changing the word expression of context words in the sentence while keeping aspect terms unchanged.In contrast, aspect-focused data augmentation aims to change aspect terms but keep context words unchanged.Context-Aspect data augmentation integrates the above two data augmentations to generate augmented samples.Furthermore, we incorporate contrastive learning into the ABSA tasks to improve performance.Extensive experiments show that all three data augmentation techniques lead to performance improvements, with the context-aspect data augmentation strategy performing best and surpassing the performance of the baseline models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11218v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11218v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this manuscript I present an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the number of variables of the satisfiable problem.I show that the model can call an external SAT solver to solve the instances, rather than solving them directly.<span class='px-1 mx-1 bg-yellow-200'>Despite using external solvers, the model reports incorrect assignments as output. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.378</span></span>Moreover, I propose and present an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11232v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11232v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) offer powerful capabilities but incur substantial computational costs, driving the need for efficient compression techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span><span class='px-1 mx-1 bg-yellow-200'>This study evaluates the impact of popular compression methods - Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on the trade-offs between model size reduction, downstream task performance, and the role of calibration data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that while SparseGPT and Wanda preserve perplexity even at 50% sparsity, they suffer significant degradation on downstream tasks, highlighting the inadequacy of perplexity as the sole evaluation metric. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.349</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, we introduce Jensen-Shannon (JS) Divergence as a more comprehensive metric that captures nuanced changes in model behavior post-compression. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.483</span></span><span class='px-1 mx-1 bg-yellow-200'>We further demonstrate that task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.398</span></span><span class='px-1 mx-1 bg-yellow-200'>This research underscores the necessity for diverse evaluation metrics and careful calibration data selection to fully understand the complexities of LLM compression and its implications for practical applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.589</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11233v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11233v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-as-a-Judge & Reward Model: What They Can and Cannot Do
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.435</span></span>Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning.However, despite their popularity, their effectiveness outside of English remains largely unexplored.In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment.First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages.Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language.Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLMs are an integral part of retrieval-augmented generation (RAG) systems.While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task.<span class='px-1 mx-1 bg-yellow-200'>Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.557</span></span>We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task.<span class='px-1 mx-1 bg-yellow-200'>Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.407</span></span><span class='px-1 mx-1 bg-yellow-200'>LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.344</span></span>We release our code at: https://github.com/declare-lab/trust-align.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11242v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11242v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative.Traditional security defenses are becoming less effective against the advanced tactics of today's cyberattacks.Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis.DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network.This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples.<span class='px-1 mx-1 bg-yellow-200'>Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.369</span></span>The embeddings are then used alongside few labeled samples of an unseen malware type.<span class='px-1 mx-1 bg-yellow-200'>This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.364</span></span>Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data.Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments.Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11254v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11254v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Task Arithmetic for Language Expansion in Speech Translation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in large language models (LLMs) have gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-based speech translation (ST). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.328</span></span><span class='px-1 mx-1 bg-yellow-200'>However, expanding language pairs from an existing instruction-tuned ST system is costly due to the necessity of re-training on a combination of new and previous datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose to expand new language pairs by merging the model trained on new language pairs and the existing model, using task arithmetic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.495</span></span><span class='px-1 mx-1 bg-yellow-200'>We find that the direct application of task arithmetic for ST causes the merged model to fail to follow instructions; thus, generating translation in incorrect languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.391</span></span>To eliminate language confusion, we propose an augmented task arithmetic method that merges an additional language control model.<span class='px-1 mx-1 bg-yellow-200'>It is trained to generate the correct target language token following the instructions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span>Our experiments demonstrate that our proposed language control model can achieve language expansion by eliminating language confusion.In our MuST-C and CoVoST-2 experiments, it shows up to 4.66 and 4.92 BLEU scores improvement, respectively.<span class='px-1 mx-1 bg-yellow-200'>In addition, we demonstrate the use of our task arithmetic framework can expand to a language pair where neither paired ST training data nor a pre-trained ST model is available. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span><span class='px-1 mx-1 bg-yellow-200'>We first synthesize the ST system from machine translation (MT) systems via task analogy, then merge the synthesized ST system to the existing ST model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.325</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11274v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11274v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity.<span class='px-1 mx-1 bg-yellow-200'>Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.396</span></span><span class='px-1 mx-1 bg-yellow-200'>Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.494</span></span>Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios.<span class='px-1 mx-1 bg-yellow-200'>To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.38</span></span>Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11276v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11276v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations.Traditional learning-based approaches face two challenges.Firstly, natural language instructions often lack explicit task planning.Secondly, extensive training is required to equip models with knowledge of the task environment.Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples.To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth.<span class='px-1 mx-1 bg-yellow-200'>Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.369</span></span>In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction.Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.427</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11279v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11279v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The surge of digital documents in various formats, including less standardized documents such as business reports and environmental assessments, underscores the growing importance of Document Understanding.While Large Language Models (LLMs) have showcased prowess across diverse natural language processing tasks, their direct application to Document Understanding remains a challenge.<span class='px-1 mx-1 bg-yellow-200'>Previous research has demonstrated the utility of LLMs in this domain, yet their significant computational demands make them challenging to deploy effectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Additionally, proprietary Blackbox LLMs often outperform their open-source counterparts, posing a barrier to widespread accessibility.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we delve into the realm of document understanding, leveraging distillation methods to harness the power of large LLMs while accommodating computational limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.327</span></span>Specifically, we present a novel approach wherein we distill document understanding knowledge from the proprietary LLM ChatGPT into FLAN-T5.Our methodology integrates labeling and curriculum-learning mechanisms to facilitate efficient knowledge transfer.This work contributes to the advancement of document understanding methodologies by offering a scalable solution that bridges the gap between resource-intensive LLMs and practical applications.Our findings underscore the potential of distillation techniques in facilitating the deployment of sophisticated language models in real-world scenarios, thereby fostering advancements in natural language processing and document comprehension domains.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11282v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11282v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.386</span></span>This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap.THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies.<span class='px-1 mx-1 bg-yellow-200'>It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.5</span></span>THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT).<span class='px-1 mx-1 bg-yellow-200'>Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.436</span></span>Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11353v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11353v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values.This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background.We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions.Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians.Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is writtenbut also how it is written.These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11360v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11360v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The deployment of multimodal large language models (MLLMs) has demonstrated remarkable success in engaging in conversations involving visual inputs, thanks to the superior power of large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Those MLLMs are typically built based on the LLMs, with an image encoder to process images into the token embedding space of the LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.424</span></span>However, the integration of visual modality has introduced a unique vulnerability: the MLLM becomes susceptible to malicious visual inputs and prone to generating sensitive or harmful responses, even though the LLM has been trained on textual dataset to align with human value.In this paper, we first raise the question: ``Do the MLLMs possess safety-awareness against malicious image inputs?".<span class='px-1 mx-1 bg-yellow-200'>We find that after adding a principle that specifies the safety requirement into the input of the MLLM, the model's safety awareness becomes boosted. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.306</span></span>This phenomenon verifies the existence of MLLM's safety-awareness against image inputs, it is only weakened by the modality gap.We then introduce a simple yet effective technique termed CoCA, which amplifies the safety-awareness of the MLLM by calibrating its output distribution.<span class='px-1 mx-1 bg-yellow-200'>Our proposed strategy helps the model reclaim its original safety awareness without losing its original capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.339</span></span>We verify the effectiveness of our approach on both multimodal safety and understanding benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11365v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11365v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the medical domain, acquiring large datasets poses significant challenges due to privacy concerns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>Nonetheless, the development of a robust deep-learning model for retinal disease diagnosis necessitates a substantial dataset for training.<span class='px-1 mx-1 bg-yellow-200'>The capacity to generalize effectively on smaller datasets remains a persistent challenge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.384</span></span><span class='px-1 mx-1 bg-yellow-200'>The scarcity of data presents a significant barrier to the practical implementation of scalable medical AI solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.321</span></span>To address this issue, we've combined a wide range of data sources to improve performance and generalization to new data by giving it a deeper understanding of the data representation from multi-modal datasets and developed a self-supervised framework based on large language models (LLMs), SwinV2 to gain a deeper understanding of multi-modal dataset representations, enhancing the model's ability to extrapolate to new data for the detection of eye diseases using optical coherence tomography (OCT) images.<span class='px-1 mx-1 bg-yellow-200'>We adopt a two-phase training methodology, self-supervised pre-training, and fine-tuning on a downstream supervised classifier. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.442</span></span>An ablation study conducted across three datasets employing various encoder backbones, without data fusion, with low data availability setting, and without self-supervised pre-training scenarios, highlights the robustness of our method.Our findings demonstrate consistent performance across these diverse conditions, showcasing superior generalization capabilities compared to the baseline model, ResNet-50.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11375v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11375v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Time Series Reasoning with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multi-modal large language models (MLLMs) have enabled numerous advances in understanding and reasoning in domains like vision, but we have not yet seen this broad success for time-series.Although prior works on time-series MLLMs have shown promising performance in time-series forecasting, very few works show how an LLM could be used for time-series reasoning in natural language.We propose a novel multi-modal time-series LLM approach that learns generalizable information across various domains with powerful zero-shot performance.<span class='px-1 mx-1 bg-yellow-200'>First, we train a lightweight time-series encoder on top of an LLM to directly extract time-series information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.44</span></span>Then, we fine-tune our model with chain-of-thought augmented time-series tasks to encourage the model to generate reasoning paths.We show that our model learns a latent representation that reflects specific time-series features (e.g. slope, frequency), as well as outperforming GPT-4o on a set of zero-shot reasoning tasks on a variety of domains.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11376v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11376v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Finetuning large language models on instruction data is crucial for enhancing pre-trained knowledge and improving instruction-following capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.461</span></span><span class='px-1 mx-1 bg-yellow-200'>As instruction datasets proliferate, selecting optimal data for effective training becomes increasingly important. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.397</span></span>This work addresses the question: How can we determine the optimal subset of data for effective training?While existing research often emphasizes local criteria like instance quality for subset selection, we argue that a global approach focused on data diversity is more critical.Our method employs k-means clustering to ensure the selected subset effectively represents the full dataset.<span class='px-1 mx-1 bg-yellow-200'>We propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, reassessing each cluster's importance and sampling weight in every training iteration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.47</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach reduces the effect of outliers and automatically filters out clusters containing low-quality data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.482</span></span>Through extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over random selection and a 3.8% improvement over state-of-the-art sampling methods.Our work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks.Our code is available at https://github.com/for-ai/iterative-data-selection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11378v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11378v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.398</span></span>However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity.Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent.This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.   Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works.Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types.Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects.Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems.This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11393v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11393v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NVLM: Open Frontier-Class Multimodal LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2).Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training.In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo).Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities.Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks.Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets.<span class='px-1 mx-1 bg-yellow-200'>Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.417</span></span>Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones.To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities.<span class='px-1 mx-1 bg-yellow-200'>To advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11402v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11402v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLM Model Architectures and Training Techniques</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Obfuscation for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>System prompts that include detailed instructions to describe the task performed by the underlying large language model (LLM) can easily transform foundation models into tools and services with minimal overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.493</span></span>Because of their crucial impact on the utility, they are often considered intellectual property, similar to the code of a software product.<span class='px-1 mx-1 bg-yellow-200'>However, extracting system prompts is easily possible by using prompt injection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.482</span></span><span class='px-1 mx-1 bg-yellow-200'>As of today, there is no effective countermeasure to prevent the stealing of system prompts and all safeguarding efforts could be evaded with carefully crafted prompt injections that bypass all protection mechanisms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span>In this work, we propose an alternative to conventional system prompts.<span class='px-1 mx-1 bg-yellow-200'>We introduce prompt obfuscation to prevent the extraction of the system prompt while maintaining the utility of the system itself with only little overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.473</span></span>The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt.<span class='px-1 mx-1 bg-yellow-200'>We implement an optimization-based method to find an obfuscated prompt representation while maintaining the functionality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.499</span></span><span class='px-1 mx-1 bg-yellow-200'>To evaluate our approach, we investigate eight different metrics to compare the performance of a system using the original and the obfuscated system prompts, and we show that the obfuscated version is constantly on par with the original one. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.498</span></span><span class='px-1 mx-1 bg-yellow-200'>We further perform three different deobfuscation attacks and show that with access to the obfuscated prompt and the LLM itself, we are not able to consistently extract meaningful information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.412</span></span>Overall, we showed that prompt obfuscation can be an effective method to protect intellectual property while maintaining the same utility as the original system prompt.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11026v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11026v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation.As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios.In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language.We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code.<span class='px-1 mx-1 bg-yellow-200'>With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.451</span></span><span class='px-1 mx-1 bg-yellow-200'>Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.455</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11041v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11041v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets.Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined.<span class='px-1 mx-1 bg-yellow-200'>This paper evaluates the performance of instruction-tuned LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q\&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span><span class='px-1 mx-1 bg-yellow-200'>Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.56</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11055v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11055v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the realm of task-oriented dialogue systems, a robust intent detection mechanism must effectively handle malformed utterances encountered in real-world scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.419</span></span><span class='px-1 mx-1 bg-yellow-200'>This study presents a novel fine-tuning framework for large language models (LLMs) aimed at enhancing in-distribution (ID) intent classification and out-of-distribution (OOD) intent detection, which utilizes semantic matching with prototypes derived from ID class names. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.491</span></span>By harnessing the highly distinguishable representations of LLMs, we construct semantic prototypes for each ID class using a diversity-grounded prompt tuning approach.We rigorously test our framework in a challenging OOD context, where ID and OOD classes are semantically close yet distinct, referred to as \emph{near} OOD detection.<span class='px-1 mx-1 bg-yellow-200'>For a thorough assessment, we benchmark our method against the prevalent fine-tuning approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>The experimental findings reveal that our method demonstrates superior performance in both few-shot ID intent classification and near-OOD intent detection tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11114v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11114v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models(LLMs) have exhibited remarkable few-shot learning capabilities and unified the paradigm of NLP tasks through the in-context learning(ICL) technique.Despite the success of ICL, the quality of the exemplar demonstrations can significantly influence the LLM's performance.Existing exemplar selection methods mainly focus on the semantic similarity between queries and candidate exemplars.On the other hand, the logical connections between reasoning steps can be beneficial to depict the problem-solving process as well.In this paper, we proposes a novel method named Reasoning Graph-enhanced Exemplar Retrieval(RGER).RGER first quires LLM to generate an initial response, then expresses intermediate problem-solving steps to a graph structure.After that, it employs graph kernel to select exemplars with semantic and structural similarity.Extensive experiments demonstrate the structural relationship is helpful to the alignment of queries and candidate exemplars.The efficacy of RGER on math and logit reasoning tasks showcases its superiority over state-of-the-art retrieval-based approaches.<span class='px-1 mx-1 bg-yellow-200'>Our code is released at https://github.com/Yukang-Lin/RGER. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.482</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present SuperCoder2.0, an advanced autonomous system designed to enhance software development through artificial intelligence.The system combines an AI-native development approach with intelligent agents to enable fully autonomous coding.<span class='px-1 mx-1 bg-yellow-200'>Key focus areas include a retry mechanism with error output traceback, comprehensive code rewriting and replacement using Abstract Syntax Tree (ast) parsing to minimize linting issues, code embedding technique for retrieval-augmented generation, and a focus on localizing methods for problem-solving rather than identifying specific line numbers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.546</span></span><span class='px-1 mx-1 bg-yellow-200'>The methodology employs a three-step hierarchical search space reduction approach for code base navigation and bug localization:utilizing Retrieval Augmented Generation (RAG) and a Repository File Level Map to identify candidate files, (2) narrowing down to the most relevant files using a File Level Schematic Map, and (3) extracting 'relevant locations' within these files. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.477</span></span>Code editing is performed through a two-part module comprising CodeGeneration and CodeEditing, which generates multiple solutions at different temperature values and replaces entire methods or classes to maintain code integrity.<span class='px-1 mx-1 bg-yellow-200'>A feedback loop executes repository-level test cases to validate and refine solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.458</span></span>Experiments conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's effectiveness, achieving correct file localization in 84.33% of cases within the top 5 candidates and successfully resolving 34% of test instances.<span class='px-1 mx-1 bg-yellow-200'>This performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.443</span></span>The system's ability to handle diverse repositories and problem types highlights its potential as a versatile tool for autonomous software development.Future work will focus on refining the code editing process and exploring advanced embedding models for improved natural language to code mapping.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11190v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11190v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Iterative preference optimization has recently become one of the de-facto training paradigms for large language models (LLMs), but the performance is still underwhelming due to too much noisy preference data yielded in the loop. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.481</span></span><span class='px-1 mx-1 bg-yellow-200'>To combat this issue, we present an \textbf{U}ncertainty-enhanced \textbf{P}reference \textbf{O}ptimization (UPO) framework to make the LLM self-evolve with reliable feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>The key idea is mitigating the noisy preference data derived from the current policy and reward models by performing pair-wise uncertainty estimation and judiciously reliable feedback sampling.To reach this goal, we thus introduce an estimator model, which incorporates Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the preference data derived from the LLM policy.Compared to the existing methods that directly filter generated responses based on the reward score, the estimator focuses on the model uncertainty in a pair-wise manner and effectively bypasses the confirmation bias problem of the reward model.Additionally, we also propose an uncertainty-enhanced self-evolution algorithm to improve the robustness of preference optimization and encourage the LLM to generate responses with both high reward and certainty.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments over multiple benchmarks demonstrate that our framework substantially alleviates the noisy problem and improves the performance of iterative preference optimization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual Speech-to-Text
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing speech-to-text tasks, including automatic speech recognition (ASR) and automatic speech translation (AST).<span class='px-1 mx-1 bg-yellow-200'>However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.494</span></span>To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically.By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations.<span class='px-1 mx-1 bg-yellow-200'>Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.522</span></span><span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard speech encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.434</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11214v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11214v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards specific aspect terms in a sentence and allows us to uncover nuanced perspectives and attitudes on particular aspects of a product, service, or topic.However, the scarcity of labeled data poses a significant challenge to training high-quality models.To address this issue, we explore the potential of data augmentation using ChatGPT, a well-performing large language model (LLM), to enhance the sentiment classification performance towards aspect terms.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we explore three data augmentation strategies based on ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.421</span></span><span class='px-1 mx-1 bg-yellow-200'>Context-focused data augmentation focuses on changing the word expression of context words in the sentence while keeping aspect terms unchanged. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.415</span></span>In contrast, aspect-focused data augmentation aims to change aspect terms but keep context words unchanged.Context-Aspect data augmentation integrates the above two data augmentations to generate augmented samples.Furthermore, we incorporate contrastive learning into the ABSA tasks to improve performance.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that all three data augmentation techniques lead to performance improvements, with the context-aspect data augmentation strategy performing best and surpassing the performance of the baseline models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.442</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11218v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11218v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this manuscript I present an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the number of variables of the satisfiable problem.I show that the model can call an external SAT solver to solve the instances, rather than solving them directly.<span class='px-1 mx-1 bg-yellow-200'>Despite using external solvers, the model reports incorrect assignments as output. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.42</span></span>Moreover, I propose and present an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11232v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11232v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) offer powerful capabilities but incur substantial computational costs, driving the need for efficient compression techniques.This study evaluates the impact of popular compression methods - Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on the trade-offs between model size reduction, downstream task performance, and the role of calibration data.<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that while SparseGPT and Wanda preserve perplexity even at 50% sparsity, they suffer significant degradation on downstream tasks, highlighting the inadequacy of perplexity as the sole evaluation metric. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.451</span></span>To address this, we introduce Jensen-Shannon (JS) Divergence as a more comprehensive metric that captures nuanced changes in model behavior post-compression.We further demonstrate that task-specific calibration data significantly enhances the downstream performance of compressed models compared to general calibration data.This research underscores the necessity for diverse evaluation metrics and careful calibration data selection to fully understand the complexities of LLM compression and its implications for practical applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11233v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11233v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-as-a-Judge & Reward Model: What They Can and Cannot Do
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLM-as-a-Judge and reward models are widely used alternatives of multiple-choice questions or human annotators for large language model (LLM) evaluation.Their efficacy shines in evaluating long-form responses, serving a critical role as evaluators of leaderboards and as proxies to align LLMs via reinforcement learning.However, despite their popularity, their effectiveness outside of English remains largely unexplored.In this paper, we conduct a comprehensive analysis on automated evaluators, reporting key findings on their behavior in a non-English environment.First, we discover that English evaluation capabilities significantly influence language-specific capabilities, often more than the language proficiency itself, enabling evaluators trained in English to easily transfer their skills to other languages.<span class='px-1 mx-1 bg-yellow-200'>Second, we identify critical shortcomings, where LLMs fail to detect and penalize errors, such as factual inaccuracies, cultural misrepresentations, and the presence of unwanted language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.41</span></span>Finally, we release Kudge, the first non-English meta-evaluation dataset containing 5,012 human annotations in Korean.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLMs are an integral part of retrieval-augmented generation (RAG) systems.While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task.Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework.We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task.<span class='px-1 mx-1 bg-yellow-200'>Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.438</span></span>LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9).<span class='px-1 mx-1 bg-yellow-200'>We release our code at: https://github.com/declare-lab/trust-align. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.43</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11242v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11242v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.442</span></span><span class='px-1 mx-1 bg-yellow-200'>Traditional security defenses are becoming less effective against the advanced tactics of today's cyberattacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.457</span></span><span class='px-1 mx-1 bg-yellow-200'>Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.485</span></span>DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network.This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples.Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets.The embeddings are then used alongside few labeled samples of an unseen malware type.This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes.Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data.Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments.<span class='px-1 mx-1 bg-yellow-200'>Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.433</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11254v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11254v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Task Arithmetic for Language Expansion in Speech Translation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in large language models (LLMs) have gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-based speech translation (ST).<span class='px-1 mx-1 bg-yellow-200'>However, expanding language pairs from an existing instruction-tuned ST system is costly due to the necessity of re-training on a combination of new and previous datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.523</span></span>We propose to expand new language pairs by merging the model trained on new language pairs and the existing model, using task arithmetic.<span class='px-1 mx-1 bg-yellow-200'>We find that the direct application of task arithmetic for ST causes the merged model to fail to follow instructions; thus, generating translation in incorrect languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.496</span></span><span class='px-1 mx-1 bg-yellow-200'>To eliminate language confusion, we propose an augmented task arithmetic method that merges an additional language control model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.434</span></span>It is trained to generate the correct target language token following the instructions.Our experiments demonstrate that our proposed language control model can achieve language expansion by eliminating language confusion.In our MuST-C and CoVoST-2 experiments, it shows up to 4.66 and 4.92 BLEU scores improvement, respectively.<span class='px-1 mx-1 bg-yellow-200'>In addition, we demonstrate the use of our task arithmetic framework can expand to a language pair where neither paired ST training data nor a pre-trained ST model is available. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.424</span></span>We first synthesize the ST system from machine translation (MT) systems via task analogy, then merge the synthesized ST system to the existing ST model.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11274v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11274v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity.Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span><span class='px-1 mx-1 bg-yellow-200'>Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.513</span></span>Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios.To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities.Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11276v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11276v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The surge of digital documents in various formats, including less standardized documents such as business reports and environmental assessments, underscores the growing importance of Document Understanding.While Large Language Models (LLMs) have showcased prowess across diverse natural language processing tasks, their direct application to Document Understanding remains a challenge.<span class='px-1 mx-1 bg-yellow-200'>Previous research has demonstrated the utility of LLMs in this domain, yet their significant computational demands make them challenging to deploy effectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span><span class='px-1 mx-1 bg-yellow-200'>Additionally, proprietary Blackbox LLMs often outperform their open-source counterparts, posing a barrier to widespread accessibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.495</span></span>In this paper, we delve into the realm of document understanding, leveraging distillation methods to harness the power of large LLMs while accommodating computational limitations.Specifically, we present a novel approach wherein we distill document understanding knowledge from the proprietary LLM ChatGPT into FLAN-T5.Our methodology integrates labeling and curriculum-learning mechanisms to facilitate efficient knowledge transfer.This work contributes to the advancement of document understanding methodologies by offering a scalable solution that bridges the gap between resource-intensive LLMs and practical applications.Our findings underscore the potential of distillation techniques in facilitating the deployment of sophisticated language models in real-world scenarios, thereby fostering advancements in natural language processing and document comprehension domains.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11282v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11282v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.459</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.542</span></span>THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies.<span class='px-1 mx-1 bg-yellow-200'>It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.416</span></span>THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT).Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL.Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11353v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11353v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the medical domain, acquiring large datasets poses significant challenges due to privacy concerns.Nonetheless, the development of a robust deep-learning model for retinal disease diagnosis necessitates a substantial dataset for training.The capacity to generalize effectively on smaller datasets remains a persistent challenge.The scarcity of data presents a significant barrier to the practical implementation of scalable medical AI solutions.To address this issue, we've combined a wide range of data sources to improve performance and generalization to new data by giving it a deeper understanding of the data representation from multi-modal datasets and developed a self-supervised framework based on large language models (LLMs), SwinV2 to gain a deeper understanding of multi-modal dataset representations, enhancing the model's ability to extrapolate to new data for the detection of eye diseases using optical coherence tomography (OCT) images.<span class='px-1 mx-1 bg-yellow-200'>We adopt a two-phase training methodology, self-supervised pre-training, and fine-tuning on a downstream supervised classifier. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.425</span></span>An ablation study conducted across three datasets employing various encoder backbones, without data fusion, with low data availability setting, and without self-supervised pre-training scenarios, highlights the robustness of our method.Our findings demonstrate consistent performance across these diverse conditions, showcasing superior generalization capabilities compared to the baseline model, ResNet-50.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11375v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11375v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Finetuning large language models on instruction data is crucial for enhancing pre-trained knowledge and improving instruction-following capabilities.<span class='px-1 mx-1 bg-yellow-200'>As instruction datasets proliferate, selecting optimal data for effective training becomes increasingly important. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.45</span></span>This work addresses the question: How can we determine the optimal subset of data for effective training?While existing research often emphasizes local criteria like instance quality for subset selection, we argue that a global approach focused on data diversity is more critical.Our method employs k-means clustering to ensure the selected subset effectively represents the full dataset.<span class='px-1 mx-1 bg-yellow-200'>We propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, reassessing each cluster's importance and sampling weight in every training iteration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.414</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach reduces the effect of outliers and automatically filters out clusters containing low-quality data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.466</span></span>Through extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over random selection and a 3.8% improvement over state-of-the-art sampling methods.Our work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks.Our code is available at https://github.com/for-ai/iterative-data-selection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11378v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11378v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NVLM: Open Frontier-Class Multimodal LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2).Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training.In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo).Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities.Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks.Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets.<span class='px-1 mx-1 bg-yellow-200'>Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.468</span></span>Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones.To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities.To advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11402v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11402v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations.We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing.We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation.<span class='px-1 mx-1 bg-yellow-200'>We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.402</span></span>Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation.Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation.This work contributes ~45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts.We will release the dialectal translation models and benchmarks curated in this study.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11404v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11404v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Programming applications of LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs.<span class='px-1 mx-1 bg-yellow-200'>To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span>As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios.In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language.We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code.With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples.Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11041v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11041v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We present SuperCoder2.0, an advanced autonomous system designed to enhance software development through artificial intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>The system combines an AI-native development approach with intelligent agents to enable fully autonomous coding.Key focus areas include a retry mechanism with error output traceback, comprehensive code rewriting and replacement using Abstract Syntax Tree (ast) parsing to minimize linting issues, code embedding technique for retrieval-augmented generation, and a focus on localizing methods for problem-solving rather than identifying specific line numbers.<span class='px-1 mx-1 bg-yellow-200'>The methodology employs a three-step hierarchical search space reduction approach for code base navigation and bug localization:utilizing Retrieval Augmented Generation (RAG) and a Repository File Level Map to identify candidate files, (2) narrowing down to the most relevant files using a File Level Schematic Map, and (3) extracting 'relevant locations' within these files. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span><span class='px-1 mx-1 bg-yellow-200'>Code editing is performed through a two-part module comprising CodeGeneration and CodeEditing, which generates multiple solutions at different temperature values and replaces entire methods or classes to maintain code integrity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>A feedback loop executes repository-level test cases to validate and refine solutions.Experiments conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's effectiveness, achieving correct file localization in 84.33% of cases within the top 5 candidates and successfully resolving 34% of test instances.This performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.The system's ability to handle diverse repositories and problem types highlights its potential as a versatile tool for autonomous software development.Future work will focus on refining the code editing process and exploring advanced embedding models for improved natural language to code mapping.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11190v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11190v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values.This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background.We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions.Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians.Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is writtenbut also how it is written.These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.11360v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.11360v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Messy Code Makes Managing ML Pipelines Difficult? Just Let LLMs Rewrite the Code!
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Machine learning (ML) applications that learn from data are increasingly used to automate impactful decisions.Unfortunately, these applications often fall short of adequately managing critical data and complying with upcoming regulations.A technical reason for the persistence of these issues is that the data pipelines in common ML libraries and cloud services lack fundamental declarative, data-centric abstractions.Recent research has shown how such abstractions enable techniques like provenance tracking and automatic inspection to help manage ML pipelines.Unfortunately, these approaches lack adoption in the real world because they require clean ML pipeline code written with declarative APIs, instead of the messy imperative Python code that data scientists typically write for data preparation.   We argue that it is unrealistic to expect data scientists to change their established development practices.<span class='px-1 mx-1 bg-yellow-200'>Instead, we propose to circumvent this "code abstraction gap" by leveraging the code generation capabilities of large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.813</span></span>Our idea is to rewrite messy data science code to a custom-tailored declarative pipeline abstraction, which we implement as a proof-of-concept in our prototype Lester.We detail its application for a challenging compliance management example involving "incremental view maintenance" of deployed ML pipelines.<span class='px-1 mx-1 bg-yellow-200'>The code rewrites for our running example show the potential of LLMs to make messy data science code declarative, e.g., by identifying hand-coded joins in Python and turning them into joins on dataframes, or by generating declarative feature encoders from NumPy code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10081v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10081v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In recent years, the application of large language models (LLMs) to code-related tasks has gained significant attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.901</span></span>However, existing evaluation benchmarks often focus on limited scenarios, such as code generation or completion, which do not reflect the diverse challenges developers face in real-world contexts.<span class='px-1 mx-1 bg-yellow-200'>To address this, we introduce ComplexCodeEval, a benchmark designed to assess LCMs in various development tasks, including code generation, completion, API recommendation, and test case generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span><span class='px-1 mx-1 bg-yellow-200'>It includes 3,897 Java samples and 7,184 Python samples from high-star GitHub repositories, each annotated with function signatures, docstrings, and API references to simulate real development environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>Our experiments across ten LCMs reveal that context improves performance and that data leakage can lead to overestimation, highlighting the need for more accurate evaluations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10280v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10280v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Code Vulnerability Detection: A Comparative Analysis of Emerging Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The growing trend of vulnerability issues in software development as a result of a large dependence on open-source projects has received considerable attention recently.<span class='px-1 mx-1 bg-yellow-200'>This paper investigates the effectiveness of Large Language Models (LLMs) in identifying vulnerabilities within codebases, with a focus on the latest advancements in LLM technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span><span class='px-1 mx-1 bg-yellow-200'>Through a comparative analysis, we assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma, and CodeGemma, alongside established state-of-the-art models such as BERT, RoBERTa, and GPT-3. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.794</span></span>Our study aims to shed light on the capabilities of LLMs in vulnerability detection, contributing to the enhancement of software security practices across diverse open-source repositories.<span class='px-1 mx-1 bg-yellow-200'>We observe that CodeGemma achieves the highest F1-score of 58\ and a Recall of 87\, amongst the recent additions of large language models to detect software security vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10490v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10490v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language.<span class='px-1 mx-1 bg-yellow-200'>While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.778</span></span>Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts.Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested.In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10506v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10506v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in automatic code generation using large language models (LLMs) have brought us closer to fully automated secure software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.869</span></span>However, existing approaches often rely on a single agent for code generation, which struggles to produce secure, vulnerability-free code.Traditional program synthesis with LLMs has primarily focused on functional correctness, often neglecting critical dynamic security implications that happen during runtime.To address these challenges, we propose AutoSafeCoder, a multi-agent framework that leverages LLM-driven agents for code generation, vulnerability analysis, and security enhancement through continuous collaboration.The framework consists of three agents: a Coding Agent responsible for code generation, a Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent performing dynamic testing using a mutation-based fuzzing approach to detect runtime errors.Our contribution focuses on ensuring the safety of multi-agent code generation by integrating dynamic and static testing in an iterative process during code generation by LLM that improves security.Experiments using the SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities compared to baseline LLMs, with no compromise in functionality.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10737v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10737v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown promise in tasks like code translation, prompting interest in their potential for automating software vulnerability detection (SVD) and patching (SVP). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.703</span></span>To further research in this area, establishing a benchmark is essential for evaluating the strengths and limitations of LLMs in these tasks.Despite their capabilities, questions remain regarding whether LLMs can accurately analyze complex vulnerabilities and generate appropriate patches.This paper introduces VulnLLMEval, a framework designed to assess the performance of LLMs in identifying and patching vulnerabilities in C code.Our study includes 307 real-world vulnerabilities extracted from the Linux kernel, creating a well-curated dataset that includes both vulnerable and patched code.This dataset, based on real-world code, provides a diverse and representative testbed for evaluating LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous assessment.Our results reveal that LLMs often struggle with distinguishing between vulnerable and patched code.Furthermore, in SVP tasks, these models tend to oversimplify the code, producing solutions that may not be directly usable without further refinement.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.10756v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.10756v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>LLM agents enhanced by tree search algorithms have yielded notable performances in code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.898</span></span>However, current search algorithms in this domain suffer from low search quality due to several reasons: 1) Ineffective design of the search space for the high-reasoning demands of code generation tasks, 2) Inadequate integration of code feedback with the search algorithm, and 3) Poor handling of negative feedback during the search, leading to reduced search efficiency and quality.To address these challenges, we propose to search for the reasoning process of the code and use the detailed feedback of code execution to refine erroneous thoughts during the search.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS) algorithm to conduct thought-level searches before generating code, thereby exploring a wider range of strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>More importantly, we construct verbal feedback from fine-grained code execution feedback to refine erroneous thoughts during the search.This ensures that the search progresses along the correct reasoning paths, thus improving the overall search quality of the tree by leveraging execution feedback.<span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-based and feedback-based code generation baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.786</span></span>On the HumanEval dataset, it improves the pass@1 of GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51.It effectively conducts more thorough exploration through thought-level searches and enhances the search quality of the entire tree by incorporating rethink operation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.09584v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.09584v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring LLMs for Malware Detection: Review, Framework Design, and Countermeasure Approaches
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rising use of Large Language Models (LLMs) to create and disseminate malware poses a significant cybersecurity challenge due to their ability to generate and distribute attacks with ease. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>A single prompt can initiate a wide array of malicious activities.This paper addresses this critical issue through a multifaceted approach.First, we provide a comprehensive overview of LLMs and their role in malware detection from diverse sources.We examine five specific applications of LLMs: Malware honeypots, identification of text-based threats, code analysis for detecting malicious intent, trend analysis of malware, and detection of non-standard disguised malware.Our review includes a detailed analysis of the existing literature and establishes guiding principles for the secure use of LLMs.We also introduce a classification scheme to categorize the relevant literature.Second, we propose performance metrics to assess the effectiveness of LLMs in these contexts.Third, we present a risk mitigation framework designed to prevent malware by leveraging LLMs.Finally, we evaluate the performance of our proposed risk mitigation strategies against various factors and demonstrate their effectiveness in countering LLM-enabled malware.The paper concludes by suggesting future advancements and areas requiring deeper exploration in this fascinating field of artificial intelligence.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07587v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07587v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance.We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code.Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost.SGCode has only a marginal cost compared with prompting LLMs.SGCode is available at: http://3.131.141.63:8501/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07368v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07368v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown great promise in vulnerability identification.As C/C++ comprises half of the Open-Source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing Commits (VCCs) is essential.<span class='px-1 mx-1 bg-yellow-200'>However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span>In this paper, we enhance the ability of BERT-based LLMs to identify C/C++ VCCs in a lightweight manner.<span class='px-1 mx-1 bg-yellow-200'>We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++ programs and LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span>Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details.Specifically, CLNX first applies structure-level naturalization to decompose complex programs, followed by token-level naturalization to interpret complex symbols.We evaluate CLNX on public datasets of 25,872 C/C++ functions with their commits.The results show that CLNX significantly enhances the performance of LLMs on identifying C/C++ VCCs.Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art and identifies 38 OSS vulnerabilities in the real world.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07407v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07407v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.799</span></span>Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work.To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories.SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories.Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development.We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise.We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios.This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.07440v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.07440v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What is the Role of Small Models in the LLM Era: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources.At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated.This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research.In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition.We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources.The code is available at https://github.com/tigerchen52/role_of_small_models</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06857v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06857v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Unlock Novel Scientific Research Ideas?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>"An idea is nothing more nor less than a new combination of old elements" (Young, J.W.).The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives.This study explores the capability of LLMs in generating novel research ideas based on information from research papers.We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics).We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0.We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas.<span class='px-1 mx-1 bg-yellow-200'>This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas.We make our datasets and codes publicly available.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06185v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06185v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown great potential for automatic code generation and form the basis for various tools such as GitHub Copilot. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.957</span></span><span class='px-1 mx-1 bg-yellow-200'>However, recent studies highlight that many LLM-generated code contains serious security vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>While previous work tries to address this by training models that generate secure code, these attempts remain constrained by limited access to training data and labor-intensive data preparation.   In this paper, we introduce HexaCoder, a novel approach to enhance the ability of LLMs to generate secure codes by automatically synthesizing secure codes, which reduces the effort of finding suitable training data.HexaCoder comprises two key components: an oracle-guided data synthesis pipeline and a two-step process for secure code generation.The data synthesis pipeline generates pairs of vulnerable and fixed codes for specific Common Weakness Enumeration (CWE) types by utilizing a state-of-the-art LLM for repairing vulnerable code.A security oracle identifies vulnerabilities, and a state-of-the-art LLM repairs them by extending and/or editing the codes, creating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA) method.Each example of our fine-tuning dataset includes the necessary security-related libraries and code that form the basis of our novel two-step generation approach.This allows the model to integrate security-relevant libraries before generating the main code, significantly reducing the number of generated vulnerable codes by up to 85% compared to the baseline methods.We perform extensive evaluations on three different benchmarks for four LLMs, demonstrating that HexaCoder not only improves the security of the generated code but also maintains a high level of functional correctness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06446v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06446v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Think-on-Process: Dynamic Process Generation for Collaborative Development of Multi-Agent System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Software development is a collaborative endeavor that requires individuals from different departments to work together in order to collectively develop a high-quality software system. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span>In this context, people have begun to explore a method that leverages multi-agent systems based on LLMs to carry out software development.However, existing research tends to rigidly fix the software development process in a framework in code form, thus failing to dynamically adjust the software development process in real-time to meet the more flexible and variable software environment.In this paper, we propose a dynamic process generation framework, named ToP (Think-on-Process).The core idea of ToP is to leverage experiential knowledge (i.e., process models) to guide LLMs in generating software development processes (i.e., instances).These instances will guide multi-agent in software development and employ a compiler to provide feedback on the development outcomes.Subsequently, we utilize heuristic algorithms to filter the instances and apply process mining algorithms to derive process model.Finally, the process model will be converted into text, formatted as prompts, to enhance the ability of LLMs to generate other instances.Experiments demonstrate that our framework ToP significantly enhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for five categories of software development tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.06568v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.06568v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                $\mathbb{USCD}$: Improving Code Generation of LLMs by Uncertainty-Aware Selective Contrastive Decoding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown remarkable capabilities in code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.949</span></span>However, the effects of hallucinations (e.g., output noise) make it particularly challenging for LLMs to generate high-quality code in one pass.In this work, we propose a simple and effective \textbf{u}ncertainty-aware \textbf{s}elective \textbf{c}ontrastive \textbf{d}ecoding ($\mathbb{USCD}$) mechanism to improve the quality of one-pass code generation in LLMs and reduce the impact of output noise.To be specific, we first elaborately designed a negative prompt (namely lame prompt) to output noise by removing input-output examples from the standard few-shot prompt.Our preliminary study shows that the Jensen-Shannon divergence (JS divergence) between token distribution uncertainty and the output noise is relatively low (approximately $0.25$), indicating their high relevance.Then, we selectively eliminate output noise induced by lame prompts based on the uncertainty of the prediction distribution from the standard prompt.Notably, our proposed plug-and-play mechanism is an inference-only method, enjoying appealing flexibility.Extensive experiments on widely used benchmarks, e.g., HumanEval, MBPP, and MultiPL-E, upon several LLMs (i.e., Inocder-6b, CodeLlama-7b, WizardCoder-15b, StarCoder, and Llama2-7b), demonstrate that our proposed USCD significantly improves one-pass code generation, with an average \textit{pass@$1$} scores increase of 16.59\%.We will release code and data on GitHub.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05923v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05923v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Harmonic Reasoning in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) are becoming very popular and are used for many different purposes, including creative tasks in the arts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>However, these models sometimes have trouble with specific reasoning tasks, especially those that involve logical thinking and counting.This paper looks at how well LLMs understand and reason when dealing with musical tasks like figuring out notes from intervals and identifying chords and scales.We tested GPT-3.5 and GPT-4o to see how they handle these tasks.Our results show that while LLMs do well with note intervals, they struggle with more complicated tasks like recognizing chords and scales.This points out clear limits in current LLM abilities and shows where we need to make them better, which could help improve how they think and work in both artistic and other complex areas.We also provide an automatically generated benchmark data set for the described tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05521v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05521v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Pair Programming Framework for Code Generation via Multi-Plan Exploration and Feedback-Driven Refinement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have achieved impressive performance on code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.957</span></span><span class='px-1 mx-1 bg-yellow-200'>Although prior studies enhanced LLMs with prompting techniques and code refinement, they still struggle with complex programming problems due to rigid solution plans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we draw on pair programming practices to propose PairCoder, a novel LLM-based framework for code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span>PairCoder incorporates two collaborative LLM agents, namely a Navigator agent for high-level planning and a Driver agent for specific implementation.The Navigator is responsible for proposing promising solution plans, selecting the current optimal plan, and directing the next iteration round based on execution feedback.The Driver follows the guidance of Navigator to undertake initial code generation, code testing, and refinement.This interleaved and iterative workflow involves multi-plan exploration and feedback-based refinement, which mimics the collaboration of pair programmers.We evaluate PairCoder with both open-source and closed-source LLMs on various code generation benchmarks.Extensive experimental results demonstrate the superior accuracy of PairCoder, achieving relative pass@1 improvements of 12.00%-162.43% compared to prompting LLMs directly.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05001v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05001v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Investigating the Role of Cultural Values in Adopting Large Language Models for Software Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As a socio-technical activity, software development involves the close interconnection of people and technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span><span class='px-1 mx-1 bg-yellow-200'>The integration of Large Language Models (LLMs) into this process exemplifies the socio-technical nature of software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.878</span></span>Although LLMs influence the development process, software development remains fundamentally human-centric, necessitating an investigation of the human factors in this adoption.Thus, with this study we explore the factors influencing the adoption of LLMs in software development, focusing on the role of professionals' cultural values.Guided by the Unified Theory of Acceptance and Use of Technology (UTAUT2) and Hofstede's cultural dimensions, we hypothesized that cultural values moderate the relationships within the UTAUT2 framework.Using Partial Least Squares-Structural Equation Modelling and data from 188 software engineers, we found that habit and performance expectancy are the primary drivers of LLM adoption, while cultural values do not significantly moderate this process.These findings suggest that, by highlighting how LLMs can boost performance and efficiency, organizations can encourage their use, no matter the cultural differences.Practical steps include offering training programs to demonstrate LLM benefits, creating a supportive environment for regular use, and continuously tracking and sharing performance improvements from using LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05055v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05055v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Insights from Benchmarking Frontier Language Models on Web App Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper presents insights from evaluating 16 frontier large language models (LLMs) on the WebApp1K benchmark, a test suite designed to assess the ability of LLMs to generate web application code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span>The results reveal that while all models possess similar underlying knowledge, their performance is differentiated by the frequency of mistakes they make.By analyzing lines of code (LOC) and failure distributions, we find that writing correct code is more complex than generating incorrect code.Furthermore, prompt engineering shows limited efficacy in reducing errors beyond specific cases.These findings suggest that further advancements in coding LLM should emphasize on model reliability and mistake minimization.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.05177v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.05177v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Automatic programming attempts to minimize human intervention in the generation of executable code, and has been a long-standing challenge in the software engineering community. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span><span class='px-1 mx-1 bg-yellow-200'>To advance automatic programming, researchers are focusing on three primary directions: (1) code search that reuses existing code snippets from external databases; (2) code generation that produces new code snippets from natural language; and (3) program repair that refines existing code snippets by fixing detected bugs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>Despite significant advancements, the effectiveness of state-of-the-art techniques is still limited, such as the usability of searched code and the correctness of generated code.   <span class='px-1 mx-1 bg-yellow-200'>Motivated by the real-world programming process, where developers usually use various external tools to aid their coding processes, such as code search engines and code testing tools, in this work, we propose \toolname{}, an automatic programming framework that leverages recent large language models (LLMs) to integrate the three research areas to address their inherent limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.804</span></span><span class='px-1 mx-1 bg-yellow-200'>In particular, our framework first leverages different code search strategies to retrieve similar code snippets, which are then used to further guide the code generation process of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.731</span></span>Our framework further validates the quality of generated code by compilers and test cases, and constructs repair prompts to query LLMs for generating correct patches.We conduct preliminary experiments to demonstrate the potential of our framework, \eg helping CodeLlama solve 267 programming problems with an improvement of 62.53\%.<span class='px-1 mx-1 bg-yellow-200'>As a generic framework, \toolname{} can integrate various code search, generation, and repair tools, combining these three research areas together for the first time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span>More importantly, it demonstrates the potential of using traditional SE tools to enhance the usability of LLMs in automatic programming.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.03267v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.03267v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sketch: A Toolkit for Streamlining LLM Operations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) represented by GPT family have achieved remarkable success. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>The characteristics of LLMs lie in their ability to accommodate a wide range of tasks through a generative approach.However, the flexibility of their output format poses challenges in controlling and harnessing the model's outputs, thereby constraining the application of LLMs in various domains.In this work, we present Sketch, an innovative toolkit designed to streamline LLM operations across diverse fields.Sketch comprises the following components: (1) a suite of task description schemas and prompt templates encompassing various NLP tasks; (2) a user-friendly, interactive process for building structured output LLM services tailored to various NLP tasks; (3) an open-source dataset for output format control, along with tools for dataset construction; and (4) an open-source model based on LLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting instructions.We anticipate this initiative to bring considerable convenience to LLM users, achieving the goal of ''plug-and-play'' for various applications.The components of Sketch will be progressively open-sourced at https://github.com/cofe-ai/Sketch.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.03346v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.03346v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CLUE: Concept-Level Uncertainty Estimation for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have demonstrated remarkable proficiency in various natural language generation (NLG) tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span>Previous studies suggest that LLMs' generation process involves uncertainty.However, existing approaches to uncertainty estimation mainly focus on sequence-level uncertainty, overlooking individual pieces of information within sequences.These methods fall short in separately assessing the uncertainty of each component in a sequence.In response, we propose a novel framework for Concept-Level Uncertainty Estimation (CLUE) for LLMs.We leverage LLMs to convert output sequences into concept-level representations, breaking down sequences into individual concepts and measuring the uncertainty of each concept separately.We conduct experiments to demonstrate that CLUE can provide more interpretable uncertainty estimation results compared with sentence-level uncertainty, and could be a useful tool for various tasks such as hallucination detection and story generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.03021v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.03021v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Debugging with Open-Source Large Language Models: An Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models have shown good potential in supporting software development tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span>This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code.While this can save time and effort, many companies prohibit it due to strict code sharing policies.To address this, companies can run open-source LLMs locally.But until now there is not much research evaluating the performance of open-source large language models in debugging.This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code.<span class='px-1 mx-1 bg-yellow-200'>The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span><span class='px-1 mx-1 bg-yellow-200'>Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.03031v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.03031v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-language Unit Test Generation using LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Implementing automated unit tests is an important but time consuming activity in software development.Developers dedicate substantial time to writing tests for validating an application and preventing regressions.To support developers in this task, software engineering research over the past few decades has developed many techniques for automating unit test generation.<span class='px-1 mx-1 bg-yellow-200'>However, despite this effort, usable tools exist for very few programming languages -- mainly Java, C, and C# and, more recently, for Python. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>Moreover, studies have found that automatically generated tests suffer poor readability and often do not resemble developer-written tests.In this work, we present a rigorous investigation of how large language models (LLMs) can help bridge the gap.We describe a generic pipeline that incorporates static analysis to guide LLMs in generating compilable and high-coverage test cases.We illustrate how the pipeline can be applied to different programming languages, specifically Java and Python, and to complex software requiring environment mocking.We conducted a through empirical study to assess the quality of the generated tests in terms of coverage, mutation score, and test naturalness -- evaluating them on standard as well as enterprise Java applications and a large Python benchmark.<span class='px-1 mx-1 bg-yellow-200'>Our results demonstrate that LLM-based test generation, when guided by static analysis, can be competitive with, and even outperform, state-of-the-art test-generation techniques in coverage achieved while also producing considerably more natural test cases that developers find easy to read and understand. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>We also present the results of a user study, conducted with 161 professional developers, that highlights the naturalness characteristics of the tests generated by our approach.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.03093v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.03093v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
  function extractGSUrl(url) {
            var regex = /\/([^/]+?)(?:v\d+)?$/;
            var matches = url.match(regex);

            if (matches) {
                var endOfUrl = matches[1];
                var prefix = "https://scholar.google.com/scholar_lookup?arxiv_id=";
                var finalUrl = prefix + endOfUrl;
                console.log("Final URL: " + finalUrl);
                
                // You can open the final URL in a new tab/window
                window.open(finalUrl, "_blank");
            } else {
                console.log("No match found");
            }
        };
</script>
</body>
</html>