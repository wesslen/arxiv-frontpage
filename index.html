<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Ryan's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-08-13.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by running a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <p class="text-sm text-gray-500 pt-2">This project was originally created by <a href="https://koaning.io/">Vincent Warmerdam</a>, modifying his original <a href="https://koaning.github.io/arxiv-frontpage/">frontpage</a> for different paper categories.</p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Prompt Engineering in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span><span class='px-1 mx-1 bg-yellow-200'>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.869</span></span><span class='px-1 mx-1 bg-yellow-200'>The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span><span class='px-1 mx-1 bg-yellow-200'>The second method involves creating natural language prompts on top of existing Arabic NLP datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.854</span></span>We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TRIZ-GPT: An LLM-augmented method for problem-solving
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>TRIZ, the Theory of Inventive Problem Solving, is derived from a comprehensive analysis of patents across various domains, offering a framework and practical tools for problem-solving.Despite its potential to foster innovative solutions, the complexity and abstractness of TRIZ methodology often make its acquisition and application challenging.This often requires users to have a deep understanding of the theory, as well as substantial practical experience and knowledge across various disciplines.The advent of Large Language Models (LLMs) presents an opportunity to address these challenges by leveraging their extensive knowledge bases and reasoning capabilities for innovative solution generation within TRIZ-based problem-solving process.This study explores and evaluates the application of LLMs within the TRIZ-based problem-solving process.The construction of TRIZ case collections establishes a solid empirical foundation for our experiments and offers valuable resources to the TRIZ community.<span class='px-1 mx-1 bg-yellow-200'>A specifically designed workflow, utilizing step-by-step reasoning and evaluation-validated prompt strategies, effectively transforms concrete problems into TRIZ problems and finally generates inventive solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>Finally, we present a case study in mechanical engineering field that highlights the practical application of this LLM-augmented method.It showcases GPT-4's ability to generate solutions that closely resonate with original solutions and suggests more implementation mechanisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05897v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05897v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Med42-v2: A Suite of Clinical LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Med42-v2 introduces a suite of clinical large language models (LLMs) designed to address the limitations of generic models in healthcare settings.These models are built on Llama3 architecture and fine-tuned using specialized clinical data.<span class='px-1 mx-1 bg-yellow-200'>They underwent multi-stage preference alignment to effectively respond to natural prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>While generic models are often preference-aligned to avoid answering clinical queries as a precaution, Med42-v2 is specifically trained to overcome this limitation, enabling its use in clinical settings.Med42-v2 models demonstrate superior performance compared to the original Llama3 models in both 8B and 70B parameter configurations and GPT-4 across various medical benchmarks.These LLMs are developed to understand clinical queries, perform reasoning tasks, and provide valuable assistance in clinical environments.The models are now publicly available at \href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06142v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06142v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Animate, or Inanimate, That is the Question for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding.Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information.Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do?<span class='px-1 mx-1 bg-yellow-200'>We then propose a systematic analysis via prompting approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts.Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies.Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06332v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06332v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Healthcare through Large Language Models: A Study on Medical Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, the application of Large Language Models (LLMs) in healthcare has shown significant promise in improving the accessibility and dissemination of medical knowledge.This paper presents a detailed study of various LLMs trained on the MedQuAD medical question-answering dataset, with a focus on identifying the most effective model for providing accurate medical information.Among the models tested, the Sentence-t5 combined with Mistral 7B demonstrated superior performance, achieving a precision score of 0.762.<span class='px-1 mx-1 bg-yellow-200'>This model's enhanced capabilities are attributed to its advanced pretraining techniques, robust architecture, and effective prompt construction methodologies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>By leveraging these strengths, the Sentence-t5 + Mistral 7B model excels in understanding and generating precise medical answers.Our findings highlight the potential of integrating sophisticated LLMs in medical contexts to facilitate efficient and accurate medical knowledge retrieval, thus significantly enhancing patient education and support.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04138v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04138v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper considers a scenario in city navigation: an AI agent is provided with language descriptions of the goal location with respect to some well-known landmarks; By only observing the scene around, including recognizing landmarks and road network connections, the agent has to make decisions to navigate to the goal location without instructions.This problem is very challenging, because it requires agent to establish self-position and acquire spatial representation of complex urban environment, where landmarks are often invisible.In the absence of navigation instructions, such abilities are vital for the agent to make high-quality decisions in long-range city navigation.<span class='px-1 mx-1 bg-yellow-200'>With the emergent reasoning ability of large language models (LLMs), a tempting baseline is to prompt LLMs to "react" on each observation and make decisions accordingly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.691</span></span>However, this baseline has very poor performance that the agent often repeatedly visits same locations and make short-sighted, inconsistent decisions.To address these issues, this paper introduces a novel agentic workflow featured by its abilities to perceive, reflect and plan.Specifically, we find LLaVA-7B can be fine-tuned to perceive the direction and distance of landmarks with sufficient accuracy for city navigation.Moreover, reflection is achieved through a memory mechanism, where past experiences are stored and can be retrieved with current perception for effective decision argumentation.Planning uses reflection results to produce long-term plans, which can avoid short-sighted decisions in long-range navigation.We show the designed workflow significantly improves navigation ability of the LLM agent compared with the state-of-the-art baselines.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04168v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04168v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs).<span class='px-1 mx-1 bg-yellow-200'>Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.815</span></span>This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains.In this paper, we present $\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished.Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation.This might be useful in some domains like education, where any LLM intervention is usually prohibited.Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains.LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New.The video describing our system is available at https://youtu.be/E8eT_bE7k8c.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04284v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04284v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span>Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions.<span class='px-1 mx-1 bg-yellow-200'>Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span><span class='px-1 mx-1 bg-yellow-200'>Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span><span class='px-1 mx-1 bg-yellow-200'>In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span><span class='px-1 mx-1 bg-yellow-200'>We use advanced prompting techniques with varying complexity for AEQG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.797</span></span>We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions.<span class='px-1 mx-1 bg-yellow-200'>Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.861</span></span>We also show that automated evaluation is not on par with human evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04394v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04394v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the NeuBAROCO Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper explores the question of how accurately current large language models can perform logical reasoning in natural language, with an emphasis on whether these models exhibit reasoning biases similar to humans.Specifically, our study focuses on syllogistic reasoning, a form of deductive reasoning extensively studied in cognitive science as a natural form of human reasoning.We present a syllogism dataset called NeuBAROCO, which consists of syllogistic reasoning problems in English and Japanese.This dataset was originally designed for psychological experiments to assess human reasoning capabilities using various forms of syllogisms.Our experiments with leading large language models indicate that these models exhibit reasoning biases similar to humans, along with other error tendencies.Notably, there is significant room for improvement in reasoning problems where the relationship between premises and hypotheses is neither entailment nor contradiction.<span class='px-1 mx-1 bg-yellow-200'>We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span>Our analysis using this method suggests that the primary limitations of LLMs lie in the reasoning process itself rather than the interpretation of syllogisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Recognizing Emotion Regulation Strategies from Human Behavior with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Human emotions are often not expressed directly, but regulated according to internal processes and social display rules.For affective computing systems, an understanding of how users regulate their emotions can be highly useful, for example to provide feedback in job interview training, or in psychotherapeutic scenarios.However, at present no method to automatically classify different emotion regulation strategies in a cross-user scenario exists.At the same time, recent studies showed that instruction-tuned Large Language Models (LLMs) can reach impressive performance across a variety of affect recognition tasks such as categorical emotion recognition or sentiment analysis.While these results are promising, it remains unclear to what extent the representational power of LLMs can be utilized in the more subtle task of classifying users' internal emotion regulation strategy.To close this gap, we make use of the recently introduced \textsc{Deep} corpus for modeling the social display of the emotion shame, where each point in time is annotated with one of seven different emotion regulation classes.<span class='px-1 mx-1 bg-yellow-200'>We fine-tune Llama2-7B as well as the recently introduced Gemma model using Low-rank Optimization on prompts generated from different sources of information on the \textsc{Deep} corpus. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>These include verbal and nonverbal behavior, person factors, as well as the results of an in-depth interview after the interaction.Our results show, that a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation strategy with high accuracy (0.84) without needing access to data from post-interaction interviews.This represents a significant improvement over previous approaches based on Bayesian Networks and highlights the importance of modeling verbal behavior in emotion regulation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04420v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04420v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding.<span class='px-1 mx-1 bg-yellow-200'>However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.746</span></span>In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience).<span class='px-1 mx-1 bg-yellow-200'>We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conversational Prompt Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompts are how humans communicate with LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.892</span></span><span class='px-1 mx-1 bg-yellow-200'>Informative prompts are essential for guiding LLMs to produce the desired output. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span><span class='px-1 mx-1 bg-yellow-200'>However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.906</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span><span class='px-1 mx-1 bg-yellow-200'>CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction.Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs.<span class='px-1 mx-1 bg-yellow-200'>The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span><span class='px-1 mx-1 bg-yellow-200'>The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04560v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04560v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence.<span class='px-1 mx-1 bg-yellow-200'>These models can process multi-modal prompts, enabling them to generate more context-aware responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>However, this integration is not without challenges.One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks.These tasks require precise and reliable responses to ensure safe and effective operation.<span class='px-1 mx-1 bg-yellow-200'>Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions.<span class='px-1 mx-1 bg-yellow-200'>This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03515v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03515v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RepoMasterEval: Evaluating Code Completion via Real-World Repositories
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical.However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model.<span class='px-1 mx-1 bg-yellow-200'>By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.863</span></span>These limitations makes the evaluation poorly align with the practical scenarios of code completion tools.In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories.Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites.To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score.Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios.The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03519v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03519v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring the extent of similarities in software failures across industries using LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid evolution of software development necessitates enhanced safety measures.Extracting information about software failures from companies is becoming increasingly more available through news articles.   This research utilizes the Failure Analysis Investigation with LLMs (FAIL) model to extract industry-specific information.Although the FAIL model's database is rich in information, it could benefit from further categorization and industry-specific insights to further assist software engineers.   In previous work news articles were collected from reputable sources and categorized by incidents inside a database.<span class='px-1 mx-1 bg-yellow-200'>Prompt engineering and Large Language Models (LLMs) were then applied to extract relevant information regarding the software failure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>This research extends these methods by categorizing articles into specific domains and types of software failures.The results are visually represented through graphs.   The analysis shows that throughout the database some software failures occur significantly more often in specific industries.This categorization provides a valuable resource for software engineers and companies to identify and address common failures.   This research highlights the synergy between software engineering and Large Language Models (LLMs) to automate and enhance the analysis of software failures.By transforming data from the database into an industry specific model, we provide a valuable resource that can be used to identify common vulnerabilities, predict potential risks, and implement proactive measures for preventing software failures.Leveraging the power of the current FAIL database and data visualization, we aim to provide an avenue for safer and more secure software in the future.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03528v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03528v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EnJa: Ensemble Jailbreak on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As Large Language Models (LLMs) are increasingly being deployed in safety-critical applications, their vulnerability to potential jailbreaks -- malicious prompts that can disable the safety mechanism of LLMs -- has attracted growing research attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>While alignment methods have been proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can still be jailbroken by carefully crafted malicious prompts, producing content that violates policy regulations.Existing jailbreak attacks on LLMs can be categorized into prompt-level methods which make up stories/logic to circumvent safety alignment and token-level attack methods which leverage gradient methods to find adversarial tokens.In this work, we introduce the concept of Ensemble Jailbreak and explore methods that can integrate prompt-level and token-level jailbreak into a more powerful hybrid jailbreak attack.Specifically, we propose a novel EnJa attack to hide harmful instructions using prompt-level jailbreak, boost the attack success rate using a gradient-based attack, and connect the two types of jailbreak attacks via a template-based connector.We evaluate the effectiveness of EnJa on several aligned models and show that it achieves a state-of-the-art attack success rate with fewer queries and is much stronger than any individual jailbreak.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03603v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03603v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Logical Fallacy-Informed Framework for Argument Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite the remarkable performance of Large Language Models (LLMs), they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation.An important factor contributing to LLMs' suboptimal performance in generating coherent arguments is their oversight of logical fallacies.<span class='px-1 mx-1 bg-yellow-200'>To address this issue, we introduce FIPO, a fallacy-informed framework that leverages preference optimization methods to steer LLMs toward logically sound arguments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>FIPO includes a classification loss, to capture the fine-grained information on fallacy categories.Our results on argumentation datasets show that our method reduces the fallacy errors by up to 17.5%.Furthermore, our human evaluation results indicate that the quality of the generated arguments by our method significantly outperforms the fine-tuned baselines, as well as prior preference optimization methods, such as DPO.<span class='px-1 mx-1 bg-yellow-200'>These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03618v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03618v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Base Station Siting: Intelligent Deployment based on Prompt or Agent
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Traditional base station siting (BSS) methods rely heavily on drive testing and user feedback, which are laborious and require extensive expertise in communication, networking, and optimization.<span class='px-1 mx-1 bg-yellow-200'>As large language models (LLMs) and their associated technologies advance, particularly in the realms of prompt engineering and agent engineering, network optimization will witness a revolutionary approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach entails the strategic use of well-crafted prompts to infuse human experience and knowledge into these sophisticated LLMs, and the deployment of autonomous agents as a communication bridge to seamlessly connect the machine language based LLMs with human users using natural language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>This integration represents the future paradigm of artificial intelligence (AI) as a service and AI for more ease.As a preliminary exploration, this research first develops a novel LLM-empowered BSS optimization framework, and heuristically proposes four different potential implementations: the strategies based on Prompt-optimized LLM (PoL), human-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and Cooperative multiple LLM-based autonomous BSS agents (CLaBa).<span class='px-1 mx-1 bg-yellow-200'>Through evaluation on real-world data, the experiments demonstrate that prompt-assisted LLMs and LLM-based agents can generate more efficient, cost-effective, and reliable network deployments, noticeably enhancing the efficiency of BSS optimization and reducing trivial manual participation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03631v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03631v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Natural language processing (NLP) has seen significant advancements with the advent of large language models (LLMs).However, substantial improvements are still needed for languages other than English, especially for specific domains like the applications of Mercosur Common Nomenclature (NCM), a Brazilian Harmonized System (HS).To address this gap, this study uses TeenyTineLLaMA, a foundational Portuguese LLM, as an LLM source to implement the NCM application processing.Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT) technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.<span class='px-1 mx-1 bg-yellow-200'>This approach retains the chain-of-thought (CoT) methodology for prompt development in a more concise and streamlined manner, utilizing brief and focused documents for training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.851</span></span>The proposed model demonstrates an efficient and cost-effective alternative for fine-tuning smaller LLMs, significantly outperforming TeenyTineLLaMA and ChatGPT-4 in the same task.Although the research focuses on NCM applications, the methodology can be easily adapted for HS applications worldwide.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03936v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03936v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human Speech Perception in Noise: Can Large Language Models Paraphrase to Improve It?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) can generate text by transferring style attributes like formality resulting in formal or informal text.However, instructing LLMs to generate text that when spoken, is more intelligible in an acoustically difficult environment, is an under-explored topic.We conduct the first study to evaluate LLMs on a novel task of generating acoustically intelligible paraphrases for better human speech perception in noise.<span class='px-1 mx-1 bg-yellow-200'>Our experiments in English demonstrated that with standard prompting, LLMs struggle to control the non-textual attribute, i.e., acoustic intelligibility, while efficiently capturing the desired textual attributes like semantic equivalence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span><span class='px-1 mx-1 bg-yellow-200'>To remedy this issue, we propose a simple prompting approach, prompt-and-select, which generates paraphrases by decoupling the desired textual and non-textual attributes in the text generation pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.835</span></span>Our approach resulted in a 40% relative improvement in human speech perception, by paraphrasing utterances that are highly distorted in a listening condition with babble noise at a signal-to-noise ratio (SNR) -5dB. This study reveals the limitation of LLMs in capturing non-textual attributes, and our proposed method showcases the potential of using LLMs for better human speech perception in noise.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04029v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04029v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Digital Avatars: Framework Development and Their Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We present a novel prompting strategy for artificial intelligence driven digital avatars. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span><span class='px-1 mx-1 bg-yellow-200'>To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span>To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars.This pipeline effectively captures an individual's essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device.Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines.In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04068v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04068v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Zero-Delay QKV Compression for Mitigating KV Cache and Network Bottlenecks in LLM Inference
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In large-language models, memory constraints in the key-value cache (KVC) pose a challenge during inference, especially with long prompts.In this work, we observed that compressing KV values is more effective than compressing the model regarding accuracy and job completion time (JCT).However, quantizing KV values and dropping less-important tokens incur significant runtime computational time overhead, delaying JCT.<span class='px-1 mx-1 bg-yellow-200'>These methods also cannot reduce computation time or high network communication time overhead in sequence-parallelism (SP) frameworks for long prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>To tackle these issues, based on our insightful observations from experimental analysis, we propose ZeroC, a Zero-delay QKV Compression system that eliminates time overhead and even reduces computation and communication time of the model operations.ZeroC innovatively embeds compression and decompression operations within model operations and adaptively determines compression ratios at a hybrid layer-token level.Further, it enables a communication-efficient SP inference framework.Trace-driven experiments demonstrate that ZeroC achieves up to 80% lower average JCT, 35% lower average perplexity, and 2.8x higher throughput with the same latency compared to state-of-the-art compression methods.ZeroC also reduces the average JCT of current LLM serving systems by up to 91% with the constraint of 0.1 perplexity increase.We open-sourced the code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04107v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04107v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) can help writers build story worlds by generating world elements, such as factions, characters, and locations.However, making sense of many generated elements can be overwhelming.<span class='px-1 mx-1 bg-yellow-200'>Moreover, if the user wants to precisely control aspects of generated elements that are difficult to specify verbally, prompting alone may be insufficient. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span>We introduce Patchview, a customizable LLM-powered system that visually aids worldbuilding by allowing users to interact with story concepts and elements through the physical metaphor of magnets and dust.Elements in Patchview are visually dragged closer to concepts with high relevance, facilitating sensemaking.The user can also steer the generation with verbally elusive concepts by indicating the desired position of the element between concepts.When the user disagrees with the LLM's visualization and generation, they can correct those by repositioning the element.These corrections can be used to align the LLM's future behaviors to the user's perception.With a user study, we show that Patchview supports the sensemaking of world elements and steering of element generation, facilitating exploration during the worldbuilding process.Patchview provides insights on how customizable visual representation can help sensemake, steer, and align generative AI model behaviors with the user's intentions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Rule-Based Insights Enhance LLMs for Radiology Report Classification? Introducing the RadPrompt Methodology
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing imaging models capable of detecting pathologies from chest X-rays can be cost and time-prohibitive for large datasets as it requires supervision to attain state-of-the-art performance.Instead, labels extracted from radiology reports may serve as distant supervision since these are routinely generated as part of clinical practice.Despite their widespread use, current rule-based methods for label extraction rely on extensive rule sets that are limited in their robustness to syntactic variability.To alleviate these limitations, we introduce RadPert, a rule-based system that integrates an uncertainty-aware information schema with a streamlined set of rules, enhancing performance.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we have developed RadPrompt, a multi-turn prompting strategy that leverages RadPert to bolster the zero-shot predictive capabilities of large language models, achieving a statistically significant improvement in weighted average F1 score over GPT-4 Turbo. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span>Most notably, RadPrompt surpasses both its underlying models, showcasing the synergistic potential of LLMs with rule-based models.We have evaluated our methods on two English Corpora: the MIMIC-CXR gold-standard test set and a gold-standard dataset collected from the Cambridge University Hospitals.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04121v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04121v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Robustness Tools in LLM Safety</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps).Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic.<span class='px-1 mx-1 bg-yellow-200'>However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp.Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps.This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis.Finally, HYPERION reports the inconsistency based on predefined detection patterns.The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies.We also implement HYPERION to analyze 835 real-world DApps.The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06037v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06037v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Effects of Steering Latent Representation for Large Language Model Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span>Despite its high performance, the underlying cause and explanation remain underexplored.In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses.Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers.Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks.Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs.To resolve this drawback, we propose Adaptive RMU -- a simple yet effective alternative method that makes unlearning effective with most layers.Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we introduce PhishLang, an open-source, lightweight Large Language Model (LLM) specifically designed for phishing website detection through contextual analysis of the website.Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats and deep learning models that are computationally intensive, our model utilizes the advanced language processing capabilities of LLMs to learn granular features that are characteristic of phishing attacks.Furthermore, PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning tools, while being significantly faster and less resource-intensive.<span class='px-1 mx-1 bg-yellow-200'>Over a 3.5-month testing period, PhishLang successfully identified approximately 26K phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to aid current detection measures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span><span class='px-1 mx-1 bg-yellow-200'>We also evaluate PhishLang against several realistic adversarial attacks and develop six patches that make it very robust against such threats. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>Furthermore, we integrate PhishLang with GPT-3.5<span class='px-1 mx-1 bg-yellow-200'>Turbo to create \textit{explainable blocklisting} - warnings that provide users with contextual information about different features that led to a website being marked as phishing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>Finally, we have open-sourced the PhishLang framework and developed a Chromium-based browser extension and URL scanner website, which implement explainable warnings for end-users.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05667v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05667v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses.This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the required skills surpassing their capabilities.<span class='px-1 mx-1 bg-yellow-200'>We first systematically conceptualize infeasible tasks for LLMs, providing formal definitions and categorizations that cover a spectrum of related hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.777</span></span>We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to test multiple LLMs' abilities on task feasibility.Furthermore, we explore the potential of training enhancements to increase LLMs' refusal capabilities with fine-tuning.Experiments validate the effectiveness of our methods, offering promising directions for refining the operational boundaries of LLMs in real applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05873v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05873v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains.However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare).Moreover, certain application-specific scenarios may require fine-tuning these models on private data.<span class='px-1 mx-1 bg-yellow-200'>This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline.These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training.Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs.This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learning to Rewrite: Generalized LLM-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) can be abused at scale to create non-factual content and spread disinformation.Detecting LLM-generated content is essential to mitigate these risks, but current classifiers often fail to generalize in open-world contexts.<span class='px-1 mx-1 bg-yellow-200'>Prior work shows that LLMs tend to rewrite LLM-generated content less frequently, which can be used for detection and naturally generalizes to unforeseen data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>However, we find that the rewriting edit distance between human and LLM content can be indistinguishable across domains, leading to detection failures.We propose training an LLM to rewrite input text, producing minimal edits for LLM-generated content and more edits for human-written text, deriving a distinguishable and generalizable edit distance difference across different domains.Experiments on text from 21 independent domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that our classifier outperforms the state-of-the-art zero-shot classifier by up to 20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score.Our work suggests that LLM can effectively detect machine-generated text if they are trained properly.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04237v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04237v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs).Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated).<span class='px-1 mx-1 bg-yellow-200'>This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>In this paper, we present $\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished.Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation.This might be useful in some domains like education, where any LLM intervention is usually prohibited.Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains.LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New.The video describing our system is available at https://youtu.be/E8eT_bE7k8c.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04284v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04284v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Explainable Network Intrusion Detection using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents.However, their applicability to threat detection problems remains unclear.This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explainability.Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS.Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments.Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge.Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS.<span class='px-1 mx-1 bg-yellow-200'>Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span>Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04342v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04342v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Competitive debate is a comprehensive and complex computational argumentation task.<span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) encounter hallucinations and lack competitiveness in this task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate.Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents (Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary.To comprehensively evaluate framework performance, we construct the Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions.We recruite ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans.The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking.Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans.Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As diverse linguistic communities and users adopt large language models (LLMs), assessing their safety across languages becomes critical.<span class='px-1 mx-1 bg-yellow-200'>Despite ongoing efforts to make LLMs safe, they can still be made to behave unsafely with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span>Research on LLM safety and jailbreaking, however, has so far mostly focused on English, limiting our understanding of LLM safety in other languages.We contribute towards closing this gap by investigating the effectiveness of many-shot jailbreaking, where models are prompted with unsafe demonstrations to induce unsafe behaviour, in Italian.To enable our analysis, we create a new dataset of unsafe Italian question-answer pairs.With this dataset, we identify clear safety vulnerabilities in four families of open-weight LLMs.<span class='px-1 mx-1 bg-yellow-200'>We find that the models exhibit unsafe behaviors even when prompted with few unsafe demonstrations, and -- more alarmingly -- that this tendency rapidly escalates with more demonstrations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04522v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04522v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learning Fine-Grained Grounded Citations for Attributed Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span><span class='px-1 mx-1 bg-yellow-200'>Attributed LLMs, which augment generated text with in-line citations, have shown potential in mitigating hallucinations and improving verifiability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning.Furthermore, the practice of citing only coarse document identifiers makes it challenging for users to perform fine-grained verification.In this work, we introduce FRONT, a training framework designed to teach LLMs to generate Fine-Grained Grounded Citations.By grounding model outputs in fine-grained supporting quotes, these quotes guide the generation of grounded and consistent responses, not only improving citation quality but also facilitating fine-grained verification.Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in generating superior grounded responses and highly supportive citations.With LLaMA-2-7B, the framework significantly outperforms all the baselines, achieving an average of 14.21% improvement in citation quality across all datasets, even surpassing ChatGPT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04568v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04568v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Harnessing the Power of LLMs in Source Code Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Software vulnerabilities, caused by unintentional flaws in source code, are a primary root cause of cyberattacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span><span class='px-1 mx-1 bg-yellow-200'>Static analysis of source code has been widely used to detect these unintentional defects introduced by software developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span>Large Language Models (LLMs) have demonstrated human-like conversational abilities due to their capacity to capture complex patterns in sequential data, such as natural languages.In this paper, we harness LLMs' capabilities to analyze source code and detect known vulnerabilities.To ensure the proposed vulnerability detection method is universal across multiple programming languages, we convert source code to LLVM IR and train LLMs on these intermediate representations.We conduct extensive experiments on various LLM architectures and compare their accuracy.<span class='px-1 mx-1 bg-yellow-200'>Our comprehensive experiments on real-world and synthetic codes from NVD and SARD demonstrate high accuracy in identifying source code vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03489v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03489v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence.These models can process multi-modal prompts, enabling them to generate more context-aware responses.However, this integration is not without challenges.<span class='px-1 mx-1 bg-yellow-200'>One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span>These tasks require precise and reliable responses to ensure safe and effective operation.Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously.<span class='px-1 mx-1 bg-yellow-200'>For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03515v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03515v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RepoMasterEval: Evaluating Code Completion via Real-World Repositories
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical.However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model.By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block.These limitations makes the evaluation poorly align with the practical scenarios of code completion tools.In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories.Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites.<span class='px-1 mx-1 bg-yellow-200'>To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios.The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03519v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03519v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EnJa: Ensemble Jailbreak on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As Large Language Models (LLMs) are increasingly being deployed in safety-critical applications, their vulnerability to potential jailbreaks -- malicious prompts that can disable the safety mechanism of LLMs -- has attracted growing research attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span><span class='px-1 mx-1 bg-yellow-200'>While alignment methods have been proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can still be jailbroken by carefully crafted malicious prompts, producing content that violates policy regulations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>Existing jailbreak attacks on LLMs can be categorized into prompt-level methods which make up stories/logic to circumvent safety alignment and token-level attack methods which leverage gradient methods to find adversarial tokens.In this work, we introduce the concept of Ensemble Jailbreak and explore methods that can integrate prompt-level and token-level jailbreak into a more powerful hybrid jailbreak attack.Specifically, we propose a novel EnJa attack to hide harmful instructions using prompt-level jailbreak, boost the attack success rate using a gradient-based attack, and connect the two types of jailbreak attacks via a template-based connector.We evaluate the effectiveness of EnJa on several aligned models and show that it achieves a state-of-the-art attack success rate with fewer queries and is much stronger than any individual jailbreak.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03603v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03603v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs).It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections.The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing.<span class='px-1 mx-1 bg-yellow-200'>Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>We make WalledEval publicly available at https://github.com/walledai/walledevalA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03837v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03837v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization.These complex tasks require AI to continuously summarize and improve.Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference.This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing.We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy.Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example.Note that as the memories continue to grow, this cumulative improvement may be substantial.<span class='px-1 mx-1 bg-yellow-200'>The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03841v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03841v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) can help writers build story worlds by generating world elements, such as factions, characters, and locations.However, making sense of many generated elements can be overwhelming.Moreover, if the user wants to precisely control aspects of generated elements that are difficult to specify verbally, prompting alone may be insufficient.We introduce Patchview, a customizable LLM-powered system that visually aids worldbuilding by allowing users to interact with story concepts and elements through the physical metaphor of magnets and dust.Elements in Patchview are visually dragged closer to concepts with high relevance, facilitating sensemaking.The user can also steer the generation with verbally elusive concepts by indicating the desired position of the element between concepts.<span class='px-1 mx-1 bg-yellow-200'>When the user disagrees with the LLM's visualization and generation, they can correct those by repositioning the element. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>These corrections can be used to align the LLM's future behaviors to the user's perception.With a user study, we show that Patchview supports the sensemaking of world elements and steering of element generation, facilitating exploration during the worldbuilding process.Patchview provides insights on how customizable visual representation can help sensemake, steer, and align generative AI model behaviors with the user's intentions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring RAG-based Vulnerability Augmentation with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Detecting vulnerabilities is a crucial task for maintaining the integrity, availability, and security of software systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span><span class='px-1 mx-1 bg-yellow-200'>Utilizing DL-based models for vulnerability detection has become commonplace in recent years. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>However, such deep learning-based vulnerability detectors (DLVD) suffer from a shortage of sizable datasets to train effectively.Data augmentation can potentially alleviate the shortage of data, but augmenting vulnerable code is challenging and requires designing a generative solution that maintains vulnerability.Hence, the work on generating vulnerable code samples has been limited and previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities.Lately, large language models (LLMs) are being used for solving various code generation and comprehension tasks and have shown inspiring results, especially when fused with retrieval augmented generation (RAG).In this study, we explore three different strategies to augment vulnerabilities both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension.We conducted an extensive evaluation of our proposed approach on three vulnerability datasets and three DLVD models, using two LLMs.Our results show that our injection-based clustering-enhanced RAG method beats the baseline setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling (ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with 15K generated vulnerable samples.Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04125v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04125v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Aided Compilation for Tensor Accelerators
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Hardware accelerators, in particular accelerators for tensor processing, have many potential application domains.However, they currently lack the software infrastructure to support the majority of domains outside of deep learning.Furthermore, a compiler that can easily be updated to reflect changes at both application and hardware levels would enable more agile development and design space exploration of accelerators, allowing hardware designers to realize closer-to-optimal performance.In this work, we discuss how large language models (LLMs) could be leveraged to build such a compiler.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we demonstrate the ability of GPT-4 to achieve high pass rates in translating code to the Gemmini accelerator, and prototype a technique for decomposing translation into smaller, more LLM-friendly steps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>Additionally, we propose a 2-phase workflow for utilizing LLMs to generate hardware-optimized code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03408v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03408v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compromising Embodied Agents with Contextual Backdoor Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have transformed the development of embodied intelligence.By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents.<span class='px-1 mx-1 bg-yellow-200'>However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \method{}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span><span class='px-1 mx-1 bg-yellow-200'>By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span><span class='px-1 mx-1 bg-yellow-200'>These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>To compromise the LLM's contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning.To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers.We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents.To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning.Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Scaling Laws for Data Poisoning in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent work shows that LLMs are vulnerable to data poisoning, in which they are trained on partially corrupted or harmful data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Poisoned data is hard to detect, breaks guardrails, and leads to undesirable and harmful behavior.Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat.We consider three threat models by which data poisoning can occur: malicious fine-tuning, imperfect data curation, and intentional data contamination.Our experiments evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three datasets which speak to each of our threat models.<span class='px-1 mx-1 bg-yellow-200'>We find that larger LLMs are increasingly vulnerable, learning harmful behavior -- including sleeper agent behavior -- significantly more quickly than smaller LLMs with even minimal data poisoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span><span class='px-1 mx-1 bg-yellow-200'>These results underscore the need for robust safeguards against data poisoning in larger LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02946v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02946v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning.This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning.<span class='px-1 mx-1 bg-yellow-200'>Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts.Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms.To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms.The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02999v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02999v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TestART: Improving LLM-based Unit Test via Co-evolution of Automated Generation and Repair Iteration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit test is crucial for detecting bugs in individual program units but consumes time and effort.The existing automated unit test generation methods are mainly based on search-based software testing (SBST) and language models to liberate developers.Recently, large language models (LLMs) have demonstrated remarkable reasoning and generation capabilities.<span class='px-1 mx-1 bg-yellow-200'>However, several problems limit their ability to generate high-quality test cases: (1) LLMs may generate invalid test cases under insufficient context, resulting in compilation errors; (2) Lack of test and coverage feedback information may cause runtime errors and low coverage rates. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span><span class='px-1 mx-1 bg-yellow-200'>(3) The repetitive suppression problem causes LLMs to get stuck into the repetition loop of self-repair or re-generation attempts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.753</span></span>In this paper, we propose TestART, a novel unit test generation method that leverages the strengths of LLMs while overcoming the limitations mentioned.TestART improves LLM-based unit test via co-evolution of automated generation and repair iteration.<span class='px-1 mx-1 bg-yellow-200'>TestART leverages the template-based repair technique to fix bugs in LLM-generated test cases, using prompt injection to guide the next-step automated generation and avoid repetition suppression. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.679</span></span>Furthermore, TestART extracts coverage information from the passed test cases and utilizes it as testing feedback to enhance the sufficiency of the final test case.<span class='px-1 mx-1 bg-yellow-200'>This synergy between generation and repair elevates the quality, effectiveness, and readability of the produced test cases significantly beyond previous methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span>In comparative experiments, the pass rate of TestART-generated test cases is 78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and the same ChatGPT-3.5-based method ChatUniTest.It also achieves an impressive line coverage rate of 90.96% on the focal methods that passed the test, exceeding EvoSuite by 3.4%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03095v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03095v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Defects Detection and Fix in Logging Statement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Developers use logging statements to monitor software, but misleading logs can complicate maintenance by obscuring actual activities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>Existing research on logging quality issues is limited, mainly focusing on single defects and manual fixes.<span class='px-1 mx-1 bg-yellow-200'>To address this, we conducted a study identifying four defect types in logging statements through real-world log changes analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span>We propose LogFixer, a two-stage framework for automatic detection and updating of logging statements.In the offline stage, LogFixer uses a similarity-based classifier on synthetic defective logs to identify defects.During the online phase, this classifier evaluates logs in code snippets to determine necessary improvements, and an LLM-based recommendation framework suggests updates based on historical log changes.We evaluated LogFixer on real-world and synthetic datasets, and new real-world projects, achieving an F1 score of 0.625.LogFixer significantly improved static text and dynamic variables suggestions by 48.12\% and 24.90\%, respectively, and achieved a 61.49\% success rate in recommending correct updates for new projects.<span class='px-1 mx-1 bg-yellow-200'>We reported 40 problematic logs to GitHub, resulting in 25 confirmed and merged changes across 11 projects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03101v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03101v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A First Look at License Compliance Capability of LLMs in Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers.<span class='px-1 mx-1 bg-yellow-200'>However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>To establish this benchmark, we conduct an empirical study to identify a reasonable standard for "striking similarity" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code.Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs.Using LiCoEval, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations.<span class='px-1 mx-1 bg-yellow-200'>Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span>These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks.Our study provides a foundation for future research and development to improve license compliance in AI-assisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02487v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02487v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context Conquers Parameters: Outperforming Proprietary LLM in Commit Message Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Commit messages provide descriptions of the modifications made in a commit using natural language, making them crucial for software maintenance and evolution.Recent developments in Large Language Models (LLMs) have led to their use in generating high-quality commit messages, such as the Omniscient Message Generator (OMG).This method employs GPT-4 to produce state-of-the-art commit messages.However, the use of proprietary LLMs like GPT-4 in coding tasks raises privacy and sustainability concerns, which may hinder their industrial adoption.<span class='px-1 mx-1 bg-yellow-200'>Considering that open-source LLMs have achieved competitive performance in developer tasks such as compiler validation, this study investigates whether they can be used to generate commit messages that are comparable with OMG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>Our experiments show that an open-source LLM can generate commit messages that are comparable to those produced by OMG.In addition, through a series of contextual refinements, we propose lOcal MessagE GenerAtor (OMEGA) , a CMG approach that uses a 4-bit quantized 8B open-source LLM.OMEGA produces state-of-the-art commit messages, surpassing the performance of GPT-4 in practitioners' preference.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02502v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02502v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial.A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming.<span class='px-1 mx-1 bg-yellow-200'>However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>To tackle these challenges, we introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving }\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself.SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety.<span class='px-1 mx-1 bg-yellow-200'>This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span>Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02632v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02632v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Security Challenges in LLM Development</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multimodal Large Language Models for Phishing Webpage Detection and Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>To address the challenging problem of detecting phishing webpages, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms.Among these, brand-based phishing detection that uses models from Computer Vision to detect if a given webpage is imitating a well-known brand has received widespread attention.However, such models are costly and difficult to maintain, as they need to be retrained with labeled dataset that has to be regularly and continuously collected.Besides, they also need to maintain a good reference list of well-known websites and related meta-data for effective performance.   In this work, we take steps to study the efficacy of large language models (LLMs), in particular the multimodal LLMs, in detecting phishing webpages.Given that the LLMs are pretrained on a large corpus of data, we aim to make use of their understanding of different aspects of a webpage (logo, theme, favicon, etc.) to identify the brand of a given webpage and compare the identified brand with the domain name in the URL to detect a phishing attack.We propose a two-phase system employing LLMs in both phases: the first phase focuses on brand identification, while the second verifies the domain.We carry out comprehensive evaluations on a newly collected dataset.Our experiments show that the LLM-based system achieves a high detection rate at high precision; importantly, it also provides interpretable evidence for the decisions.<span class='px-1 mx-1 bg-yellow-200'>Our system also performs significantly better than a state-of-the-art brand-based phishing detection system while demonstrating robustness against two known adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05941v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05941v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Effects of Steering Latent Representation for Large Language Model Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning.Despite its high performance, the underlying cause and explanation remain underexplored.In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses.Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers.<span class='px-1 mx-1 bg-yellow-200'>Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.833</span></span>Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs.To resolve this drawback, we propose Adaptive RMU -- a simple yet effective alternative method that makes unlearning effective with most layers.Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the constantly evolving field of cybersecurity, it is imperative for analysts to stay abreast of the latest attack trends and pertinent information that aids in the investigation and attribution of cyber-attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce the first question-answering (QA) model and its application that provides information to the cybersecurity experts about cyber-attacks investigations and attribution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>Our QA model is based on Retrieval Augmented Generation (RAG) techniques together with a Large Language Model (LLM) and provides answers to the users' queries based on either our knowledge base (KB) that contains curated information about cyber-attacks investigations and attribution or on outside resources provided by the users.We have tested and evaluated our QA model with various types of questions, including KB-based, metadata-based, specific documents from the KB, and external sources-based questions.We compared the answers for KB-based questions with those from OpenAI's GPT-3.5 and the latest GPT-4o LLMs.Our proposed QA model outperforms OpenAI's GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models, which is critical for cyber-attack investigation and attribution.Additionally, our analysis showed that when the RAG QA model is given few-shot examples rather than zero-shot instructions, it generates better answers compared to cases where no examples are supplied in addition to the query.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06272v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06272v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we introduce PhishLang, an open-source, lightweight Large Language Model (LLM) specifically designed for phishing website detection through contextual analysis of the website.<span class='px-1 mx-1 bg-yellow-200'>Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats and deep learning models that are computationally intensive, our model utilizes the advanced language processing capabilities of LLMs to learn granular features that are characteristic of phishing attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>Furthermore, PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning tools, while being significantly faster and less resource-intensive.Over a 3.5-month testing period, PhishLang successfully identified approximately 26K phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to aid current detection measures.<span class='px-1 mx-1 bg-yellow-200'>We also evaluate PhishLang against several realistic adversarial attacks and develop six patches that make it very robust against such threats. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.877</span></span>Furthermore, we integrate PhishLang with GPT-3.5<span class='px-1 mx-1 bg-yellow-200'>Turbo to create \textit{explainable blocklisting} - warnings that provide users with contextual information about different features that led to a website being marked as phishing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span><span class='px-1 mx-1 bg-yellow-200'>Finally, we have open-sourced the PhishLang framework and developed a Chromium-based browser extension and URL scanner website, which implement explainable warnings for end-users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05667v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05667v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Retriever Augmented Large Language Models for Attack Graph Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the complexity of modern systems increases, so does the importance of assessing their security posture through effective vulnerability management and threat modeling techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span><span class='px-1 mx-1 bg-yellow-200'>One powerful tool in the arsenal of cybersecurity professionals is the attack graph, a representation of all potential attack paths within a system that an adversary might exploit to achieve a certain objective. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>Traditional methods of generating attack graphs involve expert knowledge, manual curation, and computational algorithms that might not cover the entire threat landscape due to the ever-evolving nature of vulnerabilities and exploits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper explores the approach of leveraging large language models (LLMs), such as ChatGPT, to automate the generation of attack graphs by intelligently chaining Common Vulnerabilities and Exposures (CVEs) based on their preconditions and effects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.845</span></span><span class='px-1 mx-1 bg-yellow-200'>It also shows how to utilize LLMs to create attack graphs from threat reports. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05855v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05855v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Based Robust Product Classification in Commerce and Compliance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Product classification is a crucial task in international trade, as compliance regulations are verified and taxes and duties are applied based on product categories.Manual classification of products is time-consuming and error-prone, and the sheer volume of products imported and exported renders the manual process infeasible.Consequently, e-commerce platforms and enterprises involved in international trade have turned to automatic product classification using machine learning.However, current approaches do not consider the real-world challenges associated with product classification, such as very abbreviated and incomplete product descriptions.In addition, recent advancements in generative Large Language Models (LLMs) and their reasoning capabilities are mainly untapped in product classification and e-commerce.In this research, we explore the real-life challenges of industrial classification and we propose data perturbations that allow for realistic data simulation.Furthermore, we employ LLM-based product classification to improve the robustness of the prediction in presence of incomplete data.Our research shows that LLMs with in-context learning outperform the supervised approaches in the clean-data scenario.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we illustrate that LLMs are significantly more robust than the supervised approaches when data attacks are present. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains.However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare).Moreover, certain application-specific scenarios may require fine-tuning these models on private data.<span class='px-1 mx-1 bg-yellow-200'>This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span><span class='px-1 mx-1 bg-yellow-200'>We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span>These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training.Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs.This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VideoQA in the Era of LLMs: An Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks.As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing.This work conducts a timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering.Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents.However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments.<span class='px-1 mx-1 bg-yellow-200'>Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>Also, they do not necessarily generalize better.The findings demonstrate Video-LLMs' QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability, suggesting the urgent need on rationales in Video-LLM developing.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Explainable Network Intrusion Detection using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents.<span class='px-1 mx-1 bg-yellow-200'>However, their applicability to threat detection problems remains unclear. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explainability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS.Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments.<span class='px-1 mx-1 bg-yellow-200'>Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span><span class='px-1 mx-1 bg-yellow-200'>Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span>Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04342v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04342v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of large language models (LLMs) into robotics significantly enhances the capabilities of embodied agents in understanding and executing complex natural language instructions.However, the unmitigated deployment of LLM-based embodied systems in real-world environments may pose potential physical risks, such as property damage and personal injury.<span class='px-1 mx-1 bg-yellow-200'>Existing security benchmarks for LLMs overlook risk awareness for LLM-based embodied agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>To address this gap, we propose RiskAwareBench, an automated framework designed to assess physical risks awareness in LLM-based embodied agents.RiskAwareBench consists of four modules: safety tips generation, risky scene generation, plan generation, and evaluation, enabling comprehensive risk assessment with minimal manual intervention.Utilizing this framework, we compile the PhysicalRisk dataset, encompassing diverse scenarios with associated safety tips, observations, and instructions.Extensive experiments reveal that most LLMs exhibit insufficient physical risk awareness, and baseline risk mitigation strategies yield limited enhancement, which emphasizes the urgency and cruciality of improving risk awareness in LLM-based embodied agents in the future.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04449v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04449v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As diverse linguistic communities and users adopt large language models (LLMs), assessing their safety across languages becomes critical.<span class='px-1 mx-1 bg-yellow-200'>Despite ongoing efforts to make LLMs safe, they can still be made to behave unsafely with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span><span class='px-1 mx-1 bg-yellow-200'>Research on LLM safety and jailbreaking, however, has so far mostly focused on English, limiting our understanding of LLM safety in other languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span><span class='px-1 mx-1 bg-yellow-200'>We contribute towards closing this gap by investigating the effectiveness of many-shot jailbreaking, where models are prompted with unsafe demonstrations to induce unsafe behaviour, in Italian. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span><span class='px-1 mx-1 bg-yellow-200'>To enable our analysis, we create a new dataset of unsafe Italian question-answer pairs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span><span class='px-1 mx-1 bg-yellow-200'>With this dataset, we identify clear safety vulnerabilities in four families of open-weight LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.807</span></span><span class='px-1 mx-1 bg-yellow-200'>We find that the models exhibit unsafe behaviors even when prompted with few unsafe demonstrations, and -- more alarmingly -- that this tendency rapidly escalates with more demonstrations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04522v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04522v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the increasing demand for practical applications of Large Language Models (LLMs), many attention-efficient models have been developed to balance performance and computational cost.<span class='px-1 mx-1 bg-yellow-200'>However, the adversarial robustness of these models remains under-explored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we design a framework to investigate the trade-off between efficiency, performance, and adversarial robustness of LLMs by comparing three prominent models with varying levels of complexity and efficiency -- Transformer++, Gated Linear Attention (GLA) Transformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span><span class='px-1 mx-1 bg-yellow-200'>The AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to challenge model robustness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that while the GLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate higher efficiency and either superior or comparative robustness on AdvGLUE tasks compared to Transformer++ across different attack levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span><span class='px-1 mx-1 bg-yellow-200'>These findings highlight the potential of simplified architectures to achieve a compelling balance between efficiency, performance, and adversarial robustness, offering valuable insights for applications where resource constraints and resilience to adversarial attacks are critical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04585v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04585v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EnJa: Ensemble Jailbreak on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As Large Language Models (LLMs) are increasingly being deployed in safety-critical applications, their vulnerability to potential jailbreaks -- malicious prompts that can disable the safety mechanism of LLMs -- has attracted growing research attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.859</span></span><span class='px-1 mx-1 bg-yellow-200'>While alignment methods have been proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can still be jailbroken by carefully crafted malicious prompts, producing content that violates policy regulations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing jailbreak attacks on LLMs can be categorized into prompt-level methods which make up stories/logic to circumvent safety alignment and token-level attack methods which leverage gradient methods to find adversarial tokens. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.886</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce the concept of Ensemble Jailbreak and explore methods that can integrate prompt-level and token-level jailbreak into a more powerful hybrid jailbreak attack. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.79</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we propose a novel EnJa attack to hide harmful instructions using prompt-level jailbreak, boost the attack success rate using a gradient-based attack, and connect the two types of jailbreak attacks via a template-based connector. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.901</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate the effectiveness of EnJa on several aligned models and show that it achieves a state-of-the-art attack success rate with fewer queries and is much stronger than any individual jailbreak. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.764</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03603v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03603v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span>It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections.The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing.Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts.We make WalledEval publicly available at https://github.com/walledai/walledevalA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03837v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03837v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have excelled at language understanding and generating human-level text.<span class='px-1 mx-1 bg-yellow-200'>However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.815</span></span>LLMs also inherently encode potential biases that can cause various harmful effects during interactions.Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive.<span class='px-1 mx-1 bg-yellow-200'>In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics.We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short.We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03907v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03907v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring RAG-based Vulnerability Augmentation with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Detecting vulnerabilities is a crucial task for maintaining the integrity, availability, and security of software systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.791</span></span><span class='px-1 mx-1 bg-yellow-200'>Utilizing DL-based models for vulnerability detection has become commonplace in recent years. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.79</span></span><span class='px-1 mx-1 bg-yellow-200'>However, such deep learning-based vulnerability detectors (DLVD) suffer from a shortage of sizable datasets to train effectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span><span class='px-1 mx-1 bg-yellow-200'>Data augmentation can potentially alleviate the shortage of data, but augmenting vulnerable code is challenging and requires designing a generative solution that maintains vulnerability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span><span class='px-1 mx-1 bg-yellow-200'>Hence, the work on generating vulnerable code samples has been limited and previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>Lately, large language models (LLMs) are being used for solving various code generation and comprehension tasks and have shown inspiring results, especially when fused with retrieval augmented generation (RAG).<span class='px-1 mx-1 bg-yellow-200'>In this study, we explore three different strategies to augment vulnerabilities both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span><span class='px-1 mx-1 bg-yellow-200'>We conducted an extensive evaluation of our proposed approach on three vulnerability datasets and three DLVD models, using two LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that our injection-based clustering-enhanced RAG method beats the baseline setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling (ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with 15K generated vulnerable samples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04125v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04125v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">HCI in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) and conversational assistants necessitates dynamic, scalable, and configurable conversational datasets for training and evaluation.These datasets must accommodate diverse user interaction modes, including text and voice, each presenting unique modeling challenges.Knowledge Graphs (KGs), with their structured and evolving nature, offer an ideal foundation for current and precise knowledge.Although human-curated KG-based conversational datasets exist, they struggle to keep pace with the rapidly changing user information needs.We present ConvKGYarn, a scalable method for generating up-to-date and configurable conversational KGQA datasets.<span class='px-1 mx-1 bg-yellow-200'>Qualitative psychometric analyses confirm our method can generate high-quality datasets rivaling a popular conversational KGQA dataset while offering it at scale and covering a wide range of human-interaction configurations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.887</span></span>We showcase its utility by testing LLMs on diverse conversations - exploring model behavior on conversational KGQA sets with different configurations grounded in the same KG fact set.Our results highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate parametric knowledge of LLMs, thus offering a robust solution to the constantly evolving landscape of conversational assistants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05948v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05948v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Animate, or Inanimate, That is the Question for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding.Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information.Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do?We then propose a systematic analysis via prompting approaches.In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts.<span class='px-1 mx-1 bg-yellow-200'>Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06332v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06332v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Academic collaboration on large language model studies increases overall but varies across disciplines
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Interdisciplinary collaboration is crucial for addressing complex scientific challenges.Recent advancements in large language models (LLMs) have shown significant potential in benefiting researchers across various fields.To explore the application of LLMs in scientific disciplines and their implications for interdisciplinary collaboration, we collect and analyze 50,391 papers from OpenAlex, an open-source platform for scholarly metadata.We first employ Shannon entropy to assess the diversity of collaboration in terms of authors' institutions and departments.Our results reveal that most fields have exhibited varying degrees of increased entropy following the release of ChatGPT, with Computer Science displaying a consistent increase.Other fields such as Social Science, Decision Science, Psychology, Engineering, Health Professions, and Business, Management & Accounting have shown minor to significant increases in entropy in 2024 compared to 2023.Statistical testing further indicates that the entropy in Computer Science, Decision Science, and Engineering is significantly lower than that in health-related fields like Medicine and Biochemistry, Genetics & Molecular Biology.In addition, our network analysis based on authors' affiliation information highlights the prominence of Computer Science, Medicine, and other Computer Science-related departments in LLM research.Regarding authors' institutions, our analysis reveals that entities such as Stanford University, Harvard University, University College London, and Google are key players, either dominating centrality measures or playing crucial roles in connecting research networks.<span class='px-1 mx-1 bg-yellow-200'>Overall, this study provides valuable insights into the current landscape and evolving dynamics of collaboration networks in LLM research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.868</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04163v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04163v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMREC: LLM Based Multi-Modal Recommender System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily.This surge in content presents unique challenges for designing effective recommender systems.Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences.This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques.The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation.The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods.The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model.Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information.<span class='px-1 mx-1 bg-yellow-200'>This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04211v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04211v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Are Social Sentiments Inherent in LLMs? An Empirical Study on Extraction of Inter-demographic Sentiments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are supposed to acquire unconscious human knowledge and feelings, such as social common sense and biases, by training models from large amounts of text.<span class='px-1 mx-1 bg-yellow-200'>However, it is not clear how much the sentiments of specific social groups can be captured in various LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span><span class='px-1 mx-1 bg-yellow-200'>In this study, we focus on social groups defined in terms of nationality, religion, and race/ethnicity, and validate the extent to which sentiments between social groups can be captured in and extracted from LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.794</span></span>Specifically, we input questions regarding sentiments from one group to another into LLMs, apply sentiment analysis to the responses, and compare the results with social surveys.The validation results using five representative LLMs showed higher correlations with relatively small p-values for nationalities and religions, whose number of data points were relatively large.<span class='px-1 mx-1 bg-yellow-200'>This result indicates that the LLM responses including the inter-group sentiments align well with actual social survey results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.835</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04293v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04293v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We study an emerging and intriguing problem of multimodal temporal event forecasting with large language models.<span class='px-1 mx-1 bg-yellow-200'>Compared to using text or graph modalities, the investigation of utilizing images for temporal event forecasting has not been fully explored, especially in the era of large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>To bridge this gap, we are particularly interested in two key questions of: 1) why images will help in temporal event forecasting, and 2) how to integrate images into the LLM-based forecasting framework.To answer these research questions, we propose to identify two essential functions that images play in the scenario of temporal event forecasting, i.e., highlighting and complementary.Then, we develop a novel framework, named MM-Forecast.It employs an Image Function Identification module to recognize these functions as verbal descriptions using multimodal large language models (MLLMs), and subsequently incorporates these function descriptions into LLM-based forecasting models.To evaluate our approach, we construct a new multimodal dataset, MidEast-TE-mm, by extending an existing event dataset MidEast-TE-mini with images.Empirical studies demonstrate that our MM-Forecast can correctly identify the image functions, and further more, incorporating these verbal function descriptions significantly improves the forecasting performance.The dataset, code, and prompts are available at https://github.com/LuminosityX/MM-Forecast.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04388v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04388v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators.Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions.Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population.Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels.<span class='px-1 mx-1 bg-yellow-200'>In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>We use advanced prompting techniques with varying complexity for AEQG.We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions.Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered.We also show that automated evaluation is not on par with human evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04394v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04394v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Recognizing Emotion Regulation Strategies from Human Behavior with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Human emotions are often not expressed directly, but regulated according to internal processes and social display rules. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span>For affective computing systems, an understanding of how users regulate their emotions can be highly useful, for example to provide feedback in job interview training, or in psychotherapeutic scenarios.However, at present no method to automatically classify different emotion regulation strategies in a cross-user scenario exists.At the same time, recent studies showed that instruction-tuned Large Language Models (LLMs) can reach impressive performance across a variety of affect recognition tasks such as categorical emotion recognition or sentiment analysis.<span class='px-1 mx-1 bg-yellow-200'>While these results are promising, it remains unclear to what extent the representational power of LLMs can be utilized in the more subtle task of classifying users' internal emotion regulation strategy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>To close this gap, we make use of the recently introduced \textsc{Deep} corpus for modeling the social display of the emotion shame, where each point in time is annotated with one of seven different emotion regulation classes.We fine-tune Llama2-7B as well as the recently introduced Gemma model using Low-rank Optimization on prompts generated from different sources of information on the \textsc{Deep} corpus.These include verbal and nonverbal behavior, person factors, as well as the results of an in-depth interview after the interaction.<span class='px-1 mx-1 bg-yellow-200'>Our results show, that a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation strategy with high accuracy (0.84) without needing access to data from post-interaction interviews. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span><span class='px-1 mx-1 bg-yellow-200'>This represents a significant improvement over previous approaches based on Bayesian Networks and highlights the importance of modeling verbal behavior in emotion regulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04420v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04420v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Competitive debate is a comprehensive and complex computational argumentation task.Large Language Models (LLMs) encounter hallucinations and lack competitiveness in this task.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span><span class='px-1 mx-1 bg-yellow-200'>Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents (Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span>These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary.To comprehensively evaluate framework performance, we construct the Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions.<span class='px-1 mx-1 bg-yellow-200'>We recruite ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking.Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans.Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding.However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information.<span class='px-1 mx-1 bg-yellow-200'>In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.746</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conversational Prompt Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompts are how humans communicate with LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>Informative prompts are essential for guiding LLMs to produce the desired output.However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use.We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks.CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt.The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction.Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs.The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples.A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts.The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04560v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04560v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span>These models can process multi-modal prompts, enabling them to generate more context-aware responses.However, this integration is not without challenges.One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks.These tasks require precise and reliable responses to ensure safe and effective operation.Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously.For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions.This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks.Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03515v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03515v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Base Station Siting: Intelligent Deployment based on Prompt or Agent
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Traditional base station siting (BSS) methods rely heavily on drive testing and user feedback, which are laborious and require extensive expertise in communication, networking, and optimization.As large language models (LLMs) and their associated technologies advance, particularly in the realms of prompt engineering and agent engineering, network optimization will witness a revolutionary approach.<span class='px-1 mx-1 bg-yellow-200'>This approach entails the strategic use of well-crafted prompts to infuse human experience and knowledge into these sophisticated LLMs, and the deployment of autonomous agents as a communication bridge to seamlessly connect the machine language based LLMs with human users using natural language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>This integration represents the future paradigm of artificial intelligence (AI) as a service and AI for more ease.As a preliminary exploration, this research first develops a novel LLM-empowered BSS optimization framework, and heuristically proposes four different potential implementations: the strategies based on Prompt-optimized LLM (PoL), human-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and Cooperative multiple LLM-based autonomous BSS agents (CLaBa).Through evaluation on real-world data, the experiments demonstrate that prompt-assisted LLMs and LLM-based agents can generate more efficient, cost-effective, and reliable network deployments, noticeably enhancing the efficiency of BSS optimization and reducing trivial manual participation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03631v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03631v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Multimodal Emotional Support Conversation Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span>Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support.<span class='px-1 mx-1 bg-yellow-200'>This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities.This dataset captures the intricate interplay of user emotions, system strategies, system emotion, and system responses, setting a new precedent in the field.<span class='px-1 mx-1 bg-yellow-200'>Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span><span class='px-1 mx-1 bg-yellow-200'>Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span>Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness.By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03650v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03650v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Digital Avatars: Framework Development and Their Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We present a novel prompting strategy for artificial intelligence driven digital avatars. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span>To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts.<span class='px-1 mx-1 bg-yellow-200'>To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>This pipeline effectively captures an individual's essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device.Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines.In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04068v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04068v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) can help writers build story worlds by generating world elements, such as factions, characters, and locations.However, making sense of many generated elements can be overwhelming.Moreover, if the user wants to precisely control aspects of generated elements that are difficult to specify verbally, prompting alone may be insufficient.<span class='px-1 mx-1 bg-yellow-200'>We introduce Patchview, a customizable LLM-powered system that visually aids worldbuilding by allowing users to interact with story concepts and elements through the physical metaphor of magnets and dust. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>Elements in Patchview are visually dragged closer to concepts with high relevance, facilitating sensemaking.The user can also steer the generation with verbally elusive concepts by indicating the desired position of the element between concepts.When the user disagrees with the LLM's visualization and generation, they can correct those by repositioning the element.These corrections can be used to align the LLM's future behaviors to the user's perception.<span class='px-1 mx-1 bg-yellow-200'>With a user study, we show that Patchview supports the sensemaking of world elements and steering of element generation, facilitating exploration during the worldbuilding process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>Patchview provides insights on how customizable visual representation can help sensemake, steer, and align generative AI model behaviors with the user's intentions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compromising Embodied Agents with Contextual Backdoor Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have transformed the development of embodied intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span>By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents.However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \method{}.By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects.These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment.To compromise the LLM's contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning.To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers.We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents.To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning.Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient.In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries.Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels.In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses.Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains.GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency.RAP was particularly effective for GPT-4o to answer Expert level questions.<span class='px-1 mx-1 bg-yellow-200'>Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02964v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02964v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Targeted Visual Prompting for Medical Visual Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation.However, simple visual errors cast doubt on the actual visual understanding abilities of these models.To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation.<span class='px-1 mx-1 bg-yellow-200'>To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to several baseline models.Our code and data are available at https://github.com/sergiotasconmorales/locvqallm.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03043v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03043v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out.<span class='px-1 mx-1 bg-yellow-200'>To do this, we first discuss the scope of the field of pragmatics and suggest a subdivision into discourse pragmatics and interactional pragmatics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span>We give a non-exhaustive overview of the phenomena of those two subdomains and the methods traditionally used to analyze them.<span class='px-1 mx-1 bg-yellow-200'>We subsequently consider the resulting heterogeneous set of phenomena and methods as a starting point for our survey of work on discourse pragmatics and interactional pragmatics in the context of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03074v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03074v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks.Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances.Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions.By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning.Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs.Code and data will be available soon.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03247v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03247v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.LLMs have achieved remarkable success in areas including code generation and vulnerability detection.However, they also exhibit numerous limitations and shortcomings.LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.<span class='px-1 mx-1 bg-yellow-200'>Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain.In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering.In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance.We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering.We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02479v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02479v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The FIFA World Cup in Qatar was discussed extensively in the news and on social media.Due to news reports with allegations of human rights violations, there were calls to boycott it.Wearing a OneLove armband was part of a planned protest activity.Controversy around the armband arose when FIFA threatened to sanction captains who wear it.To understand what topics Twitter users Tweeted about and what the opinion of German Twitter users was towards the OneLove armband, we performed an analysis of German Tweets published during the World Cup using in-context learning with LLMs.We validated the labels on human annotations.<span class='px-1 mx-1 bg-yellow-200'>We found that Twitter users initially discussed the armband's impact, LGBT rights, and politics; after the ban, the conversation shifted towards politics in sports in general, accompanied by a subtle shift in sentiment towards neutrality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.781</span></span>Our evaluation serves as a framework for future research to explore the impact of sports activism and evolving public sentiment.This is especially useful in settings where labeling datasets for specific opinions is unfeasible, such as when events are unfolding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02520v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02520v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DanModCap: Designing a Danmaku Moderation Tool for Video-Sharing Platforms that Leverages Impact Captions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Online video platforms have gained increased popularity due to their ability to support information consumption and sharing and the diverse social interactions they afford.Danmaku, a real-time commentary feature that overlays user comments on a video, has been found to improve user engagement, however, the use of Danmaku can lead to toxic behaviors and inappropriate comments.<span class='px-1 mx-1 bg-yellow-200'>To address these issues, we propose a proactive moderation approach inspired by Impact Captions, a visual technique used in East Asian variety shows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span><span class='px-1 mx-1 bg-yellow-200'>Impact Captions combine textual content and visual elements to construct emotional and cognitive resonance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>Within the context of this work, Impact Captions were used to guide viewers towards positive Danmaku-related activities and elicit more pro-social behaviors.Leveraging Impact Captions, we developed DanModCap, an moderation tool that collected and analyzed Danmaku and used it as input to large generative language models to produce Impact Captions.<span class='px-1 mx-1 bg-yellow-200'>Our evaluation of DanModCap demonstrated that Impact Captions reduced negative antagonistic emotions, increased users' desire to share positive content, and elicited self-control in Danmaku social action to fostering proactive community maintenance behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>Our approach highlights the benefits of using LLM-supported content moderation methods for proactive moderation in a large-scale live content contexts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02574v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02574v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The aspect-based sentiment analysis (ABSA) is a standard NLP task with numerous approaches and benchmarks, where large language models (LLM) represent the current state-of-the-art.We focus on ABSA subtasks based on Twitter/X data in underrepresented languages.On such narrow tasks, small tuned language models can often outperform universal large ones, providing available and cheap solutions.   <span class='px-1 mx-1 bg-yellow-200'>We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for classification of sentiment towards Russia and Ukraine in the context of the ongoing military conflict. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>The training/testing dataset was obtained from the academic API from Twitter/X during 2023, narrowed to the languages of the V4 countries (Czech Republic, Slovakia, Poland, Hungary).Then we measure their performance under a variety of settings including translations, sentiment targets, in-context learning and more, using GPT4 as a reference model.We document several interesting phenomena demonstrating, among others, that some models are much better fine-tunable on multilingual Twitter tasks than others, and that they can reach the SOTA level with a very small training set.Finally we identify combinations of settings providing the best results.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02044v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02044v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Constructing Mechanical Design Agent Based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Since ancient times, mechanical design aids have been developed to assist human users, aimed at improving the efficiency and effectiveness of design.However, even with the widespread use of contemporary Computer-Aided Design (CAD) systems, there are still high learning costs, repetitive work, and other challenges.<span class='px-1 mx-1 bg-yellow-200'>In recent years, the rise of Large Language Models (LLMs) has introduced new productivity opportunities to the field of mechanical design. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>Yet, it remains unrealistic to rely on LLMs alone to complete mechanical design tasks directly.Through a series of explorations, we propose a method for constructing a comprehensive Mechanical Design Agent (MDA) by guiding LLM learning.To verify the validity of our proposed method, we conducted a series of experiments and presented relevant cases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>There is a growing interest in academia to use these models to simulate and study human behaviors.However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture.Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data.<span class='px-1 mx-1 bg-yellow-200'>This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.94</span></span>Our methodology is based on the studies of Miyamoto et al.<span class='px-1 mx-1 bg-yellow-200'>(2010), which identified distinctive emotional indicators in Japanese and American human responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span><span class='px-1 mx-1 bg-yellow-200'>We first administer their mixed emotion survey to five different LLMs and analyze their outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span><span class='px-1 mx-1 bg-yellow-200'>Second, we experiment with contextual variables to explore variations in responses considering both language and speaker origin. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>Thirdly, we expand our investigation to encompass additional East Asian and Western European origin languages to gauge their alignment with their respective cultures, anticipating a closer fit.We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02143v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02143v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Large Language Models in Social Sciences</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) and conversational assistants necessitates dynamic, scalable, and configurable conversational datasets for training and evaluation.These datasets must accommodate diverse user interaction modes, including text and voice, each presenting unique modeling challenges.Knowledge Graphs (KGs), with their structured and evolving nature, offer an ideal foundation for current and precise knowledge.Although human-curated KG-based conversational datasets exist, they struggle to keep pace with the rapidly changing user information needs.We present ConvKGYarn, a scalable method for generating up-to-date and configurable conversational KGQA datasets.<span class='px-1 mx-1 bg-yellow-200'>Qualitative psychometric analyses confirm our method can generate high-quality datasets rivaling a popular conversational KGQA dataset while offering it at scale and covering a wide range of human-interaction configurations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>We showcase its utility by testing LLMs on diverse conversations - exploring model behavior on conversational KGQA sets with different configurations grounded in the same KG fact set.Our results highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate parametric knowledge of LLMs, thus offering a robust solution to the constantly evolving landscape of conversational assistants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05948v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05948v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building Decision Making Models Through Language Model Regime
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose a novel approach for decision making problems leveraging the generalization capabilities of large language models (LLMs).Traditional methods such as expert systems, planning algorithms, and reinforcement learning often exhibit limited generalization, typically requiring the training of new models for each unique task.<span class='px-1 mx-1 bg-yellow-200'>In contrast, LLMs demonstrate remarkable success in generalizing across varied language tasks, inspiring a new strategy for training decision making models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Our approach, referred to as "Learning then Using" (LTU), entails a two-stage process.Initially, the \textit{learning} phase develops a robust foundational decision making model by integrating diverse knowledge from various domains and decision making contexts.The subsequent \textit{using} phase refines this foundation model for specific decision making scenarios.Distinct from other studies that employ LLMs for decision making through supervised learning, our LTU method embraces a versatile training methodology that combines broad pre-training with targeted fine-tuning.Experiments in e-commerce domains such as advertising and search optimization have shown that LTU approach outperforms traditional supervised learning regimes in decision making capabilities and generalization.The LTU approach is the first practical training architecture for both single-step and multi-step decision making tasks combined with LLMs, which can be applied beyond game and robot domains.It provides a robust and adaptable framework for decision making, enhances the effectiveness and flexibility of various systems in tackling various challenges.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Animate, or Inanimate, That is the Question for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span>Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information.Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do?We then propose a systematic analysis via prompting approaches.In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts.<span class='px-1 mx-1 bg-yellow-200'>Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span>Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06332v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06332v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Are Social Sentiments Inherent in LLMs? An Empirical Study on Extraction of Inter-demographic Sentiments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are supposed to acquire unconscious human knowledge and feelings, such as social common sense and biases, by training models from large amounts of text.<span class='px-1 mx-1 bg-yellow-200'>However, it is not clear how much the sentiments of specific social groups can be captured in various LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span><span class='px-1 mx-1 bg-yellow-200'>In this study, we focus on social groups defined in terms of nationality, religion, and race/ethnicity, and validate the extent to which sentiments between social groups can be captured in and extracted from LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span>Specifically, we input questions regarding sentiments from one group to another into LLMs, apply sentiment analysis to the responses, and compare the results with social surveys.The validation results using five representative LLMs showed higher correlations with relatively small p-values for nationalities and religions, whose number of data points were relatively large.<span class='px-1 mx-1 bg-yellow-200'>This result indicates that the LLM responses including the inter-group sentiments align well with actual social survey results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.779</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04293v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04293v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators.Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions.Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population.Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels.In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy.We use advanced prompting techniques with varying complexity for AEQG.We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions.Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered.<span class='px-1 mx-1 bg-yellow-200'>We also show that automated evaluation is not on par with human evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04394v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04394v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the NeuBAROCO Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper explores the question of how accurately current large language models can perform logical reasoning in natural language, with an emphasis on whether these models exhibit reasoning biases similar to humans.Specifically, our study focuses on syllogistic reasoning, a form of deductive reasoning extensively studied in cognitive science as a natural form of human reasoning.We present a syllogism dataset called NeuBAROCO, which consists of syllogistic reasoning problems in English and Japanese.This dataset was originally designed for psychological experiments to assess human reasoning capabilities using various forms of syllogisms.<span class='px-1 mx-1 bg-yellow-200'>Our experiments with leading large language models indicate that these models exhibit reasoning biases similar to humans, along with other error tendencies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span>Notably, there is significant room for improvement in reasoning problems where the relationship between premises and hypotheses is neither entailment nor contradiction.We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process.Our analysis using this method suggests that the primary limitations of LLMs lie in the reasoning process itself rather than the interpretation of syllogisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Recognizing Emotion Regulation Strategies from Human Behavior with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Human emotions are often not expressed directly, but regulated according to internal processes and social display rules. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>For affective computing systems, an understanding of how users regulate their emotions can be highly useful, for example to provide feedback in job interview training, or in psychotherapeutic scenarios.<span class='px-1 mx-1 bg-yellow-200'>However, at present no method to automatically classify different emotion regulation strategies in a cross-user scenario exists. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>At the same time, recent studies showed that instruction-tuned Large Language Models (LLMs) can reach impressive performance across a variety of affect recognition tasks such as categorical emotion recognition or sentiment analysis.<span class='px-1 mx-1 bg-yellow-200'>While these results are promising, it remains unclear to what extent the representational power of LLMs can be utilized in the more subtle task of classifying users' internal emotion regulation strategy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span><span class='px-1 mx-1 bg-yellow-200'>To close this gap, we make use of the recently introduced \textsc{Deep} corpus for modeling the social display of the emotion shame, where each point in time is annotated with one of seven different emotion regulation classes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.792</span></span>We fine-tune Llama2-7B as well as the recently introduced Gemma model using Low-rank Optimization on prompts generated from different sources of information on the \textsc{Deep} corpus.These include verbal and nonverbal behavior, person factors, as well as the results of an in-depth interview after the interaction.Our results show, that a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation strategy with high accuracy (0.84) without needing access to data from post-interaction interviews.<span class='px-1 mx-1 bg-yellow-200'>This represents a significant improvement over previous approaches based on Bayesian Networks and highlights the importance of modeling verbal behavior in emotion regulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04420v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04420v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Competitive debate is a comprehensive and complex computational argumentation task.Large Language Models (LLMs) encounter hallucinations and lack competitiveness in this task.To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate.Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents (Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary.To comprehensively evaluate framework performance, we construct the Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions.We recruite ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans.<span class='px-1 mx-1 bg-yellow-200'>The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans.Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding.However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information.In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience).<span class='px-1 mx-1 bg-yellow-200'>We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Multimodal Emotional Support Conversation Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.761</span></span>Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support.<span class='px-1 mx-1 bg-yellow-200'>This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities.This dataset captures the intricate interplay of user emotions, system strategies, system emotion, and system responses, setting a new precedent in the field.<span class='px-1 mx-1 bg-yellow-200'>Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation.<span class='px-1 mx-1 bg-yellow-200'>Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span><span class='px-1 mx-1 bg-yellow-200'>By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03650v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03650v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and Controllable Attributes for Improving Biomedical Text Readability
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this system report, we describe the models and methods we used for our participation in the PLABA2023 task on biomedical abstract simplification, part of the TAC 2023 tracks.The system outputs we submitted come from the following three categories: 1) domain fine-tuned T5-like models including Biomedical-T5 and Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes (via tokens) BART-w-CTs; 3) ChatGPTprompting.We also present the work we carried out for this task on BioGPT finetuning.In the official automatic evaluation using SARI scores, BeeManc ranks 2nd among all teams and our model LaySciFive ranks 3rd among all 13 evaluated systems.<span class='px-1 mx-1 bg-yellow-200'>In the official human evaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score 92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It also produced a high score 91.57 on Fluency in comparison to the highest score 93.53. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span>In the second round of submissions, our team using ChatGPT-prompting ranks the 2nd in several categories including simplified term accuracy score 92.26 and completeness score 96.58, and a very similar score on faithfulness score 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations.Our codes, fine-tuned models, prompts, and data splits from the system development stage will be available at https://github.com/ HECTA-UoM/PLABA-MU</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03871v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03871v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have excelled at language understanding and generating human-level text.However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text.LLMs also inherently encode potential biases that can cause various harmful effects during interactions.Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive.In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs.We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics.We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short.<span class='px-1 mx-1 bg-yellow-200'>We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03907v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03907v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Words to Worth: Newborn Article Impact Prediction with LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As the academic landscape expands, the challenge of efficiently identifying potentially high-impact articles among the vast number of newly published works becomes critical.This paper introduces a promising approach, leveraging the capabilities of fine-tuned LLMs to predict the future impact of newborn articles solely based on titles and abstracts.Moving beyond traditional methods heavily reliant on external information, the proposed method discerns the shared semantic features of highly impactful papers from a large collection of title-abstract and potential impact pairs.These semantic features are further utilized to regress an improved metric, TNCSI_SP, which has been endowed with value, field, and time normalization properties.Additionally, a comprehensive dataset has been constructed and released for fine-tuning the LLM, containing over 12,000 entries with corresponding titles, abstracts, and TNCSI_SP.The quantitative results, with an NDCG@20 of 0.901, demonstrate that the proposed approach achieves state-of-the-art performance in predicting the impact of newborn articles when compared to competitive counterparts.Finally, we demonstrate a real-world application for predicting the impact of newborn journal articles to demonstrate its noteworthy practical value.<span class='px-1 mx-1 bg-yellow-200'>Overall, our findings challenge existing paradigms and propose a shift towards a more content-focused prediction of academic impact, offering new insights for assessing newborn article impact. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03934v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03934v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Rule-Based Insights Enhance LLMs for Radiology Report Classification? Introducing the RadPrompt Methodology
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing imaging models capable of detecting pathologies from chest X-rays can be cost and time-prohibitive for large datasets as it requires supervision to attain state-of-the-art performance.Instead, labels extracted from radiology reports may serve as distant supervision since these are routinely generated as part of clinical practice.Despite their widespread use, current rule-based methods for label extraction rely on extensive rule sets that are limited in their robustness to syntactic variability.To alleviate these limitations, we introduce RadPert, a rule-based system that integrates an uncertainty-aware information schema with a streamlined set of rules, enhancing performance.Additionally, we have developed RadPrompt, a multi-turn prompting strategy that leverages RadPert to bolster the zero-shot predictive capabilities of large language models, achieving a statistically significant improvement in weighted average F1 score over GPT-4 Turbo.Most notably, RadPrompt surpasses both its underlying models, showcasing the synergistic potential of LLMs with rule-based models.<span class='px-1 mx-1 bg-yellow-200'>We have evaluated our methods on two English Corpora: the MIMIC-CXR gold-standard test set and a gold-standard dataset collected from the Cambridge University Hospitals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04121v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04121v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) excel in various natural language processing tasks, but leveraging them for dense passage embedding remains challenging.<span class='px-1 mx-1 bg-yellow-200'>This is due to their causal attention mechanism and the misalignment between their pre-training objectives and the text ranking tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>Despite some recent efforts to address these issues, existing frameworks for LLM-based text embeddings have been limited by their support for only a limited range of LLM architectures and fine-tuning strategies, limiting their practical application and versatility.In this work, we introduce the Unified framework for Large Language Model Embedding (ULLME), a flexible, plug-and-play implementation that enables bidirectional attention across various LLMs and supports a range of fine-tuning strategies.We also propose Generation-augmented Representation Learning (GRL), a novel fine-tuning method to boost LLMs for text embedding tasks.GRL enforces consistency between representation-based and generation-based relevance scores, leveraging LLMs' powerful generative abilities for learning passage embeddings.To showcase our framework's flexibility and effectiveness, we release three pre-trained models from ULLME with different backbone architectures, ranging from 1.5B to 8B parameters, all of which demonstrate strong performance on the Massive Text Embedding Benchmark.Our framework is publicly available at: https://github.com/nlp-uoregon/ullme.A demo video for ULLME can also be found at https://rb.gy/ws1ile.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03402v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03402v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the Generalization of Preference Learning with DPO
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have demonstrated remarkable capabilities but often struggle to align with human preferences, leading to harmful or undesirable outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span><span class='px-1 mx-1 bg-yellow-200'>Preference learning, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for ensuring that LLMs align with human values. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>Despite the widespread adoption in real-world systems, a thorough theoretical understanding of the generalization guarantees for these models remain lacking.This paper bridges that gap by introducing a new theoretical framework to analyze the generalization guarantees of models trained with direct preference optimization (DPO).While existing generalization theory often focuses on overparameterized models achieving near-optimal loss or models independent of the training process, our framework rigorously assesses how well models generalize after a finite number of gradient steps, reflecting real-world LLM training practices.By analyzing the reward margin associated with each sample and its trajectory throughout training, we can effectively bound the generalization error.We derive learning guarantees showing that, under specific conditions, models trained with DPO can correctly discern preferred responses on unseen data with high probability.These insights are empirically validated on contemporary LLMs, underscoring the practical relevance of our theoretical findings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03459v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03459v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compromising Embodied Agents with Contextual Backdoor Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have transformed the development of embodied intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span>By providing a few contextual demonstrations, developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents.However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called \method{}.By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects.These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment.To compromise the LLM's contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning.To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers.We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents.To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning.Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.878</span></span>Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient.In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries.Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels.In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses.Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains.GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency.RAP was particularly effective for GPT-4o to answer Expert level questions.Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02964v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02964v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out.To do this, we first discuss the scope of the field of pragmatics and suggest a subdivision into discourse pragmatics and interactional pragmatics.We give a non-exhaustive overview of the phenomena of those two subdomains and the methods traditionally used to analyze them.<span class='px-1 mx-1 bg-yellow-200'>We subsequently consider the resulting heterogeneous set of phenomena and methods as a starting point for our survey of work on discourse pragmatics and interactional pragmatics in the context of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03074v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03074v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conditioning LLMs with Emotion in Neural Machine Translation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT).In this work, we propose a novel MT pipeline that integrates emotion information extracted from a Speech Emotion Recognition (SER) model into LLMs to enhance translation quality.We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model.Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations.<span class='px-1 mx-1 bg-yellow-200'>Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03150v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03150v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Humans are not homo economicus (i.e., rational economic beings). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.808</span></span><span class='px-1 mx-1 bg-yellow-200'>As humans, we exhibit systematic behavioral biases such as loss aversion, anchoring, framing, etc., which lead us to make suboptimal economic decisions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.72</span></span><span class='px-1 mx-1 bg-yellow-200'>Insofar as such biases may be embedded in text data on which large language models (LLMs) are trained, to what extent are LLMs prone to the same behavioral biases? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span>Understanding these biases in LLMs is crucial for deploying LLMs to support human decision-making.We propose utility theory-a paradigm at the core of modern economic theory-as an approach to evaluate the economic biases of LLMs.Utility theory enables the quantification and comparison of economic behavior against benchmarks such as perfect rationality or human behavior.To demonstrate our approach, we quantify and compare the economic behavior of a variety of open- and closed-source LLMs.<span class='px-1 mx-1 bg-yellow-200'>We find that the economic behavior of current LLMs is neither entirely human-like nor entirely economicus-like. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span>We also find that most current LLMs struggle to maintain consistent economic behavior across settings.<span class='px-1 mx-1 bg-yellow-200'>Finally, we illustrate how our approach can measure the effect of interventions such as prompting on economic biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02784v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02784v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Performance of Large Language Models for SDG Mapping (Technical Report)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The use of large language models (LLMs) is expanding rapidly, and open-source versions are becoming available, offering users safer and more adaptable options.These models enable users to protect data privacy by eliminating the need to provide data to third parties and can be customized for specific tasks.<span class='px-1 mx-1 bg-yellow-200'>In this study, we compare the performance of various language models on the Sustainable Development Goal (SDG) mapping task, using the output of GPT-4o as the baseline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>The selected open-source models for comparison include Mixtral, LLaMA 2, LLaMA 3, Gemma, and Qwen2.Additionally, GPT-4o-mini, a more specialized version of GPT-4o, was included to extend the comparison.Given the multi-label nature of the SDG mapping task, we employed metrics such as F1 score, precision, and recall with micro-averaging to evaluate different aspects of the models' performance.These metrics are derived from the confusion matrix to ensure a comprehensive evaluation.We provide a clear observation and analysis of each model's performance by plotting curves based on F1 score, precision, and recall at different thresholds.According to the results of this experiment, LLaMA 2 and Gemma still have significant room for improvement.The other four models do not exhibit particularly large differences in performance.The outputs from all seven models are available on Zenodo: https://doi.org/10.5281/zenodo.12789375.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02201v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02201v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The FIFA World Cup in Qatar was discussed extensively in the news and on social media.Due to news reports with allegations of human rights violations, there were calls to boycott it.Wearing a OneLove armband was part of a planned protest activity.Controversy around the armband arose when FIFA threatened to sanction captains who wear it.To understand what topics Twitter users Tweeted about and what the opinion of German Twitter users was towards the OneLove armband, we performed an analysis of German Tweets published during the World Cup using in-context learning with LLMs.We validated the labels on human annotations.<span class='px-1 mx-1 bg-yellow-200'>We found that Twitter users initially discussed the armband's impact, LGBT rights, and politics; after the ban, the conversation shifted towards politics in sports in general, accompanied by a subtle shift in sentiment towards neutrality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.848</span></span><span class='px-1 mx-1 bg-yellow-200'>Our evaluation serves as a framework for future research to explore the impact of sports activism and evolving public sentiment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.883</span></span>This is especially useful in settings where labeling datasets for specific opinions is unfeasible, such as when events are unfolding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02520v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02520v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DanModCap: Designing a Danmaku Moderation Tool for Video-Sharing Platforms that Leverages Impact Captions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Online video platforms have gained increased popularity due to their ability to support information consumption and sharing and the diverse social interactions they afford.Danmaku, a real-time commentary feature that overlays user comments on a video, has been found to improve user engagement, however, the use of Danmaku can lead to toxic behaviors and inappropriate comments.<span class='px-1 mx-1 bg-yellow-200'>To address these issues, we propose a proactive moderation approach inspired by Impact Captions, a visual technique used in East Asian variety shows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span><span class='px-1 mx-1 bg-yellow-200'>Impact Captions combine textual content and visual elements to construct emotional and cognitive resonance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.793</span></span><span class='px-1 mx-1 bg-yellow-200'>Within the context of this work, Impact Captions were used to guide viewers towards positive Danmaku-related activities and elicit more pro-social behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>Leveraging Impact Captions, we developed DanModCap, an moderation tool that collected and analyzed Danmaku and used it as input to large generative language models to produce Impact Captions.<span class='px-1 mx-1 bg-yellow-200'>Our evaluation of DanModCap demonstrated that Impact Captions reduced negative antagonistic emotions, increased users' desire to share positive content, and elicited self-control in Danmaku social action to fostering proactive community maintenance behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>Our approach highlights the benefits of using LLM-supported content moderation methods for proactive moderation in a large-scale live content contexts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02574v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02574v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages.There is a growing interest in academia to use these models to simulate and study human behaviors.However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture.<span class='px-1 mx-1 bg-yellow-200'>Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.883</span></span><span class='px-1 mx-1 bg-yellow-200'>This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.847</span></span>Our methodology is based on the studies of Miyamoto et al.<span class='px-1 mx-1 bg-yellow-200'>(2010), which identified distinctive emotional indicators in Japanese and American human responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span><span class='px-1 mx-1 bg-yellow-200'>We first administer their mixed emotion survey to five different LLMs and analyze their outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.728</span></span>Second, we experiment with contextual variables to explore variations in responses considering both language and speaker origin.<span class='px-1 mx-1 bg-yellow-200'>Thirdly, we expand our investigation to encompass additional East Asian and Western European origin languages to gauge their alignment with their respective cultures, anticipating a closer fit. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span><span class='px-1 mx-1 bg-yellow-200'>We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02143v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02143v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs in Education Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Med42-v2: A Suite of Clinical LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Med42-v2 introduces a suite of clinical large language models (LLMs) designed to address the limitations of generic models in healthcare settings.These models are built on Llama3 architecture and fine-tuned using specialized clinical data.They underwent multi-stage preference alignment to effectively respond to natural prompts.While generic models are often preference-aligned to avoid answering clinical queries as a precaution, Med42-v2 is specifically trained to overcome this limitation, enabling its use in clinical settings.Med42-v2 models demonstrate superior performance compared to the original Llama3 models in both 8B and 70B parameter configurations and GPT-4 across various medical benchmarks.<span class='px-1 mx-1 bg-yellow-200'>These LLMs are developed to understand clinical queries, perform reasoning tasks, and provide valuable assistance in clinical environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>The models are now publicly available at \href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06142v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06142v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities.<span class='px-1 mx-1 bg-yellow-200'>There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span>In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans.We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning?(2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context?(3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement?Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT).Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06318v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06318v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Healthcare through Large Language Models: A Study on Medical Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, the application of Large Language Models (LLMs) in healthcare has shown significant promise in improving the accessibility and dissemination of medical knowledge.<span class='px-1 mx-1 bg-yellow-200'>This paper presents a detailed study of various LLMs trained on the MedQuAD medical question-answering dataset, with a focus on identifying the most effective model for providing accurate medical information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>Among the models tested, the Sentence-t5 combined with Mistral 7B demonstrated superior performance, achieving a precision score of 0.762.This model's enhanced capabilities are attributed to its advanced pretraining techniques, robust architecture, and effective prompt construction methodologies.By leveraging these strengths, the Sentence-t5 + Mistral 7B model excels in understanding and generating precise medical answers.<span class='px-1 mx-1 bg-yellow-200'>Our findings highlight the potential of integrating sophisticated LLMs in medical contexts to facilitate efficient and accurate medical knowledge retrieval, thus significantly enhancing patient education and support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.555</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04138v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04138v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs).Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated).<span class='px-1 mx-1 bg-yellow-200'>This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>In this paper, we present $\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished.Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation.This might be useful in some domains like education, where any LLM intervention is usually prohibited.Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains.LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New.The video describing our system is available at https://youtu.be/E8eT_bE7k8c.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04284v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04284v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Open-domain Implicit Format Control for Large Language Model Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Controlling the format of outputs generated by large language models (LLMs) is a critical functionality in various applications.Current methods typically employ constrained decoding with rule-based automata or fine-tuning with manually crafted format instructions, both of which struggle with open-domain format requirements.To address this limitation, we introduce a novel framework for controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.<span class='px-1 mx-1 bg-yellow-200'>This study investigates LLMs' capabilities to follow open-domain, one-shot constraints and replicate the format of the example answers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>We observe that this is a non-trivial problem for current LLMs.We also develop a dataset collection methodology for supervised fine-tuning that enhances the open-domain format control of LLMs without degrading output quality, as well as a benchmark on which we evaluate both the helpfulness and format correctness of LLM outputs.The resulting datasets, named OIFC-SFT, along with the related code, will be made publicly available at https://github.com/cofe-ai/OIFC.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04392v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04392v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions.<span class='px-1 mx-1 bg-yellow-200'>Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels.In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy.We use advanced prompting techniques with varying complexity for AEQG.<span class='px-1 mx-1 bg-yellow-200'>We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.691</span></span>We also show that automated evaluation is not on par with human evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04394v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04394v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the NeuBAROCO Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper explores the question of how accurately current large language models can perform logical reasoning in natural language, with an emphasis on whether these models exhibit reasoning biases similar to humans.Specifically, our study focuses on syllogistic reasoning, a form of deductive reasoning extensively studied in cognitive science as a natural form of human reasoning.We present a syllogism dataset called NeuBAROCO, which consists of syllogistic reasoning problems in English and Japanese.This dataset was originally designed for psychological experiments to assess human reasoning capabilities using various forms of syllogisms.Our experiments with leading large language models indicate that these models exhibit reasoning biases similar to humans, along with other error tendencies.Notably, there is significant room for improvement in reasoning problems where the relationship between premises and hypotheses is neither entailment nor contradiction.<span class='px-1 mx-1 bg-yellow-200'>We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.563</span></span>Our analysis using this method suggests that the primary limitations of LLMs lie in the reasoning process itself rather than the interpretation of syllogisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for cross-language code clone detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the involvement of multiple programming languages in modern software development, cross-lingual code clone detection has gained traction with the software engineering community.Numerous studies have explored this topic, proposing various promising approaches.Inspired by the significant advances in machine learning in recent years, particularly Large Language Models (LLMs), which have demonstrated their ability to tackle various tasks, this paper revisits cross-lingual code clone detection.   We investigate the capabilities of four (04) LLMs and eight (08) prompts for the identification of cross-lingual code clones.Additionally, we evaluate a pre-trained embedding model to assess the effectiveness of the generated representations for classifying clone and non-clone pairs.Both studies (based on LLMs and Embedding models) are evaluated using two widely used cross-lingual datasets, XLCoST and CodeNet.<span class='px-1 mx-1 bg-yellow-200'>Our results show that LLMs can achieve high F1 scores, up to 0.98, for straightforward programming examples (e.g., from XLCoST). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.533</span></span>However, they not only perform less well on programs associated with complex programming challenges but also do not necessarily understand the meaning of code clones in a cross-lingual setting.We show that embedding models used to represent code fragments from different programming languages in the same representation space enable the training of a basic classifier that outperforms all LLMs by ~2 and ~24 percentage points on the XLCoST and CodeNet datasets, respectively.This finding suggests that, despite the apparent capabilities of LLMs, embeddings provided by embedding models offer suitable representations to achieve state-of-the-art performance in cross-lingual code clone detection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04430v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04430v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding.However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information.In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience).<span class='px-1 mx-1 bg-yellow-200'>We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.582</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conversational Prompt Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompts are how humans communicate with LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>Informative prompts are essential for guiding LLMs to produce the desired output.However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use.We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks.CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt.The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction.Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs.The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples.A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts.The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04560v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04560v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Arctic-TILT. Business Document Understanding at Sub-Billion Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The vast portion of workloads employing LLMs involves answering questions grounded on PDF or scan content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span>We introduce the Arctic-TILT achieving accuracy on par with models 1000$\times$ its size on these use cases.It can be fine-tuned and deployed on a single 24GB GPU, lowering operational costs while processing Visually Rich Documents with up to 400k tokens.The model establishes state-of-the-art results on seven diverse Document Understanding benchmarks, as well as provides reliable confidence scores and quick inference, which are essential for processing files in large-scale or time-sensitive enterprise environments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04632v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04632v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EXAONE 3.0 7.8B Instruction Tuned Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research.Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations.<span class='px-1 mx-1 bg-yellow-200'>Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.526</span></span>Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning.With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI.Our EXAONE 3.0 instruction-tuned model is available at https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03541v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03541v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Natural language processing (NLP) has seen significant advancements with the advent of large language models (LLMs).However, substantial improvements are still needed for languages other than English, especially for specific domains like the applications of Mercosur Common Nomenclature (NCM), a Brazilian Harmonized System (HS).To address this gap, this study uses TeenyTineLLaMA, a foundational Portuguese LLM, as an LLM source to implement the NCM application processing.Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT) technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.<span class='px-1 mx-1 bg-yellow-200'>This approach retains the chain-of-thought (CoT) methodology for prompt development in a more concise and streamlined manner, utilizing brief and focused documents for training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.577</span></span>The proposed model demonstrates an efficient and cost-effective alternative for fine-tuning smaller LLMs, significantly outperforming TeenyTineLLaMA and ChatGPT-4 in the same task.Although the research focuses on NCM applications, the methodology can be easily adapted for HS applications worldwide.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03936v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03936v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Digital Avatars: Framework Development and Their Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present a novel prompting strategy for artificial intelligence driven digital avatars.To better quantify how our prompting strategy affects anthropomorphic features like humor, authenticity, and favorability we present Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a large language model (LLM) candidate over competitors answering the same or similar prompts.<span class='px-1 mx-1 bg-yellow-200'>To visualize the responses of our LLM, and the effectiveness of our prompting strategy we propose an end-to-end framework for creating high-fidelity artificial intelligence (AI) driven digital avatars. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.513</span></span>This pipeline effectively captures an individual's essence for interaction and our streaming algorithm delivers a high-quality digital avatar with real-time audio-video streaming from server to mobile device.Both our visualization tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have state-of-the-art humor, authenticity, and favorability outperforming all competitors and baselines.In the case of our Donald Trump and Joe Biden avatars, their authenticity and favorability are rated higher than even their real-world equivalents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04068v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04068v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As content generated by Large Language Model (LLM) has grown exponentially, the ability to accurately identify and fingerprint such text has become increasingly crucial.In this work, we introduce a novel black-box approach for fingerprinting LLMs, achieving an impressive 72% accuracy in identifying the correct family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of LLMs.We present an evolutionary strategy that leverages the capabilities of one LLM to discover the most salient features for identifying other LLMs.Our method employs a unique "Hide and Seek" algorithm, where an Auditor LLM generates discriminative prompts, and a Detective LLM analyzes the responses to fingerprint the target models.This approach not only demonstrates the feasibility of LLM-driven model identification but also reveals insights into the semantic manifolds of different LLM families.<span class='px-1 mx-1 bg-yellow-200'>By iteratively refining prompts through in-context learning, our system uncovers subtle distinctions between model outputs, providing a powerful tool for LLM analysis and verification. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.546</span></span>This research opens new avenues for understanding LLM behavior and has significant implications for model attribution, security, and the broader field of AI transparency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02871v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02871v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education.Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient.In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries.<span class='px-1 mx-1 bg-yellow-200'>Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.527</span></span>In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses.<span class='px-1 mx-1 bg-yellow-200'>Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency.<span class='px-1 mx-1 bg-yellow-200'>RAP was particularly effective for GPT-4o to answer Expert level questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.527</span></span><span class='px-1 mx-1 bg-yellow-200'>Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02964v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02964v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning.<span class='px-1 mx-1 bg-yellow-200'>This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.561</span></span>Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata.We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts.Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms.To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms.The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02999v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02999v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Targeted Visual Prompting for Medical Visual Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures.Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation.However, simple visual errors cast doubt on the actual visual understanding abilities of these models.<span class='px-1 mx-1 bg-yellow-200'>To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.537</span></span><span class='px-1 mx-1 bg-yellow-200'>To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.565</span></span>By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to several baseline models.Our code and data are available at https://github.com/sergiotasconmorales/locvqallm.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03043v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03043v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities.Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor parameter changes or Pre-Trained (PT) with substantial parameter shifts.However, existing methods often manually assign the model importance, rendering them feasible only for LLMs with similar parameter alterations, such as multiple FT LLMs.The diverse parameter changed ranges between FT and PT LLMs pose challenges for current solutions in empirically determining the optimal combination.In this paper, we make a pioneering effort to broaden the applicability of merging techniques from FT to PT LLMs.We initially examine the efficacy of current methods in merging FT and PT LLMs, discovering that they struggle to deal with PT LLMs.Subsequently, we introduce an approach based on WeIght DisENtanglement (WIDEN) to effectively extend the merging scope, which first disentangles model weights into magnitude and direction components, and then performs adaptive fusion by considering their respective contributions.In the experiments, we merge Qwen1.5-Chat (an FT LLM with instruction-following skills) with Sailor (a PT LLM with multilingual abilities) across 7B and 14B model scales.Results reveal that: (1) existing solutions usually fail when merging Sailor, either losing both abilities or only retaining instruction-following skills; (2) WIDEN successfully injects the multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in Southeast Asian languages, achieving enhancements in the fundamental capabilities.<span class='px-1 mx-1 bg-yellow-200'>In light of previous research, we also merge multiple 13B FT LLMs and observe that WIDEN achieves a balanced amalgamation of instruction following, mathematical reasoning, and code generation skills. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.587</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03092v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03092v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks.Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances.Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions.By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning.Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs.Code and data will be available soon.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03247v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03247v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Synthesizing Text-to-SQL Data from Weak and Strong LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks.In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models).The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span>The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03256v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03256v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks.However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses.<span class='px-1 mx-1 bg-yellow-200'>To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.545</span></span>Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios.To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in real retrieval scenarios.Concretely, we explore and simulate error types across diverse context combinations and learn how to avoid these negative signals through preference optimization methods.Simultaneously, by adjusting the balance between response length and the proportion of preference data representing different behavior patterns, we enhance the adherence capabilities and noise robustness of LLMs in a balanced manner.Experimental results show that KaPO outperforms previous methods for handling knowledge conflicts by over 37%, while also exhibiting robust generalization across various out-of-distribution datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03297v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03297v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>This study investigates whether such constraints on generation space impact LLMs' abilities, including reasoning and domain knowledge comprehension. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span>Specifically, we evaluate LLMs' performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks.Surprisingly, we observe a significant decline in LLMs' reasoning abilities under format restrictions.Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02442v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02442v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.LLMs have achieved remarkable success in areas including code generation and vulnerability detection.However, they also exhibit numerous limitations and shortcomings.LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.<span class='px-1 mx-1 bg-yellow-200'>Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.56</span></span>It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain.<span class='px-1 mx-1 bg-yellow-200'>In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.573</span></span><span class='px-1 mx-1 bg-yellow-200'>In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.523</span></span>We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering.<span class='px-1 mx-1 bg-yellow-200'>We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.528</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02479v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02479v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference.Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical.To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning.However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism.<span class='px-1 mx-1 bg-yellow-200'>To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured.For each category, representative methods are elaborated to describe the landscape of relevant research.In addition, comparison between latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations.Finally, we summarize the open challenges and propose the promosing avenues for future studies.All related contents are available at https://github.com/yuleiqin/fantastic-data-engineering.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02085v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02085v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Constructing Mechanical Design Agent Based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Since ancient times, mechanical design aids have been developed to assist human users, aimed at improving the efficiency and effectiveness of design.However, even with the widespread use of contemporary Computer-Aided Design (CAD) systems, there are still high learning costs, repetitive work, and other challenges.In recent years, the rise of Large Language Models (LLMs) has introduced new productivity opportunities to the field of mechanical design.Yet, it remains unrealistic to rely on LLMs alone to complete mechanical design tasks directly.<span class='px-1 mx-1 bg-yellow-200'>Through a series of explorations, we propose a method for constructing a comprehensive Mechanical Design Agent (MDA) by guiding LLM learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.583</span></span>To verify the validity of our proposed method, we conducted a series of experiments and presented relevant cases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-31</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MetaOpenFOAM: an LLM-based multi-agent framework for CFD
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Remarkable progress has been made in automated problem solving through societies of agents based on large language models (LLMs).Computational fluid dynamics (CFD), as a complex problem, presents unique challenges in automated simulations that require sophisticated solutions.<span class='px-1 mx-1 bg-yellow-200'>MetaOpenFOAM, as a novel multi-agent collaborations framework, aims to complete CFD simulation tasks with only natural language as input. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span>These simulation tasks include mesh pre-processing, simulation and post-processing, etc. MetaOpenFOAM harnesses the power of MetaGPT's assembly line paradigm, which assigns diverse roles to various agents, efficiently breaking down complex CFD tasks into manageable subtasks.Langchain further complements MetaOpenFOAM by integrating Retrieval-Augmented Generation (RAG) technology, which enhances the framework's ability by integrating a searchable database of OpenFOAM tutorials for LLMs.Tests on a benchmark for natural language-based CFD solver, consisting of 8 CFD simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per test (85%), with each test case costing only $0.22 on average.The 8 CFD simulation tasks include compressible and incompressible flows, 2D and 3D flows, heat transfer, and combustion, demonstrating the ability to automate CFD simulations using only natural language input and iteratively correct errors to achieve the desired simulation at a low cost.An ablation study was conducted to verify the necessity of each component in the multi-agent system and the RAG technology.A sensitivity study on the randomness of LLM showed that LLM with low randomness can obtain more stable and accurate results.Additionally, MetaOpenFOAM own the ability to identify and modify key parameters in user requirements and excels in correcting bugs when failures occur, with or without human participation, which demonstrates the generalization of MetaOpenFOAM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.21320v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.21320v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-31</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chat-like Asserts Prediction with the Support of Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit testing is an essential component of software testing, with the assert statements playing an important role in determining whether the tested function operates as expected.Although research has explored automated test case generation, generating meaningful assert statements remains an ongoing challenge.While several studies have investigated assert statement generation in Java, limited work addresses this task in popular dynamically-typed programming languages like Python.In this paper, we introduce Chat-like execution-based Asserts Prediction (\tool), a novel Large Language Model-based approach for generating meaningful assert statements for Python projects.<span class='px-1 mx-1 bg-yellow-200'>\tool utilizes the persona, Chain-of-Thought, and one-shot learning techniques in the prompt design, and conducts rounds of communication with LLM and Python interpreter to generate meaningful assert statements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.58</span></span>We also present a Python assert statement dataset mined from GitHub.Our evaluation demonstrates that \tool achieves 64.7\% accuracy for single assert statement generation and 62\% for overall assert statement generation, outperforming the existing approaches.We also analyze the mismatched assert statements, which may still share the same functionality and discuss the potential help \tool could offer to the automated Python unit test generation.The findings indicate that \tool has the potential to benefit the SE community through more practical usage scenarios.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.21429v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.21429v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-31</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-for-X: Application-agnostic Integration of Large Language Models to Support Personal Writing Workflows
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>To enhance productivity and to streamline workflows, there is a growing trend to embed large language model (LLM) functionality into applications, from browser-based web apps to native apps that run on personal computers.Here, we introduce LLM-for-X, a system-wide shortcut layer that seamlessly augments any application with LLM services through a lightweight popup dialog.Our native layer seamlessly connects front-end applications to popular LLM backends, such as ChatGPT and Gemini, using their uniform chat front-ends as the programming interface or their custom API calls.We demonstrate the benefits of LLM-for-X across a wide variety of applications, including Microsoft Office, VSCode, and Adobe Acrobat as well as popular web apps such as Overleaf.<span class='px-1 mx-1 bg-yellow-200'>In our evaluation, we compared LLM-for-X with ChatGPT's web interface in a series of tasks, showing that our approach can provide users with quick, efficient, and easy-to-use LLM assistance without context switching to support writing and reading tasks that is agnostic of the specific application. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.517</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.21593v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.21593v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI-Assisted Generation of Difficult Math Questions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Current LLM training positions mathematical reasoning as a core capability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.585</span></span><span class='px-1 mx-1 bg-yellow-200'>With publicly available sources fully tapped, there is unmet demand for diverse and challenging math questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.579</span></span>Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty.<span class='px-1 mx-1 bg-yellow-200'>We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.595</span></span><span class='px-1 mx-1 bg-yellow-200'>We leverage LLM metacognition skills <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.569</span></span><span class='px-1 mx-1 bg-yellow-200'>[Didolkar et al., 2024] of a strong LLM to extract core "skills" from existing math datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.56</span></span>These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills.<span class='px-1 mx-1 bg-yellow-200'>The use of two different skills within each question makes finding such questions an "out of distribution" task for both LLMs and humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span><span class='px-1 mx-1 bg-yellow-200'>Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multiturn prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.552</span></span>Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interactions.<span class='px-1 mx-1 bg-yellow-200'>Applying this pipeline on skills extracted from the MATH dataset [Hendrycks et al., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions, as evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH (b) Higher performance on MATH when using MATH$^2$ questions as in-context examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.579</span></span>Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of scalable oversight.Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH$^2$ is the square on MATH, suggesting that successfully solving the question in MATH$^2$ requires a nontrivial combination of two distinct math skills.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.21009v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.21009v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs as Recommender Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities.<span class='px-1 mx-1 bg-yellow-200'>To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles.It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions.<span class='px-1 mx-1 bg-yellow-200'>This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.736</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06276v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06276v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recently, increasing attention has been paid to LLM-based recommender systems, but their deployment is still under exploration in the industry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span>Most deployments utilize LLMs as feature enhancers, generating augmentation knowledge in the offline stage.<span class='px-1 mx-1 bg-yellow-200'>However, in recommendation scenarios, involving numerous users and items, even offline generation with LLMs consumes considerable time and resources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>This generation inefficiency stems from the autoregressive nature of LLMs, and a promising direction for acceleration is speculative decoding, a Draft-then-Verify paradigm that increases the number of generated tokens per decoding step.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we first identify that recommendation knowledge generation is suitable for retrieval-based speculative decoding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>Then, we discern two characteristics: (1) extensive items and users in RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for text generated by LLMs.<span class='px-1 mx-1 bg-yellow-200'>Based on the above insights, we propose a Decoding Acceleration Framework for LLM-based Recommendation (dubbed DARE), with Customized Retrieval Pool to improve retrieval efficiency and Relaxed Verification to increase the acceptance rate of draft tokens, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>Extensive experiments demonstrate that DARE achieves a 3-5x speedup and is compatible with various frameworks and backbone LLMs.DARE has also been deployed to online advertising scenarios within a large-scale commercial environment, achieving a 3.45x speedup while maintaining the downstream performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05676v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05676v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMREC: LLM Based Multi-Modal Recommender System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span><span class='px-1 mx-1 bg-yellow-200'>This surge in content presents unique challenges for designing effective recommender systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.766</span></span>Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences.<span class='px-1 mx-1 bg-yellow-200'>This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span><span class='px-1 mx-1 bg-yellow-200'>The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span><span class='px-1 mx-1 bg-yellow-200'>The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.866</span></span>The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model.Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information.<span class='px-1 mx-1 bg-yellow-200'>This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04211v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04211v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span><span class='px-1 mx-1 bg-yellow-200'>Current mainstream efforts mainly center around injecting personalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, leading to suboptimal performance.(2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths.(3) Existing approaches aren't scalable for large datasets due to training efficiency constraints.Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space.To address these problems, we propose RecLoRA.This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03533v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03533v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Model Aided QoS Prediction for Service Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have seen rapid improvement in the recent years, and are used in a wider range of applications.After being trained on large text corpus, LLMs obtain the capability of extracting rich features from textual data.Such capability is potentially useful for the web service recommendation task, where the web users and services have intrinsic attributes that can be described using natural language sentences and are useful for recommendation.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we explore the possibility and practicality of using LLMs for web service recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span>We propose the large language model aided QoS prediction (llmQoS) model, which use LLMs to extract useful information from attributes of web users and services via descriptive sentences.This information is then used in combination with the QoS values of historical interactions of users and services, to predict QoS values for any given user-service pair.Our proposed model is shown to overcome the data sparsity issue for QoS prediction.We show that on the WSDream dataset, llmQoS outperforms comparable baseline models consistently.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid evolution of large language models (LLMs) has opened up new possibilities for applications such as context-driven product recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>However, the effectiveness of these models in this context is heavily reliant on their comprehensive understanding of the product inventory.This paper presents a novel approach to equipping LLMs with product knowledge by training them to respond contextually to synthetic search queries that include product IDs.We delve into an extensive analysis of this method, evaluating its effectiveness, outlining its benefits, and highlighting its constraints.<span class='px-1 mx-1 bg-yellow-200'>The paper also discusses the potential improvements and future directions for this approach, providing a comprehensive understanding of the role of LLMs in product recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.20856v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.20856v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The recent emergence of Large Language Models (LLMs) has heralded a new era of human-AI interaction.These sophisticated models, exemplified by Chat-GPT and its successors, have exhibited remarkable capabilities in language understanding.However, as these LLMs have undergone exponential growth, a crucial dimension that remains understudied is the personalization of these models.Large foundation models such as GPT-3 etc. focus on creating a universal model that serves a broad range of tasks and users.This approach emphasizes the model's generalization capabilities, treating users as a collective rather than as distinct individuals.While practical for many common applications, this one-size-fits-all approach often fails to address the rich tapestry of human diversity and individual needs.To explore this issue we introduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP models for user personalization.\datasetname{} consists of a series of user-centered tasks containing diverse and individualized expressions where the preferences of users can potentially differ for the same input.<span class='px-1 mx-1 bg-yellow-200'>Using PEFT-U, we explore the challenge of efficiently personalizing LLMs to accommodate user-specific preferences in the context of diverse user-centered tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.18078v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.18078v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reinforced Prompt Personalization for Recommendation with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>However, existing research predominantly concentrates on task-wise prompting, developing fixed prompt templates composed of four patterns (i.e., role-playing, history records, reasoning guidance, and output format) and applying them to all users for a given task.Although convenient, task-wise prompting overlooks individual user differences, leading to potential mismatches in capturing user preferences.To address it, we introduce the concept of instance-wise prompting to personalize discrete prompts for individual users and propose Reinforced Prompt Personalization (RPP) to optimize the four patterns in prompts using multi-agent reinforcement learning (MARL).To boost efficiency, RPP formulates prompt personalization as selecting optimal sentences holistically across the four patterns, rather than optimizing word-by-word.<span class='px-1 mx-1 bg-yellow-200'>To ensure the quality of prompts, RPP meticulously crafts diverse expressions for each of the four patterns, considering multiple analytical perspectives for specific recommendation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>In addition to RPP, our proposal of RPP+ aims to enhance the scalability of action space by dynamically refining actions with LLMs throughout the iterative process.We evaluate the effectiveness of RPP/RPP+ in ranking tasks over various datasets.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate the superiority of RPP/RPP+ over traditional recommender models, few-shot methods, and other prompt-based methods, underscoring the significance of instance-wise prompting for LLMs in recommendation tasks and validating the effectiveness of RPP/RPP+. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.86</span></span>Our code is available at https://github.com/maowenyu-11/RPP.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.17115v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.17115v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How to Leverage Personal Textual Knowledge for Personalized Conversational Information Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Personalized conversational information retrieval (CIR) combines conversational and personalizable elements to satisfy various users' complex information needs through multi-turn interaction based on their backgrounds. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>The key promise is that the personal textual knowledge base (PTKB) can improve the CIR effectiveness because the retrieval results can be more related to the user's background.However, PTKB is noisy: not every piece of knowledge in PTKB is relevant to the specific query at hand.In this paper, we explore and test several ways to select knowledge from PTKB and use it for query reformulation by using a large language model (LLM).The experimental results show the PTKB might not always improve the search results when used alone, but LLM can help generate a more appropriate personalized query when high-quality guidance is provided.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.16192v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.16192v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Medication Recommendation with LLM Text Representation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Most of the existing medication recommendation models are predicted with only structured data such as medical codes, with the remaining other large amount of unstructured or semi-structured data underutilization.<span class='px-1 mx-1 bg-yellow-200'>To increase the utilization effectively, we proposed a method of enhancing medication recommendation with Large Language Model (LLM) text representation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>LLM harnesses powerful language understanding and generation capabilities, enabling the extraction of information from complex and lengthy unstructured data such as clinical notes which contain complex terminology.<span class='px-1 mx-1 bg-yellow-200'>This method can be applied to several existing base models we selected and improve medication recommendation performance with the combination representation of text and medical codes experiments on two different datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>LLM text representation alone can even demonstrate a comparable ability to the medical code representation alone.<span class='px-1 mx-1 bg-yellow-200'>Overall, this is a general method that can be applied to other models for improved recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.10453v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.10453v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantic Understanding and Data Imputation using Large Language Model to Accelerate Recommendation System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper aims to address the challenge of sparse and missing data in recommendation systems, a significant hurdle in the age of big data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>Traditional imputation methods struggle to capture complex relationships within the data.<span class='px-1 mx-1 bg-yellow-200'>We propose a novel approach that fine-tune Large Language Model (LLM) and use it impute missing data for recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>LLM which is trained on vast amounts of text, is able to understand complex relationship among data and intelligently fill in missing information.This enriched data is then used by the recommendation system to generate more accurate and personalized suggestions, ultimately enhancing the user experience.<span class='px-1 mx-1 bg-yellow-200'>We evaluate our LLM-based imputation method across various tasks within the recommendation system domain, including single classification, multi-classification, and regression compared to traditional data imputation methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span><span class='px-1 mx-1 bg-yellow-200'>By demonstrating the superiority of LLM imputation over traditional methods, we establish its potential for improving recommendation system performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.10078v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.10078v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recommender systems (RS) are vital for managing information overload and delivering personalized content, responding to users' diverse information needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span><span class='px-1 mx-1 bg-yellow-200'>The emergence of large language models (LLMs) offers a new horizon for redefining recommender systems with vast general knowledge and reasoning capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span><span class='px-1 mx-1 bg-yellow-200'>Standing across this LLM era, we aim to integrate recommender systems into a broader picture, and pave the way for more comprehensive solutions for future research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span><span class='px-1 mx-1 bg-yellow-200'>Therefore, we first offer a comprehensive overview of the technical progression of recommender systems, particularly focusing on language foundation models and their applications in recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span><span class='px-1 mx-1 bg-yellow-200'>We identify two evolution paths of modern recommender systems -- via list-wise recommendation and conversational recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span>These two paths finally converge at LLM agents with superior capabilities of long-term memory, reflection, and tool intelligence.<span class='px-1 mx-1 bg-yellow-200'>Along these two paths, we point out that the information effectiveness of the recommendation is increased, while the user's acquisition cost is decreased. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span><span class='px-1 mx-1 bg-yellow-200'>Technical features, research methodologies, and inherent challenges for each milestone along the path are carefully investigated -- from traditional list-wise recommendation to LLM-enhanced recommendation to recommendation with LLM agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span>Finally, we highlight several unresolved challenges crucial for the development of future personalization technologies and interfaces and discuss the future prospects.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.10081v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.10081v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Review-LLM: Harnessing Large Language Models for Personalized Review Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Product review generation is an important task in recommender systems, which could provide explanation and persuasiveness for the recommendation.Recently, Large Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling and generating ability, which could be applied in review generation.However, directly applying the LLMs for generating reviews might be troubled by the ``polite'' phenomenon of the LLMs and could not generate personalized reviews (e.g., negative reviews).In this paper, we propose Review-LLM that customizes LLMs for personalized review generation.Firstly, we construct the prompt input by aggregating user historical behaviors, which include corresponding item titles and reviews.This enables the LLMs to capture user interest features and review writing style.<span class='px-1 mx-1 bg-yellow-200'>Secondly, we incorporate ratings as indicators of satisfaction into the prompt, which could further improve the model's understanding of user preferences and the sentiment tendency control of generated reviews. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>Finally, we feed the prompt text into LLMs, and use Supervised Fine-Tuning (SFT) to make the model generate personalized reviews for the given user and target item.Experimental results on the real-world dataset show that our fine-tuned model could achieve better review generation performance than existing close-source LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.07487v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.07487v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fine-grained large-scale content recommendations for MSX sellers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>One of the most critical tasks of Microsoft sellers is to meticulously track and nurture potential business opportunities through proactive engagement and tailored solutions.<span class='px-1 mx-1 bg-yellow-200'>Recommender systems play a central role to help sellers achieve their goals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we present a content recommendation model which surfaces various types of content (technical documentation, comparison with competitor products, customer success stories etc.) that sellers can share with their customers or use for their own self-learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>The model operates at the opportunity level which is the lowest possible granularity and the most relevant one for sellers.It is based on semantic matching between metadata from the contents and carefully selected attributes of the opportunities.Considering the volume of seller-managed opportunities in organizations such as Microsoft, we show how to perform efficient semantic matching over a very large number of opportunity-content combinations.The main challenge is to ensure that the top-5 relevant contents for each opportunity are recommended out of a total of $\approx 40,000$ published contents.We achieve this target through an extensive comparison of different model architectures and feature selection.<span class='px-1 mx-1 bg-yellow-200'>Finally, we further examine the quality of the recommendations in a quantitative manner using a combination of human domain experts as well as by using the recently proposed "LLM as a judge" framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.06910v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.06910v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-03</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LANE: Logic Alignment of Non-tuning Large Language Models and Online Recommendation Systems for Explainable Reason Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The explainability of recommendation systems is crucial for enhancing user trust and satisfaction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span><span class='px-1 mx-1 bg-yellow-200'>Leveraging large language models (LLMs) offers new opportunities for comprehensive recommendation logic generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span><span class='px-1 mx-1 bg-yellow-200'>However, in existing related studies, fine-tuning LLM models for recommendation tasks incurs high computational costs and alignment issues with existing systems, limiting the application potential of proven proprietary/closed-source LLM models, such as GPT-4. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, our proposed effective strategy LANE aligns LLMs with online recommendation systems without additional LLMs tuning, reducing costs and improving explainability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>This innovative approach addresses key challenges in integrating language models with recommendation systems while fully utilizing the capabilities of powerful proprietary models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.744</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, our strategy operates through several key components: semantic embedding, user multi-preference extraction using zero-shot prompting, semantic alignment, and explainable recommendation generation using Chain of Thought (CoT) prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span><span class='px-1 mx-1 bg-yellow-200'>By embedding item titles instead of IDs and utilizing multi-head attention mechanisms, our approach aligns the semantic features of user preferences with those of candidate items, ensuring coherent and user-aligned recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.805</span></span><span class='px-1 mx-1 bg-yellow-200'>Sufficient experimental results including performance comparison, questionnaire voting, and visualization cases prove that our method can not only ensure recommendation performance, but also provide easy-to-understand and reasonable recommendation logic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.02833v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.02833v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Production workflows for LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multimodal Large Language Models for Phishing Webpage Detection and Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>To address the challenging problem of detecting phishing webpages, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.373</span></span><span class='px-1 mx-1 bg-yellow-200'>Among these, brand-based phishing detection that uses models from Computer Vision to detect if a given webpage is imitating a well-known brand has received widespread attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.319</span></span><span class='px-1 mx-1 bg-yellow-200'>However, such models are costly and difficult to maintain, as they need to be retrained with labeled dataset that has to be regularly and continuously collected. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.596</span></span><span class='px-1 mx-1 bg-yellow-200'>Besides, they also need to maintain a good reference list of well-known websites and related meta-data for effective performance.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.351</span></span>In this work, we take steps to study the efficacy of large language models (LLMs), in particular the multimodal LLMs, in detecting phishing webpages.<span class='px-1 mx-1 bg-yellow-200'>Given that the LLMs are pretrained on a large corpus of data, we aim to make use of their understanding of different aspects of a webpage (logo, theme, favicon, etc.) to identify the brand of a given webpage and compare the identified brand with the domain name in the URL to detect a phishing attack. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.39</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose a two-phase system employing LLMs in both phases: the first phase focuses on brand identification, while the second verifies the domain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.366</span></span><span class='px-1 mx-1 bg-yellow-200'>We carry out comprehensive evaluations on a newly collected dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.355</span></span>Our experiments show that the LLM-based system achieves a high detection rate at high precision; importantly, it also provides interpretable evidence for the decisions.<span class='px-1 mx-1 bg-yellow-200'>Our system also performs significantly better than a state-of-the-art brand-based phishing detection system while demonstrating robustness against two known adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.35</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05941v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05941v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid advancement of Large Language Models (LLMs) and conversational assistants necessitates dynamic, scalable, and configurable conversational datasets for training and evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.329</span></span>These datasets must accommodate diverse user interaction modes, including text and voice, each presenting unique modeling challenges.Knowledge Graphs (KGs), with their structured and evolving nature, offer an ideal foundation for current and precise knowledge.<span class='px-1 mx-1 bg-yellow-200'>Although human-curated KG-based conversational datasets exist, they struggle to keep pace with the rapidly changing user information needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.438</span></span><span class='px-1 mx-1 bg-yellow-200'>We present ConvKGYarn, a scalable method for generating up-to-date and configurable conversational KGQA datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.493</span></span>Qualitative psychometric analyses confirm our method can generate high-quality datasets rivaling a popular conversational KGQA dataset while offering it at scale and covering a wide range of human-interaction configurations.<span class='px-1 mx-1 bg-yellow-200'>We showcase its utility by testing LLMs on diverse conversations - exploring model behavior on conversational KGQA sets with different configurations grounded in the same KG fact set. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.423</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate parametric knowledge of LLMs, thus offering a robust solution to the constantly evolving landscape of conversational assistants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.392</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05948v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05948v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rise of Large Language Models (LLMs) has triggered legal and ethical concerns, especially regarding the unauthorized use of copyrighted materials in their training datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.585</span></span>This has led to lawsuits against tech companies accused of using protected content without permission.<span class='px-1 mx-1 bg-yellow-200'>Membership Inference Attacks (MIAs) aim to detect whether specific documents were used in a given LLM pretraining, but their effectiveness is undermined by biases such as time-shifts and n-gram overlaps.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.429</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper addresses the evaluation of MIAs on LLMs with partially inferable training sets, under the ex-post hypothesis, which acknowledges inherent distributional biases between members and non-members datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.559</span></span>We propose and validate algorithms to create ``non-biased'' and ``non-classifiable'' datasets for fairer MIA assessment.Experiments using the Gutenberg dataset on OpenLamma and Pythia show that neutralizing known biases alone is insufficient.<span class='px-1 mx-1 bg-yellow-200'>Our methods produce non-biased ex-post datasets with AUC-ROC scores comparable to those previously obtained on genuinely random datasets, validating our approach. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.457</span></span>Globally, MIAs yield results close to random, with only one being effective on both random and our datasets, but its performance decreases when bias is removed.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05968v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05968v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference Acceleration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As large language model (LLM) inference demands ever-greater resources, there is a rapid growing trend of using low-bit weights to shrink memory usage and boost inference efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span><span class='px-1 mx-1 bg-yellow-200'>However, these low-bit LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), which is a crucial yet under-explored operation that involves multiplying lower-precision weights with higher-precision activations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.5</span></span><span class='px-1 mx-1 bg-yellow-200'>Unfortunately, current hardware does not natively support mpGEMM, resulting in indirect and inefficient dequantization-based implementations.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.354</span></span><span class='px-1 mx-1 bg-yellow-200'>To address the mpGEMM requirements in low-bit LLMs, we explored the lookup table (LUT)-based approach for mpGEMM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span>However, a conventional LUT implementation falls short of its potential.<span class='px-1 mx-1 bg-yellow-200'>To fully harness the power of LUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design optimized for low-bit LLM inference. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.472</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we introduce software-based operator fusion and table symmetrization techniques to optimize table precompute and table storage, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.372</span></span>Then, LUT Tensor Core proposes the hardware design featuring an elongated tiling shape design to enhance table reuse and a bit-serial design to support various precision combinations in mpGEMM.<span class='px-1 mx-1 bg-yellow-200'>Moreover, we design an end-to-end compilation stack with new instructions for LUT-based mpGEMM, enabling efficient LLM compilation and optimizations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span><span class='px-1 mx-1 bg-yellow-200'>The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that LUT Tensor Core achieves more than a magnitude of improvements on both compute density and energy efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.385</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06003v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06003v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps).<span class='px-1 mx-1 bg-yellow-200'>Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.385</span></span><span class='px-1 mx-1 bg-yellow-200'>However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.385</span></span>In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.375</span></span><span class='px-1 mx-1 bg-yellow-200'>This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.317</span></span>Finally, HYPERION reports the inconsistency based on predefined detection patterns.The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies.We also implement HYPERION to analyze 835 real-world DApps.The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06037v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06037v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building Decision Making Models Through Language Model Regime
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We propose a novel approach for decision making problems leveraging the generalization capabilities of large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.441</span></span><span class='px-1 mx-1 bg-yellow-200'>Traditional methods such as expert systems, planning algorithms, and reinforcement learning often exhibit limited generalization, typically requiring the training of new models for each unique task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.321</span></span>In contrast, LLMs demonstrate remarkable success in generalizing across varied language tasks, inspiring a new strategy for training decision making models.Our approach, referred to as "Learning then Using" (LTU), entails a two-stage process.Initially, the \textit{learning} phase develops a robust foundational decision making model by integrating diverse knowledge from various domains and decision making contexts.The subsequent \textit{using} phase refines this foundation model for specific decision making scenarios.<span class='px-1 mx-1 bg-yellow-200'>Distinct from other studies that employ LLMs for decision making through supervised learning, our LTU method embraces a versatile training methodology that combines broad pre-training with targeted fine-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.336</span></span><span class='px-1 mx-1 bg-yellow-200'>Experiments in e-commerce domains such as advertising and search optimization have shown that LTU approach outperforms traditional supervised learning regimes in decision making capabilities and generalization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.478</span></span><span class='px-1 mx-1 bg-yellow-200'>The LTU approach is the first practical training architecture for both single-step and multi-step decision making tasks combined with LLMs, which can be applied beyond game and robot domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span><span class='px-1 mx-1 bg-yellow-200'>It provides a robust and adaptable framework for decision making, enhances the effectiveness and flexibility of various systems in tackling various challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.306</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Med42-v2: A Suite of Clinical LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Med42-v2 introduces a suite of clinical large language models (LLMs) designed to address the limitations of generic models in healthcare settings.These models are built on Llama3 architecture and fine-tuned using specialized clinical data.They underwent multi-stage preference alignment to effectively respond to natural prompts.While generic models are often preference-aligned to avoid answering clinical queries as a precaution, Med42-v2 is specifically trained to overcome this limitation, enabling its use in clinical settings.Med42-v2 models demonstrate superior performance compared to the original Llama3 models in both 8B and 70B parameter configurations and GPT-4 across various medical benchmarks.<span class='px-1 mx-1 bg-yellow-200'>These LLMs are developed to understand clinical queries, perform reasoning tasks, and provide valuable assistance in clinical environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.347</span></span>The models are now publicly available at \href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06142v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06142v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models.rStar decouples reasoning into a self-play mutual generation-discrimination process.First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories.Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM.The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct.Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA.<span class='px-1 mx-1 bg-yellow-200'>Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.437</span></span>Code will be available at https://github.com/zhentingqi/rStar.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06195v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06195v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Effects of Steering Latent Representation for Large Language Model Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.328</span></span>Despite its high performance, the underlying cause and explanation remain underexplored.In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses.<span class='px-1 mx-1 bg-yellow-200'>Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.324</span></span><span class='px-1 mx-1 bg-yellow-200'>Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.368</span></span>Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs.<span class='px-1 mx-1 bg-yellow-200'>To resolve this drawback, we propose Adaptive RMU -- a simple yet effective alternative method that makes unlearning effective with most layers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.46</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.37</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets.The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results.We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training.Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective.We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments.<span class='px-1 mx-1 bg-yellow-200'>The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.334</span></span><span class='px-1 mx-1 bg-yellow-200'>Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.354</span></span>Our code is available at https://github.com/ContextualAI/CLAIR_and_APO.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06266v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06266v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the constantly evolving field of cybersecurity, it is imperative for analysts to stay abreast of the latest attack trends and pertinent information that aids in the investigation and attribution of cyber-attacks.<span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce the first question-answering (QA) model and its application that provides information to the cybersecurity experts about cyber-attacks investigations and attribution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.342</span></span><span class='px-1 mx-1 bg-yellow-200'>Our QA model is based on Retrieval Augmented Generation (RAG) techniques together with a Large Language Model (LLM) and provides answers to the users' queries based on either our knowledge base (KB) that contains curated information about cyber-attacks investigations and attribution or on outside resources provided by the users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.39</span></span><span class='px-1 mx-1 bg-yellow-200'>We have tested and evaluated our QA model with various types of questions, including KB-based, metadata-based, specific documents from the KB, and external sources-based questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.312</span></span><span class='px-1 mx-1 bg-yellow-200'>We compared the answers for KB-based questions with those from OpenAI's GPT-3.5 and the latest GPT-4o LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.408</span></span><span class='px-1 mx-1 bg-yellow-200'>Our proposed QA model outperforms OpenAI's GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models, which is critical for cyber-attack investigation and attribution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>Additionally, our analysis showed that when the RAG QA model is given few-shot examples rather than zero-shot instructions, it generates better answers compared to cases where no examples are supplied in addition to the query.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06272v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06272v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have demonstrated prowess in a wide range of tasks.<span class='px-1 mx-1 bg-yellow-200'>However, many LLMs exhibit significant performance discrepancies between high- and low-resource languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.448</span></span><span class='px-1 mx-1 bg-yellow-200'>To mitigate this challenge, we present FuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the need of the research community for balanced and high-performing multilingual capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.363</span></span><span class='px-1 mx-1 bg-yellow-200'>FuxiTranyu-8B, the base model with 8 billion parameters, is trained from scratch on a meticulously balanced multilingual data repository that contains 600 billion tokens covering 43 natural languages and 16 programming languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span><span class='px-1 mx-1 bg-yellow-200'>In addition to the base model, we also develop two instruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse multilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined with DPO on a preference dataset for enhanced alignment ability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.578</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on a wide range of multilingual benchmarks demonstrate the competitive performance of FuxiTranyu against existing multilingual LLMs, e.g., BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.4</span></span>Interpretability analyses at both the neuron and representation level suggest that FuxiTranyu is able to learn consistent multilingual representations across different languages.<span class='px-1 mx-1 bg-yellow-200'>To promote further research into multilingual LLMs and their working mechanisms, we release both the base and instruction-tuned FuxiTranyu models together with 58 pretraining checkpoints at HuggingFace and Github. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.436</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06273v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06273v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Synthetic Patient-Physician Dialogue Generation from Clinical Notes Using LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Medical dialogue systems (MDS) enhance patient-physician communication, improve healthcare accessibility, and reduce costs.<span class='px-1 mx-1 bg-yellow-200'>However, acquiring suitable data to train these systems poses significant challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.309</span></span><span class='px-1 mx-1 bg-yellow-200'>Privacy concerns prevent the use of real conversations, necessitating synthetic alternatives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.386</span></span>Synthetic dialogue generation from publicly available clinical notes offers a promising solution to this issue, providing realistic data while safeguarding privacy.Our approach, SynDial, uses a single LLM iteratively with zero-shot prompting and a feedback loop to generate and refine high-quality synthetic dialogues.The feedback consists of weighted evaluation scores for similarity and extractiveness.The iterative process ensures dialogues meet predefined thresholds, achieving superior extractiveness as a result of the feedback loop.Additionally, evaluation shows that the generated dialogues excel in factuality metric compared to the baselines and has comparable diversity scores with GPT4.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06285v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06285v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLM Model Architectures and Training Techniques</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests.Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.<span class='px-1 mx-1 bg-yellow-200'>We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.557</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TRIZ-GPT: An LLM-augmented method for problem-solving
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>TRIZ, the Theory of Inventive Problem Solving, is derived from a comprehensive analysis of patents across various domains, offering a framework and practical tools for problem-solving.Despite its potential to foster innovative solutions, the complexity and abstractness of TRIZ methodology often make its acquisition and application challenging.This often requires users to have a deep understanding of the theory, as well as substantial practical experience and knowledge across various disciplines.The advent of Large Language Models (LLMs) presents an opportunity to address these challenges by leveraging their extensive knowledge bases and reasoning capabilities for innovative solution generation within TRIZ-based problem-solving process.This study explores and evaluates the application of LLMs within the TRIZ-based problem-solving process.The construction of TRIZ case collections establishes a solid empirical foundation for our experiments and offers valuable resources to the TRIZ community.<span class='px-1 mx-1 bg-yellow-200'>A specifically designed workflow, utilizing step-by-step reasoning and evaluation-validated prompt strategies, effectively transforms concrete problems into TRIZ problems and finally generates inventive solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.413</span></span>Finally, we present a case study in mechanical engineering field that highlights the practical application of this LLM-augmented method.It showcases GPT-4's ability to generate solutions that closely resonate with original solutions and suggests more implementation mechanisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05897v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05897v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A New Pipeline For Generating Instruction Dataset via RAG and Self Fine-Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rapid development of large language models in recent years, there has been an increasing demand for domain-specific Agents that can cater to the unique needs of enterprises and organizations.Unlike general models, which strive for broad coverage, these specialized Agents rely on focused datasets tailored to their intended applications.<span class='px-1 mx-1 bg-yellow-200'>This research proposes a pipeline that leverages the power of LLMs and the Retrieval-Augmented Generation related framework to construct high-quality instruction datasets for fine-tuning on specific domains using custom document collections. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.584</span></span><span class='px-1 mx-1 bg-yellow-200'>By ingesting domain-specific documents, the pipeline generates relevant and contextually appropriate instructions, thus effectively creating a comprehensive dataset for fine-tuning LLMs on the target domain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span>This approach overcomes the limitations of traditional dataset creation methods, which often rely on manual curation or web-scraping techniques that may introduce noise and irrelevant data.<span class='px-1 mx-1 bg-yellow-200'>Notably, our pipeline offers a dynamic solution that can quickly adapt to updates or modifications in the domain-specific document collection, eliminating the need for complete retraining. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span><span class='px-1 mx-1 bg-yellow-200'>Additionally, it addresses the challenge of data scarcity by enabling the generation of instruction datasets from a limited set of initial documents, rendering it suitable for unpopular or specialized domains where comprehensive datasets are scarce. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span>As a case study, we apply this approach to the domain of psychiatry, a field requiring specialized knowledge and sensitive handling of patient information.<span class='px-1 mx-1 bg-yellow-200'>The resulting fine-tuned LLM demonstrates showcases the viability of the proposed approach and underscores its potential for widespread adoption across various industries and domains where tailored, accurate, and contextually relevant language models are indispensable. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.461</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05911v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05911v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.426</span></span>This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.Based on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation.Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications.<span class='px-1 mx-1 bg-yellow-200'>We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.476</span></span>Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.To evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations.We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA.Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.Our optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments.This research has important implications for advancing information processing and intelligent production in the automotive industry.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05933v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05933v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multimodal Large Language Models for Phishing Webpage Detection and Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>To address the challenging problem of detecting phishing webpages, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms.Among these, brand-based phishing detection that uses models from Computer Vision to detect if a given webpage is imitating a well-known brand has received widespread attention.However, such models are costly and difficult to maintain, as they need to be retrained with labeled dataset that has to be regularly and continuously collected.<span class='px-1 mx-1 bg-yellow-200'>Besides, they also need to maintain a good reference list of well-known websites and related meta-data for effective performance.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.434</span></span>In this work, we take steps to study the efficacy of large language models (LLMs), in particular the multimodal LLMs, in detecting phishing webpages.Given that the LLMs are pretrained on a large corpus of data, we aim to make use of their understanding of different aspects of a webpage (logo, theme, favicon, etc.) to identify the brand of a given webpage and compare the identified brand with the domain name in the URL to detect a phishing attack.We propose a two-phase system employing LLMs in both phases: the first phase focuses on brand identification, while the second verifies the domain.We carry out comprehensive evaluations on a newly collected dataset.Our experiments show that the LLM-based system achieves a high detection rate at high precision; importantly, it also provides interpretable evidence for the decisions.Our system also performs significantly better than a state-of-the-art brand-based phishing detection system while demonstrating robustness against two known adversarial attacks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05941v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05941v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference Acceleration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As large language model (LLM) inference demands ever-greater resources, there is a rapid growing trend of using low-bit weights to shrink memory usage and boost inference efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.464</span></span><span class='px-1 mx-1 bg-yellow-200'>However, these low-bit LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), which is a crucial yet under-explored operation that involves multiplying lower-precision weights with higher-precision activations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span><span class='px-1 mx-1 bg-yellow-200'>Unfortunately, current hardware does not natively support mpGEMM, resulting in indirect and inefficient dequantization-based implementations.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span><span class='px-1 mx-1 bg-yellow-200'>To address the mpGEMM requirements in low-bit LLMs, we explored the lookup table (LUT)-based approach for mpGEMM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.541</span></span><span class='px-1 mx-1 bg-yellow-200'>However, a conventional LUT implementation falls short of its potential. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.443</span></span><span class='px-1 mx-1 bg-yellow-200'>To fully harness the power of LUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design optimized for low-bit LLM inference. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.405</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we introduce software-based operator fusion and table symmetrization techniques to optimize table precompute and table storage, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.491</span></span>Then, LUT Tensor Core proposes the hardware design featuring an elongated tiling shape design to enhance table reuse and a bit-serial design to support various precision combinations in mpGEMM.<span class='px-1 mx-1 bg-yellow-200'>Moreover, we design an end-to-end compilation stack with new instructions for LUT-based mpGEMM, enabling efficient LLM compilation and optimizations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.585</span></span>The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that LUT Tensor Core achieves more than a magnitude of improvements on both compute density and energy efficiency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06003v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06003v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps).Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic.However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness.In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.476</span></span>This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis.<span class='px-1 mx-1 bg-yellow-200'>Finally, HYPERION reports the inconsistency based on predefined detection patterns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.458</span></span>The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies.<span class='px-1 mx-1 bg-yellow-200'>We also implement HYPERION to analyze 835 real-world DApps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.427</span></span>The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06037v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06037v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building Decision Making Models Through Language Model Regime
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose a novel approach for decision making problems leveraging the generalization capabilities of large language models (LLMs).Traditional methods such as expert systems, planning algorithms, and reinforcement learning often exhibit limited generalization, typically requiring the training of new models for each unique task.In contrast, LLMs demonstrate remarkable success in generalizing across varied language tasks, inspiring a new strategy for training decision making models.<span class='px-1 mx-1 bg-yellow-200'>Our approach, referred to as "Learning then Using" (LTU), entails a two-stage process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.445</span></span>Initially, the \textit{learning} phase develops a robust foundational decision making model by integrating diverse knowledge from various domains and decision making contexts.The subsequent \textit{using} phase refines this foundation model for specific decision making scenarios.<span class='px-1 mx-1 bg-yellow-200'>Distinct from other studies that employ LLMs for decision making through supervised learning, our LTU method embraces a versatile training methodology that combines broad pre-training with targeted fine-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.493</span></span>Experiments in e-commerce domains such as advertising and search optimization have shown that LTU approach outperforms traditional supervised learning regimes in decision making capabilities and generalization.The LTU approach is the first practical training architecture for both single-step and multi-step decision making tasks combined with LLMs, which can be applied beyond game and robot domains.<span class='px-1 mx-1 bg-yellow-200'>It provides a robust and adaptable framework for decision making, enhances the effectiveness and flexibility of various systems in tackling various challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.509</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06087v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06087v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models.rStar decouples reasoning into a self-play mutual generation-discrimination process.First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories.Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM.The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct.Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA.<span class='px-1 mx-1 bg-yellow-200'>Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.476</span></span><span class='px-1 mx-1 bg-yellow-200'>Code will be available at https://github.com/zhentingqi/rStar. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.453</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06195v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06195v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Effects of Steering Latent Representation for Large Language Model Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning.<span class='px-1 mx-1 bg-yellow-200'>Despite its high performance, the underlying cause and explanation remain underexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.401</span></span>In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses.Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers.<span class='px-1 mx-1 bg-yellow-200'>Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.474</span></span><span class='px-1 mx-1 bg-yellow-200'>Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span><span class='px-1 mx-1 bg-yellow-200'>To resolve this drawback, we propose Adaptive RMU -- a simple yet effective alternative method that makes unlearning effective with most layers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.571</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.531</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets.The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results.We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training.Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective.We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments.<span class='px-1 mx-1 bg-yellow-200'>The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.468</span></span>Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%.Our code is available at https://github.com/ContextualAI/CLAIR_and_APO.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06266v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06266v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the constantly evolving field of cybersecurity, it is imperative for analysts to stay abreast of the latest attack trends and pertinent information that aids in the investigation and attribution of cyber-attacks.In this work, we introduce the first question-answering (QA) model and its application that provides information to the cybersecurity experts about cyber-attacks investigations and attribution.Our QA model is based on Retrieval Augmented Generation (RAG) techniques together with a Large Language Model (LLM) and provides answers to the users' queries based on either our knowledge base (KB) that contains curated information about cyber-attacks investigations and attribution or on outside resources provided by the users.We have tested and evaluated our QA model with various types of questions, including KB-based, metadata-based, specific documents from the KB, and external sources-based questions.<span class='px-1 mx-1 bg-yellow-200'>We compared the answers for KB-based questions with those from OpenAI's GPT-3.5 and the latest GPT-4o LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.417</span></span>Our proposed QA model outperforms OpenAI's GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models, which is critical for cyber-attack investigation and attribution.Additionally, our analysis showed that when the RAG QA model is given few-shot examples rather than zero-shot instructions, it generates better answers compared to cases where no examples are supplied in addition to the query.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06272v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06272v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have demonstrated prowess in a wide range of tasks.<span class='px-1 mx-1 bg-yellow-200'>However, many LLMs exhibit significant performance discrepancies between high- and low-resource languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span><span class='px-1 mx-1 bg-yellow-200'>To mitigate this challenge, we present FuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the need of the research community for balanced and high-performing multilingual capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.461</span></span>FuxiTranyu-8B, the base model with 8 billion parameters, is trained from scratch on a meticulously balanced multilingual data repository that contains 600 billion tokens covering 43 natural languages and 16 programming languages.<span class='px-1 mx-1 bg-yellow-200'>In addition to the base model, we also develop two instruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse multilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined with DPO on a preference dataset for enhanced alignment ability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on a wide range of multilingual benchmarks demonstrate the competitive performance of FuxiTranyu against existing multilingual LLMs, e.g., BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span>Interpretability analyses at both the neuron and representation level suggest that FuxiTranyu is able to learn consistent multilingual representations across different languages.<span class='px-1 mx-1 bg-yellow-200'>To promote further research into multilingual LLMs and their working mechanisms, we release both the base and instruction-tuned FuxiTranyu models together with 58 pretraining checkpoints at HuggingFace and Github. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.546</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06273v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06273v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities.There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks.In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans.We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning?(2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context?<span class='px-1 mx-1 bg-yellow-200'>(3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span><span class='px-1 mx-1 bg-yellow-200'>Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.551</span></span>Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06318v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06318v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code generation has been greatly enhanced by the profound advancements in Large Language Models (LLMs) recently.<span class='px-1 mx-1 bg-yellow-200'>Nevertheless, such LLM-based code generation approaches still struggle to generate error-free code in a few tries when faced with complex problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.462</span></span>To address this, the prevailing strategy is to sample a huge number of candidate programs, with the hope of any one in them could work.However, users of code generation systems usually expect to find a correct program by reviewing or testing only a small number of code candidates.<span class='px-1 mx-1 bg-yellow-200'>Otherwise, the system would be unhelpful. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.472</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose Top Pass, a code ranking approach that identifies potential correct solutions from a large number of candidates. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.45</span></span><span class='px-1 mx-1 bg-yellow-200'>Top Pass directly optimizes the pass@k loss function, enhancing the quality at the top of the candidate list. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.42</span></span>This enables the user to find the correct solution within as few tries as possible.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on four benchmarks indicate that our Top Pass method enhances the usability of code generation models by producing better ranking results, particularly achieving a 32.9\% relative improvement in pass@1 on CodeContests when compared to the state-of-the-art ranking method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.472</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05715v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05715v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Supporting Software Maintenance with Dynamically Generated Document Hierarchies
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software documentation supports a broad set of software maintenance tasks; however, creating and maintaining high-quality, multi-level software documentation can be incredibly time-consuming and therefore many code bases suffer from a lack of adequate documentation.<span class='px-1 mx-1 bg-yellow-200'>We address this problem through presenting HGEN, a fully automated pipeline that leverages LLMs to transform source code through a series of six stages into a well-organized hierarchy of formatted documents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.408</span></span>We evaluate HGEN both quantitatively and qualitatively.First, we use it to generate documentation for three diverse projects, and engage key developers in comparing the quality of the generated documentation against their own previously produced manually-crafted documentation.We then pilot HGEN in nine different industrial projects using diverse datasets provided by each project.We collect feedback from project stakeholders, and analyze it using an inductive approach to identify recurring themes.Results show that HGEN produces artifact hierarchies similar in quality to manually constructed documentation, with much higher coverage of the core concepts than the baseline approach.<span class='px-1 mx-1 bg-yellow-200'>Stakeholder feedback highlights HGEN's commercial impact potential as a tool for accelerating code comprehension and maintenance tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.45</span></span>Results and associated supplemental materials can be found at https://zenodo.org/records/11403244</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05829v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05829v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Retriever Augmented Large Language Models for Attack Graph Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the complexity of modern systems increases, so does the importance of assessing their security posture through effective vulnerability management and threat modeling techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.418</span></span>One powerful tool in the arsenal of cybersecurity professionals is the attack graph, a representation of all potential attack paths within a system that an adversary might exploit to achieve a certain objective.Traditional methods of generating attack graphs involve expert knowledge, manual curation, and computational algorithms that might not cover the entire threat landscape due to the ever-evolving nature of vulnerabilities and exploits.<span class='px-1 mx-1 bg-yellow-200'>This paper explores the approach of leveraging large language models (LLMs), such as ChatGPT, to automate the generation of attack graphs by intelligently chaining Common Vulnerabilities and Exposures (CVEs) based on their preconditions and effects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.421</span></span>It also shows how to utilize LLMs to create attack graphs from threat reports.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05855v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05855v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses.<span class='px-1 mx-1 bg-yellow-200'>This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the required skills surpassing their capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.488</span></span>We first systematically conceptualize infeasible tasks for LLMs, providing formal definitions and categorizations that cover a spectrum of related hallucinations.<span class='px-1 mx-1 bg-yellow-200'>We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to test multiple LLMs' abilities on task feasibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.423</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, we explore the potential of training enhancements to increase LLMs' refusal capabilities with fine-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span>Experiments validate the effectiveness of our methods, offering promising directions for refining the operational boundaries of LLMs in real applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05873v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05873v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Based Robust Product Classification in Commerce and Compliance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Product classification is a crucial task in international trade, as compliance regulations are verified and taxes and duties are applied based on product categories.<span class='px-1 mx-1 bg-yellow-200'>Manual classification of products is time-consuming and error-prone, and the sheer volume of products imported and exported renders the manual process infeasible. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.403</span></span>Consequently, e-commerce platforms and enterprises involved in international trade have turned to automatic product classification using machine learning.However, current approaches do not consider the real-world challenges associated with product classification, such as very abbreviated and incomplete product descriptions.In addition, recent advancements in generative Large Language Models (LLMs) and their reasoning capabilities are mainly untapped in product classification and e-commerce.In this research, we explore the real-life challenges of industrial classification and we propose data perturbations that allow for realistic data simulation.Furthermore, we employ LLM-based product classification to improve the robustness of the prediction in presence of incomplete data.Our research shows that LLMs with in-context learning outperform the supervised approaches in the clean-data scenario.Additionally, we illustrate that LLMs are significantly more robust than the supervised approaches when data attacks are present.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Programming applications of LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps).Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic.However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness.In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>Finally, HYPERION reports the inconsistency based on predefined detection patterns.The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies.We also implement HYPERION to analyze 835 real-world DApps.The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06037v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06037v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code generation has been greatly enhanced by the profound advancements in Large Language Models (LLMs) recently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.953</span></span><span class='px-1 mx-1 bg-yellow-200'>Nevertheless, such LLM-based code generation approaches still struggle to generate error-free code in a few tries when faced with complex problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>To address this, the prevailing strategy is to sample a huge number of candidate programs, with the hope of any one in them could work.However, users of code generation systems usually expect to find a correct program by reviewing or testing only a small number of code candidates.Otherwise, the system would be unhelpful.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose Top Pass, a code ranking approach that identifies potential correct solutions from a large number of candidates. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>Top Pass directly optimizes the pass@k loss function, enhancing the quality at the top of the candidate list.This enables the user to find the correct solution within as few tries as possible.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on four benchmarks indicate that our Top Pass method enhances the usability of code generation models by producing better ranking results, particularly achieving a 32.9\% relative improvement in pass@1 on CodeContests when compared to the state-of-the-art ranking method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.861</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05715v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05715v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Supporting Software Maintenance with Dynamically Generated Document Hierarchies
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software documentation supports a broad set of software maintenance tasks; however, creating and maintaining high-quality, multi-level software documentation can be incredibly time-consuming and therefore many code bases suffer from a lack of adequate documentation.<span class='px-1 mx-1 bg-yellow-200'>We address this problem through presenting HGEN, a fully automated pipeline that leverages LLMs to transform source code through a series of six stages into a well-organized hierarchy of formatted documents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>We evaluate HGEN both quantitatively and qualitatively.First, we use it to generate documentation for three diverse projects, and engage key developers in comparing the quality of the generated documentation against their own previously produced manually-crafted documentation.We then pilot HGEN in nine different industrial projects using diverse datasets provided by each project.We collect feedback from project stakeholders, and analyze it using an inductive approach to identify recurring themes.Results show that HGEN produces artifact hierarchies similar in quality to manually constructed documentation, with much higher coverage of the core concepts than the baseline approach.Stakeholder feedback highlights HGEN's commercial impact potential as a tool for accelerating code comprehension and maintenance tasks.Results and associated supplemental materials can be found at https://zenodo.org/records/11403244</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05829v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05829v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantic-Enhanced Indirect Call Analysis with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks.To tackle this issue, various types of indirect call analyzers have been proposed.However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios.To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis.Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets.This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets.<span class='px-1 mx-1 bg-yellow-200'>Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span>Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives.Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs.Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04344v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04344v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators.<span class='px-1 mx-1 bg-yellow-200'>Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population.Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels.In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy.We use advanced prompting techniques with varying complexity for AEQG.We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions.Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered.We also show that automated evaluation is not on par with human evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04394v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04394v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for cross-language code clone detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the involvement of multiple programming languages in modern software development, cross-lingual code clone detection has gained traction with the software engineering community. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Numerous studies have explored this topic, proposing various promising approaches.Inspired by the significant advances in machine learning in recent years, particularly Large Language Models (LLMs), which have demonstrated their ability to tackle various tasks, this paper revisits cross-lingual code clone detection.   We investigate the capabilities of four (04) LLMs and eight (08) prompts for the identification of cross-lingual code clones.Additionally, we evaluate a pre-trained embedding model to assess the effectiveness of the generated representations for classifying clone and non-clone pairs.Both studies (based on LLMs and Embedding models) are evaluated using two widely used cross-lingual datasets, XLCoST and CodeNet.Our results show that LLMs can achieve high F1 scores, up to 0.98, for straightforward programming examples (e.g., from XLCoST).<span class='px-1 mx-1 bg-yellow-200'>However, they not only perform less well on programs associated with complex programming challenges but also do not necessarily understand the meaning of code clones in a cross-lingual setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span><span class='px-1 mx-1 bg-yellow-200'>We show that embedding models used to represent code fragments from different programming languages in the same representation space enable the training of a basic classifier that outperforms all LLMs by ~2 and ~24 percentage points on the XLCoST and CodeNet datasets, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>This finding suggests that, despite the apparent capabilities of LLMs, embeddings provided by embedding models offer suitable representations to achieve state-of-the-art performance in cross-lingual code clone detection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04430v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04430v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.934</span></span>However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information.In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience).We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences.<span class='px-1 mx-1 bg-yellow-200'>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.868</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have exhibited remarkable proficiency across a diverse array of natural language processing (NLP) tasks.However, adapting LLMs to downstream applications typically necessitates computationally intensive and memory-demanding fine-tuning procedures.To mitigate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead.While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data.In this work, we introduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance.BA-LoRA incorporates three distinct regularization terms: (1) consistency regularizer, (2) diversity regularizer, and (3) singular vector decomposition regularizer.These regularizers collectively aim to improve the generative models' consistency, diversity, and generalization capabilities during the fine-tuning process.<span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments on a variety of natural language understanding (NLU) and natural language generation (NLG) tasks, employing prominent LLMs such as LLaMA, Mistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of LoRA and its state-of-the-art variants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>Moreover, our method effectively mitigates the deleterious effects of pre-training bias, leading to more reliable and robust model outputs.The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04556v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04556v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Harnessing the Power of LLMs in Source Code Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software vulnerabilities, caused by unintentional flaws in source code, are a primary root cause of cyberattacks.<span class='px-1 mx-1 bg-yellow-200'>Static analysis of source code has been widely used to detect these unintentional defects introduced by software developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>Large Language Models (LLMs) have demonstrated human-like conversational abilities due to their capacity to capture complex patterns in sequential data, such as natural languages.In this paper, we harness LLMs' capabilities to analyze source code and detect known vulnerabilities.To ensure the proposed vulnerability detection method is universal across multiple programming languages, we convert source code to LLVM IR and train LLMs on these intermediate representations.We conduct extensive experiments on various LLM architectures and compare their accuracy.Our comprehensive experiments on real-world and synthetic codes from NVD and SARD demonstrate high accuracy in identifying source code vulnerabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03489v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03489v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RepoMasterEval: Evaluating Code Completion via Real-World Repositories
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block.These limitations makes the evaluation poorly align with the practical scenarios of code completion tools.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.83</span></span>Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites.To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score.Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios.The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.<span class='px-1 mx-1 bg-yellow-200'>Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03519v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03519v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring the extent of similarities in software failures across industries using LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid evolution of software development necessitates enhanced safety measures.Extracting information about software failures from companies is becoming increasingly more available through news articles.   This research utilizes the Failure Analysis Investigation with LLMs (FAIL) model to extract industry-specific information.Although the FAIL model's database is rich in information, it could benefit from further categorization and industry-specific insights to further assist software engineers.   In previous work news articles were collected from reputable sources and categorized by incidents inside a database.Prompt engineering and Large Language Models (LLMs) were then applied to extract relevant information regarding the software failure.This research extends these methods by categorizing articles into specific domains and types of software failures.The results are visually represented through graphs.   The analysis shows that throughout the database some software failures occur significantly more often in specific industries.This categorization provides a valuable resource for software engineers and companies to identify and address common failures.   <span class='px-1 mx-1 bg-yellow-200'>This research highlights the synergy between software engineering and Large Language Models (LLMs) to automate and enhance the analysis of software failures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>By transforming data from the database into an industry specific model, we provide a valuable resource that can be used to identify common vulnerabilities, predict potential risks, and implement proactive measures for preventing software failures.Leveraging the power of the current FAIL database and data visualization, we aim to provide an avenue for safer and more secure software in the future.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03528v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03528v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span>These complex tasks require AI to continuously summarize and improve.Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference.This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing.We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy.Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example.Note that as the memories continue to grow, this cumulative improvement may be substantial.The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03841v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03841v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.87</span></span><span class='px-1 mx-1 bg-yellow-200'>This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks.Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications.To mitigate these limitations, we introduce \framework, a system that integrates LLM agents with graph database interfaces extracted from code repositories.By leveraging the structural properties of graph databases and the flexibility of the graph query language, \framework enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation.We assess \framework using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench.Additionally, we develop five real-world coding applications.With a unified graph database schema, \framework demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering.Our application demo: https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03910v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03910v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring RAG-based Vulnerability Augmentation with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Detecting vulnerabilities is a crucial task for maintaining the integrity, availability, and security of software systems.Utilizing DL-based models for vulnerability detection has become commonplace in recent years.However, such deep learning-based vulnerability detectors (DLVD) suffer from a shortage of sizable datasets to train effectively.Data augmentation can potentially alleviate the shortage of data, but augmenting vulnerable code is challenging and requires designing a generative solution that maintains vulnerability.Hence, the work on generating vulnerable code samples has been limited and previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities.<span class='px-1 mx-1 bg-yellow-200'>Lately, large language models (LLMs) are being used for solving various code generation and comprehension tasks and have shown inspiring results, especially when fused with retrieval augmented generation (RAG). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.881</span></span>In this study, we explore three different strategies to augment vulnerabilities both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension.We conducted an extensive evaluation of our proposed approach on three vulnerability datasets and three DLVD models, using two LLMs.Our results show that our injection-based clustering-enhanced RAG method beats the baseline setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling (ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with 15K generated vulnerable samples.Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04125v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04125v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TestART: Improving LLM-based Unit Test via Co-evolution of Automated Generation and Repair Iteration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit test is crucial for detecting bugs in individual program units but consumes time and effort.<span class='px-1 mx-1 bg-yellow-200'>The existing automated unit test generation methods are mainly based on search-based software testing (SBST) and language models to liberate developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span><span class='px-1 mx-1 bg-yellow-200'>Recently, large language models (LLMs) have demonstrated remarkable reasoning and generation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>However, several problems limit their ability to generate high-quality test cases: (1) LLMs may generate invalid test cases under insufficient context, resulting in compilation errors; (2) Lack of test and coverage feedback information may cause runtime errors and low coverage rates.(3) The repetitive suppression problem causes LLMs to get stuck into the repetition loop of self-repair or re-generation attempts.In this paper, we propose TestART, a novel unit test generation method that leverages the strengths of LLMs while overcoming the limitations mentioned.TestART improves LLM-based unit test via co-evolution of automated generation and repair iteration.TestART leverages the template-based repair technique to fix bugs in LLM-generated test cases, using prompt injection to guide the next-step automated generation and avoid repetition suppression.Furthermore, TestART extracts coverage information from the passed test cases and utilizes it as testing feedback to enhance the sufficiency of the final test case.This synergy between generation and repair elevates the quality, effectiveness, and readability of the produced test cases significantly beyond previous methods.In comparative experiments, the pass rate of TestART-generated test cases is 78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and the same ChatGPT-3.5-based method ChatUniTest.It also achieves an impressive line coverage rate of 90.96% on the focal methods that passed the test, exceeding EvoSuite by 3.4%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03095v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03095v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Translation Performance of Large Language Models Based on Euas-20
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap.Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges.Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03119v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03119v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs).We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM).We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset.The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency.<span class='px-1 mx-1 bg-yellow-200'>All the developed code is publicly available on a GitHub repository <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.736</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03127v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03127v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Large Language Models for Automatic Register Transfer Logic Generation via High-Level Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The ever-growing popularity of large language models (LLMs) has resulted in their increasing adoption for hardware design and verification.Prior research has attempted to assess the capability of LLMs to automate digital hardware design by producing superior-quality Register Transfer Logic (RTL) descriptions, particularly in Verilog.<span class='px-1 mx-1 bg-yellow-200'>However, these tests have revealed that Verilog code production using LLMs at current state-of-the-art lack sufficient functional correctness to be practically viable, compared to automatic generation of programs in general-purpose programming languages such as C, C++, Python, etc. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span><span class='px-1 mx-1 bg-yellow-200'>With this as the key insight, in this paper we assess the performance of a two-stage software pipeline for automated Verilog RTL generation: LLM based automatic generation of annotated C++ code suitable for high-level synthesis (HLS), followed by HLS to generate Verilog RTL. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>We have benchmarked the performance of our proposed scheme using the open-source VerilogEval dataset, for four different industry-scale LLMs, and the Vitis HLS tool.Our experimental results demonstrate that our two-step technique substantially outperforms previous proposed techniques of direct Verilog RTL generation by LLMs in terms of average functional correctness rates, reaching score of 0.86 in pass@1 metric.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02793v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02793v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown great potential in code-related tasks, yet open-source models lag behind their closed-source counterparts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.865</span></span>To bridge this performance gap, existing methods generate vast amounts of synthetic data for fine-tuning, leading to inefficiencies in training.Motivated by the need for more effective and efficient training, we propose the Code Adaptive Compute-efficient Tuning (CodeACT) framework.CodeACT introduces the Complexity and Diversity Aware Sampling (CDAS) method to select high-quality training data based on complexity and diversity, and the Dynamic Pack padding strategy to reduce computational resource usage by minimizing padding tokens during training.Experimental results demonstrate that CodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data, achieves an 8.6% performance increase on HumanEval, reduces training time by 78%, and decreases peak GPU memory usage by 27%.<span class='px-1 mx-1 bg-yellow-200'>These findings underscore CodeACT's ability to enhance the performance and efficiency of open-source models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span><span class='px-1 mx-1 bg-yellow-200'>By optimizing both the data selection and training processes, CodeACT offers a comprehensive approach to improving the capabilities of open-source LLMs while significantly reducing computational requirements, addressing the dual challenges of data quality and training efficiency, and paving the way for more resource-efficient and performant models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02193v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02193v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Performance of Large Language Models for SDG Mapping (Technical Report)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The use of large language models (LLMs) is expanding rapidly, and open-source versions are becoming available, offering users safer and more adaptable options. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>These models enable users to protect data privacy by eliminating the need to provide data to third parties and can be customized for specific tasks.In this study, we compare the performance of various language models on the Sustainable Development Goal (SDG) mapping task, using the output of GPT-4o as the baseline.The selected open-source models for comparison include Mixtral, LLaMA 2, LLaMA 3, Gemma, and Qwen2.Additionally, GPT-4o-mini, a more specialized version of GPT-4o, was included to extend the comparison.Given the multi-label nature of the SDG mapping task, we employed metrics such as F1 score, precision, and recall with micro-averaging to evaluate different aspects of the models' performance.These metrics are derived from the confusion matrix to ensure a comprehensive evaluation.We provide a clear observation and analysis of each model's performance by plotting curves based on F1 score, precision, and recall at different thresholds.According to the results of this experiment, LLaMA 2 and Gemma still have significant room for improvement.The other four models do not exhibit particularly large differences in performance.The outputs from all seven models are available on Zenodo: https://doi.org/10.5281/zenodo.12789375.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02201v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02201v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite advances in text-to-3D generation methods, generation of multi-object arrangements remains challenging.Current methods exhibit failures in generating physically plausible arrangements that respect the provided text description.We present SceneMotifCoder (SMC), an example-driven framework for generating 3D object arrangements through visual program learning.<span class='px-1 mx-1 bg-yellow-200'>SMC leverages large language models (LLMs) and program synthesis to overcome these challenges by learning visual programs from example arrangements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>These programs are generalized into compact, editable meta-programs.When combined with 3D object retrieval and geometry-aware optimization, they can be used to create object arrangements varying in arrangement structure and contained objects.Our experiments show that SMC generates high-quality arrangements using meta-programs learned from few examples.Evaluation results demonstrates that object arrangements generated by SMC better conform to user-specified text descriptions and are more physically plausible when compared with state-of-the-art text-to-3D generation and layout methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02211v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02211v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SpecRover: Code Intent Extraction via LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Autonomous program improvement typically involves automatically producing bug fixes and feature additions.<span class='px-1 mx-1 bg-yellow-200'>Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches.In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent.Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior.The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches.Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover.In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover.Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite.The production of explanation by SpecRover allows for a better "signal" to be given to the developer, on when the suggested patches can be accepted with confidence.SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02232v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02232v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.<span class='px-1 mx-1 bg-yellow-200'>LLMs have achieved remarkable success in areas including code generation and vulnerability detection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>However, they also exhibit numerous limitations and shortcomings.LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents.It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain.In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering.In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance.We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering.We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02479v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02479v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A First Look at License Compliance Capability of LLMs in Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.902</span></span>However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production.This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code.To establish this benchmark, we conduct an empirical study to identify a reasonable standard for "striking similarity" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code.Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs.<span class='px-1 mx-1 bg-yellow-200'>Using LiCoEval, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses.These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks.Our study provides a foundation for future research and development to improve license compliance in AI-assisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02487v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02487v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context Conquers Parameters: Outperforming Proprietary LLM in Commit Message Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Commit messages provide descriptions of the modifications made in a commit using natural language, making them crucial for software maintenance and evolution.<span class='px-1 mx-1 bg-yellow-200'>Recent developments in Large Language Models (LLMs) have led to their use in generating high-quality commit messages, such as the Omniscient Message Generator (OMG). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>This method employs GPT-4 to produce state-of-the-art commit messages.However, the use of proprietary LLMs like GPT-4 in coding tasks raises privacy and sustainability concerns, which may hinder their industrial adoption.Considering that open-source LLMs have achieved competitive performance in developer tasks such as compiler validation, this study investigates whether they can be used to generate commit messages that are comparable with OMG.Our experiments show that an open-source LLM can generate commit messages that are comparable to those produced by OMG.In addition, through a series of contextual refinements, we propose lOcal MessagE GenerAtor (OMEGA) , a CMG approach that uses a 4-bit quantized 8B open-source LLM.OMEGA produces state-of-the-art commit messages, surpassing the performance of GPT-4 in practitioners' preference.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02502v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02502v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-31</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Performance Study of LLM-Generated Code on Leetcode
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.935</span></span><span class='px-1 mx-1 bg-yellow-200'>We compare 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span><span class='px-1 mx-1 bg-yellow-200'>This research introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.894</span></span>We also find that LLMs are capable of generating code that is, on average, more efficient than the code written by humans.The paper further discusses the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform's measurement reliability.<span class='px-1 mx-1 bg-yellow-200'>We believe that our findings contribute to a better understanding of LLM capabilities in code generation and set the stage for future optimizations in the field. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.21579v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.21579v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Visual question answering (VQA) is the task of providing accurate answers to natural language questions based on visual input.Programmatic VQA (PVQA) models have been gaining attention recently.<span class='px-1 mx-1 bg-yellow-200'>These use large language models (LLMs) to formulate executable programs that address questions requiring complex visual reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>However, there are challenges in enabling LLMs to comprehend the usage of image processing modules and generate relevant code.To overcome these challenges, this paper introduces PyramidCoder, a novel prompting framework for PVQA models.PyramidCoder consists of three hierarchical levels, each serving a distinct purpose: query rephrasing, code generation, and answer aggregation.Notably, PyramidCoder utilizes a single frozen LLM and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures.Compared to the state-of-the-art PVQA model, our approach improves accuracy by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.20563v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.20563v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Cocobo: Exploring Large Language Models as the Engine for End-User Robot Programming
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>End-user development allows everyday users to tailor service robots or applications to their needs.<span class='px-1 mx-1 bg-yellow-200'>One user-friendly approach is natural language programming. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>However, it encounters challenges such as an expansive user expression space and limited support for debugging and editing, which restrict its application in end-user programming.<span class='px-1 mx-1 bg-yellow-200'>The emergence of large language models (LLMs) offers promising avenues for the translation and interpretation between human language instructions and the code executed by robots, but their application in end-user programming systems requires further study. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span><span class='px-1 mx-1 bg-yellow-200'>We introduce Cocobo, a natural language programming system with interactive diagrams powered by LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span>Cocobo employs LLMs to understand users' authoring intentions, generate and explain robot programs, and facilitate the conversion between executable code and flowchart representations.Our user study shows that Cocobo has a low learning curve, enabling even users with zero coding experience to customize robot programs successfully.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.20712v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.20712v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      //  auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      //  rendering keys, e.g.:
      throwOnError : false
    });
  });
  function extractGSUrl(url) {
            var regex = /\/([^/]+?)(?:v\d+)?$/;
            var matches = url.match(regex);

            if (matches) {
                var endOfUrl = matches[1];
                var prefix = "https://scholar.google.com/scholar_lookup?arxiv_id=";
                var finalUrl = prefix + endOfUrl;
                console.log("Final URL: " + finalUrl);
                
                // You can open the final URL in a new tab/window
                window.open(finalUrl, "_blank");
            } else {
                console.log("No match found");
            }
        };
</script>
</body>
</html>