<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Ryan's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2025-07-21.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by running a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <p class="text-sm text-gray-500 pt-2">This project was originally created by <a href="https://koaning.io/">Vincent Warmerdam</a>, modifying his original <a href="https://koaning.github.io/arxiv-frontpage/">frontpage</a> for different paper categories.</p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Prompt Engineering in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Efficient Temporal Tokenization for Mobility Prediction with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a framework that leverages large language models (LLMs) as spatio-temporal predictors and trajectory reasoners.RHYTHM partitions trajectories into daily segments encoded as discrete tokens with hierarchical attention, capturing both daily and weekly dependencies while substantially reducing the sequence length.<span class='px-1 mx-1 bg-yellow-200'>Token representations are enriched with pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability to capture interdependencies without extensive computational overhead. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span>By freezing the LLM backbone, RHYTHM achieves significant computational efficiency.Evaluation on three real-world datasets demonstrates a 2.4% improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14017v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14017v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TransEvalnia: Reasoning-based Evaluation and Ranking of Translations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We present TransEvalnia, a prompting-based translation evaluation and ranking system that uses reasoning in performing its evaluations and ranking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span>This system presents fine-grained evaluations based on a subset of the Multidimensional Quality Metrics (https://themqm.org/), returns an assessment of which translation it deems the best, and provides numerical scores for the various dimensions and for the overall translation.We show that TransEvalnia performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al. 2024) on our own English-Japanese data as well as several language pairs from various WMT shared tasks.Using Anthropic's Claude-3.5-Sonnet and Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations returned are deemed highly acceptable to human raters, and that the scores assigned to the translations by Sonnet, as well as other LLMs, correlate well with scores assigned by the human raters.We also note the sensitivity of our system -- as well as MT-Ranker -- to the order in which the translations are presented, and we propose methods to address this position bias.All data, including the system's evaluation and reasoning, human assessments, as well as code is released.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12724v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12724v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Emotional Support with LLM-based Empathetic Dialogue Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support.<span class='px-1 mx-1 bg-yellow-200'>This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses.Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks.Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12820v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12820v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span><span class='px-1 mx-1 bg-yellow-200'>It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored.In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization.We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories.Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction.Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks.We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12901v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12901v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper presents our approach for the IberLEF 2025 Task PRESTA:Preguntas y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in Spanish).Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table.This solution evolves from the MRT implementation for the Semeval 2025 related task.The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions.<span class='px-1 mx-1 bg-yellow-200'>These steps use open-source LLMs and fine-grained optimized prompts for each step. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>With this approach, we achieved an accuracy score of 85\% in the task.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Cross-task Transfer of Large Language Models via Activation Steering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown impressive abilities in leveraging pretrained knowledge through prompting, but they often struggle with unseen tasks, particularly in data-scarce scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.791</span></span>While cross-task in-context learning offers a direct solution for transferring knowledge across tasks, it still faces critical challenges in terms of robustness, scalability, and efficiency.In this paper, we investigate whether cross-task transfer can be achieved via latent space steering without parameter updates or input expansion.Through an analysis of activation patterns in the latent space of LLMs, we observe that the enhanced activations induced by in-context examples have consistent patterns across different tasks.Inspired by these findings, we propose CAST, a novel Cross-task Activation Steering Transfer framework that enables effective transfer by manipulating the model's internal activation states.Our approach first selects influential and diverse samples from high-resource tasks, then utilizes their contrastive representation-enhanced activations to adapt LLMs to low-resource tasks.Extensive experiments across both cross-domain and cross-lingual transfer settings show that our method outperforms competitive baselines and demonstrates superior scalability and lower computational costs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13236v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13236v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities.<span class='px-1 mx-1 bg-yellow-200'>While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span>To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams.<span class='px-1 mx-1 bg-yellow-200'>We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span>This approach improves model performance on Hindi analogy questions.<span class='px-1 mx-1 bg-yellow-200'>Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.868</span></span>Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span>Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress.This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities.We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25.Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13266v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13266v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Task-agnostic prompt compression leverages the redundancy in natural language to reduce computational overhead and enhance information density within prompts, especially in long-context scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>Existing methods predominantly rely on information entropy as the metric to compress lexical units, aiming to achieve minimal information loss.However, these approaches overlook two critical aspects: (i) the importance of attention-critical tokens at the algorithmic level, and (ii) shifts in information entropy during the compression process.<span class='px-1 mx-1 bg-yellow-200'>Motivated by these challenges, we propose a dynamic attention-aware approach for task-agnostic prompt compression (DAC). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach effectively integrates entropy and attention information, dynamically sensing entropy shifts during compression to achieve fine-grained prompt compression. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span>Extensive experiments across various domains, including LongBench, GSM8K, and BBH, show that DAC consistently yields robust and substantial improvements across a diverse range of tasks and LLMs, offering compelling evidence of its efficacy.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11942v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11942v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) can provide accurate word definitions and explanations for any context.However, the scope of the definition changes for different target groups, like children or language learners.This is especially relevant for homonyms, words with multiple meanings, where oversimplification might risk information loss by omitting key senses, potentially misleading users who trust LLM outputs.We investigate how simplification impacts homonym definition quality across three target groups: Normal, Simple, and ELI5.Using two novel evaluation datasets spanning multiple languages, we test DeepSeek v3, Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge and human annotations.Our results show that simplification drastically degrades definition completeness by neglecting polysemy, increasing the risk of misunderstanding.<span class='px-1 mx-1 bg-yellow-200'>Fine-tuning Llama 3.1 8B with Direct Preference Optimization substantially improves homonym response quality across all prompt types. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>These findings highlight the need to balance simplicity and completeness in educational NLP to ensure reliable, context-aware definitions for all learners.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario.The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person.<span class='px-1 mx-1 bg-yellow-200'>Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>This paper summarises and extends earlier work presented at COSIT-24.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12059v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12059v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper presents an intervention study on the effects of the combined methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3) simplified gamification and (4) formative feedback on university students' Maths learning driven by large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.778</span></span>We call our approach Mathematics Explanations through Games by AI LLMs (MEGA).Some students struggle with Maths and as a result avoid Math-related discipline or subjects despite the importance of Maths across many fields, including signal processing.<span class='px-1 mx-1 bg-yellow-200'>Oftentimes, students' Maths difficulties stem from suboptimal pedagogy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>We compared the MEGA method to the traditional step-by-step (CoT) method to ascertain which is better by using a within-group design after randomly assigning questions for the participants, who are university students.Samples (n=60) were randomly drawn from each of the two test sets of the Grade School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH) datasets, based on the error margin of 11%, the confidence level of 90%, and a manageable number of samples for the student evaluators.These samples were used to evaluate two capable LLMs at length (Generative Pretrained Transformer 4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for capability.The results showed that students agree in more instances that the MEGA method is experienced as better for learning for both datasets.It is even much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH dataset, indicating that MEGA is better at explaining difficult Maths problems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12079v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12079v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Game playing has long served as a fundamental benchmark for evaluating Artificial General Intelligence (AGI).<span class='px-1 mx-1 bg-yellow-200'>While Large Language Models (LLMs) have demonstrated impressive capabilities in general reasoning, their effectiveness in spatial strategic reasoning, which is critical for complex and fully observable board games, remains insufficiently explored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>In this work, we adopt Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate rules and spatial complexity.To advance LLMs' strategic competence in such environments, we propose a training framework tailored to Xiangqi, built upon a large-scale dataset of five million board-move pairs enhanced with expert annotations and engine evaluations.Building on this foundation, we introduce Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning for legal move prediction to capture basic spatial rules, (2) incorporating strategic annotations to improve decision-making, and (3) applying reinforcement learning via Group Relative Policy Optimization (GRPO) with multi-dimensional reward signals to enhance reasoning stability.Our Experimental results indicate that, despite their size and power, general-purpose LLMs struggle to achieve satisfactory performance in these tasks.Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an 18% rise in move legality and a 22% boost in analysis accuracy.Our results point to a promising path for creating general strategic intelligence in spatially complex areas.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12215v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12215v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Humans are more gullible than LLMs in believing common psychological myths
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite widespread debunking, many psychological myths remain deeply entrenched.This paper investigates whether Large Language Models (LLMs) mimic human behaviour of myth belief and explores methods to mitigate such tendencies.<span class='px-1 mx-1 bg-yellow-200'>Using 50 popular psychological myths, we evaluate myth belief across multiple LLMs under different prompting strategies, including retrieval-augmented generation and swaying prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.831</span></span><span class='px-1 mx-1 bg-yellow-200'>Results show that LLMs exhibit significantly lower myth belief rates than humans, though user prompting can influence responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>RAG proves effective in reducing myth belief and reveals latent debiasing potential within LLMs.Our findings contribute to the emerging field of Machine Psychology and highlight how cognitive science methods can inform the evaluation and development of LLM-based systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12296v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12296v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) show potential for enhancing robotic path planning.This paper assesses visual input's utility for multimodal LLMs in such tasks via a comprehensive benchmark.We evaluated 15 multimodal LLMs on generating valid and optimal paths in 2D grid environments, simulating simplified robotic planning, comparing text-only versus text-plus-visual inputs across varying model sizes and grid complexities.<span class='px-1 mx-1 bg-yellow-200'>Our results indicate moderate success rates on simpler small grids, where visual input or few-shot text prompting offered some benefits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>However, performance significantly degraded on larger grids, highlighting a scalability challenge.While larger models generally achieved higher average success, the visual modality was not universally dominant over well-structured text for these multimodal systems, and successful paths on simpler grids were generally of high quality.These results indicate current limitations in robust spatial reasoning, constraint adherence, and scalable multimodal integration, identifying areas for future LLM development in robotic path planning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12391v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12391v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision.In this report, we investigate the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains.Our work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization.We introduce controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains.Our model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks.To facilitate continued research, we release our model publicly.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12507v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12507v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                REVA: Supporting LLM-Generated Programming Feedback Validation at Scale Through User Attention-based Adaptation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces REVA, a human-AI system that expedites instructor review of voluminous AI-generated programming feedback by sequencing submissions to minimize cognitive context shifts and propagating instructor-driven revisions across semantically similar instances. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span><span class='px-1 mx-1 bg-yellow-200'>REVA introduces a novel approach to human-AI collaboration in educational feedback by adaptively learning from instructors' attention in the review and revision process to continuously improve the feedback validation process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>REVA's usefulness and effectiveness in improving feedback quality and the overall feedback review process were evaluated through a within-subjects lab study with 12 participants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11470v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11470v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Many Instructions Can LLMs Follow at Once?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Production-grade LLM systems require robust adherence to dozens or even hundreds of instructions simultaneously.However, the instruction-following capabilities of LLMs at high instruction densities have not yet been characterized, as existing benchmarks only evaluate models on tasks with a single or few instructions.We introduce IFScale, a simple benchmark of 500 keyword-inclusion instructions for a business report writing task to measure how instruction-following performance degrades as instruction density increases.We evaluate 20 state-of-the-art models across seven major providers and find that even the best frontier models only achieve 68% accuracy at the max density of 500 instructions.Our analysis reveals model size and reasoning capability to correlate with 3 distinct performance degradation patterns, bias towards earlier instructions, and distinct categories of instruction-following errors.<span class='px-1 mx-1 bg-yellow-200'>Our insights can help inform design of instruction-dense prompts in real-world applications and highlight important performance-latency tradeoffs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>We open-source the benchmark and all results for further analysis at https://distylai.github.io/IFScale.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11538v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11538v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Robustness Tools in LLM Safety</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Bridge maintenance and safety are essential for transportation authorities, and Non-Destructive Evaluation (NDE) techniques are critical to assessing structural integrity.However, interpreting NDE data can be time-consuming and requires expertise, potentially delaying decision-making.Recent advancements in Large Language Models (LLMs) offer new ways to automate and improve this analysis.This pilot study introduces a holistic assessment of LLM capabilities for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in providing detailed bridge condition analyses.<span class='px-1 mx-1 bg-yellow-200'>It establishes a framework for integrating LLMs into bridge inspection workflows, indicating that LLM-assisted analysis can enhance efficiency without compromising accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>In this study, several LLMs are explored with prompts specifically designed to enhance the quality of image descriptions, which are applied to interpret five different NDE contour maps obtained through technologies for assessing bridge conditions.Each LLM model is evaluated based on its ability to produce detailed descriptions, identify defects, provide actionable recommendations, and demonstrate overall accuracy.The research indicates that four of the nine models provide better image descriptions, effectively covering a wide range of topics related to the bridge's condition.The outputs from these four models are summarized using five different LLMs to form a comprehensive overview of the bridge.Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more effective summaries.The findings suggest that LLMs have the potential to significantly improve efficiency and accuracy.This pilot study presents an innovative approach that leverages LLMs for image captioning in parallel and summarization, enabling faster decision-making in bridge maintenance and enhancing infrastructure management and safety assessments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14107v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14107v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines.Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions.Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns.<span class='px-1 mx-1 bg-yellow-200'>Additionally, hallucinations in LLMs can compromise public health communication. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.883</span></span>To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique.It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence.Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12840v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12840v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of large language models (LLMs) with control systems has demonstrated significant potential in various settings, such as task completion with a robotic manipulator.A main reason for this success is the ability of LLMs to perform in-context learning, which, however, strongly relies on the design of task examples, closely related to the target tasks.Consequently, employing LLMs to formulate optimal control problems often requires task examples that contain explicit mathematical expressions, designed by trained engineers.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, there is often no principled way to evaluate for hallucination before task execution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>To address these challenges, we propose DEMONSTRATE, a novel methodology that avoids the use of LLMs for complex optimization problem generations, and instead only relies on the embedding representations of task descriptions.To do this, we leverage tools from inverse optimal control to replace in-context prompt examples with task demonstrations, as well as the concept of multitask learning, which ensures target and example task similarity by construction.Given the fact that hardware demonstrations can easily be collected using teleoperation or guidance of the robot, our approach significantly reduces the reliance on engineering expertise for designing in-context examples.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, the enforced multitask structure enables learning from few demonstrations and assessment of hallucinations prior to task execution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span>We demonstrate the effectiveness of our method through simulation and hardware experiments involving a robotic arm tasked with tabletop manipulation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12855v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12855v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Probabilistic Soundness Guarantees in LLM Reasoning Chains
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span><span class='px-1 mx-1 bg-yellow-200'>Current LLM-based error detection methods often fail to detect propagated errors because they do not properly account for how earlier errors might corrupt judgments of downstream reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>To better detect such propagated errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a novel probabilistic framework that prevents error propagation by judging each claim based only on previously-assessed sound premises.This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label.ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12948v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12948v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Static to Intelligent: Evolving SaaS Pricing with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The SaaS paradigm has revolutionized software distribution by offering flexible pricing options to meet diverse customer needs.However, the rapid expansion of the SaaS market has introduced significant complexity for DevOps teams, who must manually manage and evolve pricing structures, an approach that is both time-consuming and prone to errors.The absence of automated tools for pricing analysis restricts the ability to efficiently evaluate, optimize, and scale these models.This paper proposes leveraging intelligent pricing (iPricing), dynamic, machine-readable pricing models, as a solution to these challenges.Intelligent pricing enables competitive analysis, streamlines operational decision-making, and supports continuous pricing evolution in response to market dynamics, leading to improved efficiency and accuracy.We present an LLM-driven approach that automates the transformation of static HTML pricing into iPricing, significantly improving efficiency and consistency while minimizing human error.Our implementation, AI4Pricing2Yaml, features a basic Information Extractor that uses web scraping and LLMs technologies to extract essential pricing components, plans, features, usage limits, and add-ons, from SaaS websites.Validation against a dataset of 30 distinct commercial SaaS, encompassing over 150 intelligent pricings, demonstrates the system's effectiveness in extracting the desired elements across all steps.<span class='px-1 mx-1 bg-yellow-200'>However, challenges remain in addressing hallucinations, complex structures, and dynamic content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>This work highlights the potential of automating intelligent pricing transformation to streamline SaaS pricing management, offering implications for improved consistency and scalability in an increasingly intricate pricing landscape.Future research will focus on refining extraction capabilities and enhancing the system's adaptability to a wider range of SaaS websites.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12104v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12104v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Based Config Synthesis requires Disambiguation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Beyond hallucinations, another problem in program synthesis using LLMs is ambiguity in user intent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>We illustrate the ambiguity problem in a networking context for LLM-based incremental configuration synthesis of route-maps and ACLs.These structures frequently overlap in header space, making the relative priority of actions impossible for the LLM to infer without user interaction.Measurements in a large cloud identify complex ACLs with 100's of overlaps, showing ambiguity is a real problem.We propose a prototype system, Clarify, which uses an LLM augmented with a new module called a Disambiguator that helps elicit user intent.On a small synthetic workload, Clarify incrementally synthesizes routing policies after disambiguation and then verifies them.Our treatment of ambiguities is useful more generally when the intent of updates can be correctly synthesized by LLMs, but their integration is ambiguous and can lead to different global behaviors.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12443v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12443v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have become a cornerstone of modern AI, driving breakthroughs in natural language processing and expanding into multimodal jobs involving images, audio, and video.As with most computational software, it is important to distinguish between ordinary runtime performance and startup overhead.Prior research has focused on runtime performance: improving training efficiency and stability.This work focuses instead on the increasingly critical issue of startup overhead in training: the delay before training jobs begin execution.<span class='px-1 mx-1 bg-yellow-200'>Startup overhead is particularly important in large, industrial-scale LLMs, where failures occur more frequently and multiple teams operate in iterative update-debug cycles. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>In one of our training clusters, more than 3.5% of GPU time is wasted due to startup overhead alone.   In this work, we present the first in-depth characterization of LLM training startup overhead based on real production data.We analyze the components of startup cost, quantify its direct impact, and examine how it scales with job size.These insights motivate the design of Bootseer, a system-level optimization framework that addresses three primary startup bottlenecks: (a) container image loading, (b) runtime dependency installation, and (c) model checkpoint resumption.To mitigate these bottlenecks, Bootseer introduces three techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and (c) striped HDFS-FUSE.Bootseer has been deployed in a production environment and evaluated on real LLM training workloads, demonstrating a 50% reduction in startup overhead.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12619v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12619v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                QSpark: Towards Reliable Qiskit Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and StarCoder often output flawed Qiskit code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span>We fine-tuned a 32 B model with two RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic dataset.On the Qiskit HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines; on the original HumanEval they score 65.90\% and 63.00\%.GRPO excels on basic tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five advanced tasks, highlighting clear gains yet room for progress in AI-assisted quantum programming.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12642v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12642v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) in software engineering has revealed critical limitations in existing benchmarks, particularly the widely used SWE-bench dataset.<span class='px-1 mx-1 bg-yellow-200'>Recent studies have uncovered severe data contamination issues, e.g. SWE-bench reports 32.67% of successful patches involve direct solution leakage and 31.08\% pass due to inadequate test cases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to address these fundamental challenges through an automated collection of real-world GitHub issues and rigorous quality validation.Our approach implements a reliable pipeline that ensures quality while minimizing contamination risks, resulting in approximately 10,000 potential tasks with 300 samples currently available.Evaluation using the Aider coding agent demonstrates strong discriminative power in state-of-the-art models.We report performance across a dozen recent LLMs evaluated on tasks collected between September 2024 and June 2025.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11059v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11059v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Logistics operators, from battlefield coordinators rerouting airlifts ahead of a storm to warehouse managers juggling late trucks, often face life-critical decisions that demand both domain expertise and rapid and continuous replanning.While popular methods like integer programming yield logistics plans that satisfy user-defined logical constraints, they are slow and assume an idealized mathematical model of the environment that does not account for uncertainty.<span class='px-1 mx-1 bg-yellow-200'>On the other hand, large language models (LLMs) can handle uncertainty and promise to accelerate replanning while lowering the barrier to entry by translating free-form utterances into executable plans, yet they remain prone to misinterpretations and hallucinations that jeopardize safety and cost. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>We introduce a neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable guarantees on goal interpretation.It converts user requests into structured planning specifications, quantifies its own uncertainty at the field and token level, and invokes an interactive clarification loop whenever confidence falls below an adaptive threshold.A lightweight model, fine-tuned on just 100 uncertainty-filtered examples, surpasses the zero-shot performance of GPT-4.1 while cutting inference latency by nearly 50%.These preliminary results highlight a practical path toward certifiable, real-time, and user-aligned decision-making for complex logistics.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11352v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11352v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory.To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading.<span class='px-1 mx-1 bg-yellow-200'>This involves swapping KV cache between GPU and CPU memory. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span>However, because the cache updates dynamically, such swapping incurs high CPU memory traffic.We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache.<span class='px-1 mx-1 bg-yellow-200'>Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed.<span class='px-1 mx-1 bg-yellow-200'>Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11507v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11507v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recognizing the information flows and operations comprising data science and machine learning Python notebooks is critical for evaluating, reusing, and adapting notebooks for new tasks.Investigating a notebook via re-execution often is impractical due to the challenges of resolving data and software dependencies.<span class='px-1 mx-1 bg-yellow-200'>While Large Language Models (LLMs) pre-trained on large codebases have demonstrated effectiveness in understanding code without running it, we observe that they fail to understand some realistic notebooks due to hallucinations and long-context challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>To address these issues, we propose a notebook understanding task yielding an information flow graph and corresponding cell execution dependency graph for a notebook, and demonstrate the effectiveness of a pincer strategy that uses limited syntactic analysis to assist full comprehension of the notebook using an LLM.Our Capture and Resolve Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning, thereby identifying the true data inputs and outputs of each cell.We evaluate and demonstrate the effectiveness of our approach using an annotated dataset of 50 representative, highly up-voted Kaggle notebooks that together represent 3454 actual cell inputs and outputs.The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the syntactic structure of these notebooks.Across 50 notebooks, CRABS achieves average F1 scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11742v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11742v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Security Challenges in LLM Development</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Injection 2.0: Hybrid AI Threats
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompt injection attacks, where malicious input is designed to manipulate AI systems into ignoring their original instructions and following unauthorized commands instead, were first discovered by Preamble, Inc. in May 2022 and responsibly disclosed to OpenAI. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span><span class='px-1 mx-1 bg-yellow-200'>Over the last three years, these attacks have continued to pose a critical security threat to LLM-integrated systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.766</span></span>The emergence of agentic AI systems, where LLMs autonomously perform multistep tasks through tools and coordination with other agents, has fundamentally transformed the threat landscape.<span class='px-1 mx-1 bg-yellow-200'>Modern prompt injection attacks can now combine with traditional cybersecurity exploits to create hybrid threats that systematically evade traditional security controls. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.853</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper presents a comprehensive analysis of Prompt Injection 2.0, examining how prompt injections integrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and other web security vulnerabilities to bypass traditional security measures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.828</span></span><span class='px-1 mx-1 bg-yellow-200'>We build upon Preamble's foundational research and mitigation technologies, evaluating them against contemporary threats, including AI worms, multi-agent infections, and hybrid cyber-AI attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span><span class='px-1 mx-1 bg-yellow-200'>Our analysis incorporates recent benchmarks that demonstrate how traditional web application firewalls, XSS filters, and CSRF tokens fail against AI-enhanced attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.804</span></span><span class='px-1 mx-1 bg-yellow-200'>We also present architectural solutions that combine prompt isolation, runtime security, and privilege separation with novel threat detection capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.828</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13169v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13169v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs Encode Harmfulness and Refusal Separately
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing?Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction.<span class='px-1 mx-1 bg-yellow-200'>In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span>There exists a harmfulness direction that is distinct from the refusal direction.<span class='px-1 mx-1 bg-yellow-200'>As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without reversing the model's internal belief of harmfulness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span><span class='px-1 mx-1 bg-yellow-200'>We also find that adversarially finetuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span><span class='px-1 mx-1 bg-yellow-200'>These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span><span class='px-1 mx-1 bg-yellow-200'>For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.746</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11878v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11878v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored.<span class='px-1 mx-1 bg-yellow-200'>Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.799</span></span>In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs.<span class='px-1 mx-1 bg-yellow-200'>First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.703</span></span><span class='px-1 mx-1 bg-yellow-200'>Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content.<span class='px-1 mx-1 bg-yellow-200'>We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.837</span></span>Our dataset and findings provide crucial insights for developing more robust and safe MLLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11968v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11968v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing remains an important approach to securing smart contracts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.717</span></span>Even though mutation scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers have primarily explored seed scheduling and generation, while mutation scheduling has been rarely addressed by prior work.In this work, we propose a Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and hybrid testing techniques.<span class='px-1 mx-1 bg-yellow-200'>Key components of the proposed LLAMA include: (i) a hierarchical prompting strategy that guides LLMs to generate semantically valid initial seeds, coupled with a lightweight pre-fuzzing phase to select high-potential inputs; (ii) a multi-feedback optimization mechanism that simultaneously improves seed generation, seed selection, and mutation scheduling by leveraging runtime coverage and dependency feedback; and (iii) an evolutionary fuzzing engine that dynamically adjusts mutation operator probabilities based on effectiveness, while incorporating symbolic execution to escape stagnation and uncover deeper vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage and vulnerability detection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.758</span></span>Specifically, it achieves 91% instruction coverage and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities across diverse categories.<span class='px-1 mx-1 bg-yellow-200'>These results highlight LLAMA's effectiveness, adaptability, and practicality in real-world smart contract security testing scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12084v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12084v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>ELOQUENT is a set of shared tasks that aims to create easily testable high-level criteria for evaluating generative language models.Sensemaking is one such shared task.   In Sensemaking, we try to assess how well generative models ``make sense out of a given text'' in three steps inspired by exams in a classroom setting: (1) Teacher systems should prepare a set of questions, (2) Student systems should answer these questions, and (3) Evaluator systems should score these answers, all adhering rather strictly to a given set of input materials.   We report on the 2025 edition of Sensemaking, where we had 7 sources of test materials (fact-checking analyses of statements, textbooks, transcribed recordings of a lecture, and educational videos) spanning English, German, Ukrainian, and Czech languages.   This year, 4 teams participated, providing us with 2 Teacher submissions, 2 Student submissions, and 2 Evaluator submissions.We added baselines for Teacher and Student using commercial large language model systems.We devised a fully automatic evaluation procedure, which we compare to a minimalistic manual evaluation.   We were able to make some interesting observations.For the first task, the creation of questions, better evaluation strategies will still have to be devised because it is difficult to discern the quality of the various candidate question sets.In the second task, question answering, the LLMs examined overall perform acceptably, but restricting their answers to the given input texts remains problematic.<span class='px-1 mx-1 bg-yellow-200'>In the third task, evaluation of question answers, our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm erroneously rate both garbled question-answer pairs and answers to mixed-up questions as acceptable. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12143v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12143v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Thought Purity: Defense Paradigm For Chain-of-Thought Attack
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.779</span></span><span class='px-1 mx-1 bg-yellow-200'>This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span><span class='px-1 mx-1 bg-yellow-200'>The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.841</span></span>Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics.<span class='px-1 mx-1 bg-yellow-200'>Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12314v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12314v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling.<span class='px-1 mx-1 bg-yellow-200'>However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span><span class='px-1 mx-1 bg-yellow-200'>To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.853</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span><span class='px-1 mx-1 bg-yellow-200'>Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span>This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt.<span class='px-1 mx-1 bg-yellow-200'>Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt.<span class='px-1 mx-1 bg-yellow-200'>Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>Code is available at https://github.com/ZichenWen1/DIJA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11097v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11097v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent studies have shown that Large Language Models (LLMs) are vulnerable to data poisoning attacks, where malicious training examples embed hidden behaviours triggered by specific input patterns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.759</span></span>However, most existing works assume a phrase and focus on the attack's effectiveness, offering limited understanding of trigger mechanisms and how multiple triggers interact within the model.In this paper, we present a framework for studying poisoning in LLMs.<span class='px-1 mx-1 bg-yellow-200'>We show that multiple distinct backdoor triggers can coexist within a single model without interfering with each other, enabling adversaries to embed several triggers concurrently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span>Using multiple triggers with high embedding similarity, we demonstrate that poisoned triggers can achieve robust activation even when tokens are substituted or separated by long token spans.<span class='px-1 mx-1 bg-yellow-200'>Our findings expose a broader and more persistent vulnerability surface in LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span>To mitigate this threat, we propose a post hoc recovery method that selectively retrains specific model components based on a layer-wise weight difference analysis.<span class='px-1 mx-1 bg-yellow-200'>Our method effectively removes the trigger behaviour with minimal parameter updates, presenting a practical and efficient defence against multi-trigger poisoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Verifying the credibility of Cyber Threat Intelligence (CTI) is essential for reliable cybersecurity defense. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>However, traditional approaches typically treat this task as a static classification problem, relying on handcrafted features or isolated deep learning models.These methods often lack the robustness needed to handle incomplete, heterogeneous, or noisy intelligence, and they provide limited transparency in decision-making-factors that reduce their effectiveness in real-world threat environments.To address these limitations, we propose LRCTI, a Large Language Model (LLM)-based framework designed for multi-step CTI credibility verification.The framework first employs a text summarization module to distill complex intelligence reports into concise and actionable threat claims.It then uses an adaptive multi-step evidence retrieval mechanism that iteratively identifies and refines supporting information from a CTI-specific corpus, guided by LLM feedback.Finally, a prompt-based Natural Language Inference (NLI) module is applied to evaluate the credibility of each claim while generating interpretable justifications for the classification outcome.Experiments conducted on two benchmark datasets, CTI-200 and PolitiFact show that LRCTI improves F1-Macro and F1-Micro scores by over 5%, reaching 90.9% and 93.6%, respectively, compared to state-of-the-art baselines.These results demonstrate that LRCTI effectively addresses the core limitations of prior methods, offering a scalable, accurate, and explainable solution for automated CTI credibility verification</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11310v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11310v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Internal Value Alignment in Large Language Models through Controlled Value Vector Activation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios.In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs.To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method.To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control.<span class='px-1 mx-1 bg-yellow-200'>Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.779</span></span>Source code and data are available at~ https://github.com/hr-jin/ConVA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11316v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11316v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">HCI in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services.However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration.<span class='px-1 mx-1 bg-yellow-200'>Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.717</span></span>Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy.This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance.Between these poles, the framework specifies four distinct intermediate structures.These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks.The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI.The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures.By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14034v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14034v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios.Existing large language model (LLM) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct communication.<span class='px-1 mx-1 bg-yellow-200'>To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span><span class='px-1 mx-1 bg-yellow-200'>Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14088v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14088v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Multi-agent debate (MAD) systems leverage collaborative interactions among large language models (LLMs) agents to improve reasoning capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span>While recent studies have focused on increasing the accuracy and scalability of MAD systems, their security vulnerabilities have received limited attention.In this work, we introduce MAD-Spear, a targeted prompt injection attack that compromises a small subset of agents but significantly disrupts the overall MAD process.Manipulated agents produce multiple plausible yet incorrect responses, exploiting LLMs' conformity tendencies to propagate misinformation and degrade consensus quality.Furthermore, the attack can be composed with other strategies, such as communication attacks, to further amplify its impact by increasing the exposure of agents to incorrect responses.To assess MAD's resilience under attack, we propose a formal definition of MAD fault-tolerance and develop a comprehensive evaluation framework that jointly considers accuracy, consensus efficiency, and scalability.Extensive experiments on five benchmark datasets with varying difficulty levels demonstrate that MAD-Spear consistently outperforms the baseline attack in degrading system performance.Additionally, we observe that agent diversity substantially improves MAD performance in mathematical reasoning tasks, which challenges prior work suggesting that agent diversity has minimal impact on performance.These findings highlight the urgent need to improve the security in MAD design.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13038v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13038v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Humour, as a complex language form, is derived from myriad aspects of life, whilst existing work on computational humour has focussed almost exclusively on short pun-based jokes.In this work, we investigate whether the ability of Large Language Models (LLMs) to explain humour depends on the particular humour form.We compare models on simple puns and more complex topical humour that requires knowledge of real-world entities and events.In doing so, we curate a dataset of 600 jokes split across 4 joke types and manually write high-quality explanations.<span class='px-1 mx-1 bg-yellow-200'>These jokes include heterographic and homographic puns, contemporary internet humour, and topical jokes, where understanding relies on reasoning beyond "common sense", rooted instead in world knowledge regarding news events and pop culture. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>Using this dataset, we compare the zero-shot abilities of a range of LLMs to accurately and comprehensively explain jokes of different types, identifying key research gaps in the task of humour explanation.We find that none of the tested models (inc. reasoning models) are capable of reliably generating adequate explanations of all joke types, further highlighting the narrow focus of most works in computational humour on overly simple joke forms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13335v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13335v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Reading comprehension is a fundamental skill in human cognitive development.<span class='px-1 mx-1 bg-yellow-200'>With the advancement of Large Language Models (LLMs), there is a growing need to compare how humans and LLMs understand language across different contexts and apply this understanding to functional tasks such as inference, emotion interpretation, and information retrieval. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.728</span></span>Our previous work used LLMs and human biomarkers to study the reading comprehension process.The results showed that the biomarkers corresponding to words with high and low relevance to the inference target, as labeled by the LLMs, exhibited distinct patterns, particularly when validated using eye-tracking data.However, focusing solely on individual words limited the depth of understanding, which made the conclusions somewhat simplistic despite their potential significance.<span class='px-1 mx-1 bg-yellow-200'>This study used an LLM-based AI agent to group words from a reading passage into nodes and edges, forming a graph-based text representation based on semantic meaning and question-oriented prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>We then compare the distribution of eye fixations on important nodes and edges.Our findings indicate that LLMs exhibit high consistency in language understanding at the level of graph topological structure.These results build on our previous findings and offer insights into effective human-AI co-learning strategies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11972v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11972v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have emerged as powerful tools for simulating complex social phenomena using human-like agents with specific traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>In human societies, value similarity is important for building trust and close relationships; however, it remains unexplored whether this principle holds true in artificial societies comprising LLM agents.<span class='px-1 mx-1 bg-yellow-200'>Therefore, this study investigates the influence of value similarity on relationship-building among LLM agents through two experiments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span>First, in a preliminary experiment, we evaluated the controllability of values in LLMs to identify the most effective model and prompt design for controlling the values.<span class='px-1 mx-1 bg-yellow-200'>Subsequently, in the main experiment, we generated pairs of LLM agents imbued with specific values and analyzed their mutual evaluations of trust and interpersonal closeness following a dialogue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>The experiments were conducted in English and Japanese to investigate language dependence.The results confirmed that pairs of agents with higher value similarity exhibited greater mutual trust and interpersonal closeness.<span class='px-1 mx-1 bg-yellow-200'>Our findings demonstrate that the LLM agent simulation serves as a valid testbed for social science theories, contributes to elucidating the mechanisms by which values influence relationship building, and provides a foundation for inspiring new theories and insights into the social sciences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.776</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11979v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11979v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The following paper introduces a general linguistic creativity test for humans and Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span>The test consists of various tasks aimed at assessing their ability to generate new original words and phrases based on word formation processes (derivation and compounding) and on metaphorical language use.We administered the test to 24 humans and to an equal number of LLMs, and we automatically evaluated their answers using OCSAI tool for three criteria: Originality, Elaboration, and Flexibility.The results show that LLMs not only outperformed humans in all the assessed criteria, but did better in six out of the eight test tasks.We then computed the uniqueness of the individual answers, which showed some minor differences between humans and LLMs.Finally, we performed a short manual analysis of the dataset, which revealed that humans are more inclined towards E(extending)-creativity, while LLMs favor F(ixed)-creativity.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12039v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12039v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Autonomous robots are increasingly being tested into public spaces to enhance user experiences, particularly in cultural and educational settings.This paper presents the design, implementation, and evaluation of the autonomous museum guide robot Alter-Ego equipped with advanced navigation and interactive capabilities.<span class='px-1 mx-1 bg-yellow-200'>The robot leverages state-of-the-art Large Language Models (LLMs) to provide real-time, context aware question-and-answer (Q&A) interactions, allowing visitors to engage in conversations about exhibits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>It also employs robust simultaneous localization and mapping (SLAM) techniques, enabling seamless navigation through museum spaces and route adaptation based on user requests.<span class='px-1 mx-1 bg-yellow-200'>The system was tested in a real museum environment with 34 participants, combining qualitative analysis of visitor-robot conversations and quantitative analysis of pre and post interaction surveys. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>Results showed that the robot was generally well-received and contributed to an engaging museum experience, despite some limitations in comprehension and responsiveness.This study sheds light on HRI in cultural spaces, highlighting not only the potential of AI-driven robotics to support accessibility and knowledge acquisition, but also the current limitations and challenges of deploying such technologies in complex, real-world environments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12273v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12273v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>However, they face challenges such as ambiguity in user requests processed by LLMs.To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models.The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities.The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7% success rate and proving particularly effective for complex ambiguities and efficient consensus.<span class='px-1 mx-1 bg-yellow-200'>While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12370v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12370v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Novelty is a crucial criterion in the peer review process for evaluating academic papers.Traditionally, it's judged by experts or measure by unique reference combinations.Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain.Moreover, it's unclear if unique citations truly measure novelty.<span class='px-1 mx-1 bg-yellow-200'>The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment.One of the most common types of novelty in academic papers is the introduction of new methods.In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.)in predicting the method novelty of papers.Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs.In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge.We compared the method we proposed with a large number of baselines.Extensive experiments demonstrate that our method achieves superior performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11330v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11330v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                General Modular Harness for LLM Agents in Multi-Turn Gaming Environments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering.Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings.Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades.<span class='px-1 mx-1 bg-yellow-200'>These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11633v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11633v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts.Existing decision support systems face scalability limitations.<span class='px-1 mx-1 bg-yellow-200'>Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts.The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases.We compared MAS performance with single-agent approaches and real-world benchmarks.An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark.Our results show that with current LLMs, a single agent GP performs as well as MDTs.The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete.Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10911v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10911v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Artificial Finance: How AI Thinks About Money
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we explore how large language models (LLMs) approach financial decision-making by systematically comparing their responses to those of human participants across the globe.We posed a set of commonly used financial decision-making questions to seven leading LLMs, including five models from the GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1.We then compared their outputs to human responses drawn from a dataset covering 53 nations.Our analysis reveals three main results.First, LLMs generally exhibit a risk-neutral decision-making pattern, favoring choices aligned with expected value calculations when faced with lottery-type questions.Second, when evaluating trade-offs between present and future, LLMs occasionally produce responses that appear inconsistent with normative reasoning.Third, when we examine cross-national similarities, we find that the LLMs' aggregate responses most closely resemble those of participants from Tanzania.<span class='px-1 mx-1 bg-yellow-200'>These findings contribute to the understanding of how LLMs emulate human-like decision behaviors and highlight potential cultural and training influences embedded within their outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.809</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10933v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10933v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Modeling Understanding of Story-Based Analogies Using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in Large Language Models (LLMs) have brought them closer to matching human cognition across a variety of tasks.How well do these models align with human performance in detecting and mapping analogies?Prior research has shown that LLMs can extract similarities from analogy problems but lack robust human-like reasoning.<span class='px-1 mx-1 bg-yellow-200'>Building on Webb, Holyoak, and Lu (2023), the current study focused on a story-based analogical mapping task and conducted a fine-grained evaluation of LLM reasoning abilities compared to human performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span>First, it explored the semantic representation of analogies in LLMs, using sentence embeddings to assess whether they capture the similarity between the source and target texts of an analogy, and the dissimilarity between the source and distractor texts.Second, it investigated the effectiveness of explicitly prompting LLMs to explain analogies.Throughout, we examine whether LLMs exhibit similar performance profiles to those observed in humans by evaluating their reasoning at the level of individual analogies, and not just at the level of overall accuracy (as prior studies have done).Our experiments include evaluating the impact of model size (8B vs. 70B parameters) and performance variation across state-of-the-art model architectures such as GPT-4 and LLaMA3.This work advances our understanding of the analogical reasoning abilities of LLMs and their potential as models of human reasoning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10957v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10957v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This Working Note summarizes the participation of the DS@GT team in two eRisk 2025 challenges.For the Pilot Task on conversational depression detection with large language-models (LLMs), we adopted a prompt-engineering strategy in which diverse LLMs conducted BDI-II-based assessments and produced structured JSON outputs.Because ground-truth labels were unavailable, we evaluated cross-model agreement and internal consistency.<span class='px-1 mx-1 bg-yellow-200'>Our prompt design methodology aligned model outputs with BDI-II criteria and enabled the analysis of conversational cues that influenced the prediction of symptoms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>Our best submission, second on the official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10958v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10958v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood.<span class='px-1 mx-1 bg-yellow-200'>We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span>Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration.Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions.Temperature significantly impacted whether and when consensus was reached across all six LLMs.<span class='px-1 mx-1 bg-yellow-200'>MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span><span class='px-1 mx-1 bg-yellow-200'>In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>However, neither temperature nor persona pairing lead to robust improvements in coding accuracy.Single agents matched or outperformed MAS consensus in most conditions.Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona.Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding.<span class='px-1 mx-1 bg-yellow-200'>We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.869</span></span>We open-source our MAS and experimentation code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11198v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11198v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Well-being in family settings involves subtle psychological dynamics that conventional metrics often overlook. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>In particular, unconscious parental expectations, termed ideal parent bias, can suppress children's emotional expression and autonomy.This suppression, referred to as suppressed emotion, often stems from well-meaning but value-driven communication, which is difficult to detect or address from outside the family.<span class='px-1 mx-1 bg-yellow-200'>Focusing on these latent dynamics, this study explores Large Language Model (LLM)-based support for psychologically safe family communication. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span>We constructed a Japanese parent-child dialogue corpus of 30 scenarios, each annotated with metadata on ideal parent bias and suppressed emotion.<span class='px-1 mx-1 bg-yellow-200'>Based on this corpus, we developed a Role-Playing LLM-based multi-agent dialogue support framework that analyzes dialogue and generates feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span>Specialized agents detect suppressed emotion, describe implicit ideal parent bias in parental speech, and infer contextual attributes such as the child's age and background.A meta-agent compiles these outputs into a structured report, which is then passed to five selected expert agents.These agents collaboratively generate empathetic and actionable feedback through a structured four-step discussion process.Experiments show that the system can detect categories of suppressed emotion with moderate accuracy and produce feedback rated highly in empathy and practicality.<span class='px-1 mx-1 bg-yellow-200'>Moreover, simulated follow-up dialogues incorporating this feedback exhibited signs of improved emotional expression and mutual understanding, suggesting the framework's potential in supporting positive transformation in family interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.873</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11210v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11210v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Novelty is a crucial criterion in the peer review process for evaluating academic papers.Traditionally, it's judged by experts or measure by unique reference combinations.Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain.Moreover, it's unclear if unique citations truly measure novelty.<span class='px-1 mx-1 bg-yellow-200'>The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment.The most common novelty in academic papers is the introduction of new methods.In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.)in predicting the method novelty of papers.Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs.In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge.We compared the method we proposed with a large number of baselines.Extensive experiments demonstrate that our method achieves superior performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11330v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11330v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Chain-of-thought traces have been shown to improve performance of large language models in a plethora of reasoning tasks, yet there is no consensus on the mechanism through which this performance boost is achieved.To shed more light on this, we introduce Causal CoT Graphs (CCGs), which are directed acyclic graphs automatically extracted from reasoning traces that model fine-grained causal dependencies in the language model output.A collection of $1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their associated CCGs are compiled into our dataset -- \textbf{KisMATH}.Our detailed empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in the CCG are mediators for the final answer, a condition necessary for reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating that models internally realise structures akin to our graphs.<span class='px-1 mx-1 bg-yellow-200'>KisMATH enables controlled, graph-aligned interventions and opens up avenues for further investigation into the role of chain-of-thought in LLM reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11408v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11408v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Background Large language models (LLMs) face challenges in inductive thematic analysis, a task requiring deep interpretive and domain-specific expertise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluated the feasibility of using LLMs to replicate expert-driven thematic analysis of social media data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.877</span></span>Methods Using two temporally non-intersecting Reddit datasets on xylazine (n=286 and n=686, for model optimization and validation, respectively) with twelve expert-derived themes, we evaluated five LLMs against expert coding.We modeled the task as a series of binary classifications, rather than a single, multi-label classification, employing zero-, single-, and few-shot prompting strategies and measuring performance via accuracy, precision, recall, and F1-score.Results On the validation set, GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71).For high-prevalence themes, model-derived thematic distributions closely mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use: 16.5% vs. 17.8%).<span class='px-1 mx-1 bg-yellow-200'>Conclusions Our findings suggest that few-shot LLM-based approaches can automate thematic analyses, offering a scalable supplement for qualitative research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span>Keywords: thematic analysis, large language models, natural language processing, qualitative analysis, social media, prompt engineering, public health</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10803v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10803v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MultiVox: Benchmarking Voice Assistants for Multimodal Interactions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid progress of Large Language Models (LLMs) has empowered omni models to act as voice assistants capable of understanding spoken dialogues.These models can process multimodal inputs beyond text, such as speech and visual data, enabling more context-aware interactions.However, current benchmarks fall short in comprehensively evaluating how well these models generate context-aware responses, particularly when it comes to implicitly understanding fine-grained speech characteristics, such as pitch, emotion, timbre, and volume or the environmental acoustic context such as background sounds.Additionally, they inadequately assess the ability of models to align paralinguistic cues with complementary visual signals to inform their responses.To address these gaps, we introduce MultiVox, the first omni voice assistant benchmark designed to evaluate the ability of voice assistants to integrate spoken and visual cues including paralinguistic speech features for truly multimodal understanding.Specifically, MultiVox includes 1000 human-annotated and recorded speech dialogues that encompass diverse paralinguistic features and a range of visual cues such as images and videos.<span class='px-1 mx-1 bg-yellow-200'>Our evaluation on 9 state-of-the-art models reveals that, although humans excel at these tasks, current models consistently struggle to produce contextually grounded responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10859v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10859v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Large Language Models in Social Sciences</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Objective:<span class='px-1 mx-1 bg-yellow-200'>Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary.Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.   Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences.Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2).For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references.Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.   Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers.In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity.<span class='px-1 mx-1 bg-yellow-200'>Automatic, reference-based metrics generally did not correlate well with manual judgments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>In Task 2, systems struggled with identifying difficult terms and classifying how to replace them.When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.   Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14096v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14096v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs).<span class='px-1 mx-1 bg-yellow-200'>However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span>This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling.We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems.Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge.We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning.Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings.This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support.For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12774v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12774v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Understanding the sources of variability in annotations is crucial for developing fair NLP systems, especially for tasks like sexism detection where demographic bias is a concern. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.873</span></span><span class='px-1 mx-1 bg-yellow-200'>This study investigates the extent to which annotator demographic features influence labeling decisions compared to text content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.776</span></span><span class='px-1 mx-1 bg-yellow-200'>Using a Generalized Linear Mixed Model, we quantify this inf luence, finding that while statistically present, demographic factors account for a minor fraction ( 8%) of the observed variance, with tweet content being the dominant factor. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span>We then assess the reliability of Generative AI (GenAI) models as annotators, specifically evaluating if guiding them with demographic personas improves alignment with human judgments.<span class='px-1 mx-1 bg-yellow-200'>Our results indicate that simplistic persona prompting often fails to enhance, and sometimes degrades, performance compared to baseline models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, explainable AI (XAI) techniques reveal that model predictions rely heavily on content-specific tokens related to sexism, rather than correlates of demographic characteristics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span><span class='px-1 mx-1 bg-yellow-200'>We argue that focusing on content-driven explanations and robust annotation protocols offers a more reliable path towards fairness than potentially persona simulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13138v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13138v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence.The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment.This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks.<span class='px-1 mx-1 bg-yellow-200'>In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span>We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field.We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment.Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques.Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions.By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13158v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13158v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automatically assessing oral narratives of Afrikaans and isiXhosa children
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing narrative and comprehension skills in early childhood is critical for later literacy.<span class='px-1 mx-1 bg-yellow-200'>However, teachers in large preschool classrooms struggle to accurately identify students who require intervention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.761</span></span>We present a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa.The system uses automatic speech recognition followed by a machine learning scoring model to predict narrative and comprehension scores.For scoring predicted transcripts, we compare a linear model to a large language model (LLM).The LLM-based system outperforms the linear model in most cases, but the linear system is competitive despite its simplicity.The LLM-based system is comparable to a human expert in flagging children who require intervention.<span class='px-1 mx-1 bg-yellow-200'>We lay the foundation for automatic oral assessments in classrooms, giving teachers extra capacity to focus on personalised support for children's learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13205v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13205v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research.AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers.In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context.Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs.<span class='px-1 mx-1 bg-yellow-200'>Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.72</span></span>To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task.We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13300v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13300v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Socio-economic indicators like regional GDP, population, and education levels, are crucial to shaping policy decisions and fostering sustainable development.This research introduces GeoReg a regression model that integrates diverse data sources, including satellite imagery and web-based geospatial information, to estimate these indicators even for data-scarce regions such as developing countries.Our approach leverages the prior knowledge of large language model (LLM) to address the scarcity of labeled data, with the LLM functioning as a data engineer by extracting informative features to enable effective estimation in few-shot settings.Specifically, our model obtains contextual relationships between data features and the target indicator, categorizing their correlations as positive, negative, mixed, or irrelevant.These features are then fed into the linear estimator with tailored weight constraints for each category.To capture nonlinear patterns, the model also identifies meaningful feature interactions and integrates them, along with nonlinear transformations.<span class='px-1 mx-1 bg-yellow-200'>Experiments across three countries at different stages of development demonstrate that our model outperforms baselines in estimating socio-economic indicators, even for low-income countries with limited data availability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13323v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13323v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Mental visualization, the ability to construct and manipulate visual representations internally, is a core component of human cognition and plays a vital role in tasks involving reasoning, prediction, and abstraction.Despite the rapid progress of Multimodal Large Language Models (MLLMs), current benchmarks primarily assess passive visual perception, offering limited insight into the more active capability of internally constructing visual patterns to support problem solving.<span class='px-1 mx-1 bg-yellow-200'>Yet mental visualization is a critical cognitive skill in humans, supporting abilities such as spatial navigation, predicting physical trajectories, and solving complex visual problems through imaginative simulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>To bridge this gap, we introduce Hyperphantasia, a synthetic benchmark designed to evaluate the mental visualization abilities of MLLMs through four carefully constructed puzzles.Each task is procedurally generated and presented at three difficulty levels, enabling controlled analysis of model performance across increasing complexity.<span class='px-1 mx-1 bg-yellow-200'>Our comprehensive evaluation of state-of-the-art models reveals a substantial gap between the performance of humans and MLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span>Additionally, we explore the potential of reinforcement learning to improve visual simulation capabilities.Our findings suggest that while some models exhibit partial competence in recognizing visual patterns, robust mental visualization remains an open challenge for current MLLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11932v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11932v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As online communication increasingly incorporates under-represented languages and colloquial dialects, standard translation systems often fail to preserve local slang, code-mixing, and culturally embedded markers of harmful speech. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>Translating toxic content between low-resource language pairs poses additional challenges due to scarce parallel data and safety filters that sanitize offensive expressions.In this work, we propose a reproducible, two-stage framework for toxicity-preserving translation, demonstrated on a code-mixed Singlish safety corpus.First, we perform human-verified few-shot prompt engineering: we iteratively curate and rank annotator-selected Singlish-target examples to capture nuanced slang, tone, and toxicity.Second, we optimize model-prompt pairs by benchmarking several large language models using semantic similarity via direct and back-translation.Quantitative human evaluation confirms the effectiveness and efficiency of our pipeline.Beyond improving translation quality, our framework contributes to the safety of multicultural LLMs by supporting culturally sensitive moderation and benchmarking in low-resource contexts.<span class='px-1 mx-1 bg-yellow-200'>By positioning Singlish as a testbed for inclusive NLP, we underscore the importance of preserving sociolinguistic nuance in real-world applications such as content moderation and regional platform governance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.887</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11966v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11966v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have traditionally relied on static training data, limiting their knowledge to fixed snapshots.Recent advancements, however, have equipped LLMs with web browsing capabilities, enabling real time information retrieval and multi step reasoning over live web content.While prior studies have demonstrated LLMs ability to access and analyze websites, their capacity to directly retrieve and analyze social media data remains unexplored.Here, we evaluate whether web browsing LLMs can infer demographic attributes of social media users given only their usernames.<span class='px-1 mx-1 bg-yellow-200'>Using a synthetic dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international participants, we show that these models can access social media content and predict user demographics with reasonable accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span><span class='px-1 mx-1 bg-yellow-200'>Analysis of the synthetic dataset further reveals how LLMs parse and interpret social media profiles, which may introduce gender and political biases against accounts with minimal activity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span>While this capability holds promise for computational social science in the post API era, it also raises risks of misuse particularly in information operations and targeted advertising underscoring the need for safeguards.We recommend that LLM providers restrict this capability in public facing applications, while preserving controlled access for verified research purposes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12372v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12372v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Previous literature has largely shown that Large Language Models (LLMs) perpetuate social biases learnt from their pre-training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.851</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the notable lack of resources for social bias evaluation in languages other than English, and for social contexts outside of the United States, this paper introduces the Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and CaBBQ). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on the original BBQ, these two parallel datasets are designed to assess social bias across 10 categories using a multiple-choice QA setting, now adapted to the Spanish and Catalan languages and to the social context of Spain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.8</span></span>We report evaluation results on different LLMs, factoring in model family, size and variant.Our results show that models tend to fail to choose the correct answer in ambiguous scenarios, and that high QA accuracy often correlates with greater reliance on social biases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11216v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11216v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging.<span class='px-1 mx-1 bg-yellow-200'>Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs.While some of these features are language-independent, the presence of language-specific features remains underexplored.In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network.We find that many such features predominantly appear in the middle to final layers of the model and are interpretable.These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability.Our code is available at https://github.com/LyzanderAndrylie/language-specific-features .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11230v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11230v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The proliferation of hate speech has inflicted significant societal harm, with its intensity and directionality closely tied to specific targets and arguments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>In recent years, numerous machine learning-based methods have been developed to detect hateful comments on online platforms automatically.<span class='px-1 mx-1 bg-yellow-200'>However, research on Chinese hate speech detection lags behind, and interpretability studies face two major challenges: first, the scarcity of span-level fine-grained annotated datasets limits models' deep semantic understanding of hate speech; second, insufficient research on identifying and interpreting coded hate speech restricts model explainability in complex real-world scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>To address these, we make the following contributions: (1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the hate semantic understanding of existing models using it.(2) We conduct the first comprehensive study on Chinese coded hate terms, LLMs' ability to interpret hate semantics.(3) We propose a method to integrate an annotated lexicon into models, significantly enhancing hate speech detection performance.Our work provides valuable resources and insights to advance the interpretability of Chinese hate speech detection research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11292v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11292v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Text-based telemedicine has become increasingly common, yet the quality of medical advice in doctor-patient interactions is often judged more on how advice is communicated rather than its clinical accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>To address this, we introduce Dr.Copilot , a multi-agent large language model (LLM) system that supports Romanian-speaking doctors by evaluating and enhancing the presentation quality of their written responses.Rather than assessing medical correctness, Dr.Copilot provides feedback along 17 interpretable axes.The system comprises of three LLM agents with prompts automatically optimized via DSPy.Designed with low-resource Romanian data and deployed using open-weight models, it delivers real-time specific feedback to doctors within a telemedicine platform.Empirical evaluations and live deployment with 41 doctors show measurable improvements in user reviews and response quality, marking one of the first real-world deployments of LLMs in Romanian medical settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11299v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11299v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Guiding LLM Decision-Making with Fairness Reward Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models are increasingly used to support high-stakes decisions, potentially influencing who is granted bail or receives a loan.Naive chain-of-thought sampling can improve average decision accuracy, but has also been shown to amplify unfair bias.To address this challenge and enable the trustworthy use of reasoning models in high-stakes decision-making, we propose a framework for training a generalizable Fairness Reward Model (FRM).Our model assigns a fairness score to LLM reasoning, enabling the system to down-weight biased trajectories and favor equitable ones when aggregating decisions across reasoning chains.We show that a single Fairness Reward Model, trained on weakly supervised, LLM-annotated examples of biased versus unbiased reasoning, transfers across tasks, domains, and model families without additional fine-tuning.<span class='px-1 mx-1 bg-yellow-200'>Applied to real-world decision-making tasks including recidivism prediction and social media moderation, we show that our approach consistently improves fairness while matching, or even surpassing, baseline accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11344v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11344v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Real-World Summarization: When Evaluation Reaches Its Limits
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We examine evaluation of faithfulness to input data in the context of hotel highlights: brief LLM-generated summaries that capture unique features of accommodations.<span class='px-1 mx-1 bg-yellow-200'>Through human evaluation campaigns involving categorical error assessment and span-level annotation, we compare traditional metrics, trainable methods, and LLM-as-a-judge approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that simpler metrics like word overlap correlate surprisingly well with human judgments (Spearman correlation rank of 0.63), often outperforming more complex methods when applied to out-of-domain data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>We further demonstrate that while LLMs can generate high-quality highlights, they prove unreliable for evaluation as they tend to severely under- or over-annotate.Our analysis of real-world business impacts shows incorrect and non-checkable information pose the greatest risks.We also highlight challenges in crowdsourced evaluations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11508v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11508v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-based ambiguity detection in natural language instructions for collaborative surgical robots
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios.Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities.A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues.<span class='px-1 mx-1 bg-yellow-200'>Individual evaluator assessments are synthesized through conformal prediction, which yields non-conformity scores based on comparison to a labeled calibration dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span>Evaluating Llama 3.2 11B and Gemma 3 12B, we observed classification accuracy exceeding 60% in differentiating ambiguous from unambiguous surgical instructions.Our approach improves the safety and reliability of human-robot collaboration in surgery by offering a mechanism to identify potentially ambiguous instructions before robot action.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11525v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11525v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs in Education Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound.<span class='px-1 mx-1 bg-yellow-200'>While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span><span class='px-1 mx-1 bg-yellow-200'>This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.51</span></span>The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients.Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient.<span class='px-1 mx-1 bg-yellow-200'>By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.571</span></span><span class='px-1 mx-1 bg-yellow-200'>This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13052v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13052v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scientific innovation is undergoing a paradigm shift driven by the rapid advancement of Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>As science faces mounting challenges including information overload, disciplinary silos, and diminishing returns on conventional research methods, LLMs are emerging as powerful agents capable not only of enhancing scientific workflows but also of participating in and potentially leading the innovation process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>Existing surveys mainly focus on different perspectives, phrases, and tasks in scientific research and discovery, while they have limitations in understanding the transformative potential and role differentiation of LLM.This survey proposes a comprehensive framework to categorize the evolving roles of LLMs in scientific innovation across three hierarchical levels: Evaluator, Collaborator, and Scientist.We distinguish between LLMs' contributions to structured scientific research processes and open-ended scientific discovery, thereby offering a unified taxonomy that clarifies capability boundaries, evaluation criteria, and human-AI interaction patterns at each level.Through an extensive analysis of current methodologies, benchmarks, systems, and evaluation metrics, this survey delivers an in-depth and systematic synthesis on LLM-driven scientific innovation.We present LLMs not only as tools for automating existing processes, but also as catalysts capable of reshaping the epistemological foundations of science itself.This survey offers conceptual clarity, practical guidance, and theoretical foundations for future research, while also highlighting open challenges and ethical considerations in the pursuit of increasingly autonomous AI-driven science.Resources related to this survey can be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11810v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11810v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Charts are a universally adopted medium for interpreting and communicating data.However, existing chart understanding benchmarks are predominantly English-centric, limiting their accessibility and applicability to global audiences.In this paper, we present PolyChartQA, the first large-scale multilingual chart question answering benchmark covering 22,606 charts and 26,151 question-answering pairs across 10 diverse languages.PolyChartQA is built using a decoupled pipeline that separates chart data from rendering code, allowing multilingual charts to be flexibly generated by simply translating the data and reusing the code.We leverage state-of-the-art LLM-based translation and enforce rigorous quality control in the pipeline to ensure the linguistic and semantic consistency of the generated multilingual charts.<span class='px-1 mx-1 bg-yellow-200'>PolyChartQA facilitates systematic evaluation of multilingual chart understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.522</span></span>Experiments on both open- and closed-source large vision-language models reveal a significant performance gap between English and other languages, especially low-resource ones with non-Latin scripts.This benchmark lays a foundation for advancing globally inclusive vision-language models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11939v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11939v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University Admission System), a real-world deployment of a conversational AI platform for higher education admissions counseling in Vietnam. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>While large language models (LLMs) offer potential for automating advisory tasks, most existing solutions remain limited to prototypes or synthetic benchmarks.<span class='px-1 mx-1 bg-yellow-200'>MARAUS addresses this gap by combining hybrid retrieval, multi-agent orchestration, and LLM-based generation into a system tailored for real-world university admissions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.537</span></span>In collaboration with the University of Transport Technology (UTT) in Hanoi, we conducted a two-phase study involving technical development and real-world evaluation.MARAUS processed over 6,000 actual user interactions, spanning six categories of queries.Results show substantial improvements over LLM-only baselines: on average 92 percent accuracy, hallucination rates reduced from 15 precent to 1.45 percent, and average response times below 4 seconds.The system operated cost-effectively, with a two-week deployment cost of 11.58 USD using GPT-4o mini.<span class='px-1 mx-1 bg-yellow-200'>This work provides actionable insights for the deployment of agentic RAG systems in low-resource educational settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.679</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11272v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11272v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Human reasoning involves different strategies, each suited to specific problems.Prior work shows that large language model (LLMs) tend to favor a single reasoning strategy, potentially limiting their effectiveness in diverse reasoning challenges.<span class='px-1 mx-1 bg-yellow-200'>In this work, we investigate whether prompting can control LLMs reasoning strategies and assess its impact on logical problem-solving. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span>While our experiments show that no single strategy consistently improves accuracy, performance could be enhanced if models could adaptively choose the optimal strategy.We propose methods to guide LLMs in strategy selection, highlighting new ways to refine their reasoning abilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11423v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11423v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry.However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering.<span class='px-1 mx-1 bg-yellow-200'>Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.592</span></span>DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total.<span class='px-1 mx-1 bg-yellow-200'>DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span><span class='px-1 mx-1 bg-yellow-200'>The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications.Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11527v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11527v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency.<span class='px-1 mx-1 bg-yellow-200'>This paper offers a structural diagnosis of such failures, revealing a persistent gap between \textit{comprehension} and \textit{competence}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.534</span></span>Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying them--a failure rooted not in knowledge access, but in computational execution.We term this phenomenon the computational \textit{split-brain syndrome}, where instruction and action pathways are geometrically and functionally dissociated.This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting.We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning.Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution.This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10624v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10624v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Warehouse Spatial Question Answering with LLM Agent
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs).Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability.In this paper, we present a data-efficient approach.<span class='px-1 mx-1 bg-yellow-200'>We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span><span class='px-1 mx-1 bg-yellow-200'>Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.54</span></span>Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation.The code is available at: https://github.com/hsiangwei0903/SpatialAgent</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10778v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10778v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs as Recommender Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Traditional information extraction systems face challenges with text only language models as it does not consider infographics (visual elements of information) such as tables, charts, images etc. often used to convey complex information to readers.Multimodal LLM (MLLM) face challenges of finding needle in the haystack problem i.e., either longer context length or substantial number of documents as search space.Late interaction mechanism over visual language models has shown state of the art performance in retrieval-based vision augmented Q&A tasks.There are yet few challenges using it for RAG based multi-modal Q&A.Firstly, many popular and widely adopted vector databases do not support native multi-vector retrieval.Secondly, late interaction requires computation which inflates space footprint and can hinder enterprise adoption.Lastly, the current state of late interaction mechanism does not leverage the approximate neighbor search indexing methods for large speed ups in retrieval process.This paper explores a pragmatic approach to make vision retrieval process scalable and efficient without compromising on performance quality.<span class='px-1 mx-1 bg-yellow-200'>We propose multi-step custom implementation utilizing widely adopted hybrid search (metadata & embedding) and state of the art late interaction re-ranker to retrieve best matching pages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>Finally, MLLM are prompted as reader to generate answers from contextualized best matching pages.Through experiments, we observe that the proposed design is scalable (significant speed up) and stable (without degrading performance quality), hence can be used as production systems at enterprises.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12378v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12378v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, much effort has been devoted to modeling users' multi-interests based on their behaviors or auxiliary signals.However, existing methods often rely on heuristic assumptions, e.g., co-occurring items indicate the same interest of users, failing to capture user multi-interests aligning with real-world scenarios.While large language models (LLMs) show significant potential for multi-interest analysis due to their extensive knowledge and powerful reasoning capabilities, two key challenges remain.First, the granularity of LLM-driven multi-interests is agnostic, possibly leading to overly fine or coarse interest grouping.Second, individual user analysis provides limited insights due to the data sparsity issue.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose an LLM-driven dual-level multi-interest modeling framework for more effective recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span>At the user-individual level, we exploit LLMs to flexibly allocate items engaged by users into different semantic clusters, indicating their diverse and distinct interests.To alleviate the agnostic generation of LLMs, we adaptively assign these semantic clusters to users' collaborative multi-interests learned from global user-item interactions, allowing the granularity to be automatically adjusted according to the user's behaviors using an alignment module.To alleviate the limited insights derived from individual users' behaviors, at the user-crowd level, we propose aggregating user cliques into synthesized users with rich behaviors for more comprehensive LLM-driven multi-interest analysis.We formulate a max covering problem to ensure the compactness and representativeness of synthesized users' behaviors, and then conduct contrastive learning based on their LLM-driven multi-interests to disentangle item representations among different interests.Experiments on real-world datasets show the superiority of our approach against state-of-the-art methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10917v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10917v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software libraries are central to the functionality, security, and maintainability of modern code.<span class='px-1 mx-1 bg-yellow-200'>As developers increasingly turn to Large Language Models (LLMs) to assist with programming tasks, understanding how these models recommend libraries is essential. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>In this paper, we conduct an empirical study of six state-of-the-art LLMs, both proprietary and open-source, by prompting them to solve real-world Python problems sourced from Stack Overflow.We analyze the types of libraries they import, the characteristics of those libraries, and the extent to which the recommendations are usable out of the box.Our results show that LLMs predominantly favour third-party libraries over standard ones, and often recommend mature, popular, and permissively licensed dependencies.However, we also identify gaps in usability: 4.6% of the libraries could not be resolved automatically due to structural mismatches between import names and installable packages, and only two models (out of six) provided installation guidance.While the generated code is technically valid, the lack of contextual support places the burden of manually resolving dependencies on the user.Our findings offer actionable insights for both developers and researchers, and highlight opportunities to improve the reliability and usability of LLM-generated code in the context of software dependencies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10818v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10818v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Scalable and Efficient Signal Integration System for Job Matching
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>LinkedIn, one of the world's largest platforms for professional networking and job seeking, encounters various modeling challenges in building recommendation systems for its job matching product, including cold-start, filter bubbles, and biases affecting candidate-job matching.To address these, we developed the STAR (Signal Integration for Talent And Recruiters) system, leveraging the combined strengths of Large Language Models (LLMs) and Graph Neural Networks (GNNs).LLMs excel at understanding textual data, such as member profiles and job postings, while GNNs capture intricate relationships and mitigate cold-start issues through network effects.STAR integrates diverse signals by uniting LLM and GNN capabilities with industrial-scale paradigms including adaptive sampling and version management.<span class='px-1 mx-1 bg-yellow-200'>It provides an end-to-end solution for developing and deploying embeddings in large-scale recommender systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Our key contributions include a robust methodology for building embeddings in industrial applications, a scalable GNN-LLM integration for high-performing recommendations, and practical insights for real-world model deployment.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09797v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09797v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Language-Driven Framework for Improving Personalized Recommendations: Merging LLMs with Traditional Algorithms
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Traditional recommendation algorithms are not designed to provide personalized recommendations based on user preferences provided through text, e.g., "I enjoy light-hearted comedies with a lot of humor". <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>Large Language Models (LLMs) have emerged as one of the most promising tools for natural language processing in recent years.<span class='px-1 mx-1 bg-yellow-200'>This research proposes a novel framework that mimics how a close friend would recommend items based on their knowledge of an individual's tastes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.872</span></span><span class='px-1 mx-1 bg-yellow-200'>We leverage LLMs to enhance movie recommendation systems by refining traditional algorithm outputs and integrating them with language-based user preference inputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span><span class='px-1 mx-1 bg-yellow-200'>We employ Singular Value Decomposition (SVD) or SVD++ algorithms to generate initial movie recommendations, implemented using the Surprise Python library and trained on the MovieLens-Latest-Small dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>We compare the performance of the base algorithms with our LLM-enhanced versions using leave-one-out validation hit rates and cumulative hit rates.<span class='px-1 mx-1 bg-yellow-200'>Additionally, to compare the performance of our framework against the current state-of-the-art recommendation systems, we use rating and ranking metrics with an item-based stratified 0.75 train, 0.25 test split. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.807</span></span>Our framework can generate preference profiles automatically based on users' favorite movies or allow manual preference specification for more personalized results.Using an automated approach, our framework overwhelmingly surpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of up to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a slight increase in computational overhead.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.07251v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.07251v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the past year, Generative Recommendations (GRs) have undergone substantial advancements, especially in leveraging the powerful sequence modeling and reasoning capabilities of Large Language Models (LLMs) to enhance overall recommendation performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.833</span></span><span class='px-1 mx-1 bg-yellow-200'>LLM-based GRs are forming a new paradigm that is distinctly different from discriminative recommendations, showing strong potential to replace traditional recommendation systems heavily dependent on complex hand-crafted features. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>In this paper, we provide a comprehensive survey aimed at facilitating further research of LLM-based GRs.Initially, we outline the general preliminaries and application cases of LLM-based GRs.Subsequently, we introduce the main considerations when LLM-based GRs are applied in real industrial scenarios.Finally, we explore promising directions for LLM-based GRs.We hope that this survey contributes to the ongoing advancement of the GR domain.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.06507v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.06507v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Boosting Parameter Efficiency in LLM-Based Recommendation through Sophisticated Pruning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>LLM-based recommender systems have made significant progress; however, the deployment cost associated with the large parameter volume of LLMs still hinders their real-world applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.862</span></span>This work explores parameter pruning to improve parameter efficiency while maintaining recommendation quality, thereby enabling easier deployment.Unlike existing approaches that focus primarily on inter-layer redundancy, we uncover intra-layer redundancy within components such as self-attention and MLP modules.Building on this analysis, we propose a more fine-grained pruning approach that integrates both intra-layer and layer-wise pruning.Specifically, we introduce a three-stage pruning strategy that progressively prunes parameters at different levels and parts of the model, moving from intra-layer to layer-wise pruning, or from width to depth.Each stage also includes a performance restoration step using distillation techniques, helping to strike a balance between performance and parameter efficiency.Empirical results demonstrate the effectiveness of our approach: across three datasets, our models achieve an average of 88% of the original model's performance while pruning more than 95% of the non-embedding parameters.This underscores the potential of our method to significantly reduce resource requirements without greatly compromising recommendation quality.Our code will be available at: https://github.com/zheng-sl/PruneRec</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.07064v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.07064v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Self-Attentive Sequential Recommendation (SASRec) effectively captures long-term user preferences by applying attention mechanisms to historical interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span><span class='px-1 mx-1 bg-yellow-200'>Concurrently, the rise of Large Language Models (LLMs) has motivated research into LLM-based recommendation, which leverages their powerful generalization and language understanding capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.792</span></span>However, LLMs often lack the domain-specific knowledge and collaborative signals essential for high-quality recommendations when relying solely on textual prompts.To address this limitation, this study proposes SASRecLLM, a novel framework that integrates SASRec as a collaborative encoder with an LLM fine-tuned using Low-Rank Adaptation (LoRA).The components are connected via a mapping layer to align their dimensional spaces, and three targeted training strategies are designed to optimize the hybrid architecture.Extensive experiments on multiple datasets demonstrate that SASRecLLM achieves robust and consistent improvements over strong baselines in both cold-start and warm-start scenarios.<span class='px-1 mx-1 bg-yellow-200'>This work advances the field of LLM-based recommendation by presenting a modular and effective paradigm for fusing structured collaborative filtering with the semantic power of fine-tuned LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>The implementation is available on GitHub: https://github.com/kechenkristin/RecLLM</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.05733v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.05733v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown strong potential in recommender systems due to their contextual learning and generalisation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing LLM-based recommendation approaches typically formulate the recommendation task using specialised prompts designed to leverage their contextual abilities, and aligning their outputs closely with human preferences to yield an improved recommendation performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.892</span></span>However, the use of LLMs for recommendation tasks is limited by the absence of domain-specific knowledge.This lack of relevant relational knowledge about the items to be recommended in the LLM's pre-training corpus can lead to inaccuracies or hallucinations, resulting in incorrect or misleading recommendations.<span class='px-1 mx-1 bg-yellow-200'>Moreover, directly using information from the knowledge graph introduces redundant and noisy information, which can affect the LLM's reasoning process or exceed its input context length, thereby reducing the performance of LLM-based recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span><span class='px-1 mx-1 bg-yellow-200'>To address the lack of domain-specific knowledge, we propose a novel model called Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation (KERAG_R). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we leverage a graph retrieval-augmented generation (GraphRAG) component to integrate additional information from a knowledge graph (KG) into instructions, enabling the LLM to collaboratively exploit recommendation signals from both text-based user interactions and the knowledge graph to better estimate the users' preferences in a recommendation context. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>In particular, we perform graph RAG by pre-training a graph attention network (GAT) to select the most relevant triple for the target users for the used LLM, thereby enhancing the LLM while reducing redundant and noisy information.<span class='px-1 mx-1 bg-yellow-200'>Our extensive experiments on three public datasets show that our proposed KERAG_R model significantly outperforms ten existing state-of-the-art recommendation methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.876</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.05863v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.05863v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>A recent Large language model (LLM)-based recommendation model, called RecRanker, has demonstrated a superior performance in the top-k recommendation task compared to other models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.836</span></span><span class='px-1 mx-1 bg-yellow-200'>In particular, RecRanker samples users via clustering, generates an initial ranking list using an initial recommendation model, and fine-tunes an LLM through hybrid instruction tuning to infer user preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>However, the contribution of each core component remains underexplored.In this work, we inspect the reproducibility of RecRanker, and study the impact and role of its various components.We begin by reproducing the RecRanker pipeline through the implementation of all its key components.Our reproduction shows that the pairwise and listwise methods achieve a performance comparable to that reported in the original paper.For the pointwise method, while we are also able to reproduce the original paper's results, further analysis shows that the performance is abnormally high due to data leakage from the inclusion of ground-truth information in the prompts.<span class='px-1 mx-1 bg-yellow-200'>To enable a fair and comprehensive evaluation of LLM-based top-k recommendations, we propose RecRankerEval, an extensible framework that covers five key dimensions: user sampling strategy, initial recommendation model, LLM backbone, dataset selection, and instruction tuning method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.797</span></span>Using the RecRankerEval framework, we show that the original results of RecRanker can be reproduced on the ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset, but not on BookCrossing due to the lack of timestamp information in the original RecRanker paper.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we demonstrate that RecRanker's performance can be improved by employing alternative user sampling methods, stronger initial recommenders, and more capable LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.764</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.05880v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.05880v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Explainable recommendations, which use the information of user and item with interaction to generate a explanation for why the user would interact with the item, are crucial for improving user trust and decision transparency to the recommender system. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span>Existing methods primarily rely on encoding features of users and items to embeddings, which often leads to information loss due to dimensionality reduction, sparse interactions, and so on.With the advancements of large language models (LLMs) in language comprehension, some methods use embeddings as LLM inputs for explanation generation.However, since embeddings lack inherent semantics, LLMs must adjust or extend their parameters to interpret them, a process that inevitably incurs information loss.<span class='px-1 mx-1 bg-yellow-200'>To address this issue, we propose a novel approach combining profile generation via hierarchical interaction summarization (PGHIS), which leverages a pretrained LLM to hierarchically summarize user-item interactions, generating structured textual profiles as explicit representations of user and item characteristics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span><span class='px-1 mx-1 bg-yellow-200'>Additionally, we propose contrastive prompting for explanation generation (CPEG) which employs contrastive learning to guide another reasoning language models in producing high-quality ground truth recommendation explanations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>Finally, we use the textual profiles of user and item as input and high-quality explanation as output to fine-tune a LLM for generating explanations.Experimental results on multiple datasets demonstrate that our approach outperforms existing state-of-the-art methods, achieving a great improvement on metrics about explainability (e.g., 5% on GPTScore) and text quality.Furthermore, our generated ground truth explanations achieve a significantly higher win rate compared to user-written reviews and those produced by other methods, demonstrating the effectiveness of CPEG in generating high-quality ground truths.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.06044v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.06044v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Data-Semantics-Aware Recommendation of Diverse Pivot Tables
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Data summarization is essential to discover insights from large datasets.In a spreadsheets, pivot tables offer a convenient way to summarize tabular data by computing aggregates over some attributes, grouped by others.However, identifying attribute combinations that will result in useful pivot tables remains a challenge, especially for high-dimensional datasets.We formalize the problem of automatically recommending insightful and interpretable pivot tables, eliminating the tedious manual process.A crucial aspect of recommending a set of pivot tables is to diversify them.Traditional works inadequately address the table-diversification problem, which leads us to consider the problem of pivot table diversification.   <span class='px-1 mx-1 bg-yellow-200'>We present SAGE, a data-semantics-aware system for recommending k-budgeted diverse pivot tables, overcoming the shortcomings of prior work for top-k recommendations that cause redundancy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>SAGE ensures that each pivot table is insightful, interpretable, and adaptive to the user's actions and preferences, while also guaranteeing that the set of pivot tables are different from each other, offering a diverse recommendation.We make two key technical contributions: (1) a data-semantics-aware model to measure the utility of a single pivot table and the diversity of a set of pivot tables, and (2) a scalable greedy algorithm that can efficiently select a set of diverse pivot tables of high utility, by leveraging data semantics to significantly reduce the combinatorial search space.Our extensive experiments on three real-world datasets show that SAGE outperforms alternative approaches, and efficiently scales to accommodate high-dimensional datasets.Additionally, we present several case studies to highlight SAGE's qualitative effectiveness over commercial software and Large Language Models (LLMs).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.06171v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.06171v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Session-based Recommendation (SBR) aims to predict the next item a user will likely engage with, using their interaction sequence within an anonymous session. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>Existing SBR models often focus only on single-session information, ignoring inter-session relationships and valuable cross-session insights.Some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance.Additionally, most models rely on item ID co-occurrence and overlook rich semantic details, limiting their ability to capture fine-grained item features.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose a novel hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations, called HIPHOP. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.8</span></span>First, we introduce a pluggable embedding module based on large language models (LLMs) to generate high-quality semantic representations, enhancing item embeddings.Second, HIPHOP utilizes graph neural networks (GNNs) to model item transition relationships and incorporates a dynamic multi-intent capturing module to address users' diverse interests within a session.Additionally, we design a hierarchical inter-session similarity learning module, guided by user intent, to capture global and local session relationships, effectively exploring users' long-term and short-term interests.To mitigate noise, an intent-guided denoising strategy is applied during inter-session learning.Finally, we enhance the model's discriminative capability by using contrastive learning to optimize session representations.<span class='px-1 mx-1 bg-yellow-200'>Experiments on multiple datasets show that HIPHOP significantly outperforms existing methods, demonstrating its effectiveness in improving recommendation quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span>Our code is available: https://github.com/hjx159/HIPHOP.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.04623v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.04623v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Heterogeneous User Modeling for LLM-based Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Leveraging Large Language Models (LLMs) for recommendation has demonstrated notable success in various domains, showcasing their potential for open-domain recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.784</span></span><span class='px-1 mx-1 bg-yellow-200'>A key challenge to advancing open-domain recommendation lies in effectively modeling user preferences from users' heterogeneous behaviors across multiple domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span>Existing approaches, including ID-based and semantic-based modeling, struggle with poor generalization, an inability to compress noisy interactions effectively, and the domain seesaw phenomenon.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose a Heterogeneous User Modeling (HUM) method, which incorporates a compression enhancer and a robustness enhancer for LLM-based recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.747</span></span>The compression enhancer uses a customized prompt to compress heterogeneous behaviors into a tailored token, while a masking mechanism enhances cross-domain knowledge extraction and understanding.The robustness enhancer introduces a domain importance score to mitigate the domain seesaw phenomenon by guiding domain optimization.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on heterogeneous datasets validate that HUM effectively models user heterogeneity by achieving both high efficacy and robustness, leading to superior performance in open-domain recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.04626v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.04626v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "This Suits You the Best": Query Focused Comparative Explainable Summarization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Product recommendations inherently involve comparisons, yet traditional opinion summarization often fails to provide holistic comparative insights. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span>We propose the novel task of generating Query-Focused Comparative Explainable Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS).<span class='px-1 mx-1 bg-yellow-200'>To address the lack of query-focused recommendation datasets, we introduce MS-Q2P, comprising 7,500 queries mapped to 22,500 recommended products with metadata. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.783</span></span>We leverage Large Language Models (LLMs) to generate tabular comparative summaries with query-specific explanations.Our approach is personalized, privacy-preserving, recommendation engine-agnostic, and category-agnostic.M-OS as an intermediate step reduces inference latency approximately by 40% compared to the direct input approach (DIA), which processes raw data directly.We evaluate open-source and proprietary LLMs for generating and assessing QF-CES.Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity, faithfulness, informativeness, format adherence, and query relevance) showed an average Spearman correlation of 0.74 with human judgments, indicating its potential for QF-CES evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.04733v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.04733v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender Systems via Bi-level Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Model-enhanced Recommender Systems (LLM-enhanced RSs) have emerged as a powerful approach to improving recommendation quality by leveraging LLMs to generate item representations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span>Despite these advancements, the integration of LLMs raises severe fairness concerns.Existing studies reveal that LLM-based RSs exhibit greater unfairness than traditional RSs, yet fairness issues in LLM-enhanced RSs remain largely unexplored.In this paper, our empirical study reveals that while LLM-enhanced RSs improve fairness across item groups, a significant fairness gap persists.Further enhancement remains challenging due to the architectural differences and varying sources of unfairness inherent in LLM-enhanced RSs.<span class='px-1 mx-1 bg-yellow-200'>To bridge this gap, we first decompose unfairness into i) \textit{prior unfairness} in LLM-generated representations and ii) \textit{training unfairness} in recommendation models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Then, we propose BiFair, a bi-level optimization-based fairness-aware training framework designed to mitigate both prior and training unfairness simultaneously.<span class='px-1 mx-1 bg-yellow-200'>BiFair optimizes two sets of learnable parameters: LLM-generated representations and a trainable projector in the recommendation model, using a two-level nested optimization process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>Additionally, we introduce an adaptive inter-group balancing mechanism, leveraging multi-objective optimization principles to dynamically balance fairness across item groups.Extensive experiments on three real-world datasets demonstrate that BiFair significantly mitigates unfairness and outperforms previous state-of-the-art methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.04294v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.04294v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-03</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Listwise Preference Alignment Optimization for Tail Item Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Preference alignment has achieved greater success on Large Language Models (LLMs) and drawn broad interest in recommendation research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.777</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing preference alignment methods for recommendation either require explicit reward modeling or only support pairwise preference comparison. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>The former directly increases substantial computational costs, while the latter hinders training efficiency on negative samples.<span class='px-1 mx-1 bg-yellow-200'>Moreover, no existing effort has explored preference alignment solutions for tail-item recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span>To bridge the above gaps, we propose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison to listwise comparison, to improve the efficiency of model training.Specifically, we derive a closed form optimal policy to enable more efficient and effective training without explicit reward modeling.<span class='px-1 mx-1 bg-yellow-200'>We also present an adaptive negative sampling and reweighting strategy to prioritize tail items during optimization and enhance performance in tail-item recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>Besides, we theoretically prove that optimizing the listwise preference optimization (LPO) loss is equivalent to maximizing the upper bound of the optimal reward.<span class='px-1 mx-1 bg-yellow-200'>Our experiments on three public datasets show that our method outperforms 10 baselines by a large margin, achieving up to 50% performance improvement while reducing 17.9% GPU memory usage when compared with direct preference optimization (DPO) in tail-item recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>Our code is available at https://github.com/Yuhanleeee/LPO4Rec.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.02255v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.02255v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-03</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Content filtering methods for music recommendation: A review
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recommendation systems have become essential in modern music streaming platforms, shaping how users discover and engage with songs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span><span class='px-1 mx-1 bg-yellow-200'>One common approach in recommendation systems is collaborative filtering, which suggests content based on the preferences of users with similar listening patterns to the target user. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span>However, this method is less effective on media where interactions are sparse.Music is one such medium, since the average user of a music streaming service will never listen to the vast majority of tracks.Due to this sparsity, there are several challenges that have to be addressed with other methods.<span class='px-1 mx-1 bg-yellow-200'>This review examines the current state of research in addressing these challenges, with an emphasis on the role of content filtering in mitigating biases inherent in collaborative filtering approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>We explore various methods of song classification for content filtering, including lyrical analysis using Large Language Models (LLMs) and audio signal processing techniques.Additionally, we discuss the potential conflicts between these different analysis methods and propose avenues for resolving such discrepancies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.02282v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.02282v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-03</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Owing to powerful natural language processing and generative capabilities, large language model (LLM) agents have emerged as a promising solution for enhancing recommendation systems via user simulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>However, in the realm of video recommendation, existing studies predominantly resort to prompt-based simulation using frozen LLMs and encounter the intricate challenge of multimodal content understanding.<span class='px-1 mx-1 bg-yellow-200'>This frequently results in suboptimal item modeling and user preference learning, thereby ultimately constraining recommendation performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>To address these challenges, we introduce VRAgent-R1, a novel agent-based paradigm that incorporates human-like intelligence in user simulation.Specifically, VRAgent-R1 comprises two distinct agents: the Item Perception (IP) Agent and the User Simulation (US) Agent, designed for interactive user-item modeling.Firstly, the IP Agent emulates human-like progressive thinking based on MLLMs, effectively capturing hidden recommendation semantics in videos.With a more comprehensive multimodal content understanding provided by the IP Agent, the video recommendation system is equipped to provide higher-quality candidate items.Subsequently, the US Agent refines the recommended video sets based on in-depth chain-of-thought (CoT) reasoning and achieves better alignment with real user preferences through reinforcement learning.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on a large-scale video recommendation benchmark have demonstrated the effectiveness of our proposed VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10 on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\% higher accuracy in user decision simulation compared to state-of-the-art baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.02626v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.02626v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Production workflows for LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Photonic Fabric Platform for AI Accelerators
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM (PFA), a photonic-enabled switch and memory subsystem that delivers low latency, high bandwidth, and low per-bit energy.<span class='px-1 mx-1 bg-yellow-200'>By integrating high-bandwidth HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D electro-optical system-in-package, the PFA offers up to 32 TB of shared memory alongside 115 Tbps of all-to-all digital switching. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.336</span></span><span class='px-1 mx-1 bg-yellow-200'>The Photonic FabricTM enables distributed AI training and inference to execute parallelism strategies more efficiently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.399</span></span>The Photonic Fabric removes the silicon beachfront constraint that limits the fixed memory-to-compute ratio observed in virtually all current XPU accelerator designs.<span class='px-1 mx-1 bg-yellow-200'>Replacing a local HBM stack on an XPU with a chiplet that connects to the Photonic Fabric increases its memory capacity and correspondingly its memory bandwidth by offering a flexible path to scaling well beyond the limitations of on-package HBM alone. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.361</span></span>We introduce CelestiSim, a lightweight analytical simulator validated on NVIDIA H100 and H200 systems.<span class='px-1 mx-1 bg-yellow-200'>It is used to evaluate the performance of LLM reference and energy savings on PFA, without any significant change to the GPU core design. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.39</span></span><span class='px-1 mx-1 bg-yellow-200'>With the PFA, the simulation results show that up to 3.66x throughput and 1.40x latency improvements in LLM inference at 405B parameters, up to 7.04x throughput and 1.41x latency improvements at 1T parameters, and 60-90% energy savings in data movement for heavy collective operations in all LLM training scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>While these results are shown for NVIDIA GPUs, they can be applied similarly to other AI accelerator designs (XPUs) that share the same fundamental limitation of fixed memory to compute.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14000v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14000v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper presents a comprehensive evaluation of cost-efficient Large Language Models (LLMs) for diverse biomedical tasks spanning both text and image modalities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.301</span></span>We evaluated a range of closed-source and open-source LLMs on tasks such as biomedical text classification and generation, question answering, and multimodal image processing.<span class='px-1 mx-1 bg-yellow-200'>Our experimental findings indicate that there is no single LLM that can consistently outperform others across all tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.348</span></span>Instead, different LLMs excel in different tasks.<span class='px-1 mx-1 bg-yellow-200'>While some closed-source LLMs demonstrate strong performance on specific tasks, their open-source counterparts achieve comparable results (sometimes even better), with additional benefits like faster inference and enhanced privacy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>Our experimental results offer valuable insights for selecting models that are optimally suited for specific biomedical applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14045v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14045v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.315</span></span><span class='px-1 mx-1 bg-yellow-200'>Despite their importance, they are severely underrepresented in large-scale EHR datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.378</span></span><span class='px-1 mx-1 bg-yellow-200'>For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.316</span></span>In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.   We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes.The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs.At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits.This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.   We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation.The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes.By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14079v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14079v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The evaluation of large language models is a complex task, in which several approaches have been proposed.The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics.However, this method has certain limitations, being the most concerning, the poor correlation with the humans.An alternative approach, is to have humans evaluate the LLMs.This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models.<span class='px-1 mx-1 bg-yellow-200'>An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.351</span></span>The results are then elaborated into a model ranking.An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest.In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process.Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models.This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13302v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13302v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.539</span></span>Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance.To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine.<span class='px-1 mx-1 bg-yellow-200'>From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.453</span></span><span class='px-1 mx-1 bg-yellow-200'>TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.333</span></span><span class='px-1 mx-1 bg-yellow-200'>To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.313</span></span><span class='px-1 mx-1 bg-yellow-200'>Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.384</span></span>The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers.This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13332v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13332v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLM Model Architectures and Training Techniques</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability.We present KROMA, a novel OM framework that harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge.<span class='px-1 mx-1 bg-yellow-200'>To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable.Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14032v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14032v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Sixth generation (6G) networks demand tight integration of artificial intelligence (AI) into radio access networks (RANs) to meet stringent quality of service (QoS) and resource efficiency requirements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing solutions struggle to bridge the gap between high level user intents and the low level, parameterized configurations required for optimal performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.549</span></span>To address this challenge, we propose RIDAS, a multi agent framework composed of representation driven agents (RDAs) and an intention driven agent (IDA).<span class='px-1 mx-1 bg-yellow-200'>RDAs expose open interface with tunable control parameters (rank and quantization bits, enabling explicit trade) offs between distortion and transmission rate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.567</span></span><span class='px-1 mx-1 bg-yellow-200'>The IDA employs a two stage planning scheme (bandwidth pre allocation and reallocation) driven by a large language model (LLM) to map user intents and system state into optimal RDA configurations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span><span class='px-1 mx-1 bg-yellow-200'>Experiments demonstrate that RIDAS supports 44.71\% more users than WirelessAgent under equivalent QoS constraints. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.544</span></span>These results validate ability of RIDAS to capture user intent and allocate resources more efficiently in AI RAN environments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13140v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13140v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs).These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning.However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity.<span class='px-1 mx-1 bg-yellow-200'>To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.404</span></span><span class='px-1 mx-1 bg-yellow-200'>To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.482</span></span>Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution.<span class='px-1 mx-1 bg-yellow-200'>Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.429</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.414</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13152v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13152v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs).<span class='px-1 mx-1 bg-yellow-200'>Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.49</span></span>This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch.Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination.These guideposts, applied to what we term "SMA-LLS" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years.We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13175v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13175v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey of Context Engineering for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference.<span class='px-1 mx-1 bg-yellow-200'>This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.456</span></span><span class='px-1 mx-1 bg-yellow-200'>We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.428</span></span><span class='px-1 mx-1 bg-yellow-200'>We first examine the foundational components: context retrieval and generation, context processing and context management. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.428</span></span>We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems.Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities.While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs.<span class='px-1 mx-1 bg-yellow-200'>Addressing this gap is a defining priority for future research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.43</span></span>Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13334v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13334v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens.However, we observe that most real-world scenarios do not require such an extensive number of visual tokens.While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution.Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink.It starts with a downsampled image and smartly decides whether it is sufficient for problem solving.Otherwise, the model could output a special token to request the higher-resolution image.Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case.As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks.We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks.<span class='px-1 mx-1 bg-yellow-200'>Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.514</span></span>Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method.Our code is available at https://github.com/dvlab-research/VisionThink.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13348v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13348v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs).Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding.We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions.The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process.First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence.Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations.<span class='px-1 mx-1 bg-yellow-200'>We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.429</span></span>Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13353v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13353v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Programming applications of LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies.<span class='px-1 mx-1 bg-yellow-200'>While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.   CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449.Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.   The capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge.More importantly, the trained RL model extend the acquired reasoning abilities to new kernels.This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.14111v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.14111v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models' Internal Perception of Symbolic Music
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>However, the extent to which they implicitly model symbolic music remains underexplored.This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks.We produce a dataset of LLM-generated MIDI files without relying on explicit musical training.We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models.Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12808v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12808v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detecting LLM-generated Code with Subtle Modification by Adversarial Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the rapid development of Large Language Models (LLMs), their powerful code-generation capabilities have been widely applied in tasks like code completion and automated development, demonstrating the value of improving coding efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.912</span></span><span class='px-1 mx-1 bg-yellow-200'>However, the extensive use of LLM-generated code also raises several new challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>On the one hand, issues such as the regulation of code provenance, copyright disputes, and code quality have become increasingly concerning.<span class='px-1 mx-1 bg-yellow-200'>How to effectively detect LLM-generated code and ensure its compliant and responsible use has become a critical and urgent issue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>On the other hand, in practical applications, LLM-generated code is often subject to manual modifications, such as variable renaming or structural adjustments.<span class='px-1 mx-1 bg-yellow-200'>Although some recent studies have proposed training-based and zero-shot methods for detecting LLM-generated code, these approaches show insufficient robustness when facing modified LLM-generated code, and there is a lack of an effective solution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>To address the real-world scenario where LLM-generated code may undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of CodeGPTSensor, which employs adversarial training to improve robustness against input perturbations.CodeGPTSensor+ integrates an adversarial sample generation module, Multi-objective Identifier and Structure Transformation (MIST), which systematically generates both high-quality and representative adversarial samples.This module effectively enhances the model's resistance against diverse adversarial attacks.Experimental results on the HMCorp dataset demonstrate that CodeGPTSensor+ significantly improves detection accuracy on the adversarial test set while maintaining high accuracy on the original test set, showcasing superior robustness compared to CodeGPTSensor.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13123v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13123v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Formal Verification of LLM-Generated Code from Natural Language Prompts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the past few years LLMs have emerged as a tool that can aid programmers by taking natural language descriptions and generating code based on it. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.933</span></span>However, LLMs often generate incorrect code that users need to fix and the literature suggests users often struggle to detect these errors.In this work we seek to offer formal guarantees of correctness to LLM generated code; such guarantees could improve the experience of using AI Code Assistants and potentially enable natural language programming for users with little or no programming knowledge.To address this challenge we propose to incorporate a formal query language that can represent a user's intent in a formally defined but natural language-like manner that a user can confirm matches their intent.Then, using such a query we propose to verify LLM generated code to ensure it matches the user's intent.We implement these ideas in our system, Astrogator, for the Ansible programming language which includes such a formal query language, a calculus for representing the behavior of Ansible programs, and a symbolic interpreter which is used for the verification.On a benchmark suite of 21 code-generation tasks, our verifier is able to verify correct code in 83% of cases and identify incorrect code in 92%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.13290v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.13290v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Advancements in LLMs have enhanced task automation in software engineering; however, current evaluations primarily focus on natural language tasks, overlooking code quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production.<span class='px-1 mx-1 bg-yellow-200'>To address this issue, we propose MERA Code, a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span>This benchmark includes 11 evaluation tasks that span 8 programming languages.Our proposed evaluation methodology features a taxonomy that outlines the practical coding skills necessary for models to complete these tasks.The benchmark comprises an open-source codebase for users to conduct MERA assessments, a scoring system compatible with various programming environments, and a platform featuring a leaderboard and submission system.<span class='px-1 mx-1 bg-yellow-200'>We evaluate open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>We are publicly releasing MERA to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12284v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12284v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness.In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization.However, despite the proliferation of LLMs for general code-related tasks, there's a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL.<span class='px-1 mx-1 bg-yellow-200'>In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.831</span></span>The latter, an in-house dataset, aims to gauge LLMs' understanding of functionally equivalent code.Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain.<span class='px-1 mx-1 bg-yellow-200'>To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.8</span></span><span class='px-1 mx-1 bg-yellow-200'>CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output.Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets.<span class='px-1 mx-1 bg-yellow-200'>This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12308v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12308v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span><span class='px-1 mx-1 bg-yellow-200'>While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, we introduce GitChameleon, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span><span class='px-1 mx-1 bg-yellow-200'>GitChameleon rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span>Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51\% range, underscoring the intricacy of the problem.By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods.We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12367v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12367v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code performance optimization is paramount in real-world software engineering and critical for production-level systems.<span class='px-1 mx-1 bg-yellow-200'>While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.912</span></span>To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts.SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories.Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments.Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12415v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12415v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We propose the Single Conversation Methodology (SCM), a novel and pragmatic approach to software development using large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span>In contrast to ad hoc interactions with generative AI, SCM emphasizes a structured and persistent development dialogue, where all stages of a project - from requirements to architecture and implementation - unfold within a single, long-context conversation.The methodology is grounded on principles of cognitive clarity, traceability, modularity, and documentation.We define its phases, best practices, and philosophical stance, while arguing that SCM offers a necessary correction to the passive reliance on LLMs prevalent in current practices.We aim to reassert the active role of the developer as architect and supervisor of the intelligent tool.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12665v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12665v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.889</span></span><span class='px-1 mx-1 bg-yellow-200'>We present ParaStudent, a systematic study of LLM-based "student-like" code generation in an introductory programming course setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.976</span></span>Using a dataset of timestamped student submissions across multiple semesters, we design low-<span class='px-1 mx-1 bg-yellow-200'>and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span>Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully.<span class='px-1 mx-1 bg-yellow-200'>This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>Code for experiments and evaluation is available at \href{https://github.com/mmiroyan/ParaStudent}{\texttt{github.com/mmiroyan/ParaStudent}}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.12674v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.12674v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Generated Commit Messages with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Commit messages are essential in software development as they serve to document and explain code changes.Yet, their quality often falls short in practice, with studies showing significant proportions of empty or inadequate messages.<span class='px-1 mx-1 bg-yellow-200'>While automated commit message generation has advanced significantly, particularly with Large Language Models (LLMs), the evaluation of generated messages remains challenging. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>Traditional reference-based automatic metrics like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit message quality, as they assume a one-to-one mapping between code changes and commit messages, leading researchers to rely on resource-intensive human evaluation.This study investigates the potential of LLMs as automated evaluators for commit message quality.Through systematic experimentation with various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs combining Chain-of-Thought reasoning with few-shot demonstrations achieve near human-level evaluation proficiency.Our LLM-based evaluator significantly outperforms traditional metrics while maintaining acceptable reproducibility, robustness, and fairness levels despite some inherent variability.This work conducts a comprehensive preliminary study on using LLMs for commit message evaluation, offering a scalable alternative to human assessment while maintaining high-quality evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10906v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10906v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SIMCODE: A Benchmark for Natural Language to ns-3 Network Simulation Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have demonstrated remarkable capabilities in code generation across various domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.923</span></span>However, their effectiveness in generating simulation scripts for domain-specific environments like ns-3 remains underexplored.Despite the growing interest in automating network simulations, existing tools primarily focus on interactive automation over rigorous evaluation.To facilitate systematic evaluation, we introduce SIMCODE, the first benchmark to evaluate LLMs' ability to generate ns-3 simulation code from natural language.SIMCODE includes 400 tasks across introductory, intermediate, and advanced levels, with solutions and test cases.Using SIMCODE, we evaluate three prominent LLMs, Gemini-2.0, GPT-4.1, and Qwen-3, across six prompt techniques.Furthermore, investigating task-specific fine-tuning's impact reveals that while GPT-4.1 outperforms others, execution accuracy remains modest, with substantial room for improvement.Error analysis identifies missing headers and API mismatches as dominant failures.Nevertheless, SIMCODE provides a foundational step toward evaluating LLMs and research in domain-aware generative systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11014v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11014v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Function-to-Style Guidance of LLMs for Code Translation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have made significant strides in code translation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.876</span></span>However, ensuring both the correctness and readability of translated code remains a challenge, limiting their effective adoption in real-world software development.<span class='px-1 mx-1 bg-yellow-200'>In this work, we propose F2STrans, a function-to-style guiding paradigm designed to progressively improve the performance of LLMs in code translation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>Our approach comprises two key stages: (1) Functional learning, which optimizes translation correctness using high-quality source-target code pairs mined from online programming platforms, and (2) Style learning, which improves translation readability by incorporating both positive and negative style examples.Additionally, we introduce a novel code translation benchmark that includes up-to-date source code, extensive test cases, and manually annotated ground-truth translations, enabling comprehensive functional and stylistic evaluations.<span class='px-1 mx-1 bg-yellow-200'>Experiments on both our new benchmark and existing datasets demonstrate that our approach significantly improves code translation performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>Notably, our approach enables Qwen-1.5B to outperform prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code translation scenarios.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11083v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11083v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Seq vs Seq: An Open Suite of Paired Encoders and Decoders
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span>However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval.Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets.We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens.Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders.Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks.However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks).We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11412v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11412v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Modeling Code: Is Text All You Need?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code LLMs have become extremely popular recently for modeling source code across a variety of tasks, such as generation, translation, and summarization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.933</span></span>However, transformer-based models are limited in their capabilities to reason through structured, analytical properties of code, such as control and data flow.Previous work has explored the modeling of these properties with structured data and graph neural networks.However, these approaches lack the generative capabilities and scale of modern LLMs.<span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce a novel approach to combine the strengths of modeling both code as text and more structured forms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.792</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.11467v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.11467v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Turning the Tide: Repository-based Code Reflection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code large language models (LLMs) enhance programming by understanding and generating code across languages, offering intelligent feedback, bug detection, and code updates through reflection, improving development efficiency and accessibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.967</span></span>While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code generation and real-world relevance, previous works ignore the scenario of modifying code in repositories.Considering challenges remaining in improving reflection capabilities and avoiding data contamination in dynamic benchmarks, we introduce LiveRepoReflection, a challenging benchmark for evaluating code understanding and generation in multi-file repository contexts, featuring 1,888 rigorously filtered test cases across $6$ programming languages to ensure diversity, correctness, and high difficulty.Further, we create RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning dataset derived from diverse sources, used to train RepoReflectionCoder through a two-turn dialogue process involving code generation and error-driven repair.The leaderboard evaluates over 40 LLMs to reflect the model performance of repository-based code reflection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09866v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09866v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Design studies aim to create visualization solutions for real-world problems of different application domains.<span class='px-1 mx-1 bg-yellow-200'>Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies.In this paper, we conducted a multi-stage qualitative study to fill this gap, involving 30 design study researchers from diverse backgrounds and expertise levels.Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them.We further compiled and summarized the roles that LLMs can play across different stages of the design study process.Our findings highlight practical implications to inform visualization practitioners, and provide a framework for leveraging LLMs to enhance the design study process in visualization research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10024v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10024v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) are increasingly used as code assistants, yet their behavior when explicitly asked to generate insecure code remains poorly understood. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>While prior research has focused on unintended vulnerabilities or adversarial prompting techniques, this study examines a more direct threat scenario: open-source LLMs generating vulnerable code when prompted either directly or indirectly.We propose a dual experimental design: (1) Dynamic Prompting, which systematically varies vulnerability type, user persona, and directness across structured templates; and (2) Reverse Prompting, which derives prompts from real vulnerable code samples to assess vulnerability reproduction accuracy.We evaluate three open-source 7B-parameter models (Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the presence of vulnerabilities and the correctness of the generated vulnerability type.Results show all models frequently produce vulnerable outputs, with Qwen2 achieving highest correctness rates.User persona significantly affects success, where student personas achieved higher vulnerability rates than professional roles, while direct prompts were marginally more effective.Vulnerability reproduction followed an inverted-U pattern with cyclomatic complexity, peaking at moderate ranges.Our findings expose limitations of safety mechanisms in open-source models, particularly for seemingly benign educational requests.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10054v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10054v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Prompt engineering has proven to be a crucial step in leveraging pretrained large language models (LLMs) in solving various real-world tasks.Numerous solutions have been proposed that seek to automate prompt engineering by using the model itself to edit prompts.However, the majority of state-of-the-art approaches are evaluated on tasks that require minimal prompt templates and on very large and highly capable LLMs.In contrast, solving complex tasks that require detailed information to be included in the prompt increases the amount of text that needs to be optimised.Furthermore, smaller models have been shown to be more sensitive to prompt design.To address these challenges, we propose an evolutionary search approach to automated discrete prompt optimisation consisting of two phases.<span class='px-1 mx-1 bg-yellow-200'>In the first phase, grammar-guided genetic programming is invoked to synthesise prompt-creating programmes by searching the space of programmes populated by function compositions of syntactic, dictionary-based and LLM-based prompt-editing functions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>In the second phase, local search is applied to explore the neighbourhoods of best-performing programmes in an attempt to further fine-tune their performance.Our approach outperforms three state-of-the-art prompt optimisation approaches, PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose LLMs in four domain-specific challenging tasks.We also illustrate several examples where these benchmark methods suffer relatively severe performance degradation, while our approach improves performance in almost all task-model combinations, only incurring minimal degradation when it does not.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10326v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10326v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span>Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models.Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking.However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks.To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation.Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks.Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size.Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks.For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy.In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance.This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios.Lastly, we study optimal prompting strategies for LLM-as-a-Judge.We find that using pair-wise comparison outperforms scalar point-wise judging.Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10535v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10535v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Programming assistants powered by large language models have transformed software development, yet most benchmarks focus narrowly on code generation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.896</span></span>Recent efforts like InfiBench and StackEval attempt to address this gap using Stack Overflow data but remain limited to single-turn interactions in isolated contexts, require significant manual curation, and fail to represent complete project environments.<span class='px-1 mx-1 bg-yellow-200'>We introduce CodeAssistBench (CAB), the first benchmark framework for evaluating multi-turn programming assistance in realistic settings that address real-world questions about actual codebases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>Unlike existing programming Q&A benchmarks, CAB automatically generates scalable datasets from question-related GitHub issues using configurable parameters (e.g., repository creation date, star count, programming languages), and includes automatic containerization of codebases for evaluation.It then evaluates models through simulated users in these containerized environments with full codebase access.Using this framework, we constructed a test set of 3,286 real-world programming questions across 231 repositories, spanning seven programming languages and diverse problem domains.Our evaluation of leading LLMs reveals a substantial capability gap: while models perform well on Stack Overflow questions with success rates of 70-83%, they resolve only up to 16.49% of CAB's recent issues.This discrepancy highlights the challenges of providing assistance in complex, project-specific contexts versus answering standalone questions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.10646v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.10646v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Mixture of Linear Corrections Generates Secure Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have become proficient at sophisticated code-generation tasks, yet remain ineffective at reliably detecting or avoiding code vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span>Does this deficiency stem from insufficient learning about code vulnerabilities, or is it merely a result of ineffective prompting?Using representation engineering techniques, we investigate whether LLMs internally encode the concepts necessary to identify code vulnerabilities.We find that current LLMs encode precise internal representations that distinguish vulnerable from secure code--achieving greater accuracy than standard prompting approaches.Leveraging these vulnerability-sensitive representations, we develop an inference-time steering technique that subtly modulates the model's token-generation probabilities through a mixture of corrections (MoC).Our method effectively guides LLMs to produce less vulnerable code without compromising functionality, demonstrating a practical approach to controlled vulnerability management in generated code.Notably, MoC enhances the security ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving functionality on HumanEval pass@1 by 2.1\%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09508v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09508v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Is Quantization a Deal-breaker? Empirical Insights from Large Code Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The growing scale of large language models (LLMs) not only demands extensive computational resources but also raises environmental concerns due to their increasing carbon footprint.Model quantization emerges as an effective approach that can reduce the resource demands of LLMs by decreasing parameter precision without substantially affecting performance (e.g., 16 bit to 4 bit).<span class='px-1 mx-1 bg-yellow-200'>While recent studies have established quantization as a promising approach for optimizing large code models (LCMs), a specialized subset of LLMs tailored for automated software engineering, their findings offer only limited insights into its practical implications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>Specifically, current investigations focus only on the functional correctness of the code generated by quantized models, neglecting how quantization impacts critical aspects of code quality such as reliability, maintainability, and security.<span class='px-1 mx-1 bg-yellow-200'>To bridge this gap, our study investigates the effects of quantization on the qualitative aspects of automatically generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span>We apply Activation-aware Weight Quantization (AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate Java and Python code.Using state-of-the-art static analysis tools, we evaluate software quality metrics and static features including cyclomatic complexity, cognitive complexity, and lines of code.Our findings reveal that quantization is a robust technique that not only preserves functional correctness, but also retains key qualitative code attributes sought after by developers, such as maintainability and structural simplicity.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09665v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09665v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompting for Performance: Exploring LLMs for Configuring Software
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software systems usually provide numerous configuration options that can affect performance metrics such as execution time, memory usage, binary size, or bitrate.On the one hand, making informed decisions is challenging and requires domain expertise in options and their combinations.On the other hand, machine learning techniques can search vast configuration spaces, but with a high computational cost, since concrete executions of numerous configurations are required.<span class='px-1 mx-1 bg-yellow-200'>In this exploratory study, we investigate whether large language models (LLMs) can assist in performance-oriented software configuration through prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>We evaluate several LLMs on tasks including identifying relevant options, ranking configurations, and recommending performant configurations across various configurable systems, such as compilers, video encoders, and SAT solvers.Our preliminary results reveal both positive abilities and notable limitations: depending on the task and systems, LLMs can well align with expert knowledge, whereas hallucinations or superficial reasoning can emerge in other cases.These findings represent a first step toward systematic evaluations and the design of LLM-based solutions to assist with software configuration.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09790v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09790v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Computer-aided design (CAD) is the digital construction of 2D and 3D objects, and is central to a wide range of engineering and manufacturing applications like automobile and aviation.Despite its importance, CAD modeling remains largely a time-intensive, manual task.Recent works have attempted to automate this process with small transformer-based models and handcrafted CAD sequence representations.However, there has been little effort to leverage the potential of large language models (LLMs) for sequential CAD design.In this work, we introduce a new large-scale dataset of more than 170k CAD models annotated with high-quality, human-like descriptions generated with our pipeline based on GPT-4.1.<span class='px-1 mx-1 bg-yellow-200'>Using this dataset, we fine-tune powerful code-LLMs to generate CAD sequences represented in a JSON-based format from natural language descriptions, demonstrating the viability and effectiveness of this approach for text-conditioned CAD generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>Because simple metrics often fail to reflect the quality of generated objects, we introduce geometric and topological metrics based on sphericity, mean curvature, and Euler characteristic to provide richer structural insights.Our experiments and ablation studies on both synthetic and human-annotated data demonstrate that CADmium is able to automate CAD design, drastically speeding up the design of new objects.The dataset, code, and fine-tuned models are available online.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09792v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09792v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have transformed software development and automated code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.948</span></span><span class='px-1 mx-1 bg-yellow-200'>Motivated by these advancements, this paper explores the feasibility of LLMs in modifying malware source code to generate variants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>We introduce LLMalMorph, a semi-automated framework that leverages semantical and syntactical code comprehension by LLMs to generate new malware variants.LLMalMorph extracts function-level information from the malware source code and employs custom-engineered prompts coupled with strategically defined code transformations to guide the LLM in generating variants without resource-intensive fine-tuning.To evaluate LLMalMorph, we collected 10 diverse Windows malware samples of varying types, complexity and functionality and generated 618 variants.Our thorough experiments demonstrate that it is possible to reduce the detection rates of antivirus engines of these malware variants to some extent while preserving malware functionalities.In addition, despite not optimizing against any Machine Learning (ML)-based malware detectors, several variants also achieved notable attack success rates against an ML-based malware classifier.<span class='px-1 mx-1 bg-yellow-200'>We also discuss the limitations of current LLM capabilities in generating malware variants from source code and assess where this emerging technology stands in the broader context of malware variant generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.09411v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.09411v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2025-07-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multilingual Multimodal Software Developer for Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid advancement of Large Language Models (LLMs) has significantly improved code generation, yet most models remain text-only, neglecting crucial visual aids like diagrams and flowcharts used in real-world software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.932</span></span>To bridge this gap, we introduce MM-Coder, a Multilingual Multimodal software developer.<span class='px-1 mx-1 bg-yellow-200'>MM-Coder integrates visual design inputs-Unified Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with textual instructions to enhance code generation accuracy and architectural alignment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.713</span></span>To enable this, we developed MMc-Instruct, a diverse multimodal instruction-tuning dataset including visual-workflow-based code generation, allowing MM-Coder to synthesize textual and graphical information like human developers, distinct from prior work on narrow tasks.Furthermore, we introduce MMEval, a new benchmark for evaluating multimodal code generation, addressing existing text-only limitations.Our evaluations using MMEval highlight significant remaining challenges for models in precise visual information capture, instruction following, and advanced programming knowledge.Our work aims to revolutionize industrial programming by enabling LLMs to interpret and implement complex specifications conveyed through both text and visual designs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2507.08719v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2507.08719v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
  function extractGSUrl(url) {
            var regex = /\/([^/]+?)(?:v\d+)?$/;
            var matches = url.match(regex);

            if (matches) {
                var endOfUrl = matches[1];
                var prefix = "https://scholar.google.com/scholar_lookup?arxiv_id=";
                var finalUrl = prefix + endOfUrl;
                console.log("Final URL: " + finalUrl);
                
                // You can open the final URL in a new tab/window
                window.open(finalUrl, "_blank");
            } else {
                console.log("No match found");
            }
        };
</script>
</body>
</html>