<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Ryan's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-12-18.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by running a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <p class="text-sm text-gray-500 pt-2">This project was originally created by <a href="https://koaning.io/">Vincent Warmerdam</a>, modifying his original <a href="https://koaning.github.io/arxiv-frontpage/">frontpage</a> for different paper categories.</p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Prompt Engineering in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph Reasoning for Cold-start Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Knowledge Graphs (KGs) represent relationships between entities in a graph structure and have been widely studied as promising tools for realizing recommendations that consider the accurate content information of items.However, traditional KG-based recommendation methods face fundamental challenges: insufficient consideration of temporal information and poor performance in cold-start scenarios.On the other hand, Large Language Models (LLMs) can be considered databases with a wealth of knowledge learned from the web data, and they have recently gained attention due to their potential application as recommendation systems.Although approaches that treat LLMs as recommendation systems can leverage LLMs' high recommendation literacy, their input token limitations make it impractical to consider the entire recommendation domain dataset and result in scalability issues.To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR).Our main idea is to treat LLMs as reasoners that output intuitive exploration strategies for KGs.To integrate the knowledge of LLMs and KGs, we trained a recommendation agent through reinforcement learning using a reward function that integrates different recommendation strategies, including LLM's intuition and KG embeddings.<span class='px-1 mx-1 bg-yellow-200'>By incorporating temporal awareness through prompt engineering and generating textual representations of user preferences from limited interactions, LIKR can improve recommendation performance in cold-start scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Furthermore, LIKR can avoid scalability issues by using KGs to represent recommendation domain datasets and limiting the LLM's output to KG exploration strategies.Experiments on real-world datasets demonstrate that our model outperforms state-of-the-art recommendation methods in cold-start sequential recommendation scenarios.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12464v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12464v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the huge number of diseases.The complexity of symptoms and the shortage of specialized doctors with relevant experience make diagnosing and treating rare diseases more challenging than common diseases.Recently, agents powered by large language models (LLMs) have demonstrated notable improvements across various domains.<span class='px-1 mx-1 bg-yellow-200'>In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical exams. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>However, current agent frameworks lack adaptation for real-world clinical scenarios, especially those involving the intricate demands of rare diseases.To address these challenges, we present RareAgents, the first multi-disciplinary team of LLM-based agents tailored to the complex clinical context of rare diseases.RareAgents integrates advanced planning capabilities, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model.Experimental results show that RareAgents surpasses state-of-the-art domain-specific models, GPT-4o, and existing agent frameworks in both differential diagnosis and medication recommendation for rare diseases.Furthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from MIMIC-IV, to support further advancements in this field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12475v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12475v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for Low-Resource Language Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have demonstrated impressive multilingual understanding and reasoning capabilities, driven by extensive pre-training multilingual corpora and fine-tuning instruction data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>However, a performance gap persists between high-resource and low-resource language tasks due to language imbalance in the pre-training corpus, even using more low-resource data during fine-tuning.To alleviate this issue, we propose LinguaLIFT, a two-stage instruction tuning framework for advancing low-resource language tasks.An additional language alignment layer is first integrated into the LLM to adapt a pre-trained multilingual encoder, thereby enhancing multilingual alignment through code-switched fine-tuning.The second stage fine-tunes LLM with English-only instruction data while freezing the language alignment layer, allowing LLM to transfer task-specific capabilities from English to low-resource language tasks.Additionally, we introduce the Multilingual Math World Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and 10 high-resource languages, enabling comprehensive evaluation of multilingual reasoning.Experimental results show that LinguaLIFT outperforms several competitive baselines across MMWP and other widely used benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12499v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12499v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Generating Move Smart Contracts based on Concepts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The growing adoption of formal verification for smart contracts has spurred the development of new verifiable languages like Move.However, the limited availability of training data for these languages hinders effective code generation by large language models (LLMs).This paper presents ConMover, a novel framework that enhances LLM-based code generation for Move by leveraging a knowledge graph of Move concepts and a small set of verified code examples.<span class='px-1 mx-1 bg-yellow-200'>ConMover integrates concept retrieval, planning, coding, and debugging agents in an iterative process to refine generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span>Evaluations with various open-source LLMs demonstrate substantial accuracy improvements over baseline models.These results underscore ConMover's potential to address low-resource code generation challenges, bridging the gap between natural language descriptions and reliable smart contract development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12513v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12513v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC).<span class='px-1 mx-1 bg-yellow-200'>Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data.Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums.Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models.Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12541v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12541v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Aspect-based sentiment analysis (ABSA), a sequence labeling task, has attracted increasing attention in multilingual contexts.While previous research has focused largely on fine-tuning or training models specifically for ABSA, we evaluate large language models (LLMs) under zero-shot conditions to explore their potential to tackle this challenge with minimal task-specific adaptation.<span class='px-1 mx-1 bg-yellow-200'>We conduct a comprehensive empirical evaluation of a series of LLMs on multilingual ABSA tasks, investigating various prompting strategies, including vanilla zero-shot, chain-of-thought (CoT), self-improvement, self-debate, and self-consistency, across nine different models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span>Results indicate that while LLMs show promise in handling multilingual ABSA, they generally fall short of fine-tuned, task-specific models.<span class='px-1 mx-1 bg-yellow-200'>Notably, simpler zero-shot prompts often outperform more complex strategies, especially in high-resource languages like English. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>These findings underscore the need for further refinement of LLM-based approaches to effectively address ABSA task across diverse languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12564v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12564v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs are Also Effective Embedding Models: An In-depth Overview
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have revolutionized natural language processing by achieving state-of-the-art performance across various tasks.Recently, their effectiveness as embedding models has gained attention, marking a paradigm shift from traditional encoder-only models like ELMo and BERT to decoder-only, large-scale LLMs such as GPT, LLaMA, and Mistral.This survey provides an in-depth overview of this transition, beginning with foundational techniques before the LLM era, followed by LLM-based embedding models through two main strategies to derive embeddings from LLMs.<span class='px-1 mx-1 bg-yellow-200'>1) Direct prompting: We mainly discuss the prompt designs and the underlying rationale for deriving competitive embeddings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>2) Data-centric tuning: We cover extensive aspects that affect tuning an embedding model, including model architecture, training objectives, data constructions, etc.Upon the above, we also cover advanced methods, such as handling longer texts, and multilingual and cross-modal data.Furthermore, we discuss factors affecting choices of embedding models, such as performance/efficiency comparisons, dense vs sparse embeddings, pooling strategies, and scaling law.Lastly, the survey highlights the limitations and challenges in adapting LLMs for embeddings, including cross-task embedding quality, trade-offs between efficiency and accuracy, low-resource, long-context, data bias, robustness, etc.This survey serves as a valuable resource for researchers and practitioners by synthesizing current advancements, highlighting key challenges, and offering a comprehensive framework for future work aimed at enhancing the effectiveness and efficiency of LLMs as embedding models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12591v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12591v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program Fine-tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Program-of-Thought (PoT), which aims to use programming language instead of natural language as an intermediate step in reasoning, is an important way for LLMs to solve mathematical problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span><span class='px-1 mx-1 bg-yellow-200'>Since different programming languages excel in different areas, it is natural to use the most suitable language for solving specific problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>However, current PoT research only focuses on single language PoT, ignoring the differences between different programming languages.<span class='px-1 mx-1 bg-yellow-200'>Therefore, this paper proposes an multilingual program reasoning method, MultiLingPoT. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>This method allows the model to answer questions using multiple programming languages by fine-tuning on multilingual data.Additionally, prior and posterior hybrid methods are used to help the model select the most suitable language for each problem.<span class='px-1 mx-1 bg-yellow-200'>Our experimental results show that the training of MultiLingPoT improves each program's mathematical reasoning by about 2.5\%. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>Moreover, with proper mixing, the performance of MultiLingPoT can be further improved, achieving a 6\% increase compared to the single-language PoT with the data augmentation.Resources of this paper can be found at https://github.com/Nianqi-Li/MultiLingPoT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12609v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12609v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Predicting User Behavior in Smart Spaces with LLM-Enhanced Logs and Personalized Prompts (Data Description)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Enhancing the intelligence of smart systems, such as smart home, and smart vehicle, and smart grids, critically depends on developing sophisticated planning capabilities that can anticipate the next desired function based on historical interactions.While existing methods view user behaviors as sequential data and apply models like RNNs and Transformers to predict future actions, they often fail to incorporate domain knowledge and capture personalized user preferences.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose a novel approach that incorporates LLM-enhanced logs and personalized prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>Our approach first constructs a graph that captures individual behavior preferences derived from their interaction histories.<span class='px-1 mx-1 bg-yellow-200'>This graph effectively transforms into a soft continuous prompt that precedes the sequence of user behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span>Then our approach leverages the vast general knowledge and robust reasoning capabilities of a pretrained LLM to enrich the oversimplified and incomplete log records.By enhancing these logs semantically, our approach better understands the user's actions and intentions, especially for those rare events in the dataset.We evaluate the method across four real-world datasets from both smart vehicle and smart home settings.The findings validate the effectiveness of our LLM-enhanced description and personalized prompt, shedding light on potential ways to advance the intelligence of smart space.Note: While this manuscript provides description of the data, we are \textbf{not} permitted to make these datasets publicly available due to restrictions imposed by the data provider.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12653v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12653v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detecting Document-level Paraphrased Machine Generated Content: Mimicking Human Writing Style and Involving Discourse Features
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The availability of high-quality APIs for Large Language Models (LLMs) has facilitated the widespread creation of Machine-Generated Content (MGC), posing challenges such as academic plagiarism and the spread of misinformation.Existing MGC detectors often focus solely on surface-level information, overlooking implicit and structural features.<span class='px-1 mx-1 bg-yellow-200'>This makes them susceptible to deception by surface-level sentence patterns, particularly for longer texts and in texts that have been subsequently paraphrased.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>To overcome these challenges, we introduce novel methodologies and datasets.<span class='px-1 mx-1 bg-yellow-200'>Besides the publicly available dataset Plagbench, we developed the paraphrased Long-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts (paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by extending artifacts from their original versions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>To address the challenge of detecting highly similar paraphrased texts, we propose MhBART, an encoder-decoder model designed to emulate human writing style while incorporating a novel difference score mechanism.This model outperforms strong classifier baselines and identifies deceptive sentence patterns.To better capture the structure of longer texts at document level, we propose DTransformer, a model that integrates discourse analysis through PDTB preprocessing to encode structural features.It results in substantial performance gains across both datasets -- 15.5\% absolute improvement on paraLFQA, 4\% absolute improvement on paraWP, and 1.5\% absolute improvement on M4 compared to SOTA approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12679v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12679v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation Detecting and Grounding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present ASAP, a new framework for detecting and grounding multi-modal media manipulation (DGM4).Upon thorough examination, we observe that accurate fine-grained cross-modal semantic alignment between the image and text is vital for accurately manipulation detection and grounding.While existing DGM4 methods pay rare attention to the cross-modal alignment, hampering the accuracy of manipulation detecting to step further.To remedy this issue, this work targets to advance the semantic alignment learning to promote this task.Particularly, we utilize the off-the-shelf Multimodal Large-Language Models (MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs, especially for the manipulated instances.Subsequently, a cross-modal alignment learning is performed to enhance the semantic alignment.<span class='px-1 mx-1 bg-yellow-200'>Besides the explicit auxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA) to provide implicit guidance for augmenting the manipulation perceiving. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>With the grounding truth available during training, MGCA encourages the model to concentrate more on manipulated components while downplaying normal ones, enhancing the model's ability to capture manipulations.Extensive experiments are conducted on the DGM4 dataset, the results demonstrate that our model can surpass the comparison method with a clear margin.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12718v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12718v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using LLM-Generated Draft Replies to Support Human Experts in Responding to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of Industrial AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The maritime industry requires effective communication among diverse stakeholders to address complex, safety-critical challenges.Industrial AI, including Large Language Models (LLMs), has the potential to augment human experts' workflows in this specialized domain.<span class='px-1 mx-1 bg-yellow-200'>Our case study investigated the utility of LLMs in drafting replies to stakeholder inquiries and supporting case handlers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>We conducted a preliminary study (observations and interviews), a survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding Similarity).We discover that while LLM drafts can streamline workflows, they often require significant modifications to meet the specific demands of maritime communications.Though LLMs are not yet mature enough for safety-critical applications without human oversight, they can serve as valuable augmentative tools.Final decision-making thus must remain with human experts.However, by leveraging the strengths of both humans and LLMs, fostering human-AI collaboration, industries can increase efficiency while maintaining high standards of quality and precision tailored to each case.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12732v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12732v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FocusChat: Text-guided Long Video Understanding via Spatiotemporal Information Filtering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, multi-modal large language models have made significant progress.However, visual information lacking of guidance from the user's intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos.<span class='px-1 mx-1 bg-yellow-200'>To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the user's prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span>In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively.The two branches are combined using the Spatial-Temporal Filtering Module (STFM).STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the user's query.It lowers the essential number of visual tokens inputted into the LLM.FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied.It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12833v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12833v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks.Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose \textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span><span class='px-1 mx-1 bg-yellow-200'>By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs.Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12881v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12881v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Question: How do Large Language Models perform on the Question Answering tasks? Answer:
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span>A common NLP-task is question-answering (QA).<span class='px-1 mx-1 bg-yellow-200'>In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>Since the dataset contains unanswerable questions, previous work used a double inference method.<span class='px-1 mx-1 bg-yellow-200'>We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.861</span></span>Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.   Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12893v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12893v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural Language Descriptions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Piping and Instrumentation Diagrams (P&IDs) are foundational to the design, construction, and operation of workflows in the engineering and process industries.However, their manual creation is often labor-intensive, error-prone, and lacks robust mechanisms for error detection and correction.While recent advancements in Generative AI, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), have demonstrated significant potential across various domains, their application in automating generation of engineering workflows remains underexplored.In this work, we introduce a novel copilot for automating the generation of P&IDs from natural language descriptions.<span class='px-1 mx-1 bg-yellow-200'>Leveraging a multi-step agentic workflow, our copilot provides a structured and iterative approach to diagram creation directly from Natural Language prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>We demonstrate the feasibility of the generation process by evaluating the soundness and completeness of the workflow, and show improved results compared to vanilla zero-shot and few-shot generation approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12898v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12898v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adaptations of AI models for querying the LandMatrix database in natural language
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Land Matrix initiative (https://landmatrix.org) and its global observatory aim to provide reliable data on large-scale land acquisitions to inform debates and actions in sectors such as agriculture, extraction, or energy in low- and middle-income countries.Although these data are recognized in the academic world, they remain underutilized in public policy, mainly due to the complexity of access and exploitation, which requires technical expertise and a good understanding of the database schema.   The objective of this work is to simplify access to data from different database systems.The methods proposed in this article are evaluated using data from the Land Matrix.<span class='px-1 mx-1 bg-yellow-200'>This work presents various comparisons of Large Language Models (LLMs) as well as combinations of LLM adaptations (Prompt Engineering, RAG, Agents) to query different database systems (GraphQL and REST queries). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span>The experiments are reproducible, and a demonstration is available online: https://github.com/tetis-nlp/landmatrix-graphql-python.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12961v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12961v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown promising capabilities in healthcare analysis but face several challenges like hallucinations, parroting, and bias manifestation.These challenges are exacerbated in complex, sensitive, and low-resource domains.Therefore, in this work we introduce IC-AnnoMI, an expert-annotated motivational interviewing (MI) dataset built upon AnnoMI by generating in-context conversational dialogues leveraging LLMs, particularly ChatGPT.<span class='px-1 mx-1 bg-yellow-200'>IC-AnnoMI employs targeted prompts accurately engineered through cues and tailored information, taking into account therapy style (empathy, reflection), contextual relevance, and false semantic change. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span>Subsequently, the dialogues are annotated by experts, strictly adhering to the Motivational Interviewing Skills Code (MISC), focusing on both the psychological and linguistic dimensions of MI dialogues.We comprehensively evaluate the IC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding of domain intricacies by modeling novel classification tasks employing several classical machine learning and current state-of-the-art transformer approaches.<span class='px-1 mx-1 bg-yellow-200'>Finally, we discuss the effects of progressive prompting strategies and the impact of augmented data in mitigating the biases manifested in IC-AnnoM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.728</span></span>Our contributions provide the MI community with not only a comprehensive dataset but also valuable insights for using LLMs in empathetic text generation for conversational therapy in supervised settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Emergence of Strategic Reasoning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As Large Language Models (LLMs) are increasingly used for a variety of complex and critical tasks, it is vital to assess their logical capabilities in strategic environments.<span class='px-1 mx-1 bg-yellow-200'>This paper examines their ability in strategic reasoning -- the process of choosing an optimal course of action by predicting and adapting to other agents' behavior. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>Using six LLMs, we analyze responses from play in classical games from behavioral economics (p-Beauty Contest, 11-20 Money Request Game, and Guessing Game) and evaluate their performance through hierarchical models of reasoning (level-$k$ theory and cognitive hierarchy theory).<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that while LLMs show understanding of the games, the majority struggle with higher-order strategic reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span><span class='px-1 mx-1 bg-yellow-200'>Although most LLMs did demonstrate learning ability with games involving repeated interactions, they still consistently fall short of the reasoning levels demonstrated by typical behavior from human subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span>The exception to these overall findings is with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning tasks -- which consistently outperforms other LLMs and human subjects.These findings highlight the challenges and pathways in advancing LLMs toward robust strategic reasoning from the perspective of behavioral economics.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13013v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13013v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent research, large language models (LLMs) have been increasingly used to investigate public opinions.This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants.Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts.Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups.Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD.<span class='px-1 mx-1 bg-yellow-200'>Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13169v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13169v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite demonstrating impressive capabilities, Large Language Models (LLMs) still often struggle to accurately express the factual knowledge they possess, especially in cases where the LLMs' knowledge boundaries are ambiguous.To improve LLMs' factual expressions, we propose the UAlign framework, which leverages Uncertainty estimations to represent knowledge boundaries, and then explicitly incorporates these representations as input features into prompts for LLMs to Align with factual knowledge.First, we prepare the dataset on knowledge question-answering (QA) samples by calculating two uncertainty estimations, including confidence score and semantic entropy, to represent the knowledge boundaries for LLMs.Subsequently, using the prepared dataset, we train a reward model that incorporates uncertainty estimations and then employ the Proximal Policy Optimization (PPO) algorithm for factuality alignment on LLMs.<span class='px-1 mx-1 bg-yellow-200'>Experimental results indicate that, by integrating uncertainty representations in LLM alignment, the proposed UAlign can significantly enhance the LLMs' capacities to confidently answer known questions and refuse unknown questions on both in-domain and out-of-domain tasks, showing reliability improvements and good generalizability over various prompt- and training-based baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11803v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11803v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Distributed Collaborative Retrieval Framework Excelling in All Queries and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Numerous retrieval models, including sparse, dense and llm-based methods, have demonstrated remarkable performance in predicting the relevance between queries and corpora.However, the preliminary effectiveness analysis experiments indicate that these models fail to achieve satisfactory performance on the majority of queries and corpora, revealing their effectiveness restricted to specific scenarios.Thus, to tackle this problem, we propose a novel Distributed Collaborative Retrieval Framework (DCRF), outperforming each single model across all queries and corpora.Specifically, the framework integrates various retrieval models into a unified system and dynamically selects the optimal results for each user's query.It can easily aggregate any retrieval model and expand to any application scenarios, illustrating its flexibility and scalability.<span class='px-1 mx-1 bg-yellow-200'>Moreover, to reduce maintenance and training costs, we design four effective prompting strategies with large language models (LLMs) to evaluate the quality of ranks without reliance of labeled data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>Extensive experiments demonstrate that proposed framework, combined with 8 efficient retrieval models, can achieve performance comparable to effective listwise methods like RankGPT and ListT5, while offering superior efficiency.<span class='px-1 mx-1 bg-yellow-200'>Besides, DCRF surpasses all selected retrieval models on the most datasets, indicating the effectiveness of our prompting strategies on rank-oriented automatic evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11832v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11832v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Instruction-Tuned Large Language Models to Identify Indicators of Vulnerability in Police Incident Narratives
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Objectives: Compare qualitative coding of instruction tuned large language models (IT-LLMs) against human coders in classifying the presence or absence of vulnerability in routinely collected unstructured text that describes police-public interactions.Evaluate potential bias in IT-LLM codings.Methods: Analyzing publicly available text narratives of police-public interactions recorded by Boston Police Department, we provide humans and IT-LLMs with qualitative labelling codebooks and compare labels generated by both, seeking to identify situations associated with (i) mental ill health; (ii) substance misuse; (iii) alcohol dependence; and (iv) homelessness.<span class='px-1 mx-1 bg-yellow-200'>We explore multiple prompting strategies and model sizes, and the variability of labels generated by repeated prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span>Additionally, to explore model bias, we utilize counterfactual methods to assess the impact of two protected characteristics - race and gender - on IT-LLM classification.Results:Results demonstrate that IT-LLMs can effectively support human qualitative coding of police incident narratives.While there is some disagreement between LLM and human generated labels, IT-LLMs are highly effective at screening narratives where no vulnerabilities are present, potentially vastly reducing the requirement for human coding.Counterfactual analyses demonstrate that manipulations to both gender and race of individuals described in narratives have very limited effects on IT-LLM classifications beyond those expected by chance.Conclusions: IT-LLMs offer effective means to augment human qualitative coding in a way that requires much lower levels of resource to analyze large unstructured datasets.Moreover, they encourage specificity in qualitative coding, promote transparency, and provide the opportunity for more standardized, replicable approaches to analyzing large free-text police data sources.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11878v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11878v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Stepwise Reasoning Error Disruption Attack of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored.Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.713</span></span><span class='px-1 mx-1 bg-yellow-200'>Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11934v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11934v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ExecRepoBench: Multi-level Executable Code Completion Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code completion has become an essential tool for daily software development.Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets.In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files.ExecRepoBench includes 1.2K samples from active Python repositories.<span class='px-1 mx-1 bg-yellow-200'>Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model.Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages.The deployment of \ourmethod{} can be used as a high-performance, local service for programming development\footnote{\url{https://execrepobench.github.io/}}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11990v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11990v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics.<span class='px-1 mx-1 bg-yellow-200'>Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span>An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support.Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them.<span class='px-1 mx-1 bg-yellow-200'>Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span><span class='px-1 mx-1 bg-yellow-200'>We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span><span class='px-1 mx-1 bg-yellow-200'>Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span><span class='px-1 mx-1 bg-yellow-200'>This LLM generated message recommendations for caregivers supporting their child's math practice via chat. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span><span class='px-1 mx-1 bg-yellow-200'>Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span>These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation.<span class='px-1 mx-1 bg-yellow-200'>We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11995v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11995v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite their remarkable success, large language models (LLMs) have shown limited ability on applied tasks such as vulnerability detection.<span class='px-1 mx-1 bg-yellow-200'>We investigate various prompting strategies for vulnerability detection and, as part of this exploration, propose a prompting strategy that integrates natural language descriptions of vulnerabilities with a contrastive chain-of-thought reasoning approach, augmented using contrastive samples from a synthetic dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.869</span></span><span class='px-1 mx-1 bg-yellow-200'>Our study highlights the potential of LLMs to detect vulnerabilities by integrating natural language descriptions, contrastive reasoning, and synthetic examples into a comprehensive prompting framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span>Our results show that this approach can enhance LLM understanding of vulnerabilities.<span class='px-1 mx-1 bg-yellow-200'>On a high-quality vulnerability detection dataset such as SVEN, our prompting strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%, 11%, and 14%, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12039v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12039v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Robustness Tools in LLM Safety</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>DNN-based language models perform excellently on various tasks, but even SOTA LLMs are susceptible to textual adversarial attacks.Adversarial texts play crucial roles in multiple subfields of NLP.However, current research has the following issues.(1) Most textual adversarial attack methods target rich-resourced languages.How do we generate adversarial texts for less-studied languages?<span class='px-1 mx-1 bg-yellow-200'>(2) Most textual adversarial attack methods are prone to generating invalid or ambiguous adversarial texts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>How do we construct high-quality adversarial robustness benchmarks?<span class='px-1 mx-1 bg-yellow-200'>(3) New language models may be immune to part of previously generated adversarial texts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span>How do we update adversarial robustness benchmarks?To address the above issues, we introduce HITL-GAT, a system based on a general approach to human-in-the-loop generation of adversarial texts.HITL-GAT contains four stages in one pipeline: victim model construction, adversarial example generation, high-quality benchmark construction, and adversarial robustness evaluation.Additionally, we utilize HITL-GAT to make a case study on Tibetan script which can be a reference for the adversarial research of other less-studied languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12478v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12478v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Simple and Fast Way to Handle Semantic Errors in Transactions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Many computer systems are now being redesigned to incorporate LLM-powered agents, enabling natural language input and more flexible operations.This paper focuses on handling database transactions created by large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Transactions generated by LLMs may include semantic errors, requiring systems to treat them as long-lived. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span><span class='px-1 mx-1 bg-yellow-200'>This allows for human review and, if the transaction is incorrect, removal from the database history. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>Any removal action must ensure the database's consistency (the "C" in ACID principles) is maintained throughout the process.   We propose a novel middleware framework based on Invariant Satisfaction (I-Confluence), which ensures consistency by identifying and coordinating dependencies between long-lived transactions and new transactions.<span class='px-1 mx-1 bg-yellow-200'>This middleware buffers suspicious or compensating transactions to manage coordination states. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>Using the TPC-C benchmark, we evaluate how transaction generation frequency, user reviews, and invariant completeness impact system performance.For system researchers, this study establishes an interactive paradigm between LLMs and database systems, providing an "undoing" mechanism for handling incorrect operations while guaranteeing database consistency.<span class='px-1 mx-1 bg-yellow-200'>For system engineers, this paper offers a middleware design that integrates removable LLM-generated transactions into existing systems with minimal modifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12493v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12493v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NLSR: Neuron-Level Safety Realignment of Large Language Models Against Harmful Fine-Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The emergence of finetuning-as-a-service has revealed a new vulnerability in large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>A mere handful of malicious data uploaded by users can subtly manipulate the finetuning process, resulting in an alignment-broken model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>Existing methods to counteract fine-tuning attacks typically require substantial computational resources.Even with parameter-efficient techniques like LoRA, gradient updates remain essential.To address these challenges, we propose \textbf{N}euron-\textbf{L}evel \textbf{S}afety \textbf{R}ealignment (\textbf{NLSR}), a training-free framework that restores the safety of LLMs based on the similarity difference of safety-critical neurons before and after fine-tuning.The core of our framework is first to construct a safety reference model from an initially aligned model to amplify safety-related features in neurons.We then utilize this reference model to identify safety-critical neurons, which we prepare as patches.Finally, we selectively restore only those neurons that exhibit significant similarity differences by transplanting these prepared patches, thereby minimally altering the fine-tuned model.Extensive experiments demonstrate significant safety enhancements in fine-tuned models across multiple downstream tasks, while greatly maintaining task-level accuracy.Our findings suggest regions of some safety-critical neurons show noticeable differences after fine-tuning, which can be effectively corrected by transplanting neurons from the reference model without requiring additional training.The code will be available at \url{https://github.com/xinykou/NLSR}</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12497v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12497v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs.While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model's probability distribution can still be misleading.Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega.We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability.By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications.Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations.<span class='px-1 mx-1 bg-yellow-200'>This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12509v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12509v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                When to Speak, When to Abstain: Contrastive Decoding with Abstention
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging both pre-trained knowledge (i.e., parametric knowledge) and external knowledge (i.e., contextual knowledge).While substantial efforts have been made to leverage both forms of knowledge, scenarios in which the model lacks any relevant knowledge remain underexplored.<span class='px-1 mx-1 bg-yellow-200'>Such limitations can result in issues like hallucination, causing reduced reliability and potential risks in high-stakes applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span>To address such limitations, this paper extends the task scope to encompass cases where the user's request cannot be fulfilled due to the lack of relevant knowledge.To this end, we introduce Contrastive Decoding with Abstention (CDA), a training-free decoding method that empowers LLMs to generate responses when relevant knowledge is available and to abstain otherwise.CDA evaluates the relevance of each knowledge for a given query, adaptively determining which knowledge to prioritize or which to completely ignore.Extensive experiments with four LLMs on three question-answering datasets demonstrate that CDA can effectively perform accurate generation and abstention simultaneously.These findings highlight CDA's potential to broaden the applicability of LLMs, enhancing reliability and preserving user trust.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12527v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12527v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Jailbreaking? One Step Is Enough!
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) excel in various tasks but remain vulnerable to jailbreak attacks, where adversaries manipulate prompts to generate harmful outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span><span class='px-1 mx-1 bg-yellow-200'>Examining jailbreak prompts helps uncover the shortcomings of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>However, current jailbreak methods and the target model's defenses are engaged in an independent and adversarial process, resulting in the need for frequent attack iterations and redesigning attacks for different models.To address these gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that disguises the attack intention as the "defense".intention against harmful content.Specifically, REDA starts from the target response, guiding the model to embed harmful content within its defensive measures, thereby relegating harmful content to a secondary role and making the model believe it is performing a defensive task.The attacking model considers that it is guiding the target model to deal with harmful content, while the target model thinks it is performing a defensive task, creating an illusion of cooperation between the two.Additionally, to enhance the model's confidence and guidance in "defensive" intentions, we adopt in-context learning (ICL) with a small number of attack examples and construct a corresponding dataset of attack examples.Extensive evaluations demonstrate that the REDA method enables cross-model attacks without the need to redesign attack strategies for different models, enables successful jailbreak in one iteration, and outperforms existing methods on both open-source and closed-source models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12621v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12621v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detecting Document-level Paraphrased Machine Generated Content: Mimicking Human Writing Style and Involving Discourse Features
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The availability of high-quality APIs for Large Language Models (LLMs) has facilitated the widespread creation of Machine-Generated Content (MGC), posing challenges such as academic plagiarism and the spread of misinformation.Existing MGC detectors often focus solely on surface-level information, overlooking implicit and structural features.<span class='px-1 mx-1 bg-yellow-200'>This makes them susceptible to deception by surface-level sentence patterns, particularly for longer texts and in texts that have been subsequently paraphrased.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>To overcome these challenges, we introduce novel methodologies and datasets.Besides the publicly available dataset Plagbench, we developed the paraphrased Long-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts (paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by extending artifacts from their original versions.To address the challenge of detecting highly similar paraphrased texts, we propose MhBART, an encoder-decoder model designed to emulate human writing style while incorporating a novel difference score mechanism.This model outperforms strong classifier baselines and identifies deceptive sentence patterns.To better capture the structure of longer texts at document level, we propose DTransformer, a model that integrates discourse analysis through PDTB preprocessing to encode structural features.It results in substantial performance gains across both datasets -- 15.5\% absolute improvement on paraLFQA, 4\% absolute improvement on paraWP, and 1.5\% absolute improvement on M4 compared to SOTA approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12679v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12679v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Trigger$^3$: Refining Query Correction via Adaptive Model Selector
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In search scenarios, user experience can be hindered by erroneous queries due to typos, voice errors, or knowledge gaps.Therefore, query correction is crucial for search engines.Current correction models, usually small models trained on specific data, often struggle with queries beyond their training scope or those requiring contextual understanding.While the advent of Large Language Models (LLMs) offers a potential solution, they are still limited by their pre-training data and inference cost, particularly for complex queries, making them not always effective for query correction.To tackle these, we propose Trigger$^3$, a large-small model collaboration framework that integrates the traditional correction model and LLM for query correction, capable of adaptively choosing the appropriate correction method based on the query and the correction results from the traditional correction model and LLM.Trigger$^3$ first employs a correction trigger to filter out correct queries.Incorrect queries are then corrected by the traditional correction model.<span class='px-1 mx-1 bg-yellow-200'>If this fails, an LLM trigger is activated to call the LLM for correction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>Finally, for queries that no model can correct, a fallback trigger decides to return the original query.Extensive experiments demonstrate Trigger$^3$ outperforms correction baselines while maintaining efficiency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12701v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12701v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FocusChat: Text-guided Long Video Understanding via Spatiotemporal Information Filtering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, multi-modal large language models have made significant progress.However, visual information lacking of guidance from the user's intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos.To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the user's prompt.In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively.The two branches are combined using the Spatial-Temporal Filtering Module (STFM).STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the user's query.<span class='px-1 mx-1 bg-yellow-200'>It lowers the essential number of visual tokens inputted into the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied.It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12833v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12833v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Truthful Text Sanitization Guided by Inference Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The purpose of text sanitization is to rewrite those text spans in a document that may directly or indirectly identify an individual, to ensure they no longer disclose personal information.Text sanitization must strike a balance between preventing the leakage of personal information (privacy protection) while also retaining as much of the document's original content as possible (utility preservation).We present an automated text sanitization strategy based on generalizations, which are more abstract (but still informative) terms that subsume the semantic content of the original text spans.The approach relies on instruction-tuned large language models (LLMs) and is divided into two stages.The LLM is first applied to obtain truth-preserving replacement candidates and rank them according to their abstraction level.Those candidates are then evaluated for their ability to protect privacy by conducting inference attacks with the LLM.<span class='px-1 mx-1 bg-yellow-200'>Finally, the system selects the most informative replacement shown to be resistant to those attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>As a consequence of this two-stage process, the chosen replacements effectively balance utility and privacy.We also present novel metrics to automatically evaluate these two aspects without the need to manually annotate data.Empirical results on the Text Anonymization Benchmark show that the proposed approach leads to enhanced utility, with only a marginal increase in the risk of re-identifying protected individuals compared to fully suppressing the original information.Furthermore, the selected replacements are shown to be more truth-preserving and abstractive than previous methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are transforming the robotics domain by enabling robots to comprehend and execute natural language instructions.The cornerstone benefits of LLM include processing textual data from technical manuals, instructions, academic papers, and user queries based on the knowledge provided.<span class='px-1 mx-1 bg-yellow-200'>However, deploying LLM-generated code in robotic systems without safety verification poses significant risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span>This paper outlines a safety layer that verifies the code generated by ChatGPT before executing it to control a drone in a simulated environment.The safety layer consists of a fine-tuned GPT-4o model using Few-Shot learning, supported by knowledge graph prompting (KGP).Our approach improves the safety and compliance of robotic actions, ensuring that they adhere to the regulations of drone operations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11387v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11387v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advanced large language models (LLMs) can generate text almost indistinguishable from human-written text, highlighting the importance of LLM-generated text detection.However, current zero-shot techniques face challenges as white-box methods are restricted to use weaker open-source LLMs, and black-box methods are limited by partial observation from stronger proprietary LLMs.It seems impossible to enable white-box methods to use proprietary models because API-level access to the models neither provides full predictive distributions nor inner embeddings.To traverse the divide, we propose Glimpse, a probability distribution estimation approach, predicting the full distributions from partial observations.Despite the simplicity of Glimpse, we successfully extend white-box methods like Entropy, Rank, Log-Rank, and Fast-DetectGPT to latest proprietary models.Experiments show that Glimpse with Fast-DetectGPT and GPT-3.5 achieves an average AUROC of about 0.95 in five latest source models, improving the score by 51% relative to the remaining space of the open source baseline (Table 1).<span class='px-1 mx-1 bg-yellow-200'>It demonstrates that the latest LLMs can effectively detect their own outputs, suggesting that advanced LLMs may be the best shield against themselves. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11506v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11506v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While Large Language Models (LLMs) have become central tools in various fields, they often provide inaccurate or false information.This study examines user preferences regarding falsehood responses from LLMs.Specifically, we evaluate preferences for LLM responses where false statements are explicitly marked versus unmarked responses and preferences for confident falsehoods compared to LLM disclaimers acknowledging a lack of knowledge.Additionally, we investigate how requiring users to assess the truthfulness of statements influences these preferences.   Surprisingly, 61\% of users prefer unmarked falsehood responses over marked ones, and 69\% prefer confident falsehoods over LLMs admitting lack of knowledge.In all our experiments, a total of 300 users participated, contributing valuable data to our analysis and conclusions.When users are required to evaluate the truthfulness of statements, preferences for unmarked and falsehood responses decrease slightly but remain high.<span class='px-1 mx-1 bg-yellow-200'>These findings suggest that user preferences, which influence LLM training via feedback mechanisms, may inadvertently encourage the generation of falsehoods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span>Future research should address the ethical and practical implications of aligning LLM behavior with such preferences.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11625v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11625v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Private Yet Social: How LLM Chatbots Support and Challenge Eating Disorder Recovery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Eating disorders (ED) are complex mental health conditions that require long-term management and support.Recent advancements in large language model (LLM)-based chatbots offer the potential to assist individuals in receiving immediate support.Yet, concerns remain about their reliability and safety in sensitive contexts such as ED.We explore the opportunities and potential harms of using LLM-based chatbots for ED recovery.We observe the interactions between 26 participants with ED and an LLM-based chatbot, WellnessBot, designed to support ED recovery, over 10 days.We discovered that our participants have felt empowered in recovery by discussing ED-related stories with the chatbot, which served as a personal yet social avenue.<span class='px-1 mx-1 bg-yellow-200'>However, we also identified harmful chatbot responses, especially concerning individuals with ED, that went unnoticed partly due to participants' unquestioning trust in the chatbot's reliability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Based on these findings, we provide design implications for safe and effective LLM-based interventions in ED management.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11656v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11656v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User Requests
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose LLM-DaaS, a novel Drone-as-a-Service (DaaS) framework that leverages Large Language Models (LLMs) to transform free-text user requests into structured, actionable DaaS operation tasks.Our approach addresses the key challenge of interpreting and structuring natural language input to automate drone service operations under uncertain conditions.The system is composed of three main components: free-text request processing, structured request generation, and dynamic DaaS selection and composition.First, we fine-tune different LLM models such as Phi-3.5, LLaMA-3.2 7b and Gemma 2b on a dataset of text user requests mapped to structured DaaS requests.Users interact with our model in a free conversational style, discussing package delivery requests, while the fine-tuned LLM extracts DaaS metadata such as delivery time, source and destination locations, and package weight.The DaaS service selection model is designed to select the best available drone capable of delivering the requested package from the delivery point to the nearest optimal destination.Additionally, the DaaS composition model composes a service from a set of the best available drones to deliver the package from the source to the final destination.Second, the system integrates real-time weather data to optimize drone route planning and scheduling, ensuring safe and efficient operations.<span class='px-1 mx-1 bg-yellow-200'>Simulations demonstrate the system's ability to significantly improve task accuracy, operational efficiency, and establish LLM-DaaS as a robust solution for DaaS operations in uncertain environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11672v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11672v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span><span class='px-1 mx-1 bg-yellow-200'>Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>This problem is particularly evident in open-source projects and impacts the overall quality of the software ecosystem.To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code.<span class='px-1 mx-1 bg-yellow-200'>Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Block, and Distorted Handling Solution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span><span class='px-1 mx-1 bg-yellow-200'>These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>In response, we propose Seeker, a multi-agent framework inspired by expert developer strategies for exception handling.Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively.<span class='px-1 mx-1 bg-yellow-200'>Our work is the first systematic study on leveraging LLMs to enhance exception handling practices in real development scenarios, providing valuable insights for future improvements in code reliability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11713v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11713v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span>Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation.To address these issues, we propose \textbf{RetroLLM}, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding.Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy.Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks.The code is available at \url{https://github.com/sunnynexus/RetroLLM}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11919v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11919v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Stepwise Reasoning Error Disruption Attack of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored.Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction.Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes.These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11934v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11934v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with a GAN-Inspired Approach to Synthetic Dataset Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Consider the problem: ``If one man and one woman can produce one child in one year, how many children will be produced by one woman and three men in 0.5 years?"Current large language models (LLMs) such as GPT-4o, GPT-o1-preview, and Gemini Flash frequently answer "0.5," which does not make sense.While these models sometimes acknowledge the unrealistic nature of the question, in many cases (8 out of 10 trials), they provide the nonsensical answer of "0.5 child."Additionally, temporal variation has been observed: if an LLM answers correctly once (by recognizing the faulty nature of the question), subsequent responses are more likely to also reflect this understanding.However, this is inconsistent.   These types of questions have motivated us to develop a dataset of science questions, SciFaultyQA, where the questions themselves are intentionally faulty.<span class='px-1 mx-1 bg-yellow-200'>We observed that LLMs often proceed to answer these flawed questions without recognizing their inherent issues, producing results that are logically or scientifically invalid. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span>By analyzing such patterns, we developed a novel method for generating synthetic datasets to evaluate and benchmark the performance of various LLMs in identifying these flawed questions.We have also developed novel approaches to reduce the errors.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11988v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11988v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Drafting radiology reports is a complex task requiring flexibility, where radiologists tail content to available information and particular clinical demands.However, most current radiology report generation (RRG) models are constrained to a fixed task paradigm, such as predicting the full ``finding'' section from a single image, inherently involving a mismatch between inputs and outputs.<span class='px-1 mx-1 bg-yellow-200'>The trained models lack the flexibility for diverse inputs and could generate harmful, input-agnostic hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>To bridge the gap between current RRG models and the clinical demands in practice, we first develop a data generation pipeline to create a new MIMIC-RG4 dataset, which considers four common radiology report drafting scenarios and has perfectly corresponded input and output.Secondly, we propose a novel large language model (LLM) based RRG framework, namely LLM-RG4, which utilizes LLM's flexible instruction-following capabilities and extensive general knowledge.We further develop an adaptive token fusion module that offers flexibility to handle diverse scenarios with different input combinations, while minimizing the additional computational burden associated with increased input volumes.Besides, we propose a token-level loss weighting strategy to direct the model's attention towards positive and uncertain descriptions.Experimental results demonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical efficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR datasets.<span class='px-1 mx-1 bg-yellow-200'>We quantitatively demonstrate that our model has minimal input-agnostic hallucinations, whereas current open-source models commonly suffer from this problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.842</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12001v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12001v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Large Language Model Approach to Identify Flakiness in C++ Projects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The role of regression testing in software testing is crucial as it ensures that any new modifications do not disrupt the existing functionality and behaviour of the software system.The desired outcome is for regression tests to yield identical results without any modifications made to the system being tested.In practice, however, the presence of Flaky Tests introduces non-deterministic behaviour and undermines the reliability of regression testing results.   <span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose an LLM-based approach for identifying the root cause of flaky tests in C++ projects at the code level, with the intention of assisting developers in debugging and resolving them more efficiently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>We compile a comprehensive collection of C++ project flaky tests sourced from GitHub repositories.We fine-tune Mistral-7b, Llama2-7b and CodeLlama-7b models on the C++ dataset and an existing Java dataset and evaluate the performance in terms of precision, recall, accuracy, and F1 score.We assess the performance of the models across various datasets and offer recommendations for both research and industry applications.   The results indicate that our models exhibit varying performance on the C++ dataset, while their performance is comparable to that of the Java dataset.The Mistral-7b surpasses the other two models regarding all metrics, achieving a score of 1.Our results demonstrate the exceptional capability of LLMs to accurately classify flakiness in C++ and Java projects, providing a promising approach to enhance the efficiency of debugging flaky tests in practice.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12340v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12340v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dual Traits in Probabilistic Reasoning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We conducted three experiments to investigate how large language models (LLMs) evaluate posterior probabilities.Our results reveal the coexistence of two modes in posterior judgment among state-of-the-art models: a normative mode, which adheres to Bayes' rule, and a representative-based mode, which relies on similarity -- paralleling human System 1 and System 2 thinking.Additionally, we observed that LLMs struggle to recall base rate information from their memory, and developing prompt engineering strategies to mitigate representative-based judgment may be challenging.We further conjecture that the dual modes of judgment may be a result of the contrastive loss function employed in reinforcement learning from human feedback.<span class='px-1 mx-1 bg-yellow-200'>Our findings underscore the potential direction for reducing cognitive biases in LLMs and the necessity for cautious deployment of LLMs in critical areas. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11009v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11009v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PromptV: Leveraging LLM-powered Multi-Agent Prompting for High-quality Verilog Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in agentic LLMs have demonstrated remarkable automated Verilog code generation capabilities.However, existing approaches either demand substantial computational resources or rely on LLM-assisted single-agent prompt learning techniques, which we observe for the first time has a degeneration issue - characterized by deteriorating generative performance and diminished error detection and correction capabilities.This paper proposes a novel multi-agent prompt learning framework to address these limitations and enhance code generation quality.<span class='px-1 mx-1 bg-yellow-200'>We show for the first time that multi-agent architectures can effectively mitigate the degeneration risk while improving code error correction capabilities, resulting in higher-quality Verilog code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Experimental results show that the proposed method could achieve 96.4% and 96.5% pass@10 scores on VerilogEval Machine and Human benchmarks, respectively while attaining 100% Syntax and 99.9% Functionality pass@5 metrics on the RTLLM benchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11014v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11014v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although large language models (LLMs) achieve effective safety alignment at the time of release, they still face various safety challenges.<span class='px-1 mx-1 bg-yellow-200'>A key issue is that fine-tuning often compromises the safety alignment of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>To address this issue, we propose a method named \textbf{IRR} (\textbf{I}dentify, \textbf{R}emove, and \textbf{R}ecalibrate for Safety Realignment) that performs safety realignment for LLMs.The core of IRR is to identify and remove unsafe delta parameters from the fine-tuned models, while recalibrating the retained ones.We evaluate the effectiveness of IRR across various datasets, including both full fine-tuning and LoRA methods.Our results demonstrate that IRR significantly enhances the safety performance of fine-tuned models on safety benchmarks, such as harmful queries and jailbreak attacks, while maintaining their performance on downstream tasks.The source code is available at: \url{https://anonymous.4open.science/r/IRR-BD4F}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11041v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11041v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are increasingly capable, aiding in tasks such as content generation, yet they also pose risks, particularly in generating harmful spear-phishing emails.<span class='px-1 mx-1 bg-yellow-200'>These emails, crafted to entice clicks on malicious URLs, threaten personal information security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>This paper proposes an adversarial framework, SpearBot, which utilizes LLMs to generate spear-phishing emails with various phishing strategies.<span class='px-1 mx-1 bg-yellow-200'>Through specifically crafted jailbreak prompts, SpearBot circumvents security policies and introduces other LLM instances as critics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span><span class='px-1 mx-1 bg-yellow-200'>When a phishing email is identified by the critic, SpearBot refines the generated email based on the critique feedback until it can no longer be recognized as phishing, thereby enhancing its deceptive quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>To evaluate the effectiveness of SpearBot, we implement various machine-based defenders and assess how well the phishing emails generated could deceive them.<span class='px-1 mx-1 bg-yellow-200'>Results show these emails often evade detection to a large extent, underscoring their deceptive quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>Additionally, human evaluations of the emails' readability and deception are conducted through questionnaires, confirming their convincing nature and the significant potential harm of the generated phishing emails.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11109v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11109v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combating Multimodal LLM Hallucination via Bottom-up Holistic Reasoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in multimodal large language models (MLLMs) have shown unprecedented capabilities in advancing various vision-language tasks.<span class='px-1 mx-1 bg-yellow-200'>However, MLLMs face significant challenges with hallucinations, and misleading outputs that do not align with the input data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span><span class='px-1 mx-1 bg-yellow-200'>While existing efforts are paid to combat MLLM hallucinations, several pivotal challenges are still unsolved. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>First, while current approaches aggressively focus on addressing errors at the perception level, another important type at the cognition level requiring factual commonsense can be overlooked.<span class='px-1 mx-1 bg-yellow-200'>In addition, existing methods might fall short in finding a more effective way to represent visual input, which is yet a key bottleneck that triggers visual hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, MLLMs can frequently be misled by faulty textual inputs and cause hallucinations, while unfortunately, this type of issue has long been overlooked by existing studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span><span class='px-1 mx-1 bg-yellow-200'>Inspired by human intuition in handling hallucinations, this paper introduces a novel bottom-up reasoning framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>Our framework systematically addresses potential issues in both visual and textual inputs by verifying and integrating perception-level information with cognition-level commonsense knowledge, ensuring more reliable outputs.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate significant improvements in multiple hallucination benchmarks after integrating MLLMs with the proposed framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span><span class='px-1 mx-1 bg-yellow-200'>In-depth analyses reveal the great potential of our methods in addressing perception- and cognition-level hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.839</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11124v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11124v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Task-Oriented Dialog Systems for the Senegalese Wolof Language
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, we are seeing considerable interest in conversational agents with the rise of large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Although they offer considerable advantages, LLMs also present significant risks, such as hallucination, which hinder their widespread deployment in industry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span>Moreover, low-resource languages such as African ones are still underrepresented in these systems limiting their performance in these languages.In this paper, we illustrate a more classical approach based on modular architectures of Task-oriented Dialog Systems (ToDS) offering better control over outputs.We propose a chatbot generation engine based on the Rasa framework and a robust methodology for projecting annotations onto the Wolof language using an in-house machine translation system.After evaluating a generated chatbot trained on the Amazon Massive dataset, our Wolof Intent Classifier performs similarly to the one obtained for French, which is a resource-rich language.We also show that this approach is extensible to other low-resource languages, thanks to the intent classifier's language-agnostic pipeline, simplifying the design of chatbots in these languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11203v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11203v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Zero-Shot Prompting Approaches for LLM-based Graphical User Interface Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Graphical user interface (GUI) prototyping represents an essential activity in the development of interactive systems, which are omnipresent today.GUI prototypes facilitate elicitation of requirements and help to test, evaluate, and validate ideas with users and the development team.However, creating GUI prototypes is a time-consuming process and often requires extensive resources.<span class='px-1 mx-1 bg-yellow-200'>While existing research for automatic GUI generation focused largely on resource-intensive training and fine-tuning of LLMs, mainly for low-fidelity GUIs, we investigate the potential and effectiveness of Zero-Shot (ZS) prompting for high-fidelity GUI generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span>We propose a Retrieval-Augmented GUI Generation (RAGG) approach, integrated with an LLM-based GUI retrieval re-ranking and filtering mechanism based on a large-scale GUI repository.In addition, we adapt Prompt Decomposition (PDGG) and Self-Critique (SCGG) for GUI generation.To evaluate the effectiveness of the proposed ZS prompting approaches for GUI generation, we extensively evaluated the accuracy and subjective satisfaction of the generated GUI prototypes.Our evaluation, which encompasses over 3,000 GUI annotations from over 100 crowd-workers with UI/UX experience, shows that SCGG, in contrast to PDGG and RAGG, can lead to more effective GUI generation, and provides valuable insights into the defects that are produced by the LLMs in the generated GUI prototypes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11328v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11328v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Obfuscated Activations Bypass LLM Latent-Space Defenses
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent latent-space monitoring techniques have shown promise as defenses against LLM attacks.<span class='px-1 mx-1 bg-yellow-200'>These defenses act as scanners that seek to detect harmful activations before they lead to undesirable actions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span><span class='px-1 mx-1 bg-yellow-200'>This prompts the question: Can models execute harmful behavior via inconspicuous latent states? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>Here, we study such obfuscated activations.We show that state-of-the-art latent-space defenses -- including sparse autoencoders, representation probing, and latent OOD detection -- are all vulnerable to obfuscated activations.<span class='px-1 mx-1 bg-yellow-200'>For example, against probes trained to classify harmfulness, our attacks can often reduce recall from 100% to 0% while retaining a 90% jailbreaking rate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>However, obfuscation has limits: we find that on a complex task (writing SQL code), obfuscation reduces model performance.Together, our results demonstrate that neural activations are highly malleable: we can reshape activation patterns in a variety of ways, often while preserving a network's behavior.This poses a fundamental challenge to latent-space defenses.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09565v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09565v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Quantifying the uncertainty in the factual parametric knowledge of Large Language Models (LLMs), especially in a black-box setting, poses a significant challenge.Existing methods, which gauge a model's uncertainty through evaluating self-consistency in responses to the original query, do not always capture true uncertainty.Models might respond consistently to the origin query with a wrong answer, yet respond correctly to varied questions from different perspectives about the same query, and vice versa.In this paper, we propose a novel method, DiverseAgentEntropy, for evaluating a model's uncertainty using multi-agent interaction under the assumption that if a model is certain, it should consistently recall the answer to the original query across a diverse collection of questions about the same original query.We further implement an abstention policy to withhold responses when uncertainty is high.<span class='px-1 mx-1 bg-yellow-200'>Our method offers a more accurate prediction of the model's reliability and further detects hallucinations, outperforming other self-consistency-based methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.836</span></span>Additionally, it demonstrates that existing models often fail to consistently retrieve the correct answer to the same query under diverse varied questions even when knowing the correct answer.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09572v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09572v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Security Challenges in LLM Development</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>DNN-based language models perform excellently on various tasks, but even SOTA LLMs are susceptible to textual adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span><span class='px-1 mx-1 bg-yellow-200'>Adversarial texts play crucial roles in multiple subfields of NLP. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>However, current research has the following issues.<span class='px-1 mx-1 bg-yellow-200'>(1) Most textual adversarial attack methods target rich-resourced languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span><span class='px-1 mx-1 bg-yellow-200'>How do we generate adversarial texts for less-studied languages? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span><span class='px-1 mx-1 bg-yellow-200'>(2) Most textual adversarial attack methods are prone to generating invalid or ambiguous adversarial texts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.884</span></span><span class='px-1 mx-1 bg-yellow-200'>How do we construct high-quality adversarial robustness benchmarks? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span><span class='px-1 mx-1 bg-yellow-200'>(3) New language models may be immune to part of previously generated adversarial texts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.807</span></span><span class='px-1 mx-1 bg-yellow-200'>How do we update adversarial robustness benchmarks? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.822</span></span><span class='px-1 mx-1 bg-yellow-200'>To address the above issues, we introduce HITL-GAT, a system based on a general approach to human-in-the-loop generation of adversarial texts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span>HITL-GAT contains four stages in one pipeline: victim model construction, adversarial example generation, high-quality benchmark construction, and adversarial robustness evaluation.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we utilize HITL-GAT to make a case study on Tibetan script which can be a reference for the adversarial research of other less-studied languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12478v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12478v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NLSR: Neuron-Level Safety Realignment of Large Language Models Against Harmful Fine-Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The emergence of finetuning-as-a-service has revealed a new vulnerability in large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span><span class='px-1 mx-1 bg-yellow-200'>A mere handful of malicious data uploaded by users can subtly manipulate the finetuning process, resulting in an alignment-broken model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing methods to counteract fine-tuning attacks typically require substantial computational resources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span>Even with parameter-efficient techniques like LoRA, gradient updates remain essential.To address these challenges, we propose \textbf{N}euron-\textbf{L}evel \textbf{S}afety \textbf{R}ealignment (\textbf{NLSR}), a training-free framework that restores the safety of LLMs based on the similarity difference of safety-critical neurons before and after fine-tuning.The core of our framework is first to construct a safety reference model from an initially aligned model to amplify safety-related features in neurons.We then utilize this reference model to identify safety-critical neurons, which we prepare as patches.Finally, we selectively restore only those neurons that exhibit significant similarity differences by transplanting these prepared patches, thereby minimally altering the fine-tuned model.Extensive experiments demonstrate significant safety enhancements in fine-tuned models across multiple downstream tasks, while greatly maintaining task-level accuracy.Our findings suggest regions of some safety-critical neurons show noticeable differences after fine-tuning, which can be effectively corrected by transplanting neurons from the reference model without requiring additional training.The code will be available at \url{https://github.com/xinykou/NLSR}</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12497v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12497v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, large language models (LLMs) have significantly improved the performance of text-to-SQL systems.Nevertheless, many state-of-the-art (SOTA) approaches have overlooked the critical aspect of system robustness.<span class='px-1 mx-1 bg-yellow-200'>Our experiments reveal that while LLM-driven methods excel on standard datasets, their accuracy is notably compromised when faced with adversarial perturbations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span>To address this challenge, we propose a robust text-to-SQL solution, called Solid-SQL, designed to integrate with various LLMs.We focus on the pre-processing stage, training a robust schema-linking model enhanced by LLM-based data augmentation.Additionally, we design a two-round, structural similarity-based example retrieval strategy for in-context learning.Our method achieves SOTA SQL execution accuracy levels of 82.1% and 58.9% on the general Spider and Bird benchmarks, respectively.Furthermore, experimental results show that Solid-SQL delivers an average improvement of 11.6% compared to baselines on the perturbed Spider-Syn, Spider-Realistic, and Dr. Spider benchmarks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12522v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12522v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Jailbreaking? One Step Is Enough!
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) excel in various tasks but remain vulnerable to jailbreak attacks, where adversaries manipulate prompts to generate harmful outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.844</span></span><span class='px-1 mx-1 bg-yellow-200'>Examining jailbreak prompts helps uncover the shortcomings of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>However, current jailbreak methods and the target model's defenses are engaged in an independent and adversarial process, resulting in the need for frequent attack iterations and redesigning attacks for different models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.853</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that disguises the attack intention as the "defense". <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span>intention against harmful content.Specifically, REDA starts from the target response, guiding the model to embed harmful content within its defensive measures, thereby relegating harmful content to a secondary role and making the model believe it is performing a defensive task.<span class='px-1 mx-1 bg-yellow-200'>The attacking model considers that it is guiding the target model to deal with harmful content, while the target model thinks it is performing a defensive task, creating an illusion of cooperation between the two. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span>Additionally, to enhance the model's confidence and guidance in "defensive" intentions, we adopt in-context learning (ICL) with a small number of attack examples and construct a corresponding dataset of attack examples.<span class='px-1 mx-1 bg-yellow-200'>Extensive evaluations demonstrate that the REDA method enables cross-model attacks without the need to redesign attack strategies for different models, enables successful jailbreak in one iteration, and outperforms existing methods on both open-source and closed-source models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.905</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12621v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12621v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Truthful Text Sanitization Guided by Inference Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The purpose of text sanitization is to rewrite those text spans in a document that may directly or indirectly identify an individual, to ensure they no longer disclose personal information.Text sanitization must strike a balance between preventing the leakage of personal information (privacy protection) while also retaining as much of the document's original content as possible (utility preservation).We present an automated text sanitization strategy based on generalizations, which are more abstract (but still informative) terms that subsume the semantic content of the original text spans.The approach relies on instruction-tuned large language models (LLMs) and is divided into two stages.The LLM is first applied to obtain truth-preserving replacement candidates and rank them according to their abstraction level.Those candidates are then evaluated for their ability to protect privacy by conducting inference attacks with the LLM.<span class='px-1 mx-1 bg-yellow-200'>Finally, the system selects the most informative replacement shown to be resistant to those attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span>As a consequence of this two-stage process, the chosen replacements effectively balance utility and privacy.We also present novel metrics to automatically evaluate these two aspects without the need to manually annotate data.Empirical results on the Text Anonymization Benchmark show that the proposed approach leads to enhanced utility, with only a marginal increase in the risk of re-identifying protected individuals compared to fully suppressing the original information.Furthermore, the selected replacements are shown to be more truth-preserving and abstractive than previous methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are transforming the robotics domain by enabling robots to comprehend and execute natural language instructions.The cornerstone benefits of LLM include processing textual data from technical manuals, instructions, academic papers, and user queries based on the knowledge provided.<span class='px-1 mx-1 bg-yellow-200'>However, deploying LLM-generated code in robotic systems without safety verification poses significant risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>This paper outlines a safety layer that verifies the code generated by ChatGPT before executing it to control a drone in a simulated environment.The safety layer consists of a fine-tuned GPT-4o model using Few-Shot learning, supported by knowledge graph prompting (KGP).Our approach improves the safety and compliance of robotic actions, ensuring that they adhere to the regulations of drone operations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11387v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11387v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Context.<span class='px-1 mx-1 bg-yellow-200'>The security of critical infrastructure has been a fundamental concern since the advent of computers, and this concern has only intensified in today's cyber warfare landscape. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>Protecting mission-critical systems (MCSs), including essential assets like healthcare, telecommunications, and military coordination, is vital for national security.These systems require prompt and comprehensive governance to ensure their resilience, yet recent events have shown that meeting these demands is increasingly challenging.Aim.Building on prior research that demonstrated the potential of GAI, particularly Large Language Models (LLMs), in improving risk analysis tasks, we aim to explore practitioners' perspectives, specifically developers and security personnel, on using generative AI (GAI) in the governance of IT MCSs seeking to provide insights and recommendations for various stakeholders, including researchers, practitioners, and policymakers.Method.<span class='px-1 mx-1 bg-yellow-200'>We designed a survey to collect practical experiences, concerns, and expectations of practitioners who develop and implement security solutions in the context of MCSs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span>Analyzing this data will help identify key trends, challenges, and opportunities for introducing GAIs in this niche domain.Conclusions and Future Works.Our findings highlight that the safe use of LLMs in MCS governance requires interdisciplinary collaboration.Researchers should focus on designing regulation-oriented models and focus on accountability; practitioners emphasize data protection and transparency, while policymakers must establish a unified AI framework with global benchmarks to ensure ethical and secure LLMs-based MCS governance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11698v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11698v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Seeker: Towards Exception Safety Code Generation with Intermediate Language Agents Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span><span class='px-1 mx-1 bg-yellow-200'>Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.679</span></span>This problem is particularly evident in open-source projects and impacts the overall quality of the software ecosystem.To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code.<span class='px-1 mx-1 bg-yellow-200'>Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Block, and Distorted Handling Solution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled.In response, we propose Seeker, a multi-agent framework inspired by expert developer strategies for exception handling.Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively.Our work is the first systematic study on leveraging LLMs to enhance exception handling practices in real development scenarios, providing valuable insights for future improvements in code reliability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11713v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11713v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper we look at the ability of recent large language models (LLMs) at solving mathematical problems in combinatorics.We compare models LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and undergraduates with prior experience in mathematical olympiads.To facilitate these comparisons we introduce the Combi-Puzzles dataset, which contains 125 problem variants based on 25 combinatorial reasoning problems.<span class='px-1 mx-1 bg-yellow-200'>Each problem is presented in one of five distinct forms, created by systematically manipulating the problem statements through adversarial additions, numeric parameter changes, and linguistic obfuscation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>Our variations preserve the mathematical core and are designed to measure the generalisability of LLM problem-solving abilities, while also increasing confidence that problems are submitted to LLMs in forms that have not been seen as training instances.We found that a model based on GPT-4 outperformed all other models in producing correct responses, and performed significantly better in the mathematical variation of the problems than humans.We also found that modifications to problem statements significantly impact the LLM's performance, while human performance remains unaffected.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11908v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11908v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Stepwise Reasoning Error Disruption Attack of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored.<span class='px-1 mx-1 bg-yellow-200'>Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.805</span></span>Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction.Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes.<span class='px-1 mx-1 bg-yellow-200'>These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11934v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11934v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Despite their remarkable success, large language models (LLMs) have shown limited ability on applied tasks such as vulnerability detection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span><span class='px-1 mx-1 bg-yellow-200'>We investigate various prompting strategies for vulnerability detection and, as part of this exploration, propose a prompting strategy that integrates natural language descriptions of vulnerabilities with a contrastive chain-of-thought reasoning approach, augmented using contrastive samples from a synthetic dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span><span class='px-1 mx-1 bg-yellow-200'>Our study highlights the potential of LLMs to detect vulnerabilities by integrating natural language descriptions, contrastive reasoning, and synthetic examples into a comprehensive prompting framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.803</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that this approach can enhance LLM understanding of vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span><span class='px-1 mx-1 bg-yellow-200'>On a high-quality vulnerability detection dataset such as SVEN, our prompting strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%, 11%, and 14%, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12039v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12039v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Large Language Model Approach to Identify Flakiness in C++ Projects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The role of regression testing in software testing is crucial as it ensures that any new modifications do not disrupt the existing functionality and behaviour of the software system.The desired outcome is for regression tests to yield identical results without any modifications made to the system being tested.<span class='px-1 mx-1 bg-yellow-200'>In practice, however, the presence of Flaky Tests introduces non-deterministic behaviour and undermines the reliability of regression testing results.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose an LLM-based approach for identifying the root cause of flaky tests in C++ projects at the code level, with the intention of assisting developers in debugging and resolving them more efficiently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>We compile a comprehensive collection of C++ project flaky tests sourced from GitHub repositories.We fine-tune Mistral-7b, Llama2-7b and CodeLlama-7b models on the C++ dataset and an existing Java dataset and evaluate the performance in terms of precision, recall, accuracy, and F1 score.We assess the performance of the models across various datasets and offer recommendations for both research and industry applications.   The results indicate that our models exhibit varying performance on the C++ dataset, while their performance is comparable to that of the Java dataset.The Mistral-7b surpasses the other two models regarding all metrics, achieving a score of 1.Our results demonstrate the exceptional capability of LLMs to accurately classify flakiness in C++ and Java projects, providing a promising approach to enhance the efficiency of debugging flaky tests in practice.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12340v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12340v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Although large language models (LLMs) achieve effective safety alignment at the time of release, they still face various safety challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>A key issue is that fine-tuning often compromises the safety alignment of LLMs.To address this issue, we propose a method named \textbf{IRR} (\textbf{I}dentify, \textbf{R}emove, and \textbf{R}ecalibrate for Safety Realignment) that performs safety realignment for LLMs.The core of IRR is to identify and remove unsafe delta parameters from the fine-tuned models, while recalibrating the retained ones.We evaluate the effectiveness of IRR across various datasets, including both full fine-tuning and LoRA methods.<span class='px-1 mx-1 bg-yellow-200'>Our results demonstrate that IRR significantly enhances the safety performance of fine-tuned models on safety benchmarks, such as harmful queries and jailbreak attacks, while maintaining their performance on downstream tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.793</span></span>The source code is available at: \url{https://anonymous.4open.science/r/IRR-BD4F}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11041v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11041v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are increasingly capable, aiding in tasks such as content generation, yet they also pose risks, particularly in generating harmful spear-phishing emails.These emails, crafted to entice clicks on malicious URLs, threaten personal information security.<span class='px-1 mx-1 bg-yellow-200'>This paper proposes an adversarial framework, SpearBot, which utilizes LLMs to generate spear-phishing emails with various phishing strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.809</span></span><span class='px-1 mx-1 bg-yellow-200'>Through specifically crafted jailbreak prompts, SpearBot circumvents security policies and introduces other LLM instances as critics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.797</span></span><span class='px-1 mx-1 bg-yellow-200'>When a phishing email is identified by the critic, SpearBot refines the generated email based on the critique feedback until it can no longer be recognized as phishing, thereby enhancing its deceptive quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span><span class='px-1 mx-1 bg-yellow-200'>To evaluate the effectiveness of SpearBot, we implement various machine-based defenders and assess how well the phishing emails generated could deceive them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span>Results show these emails often evade detection to a large extent, underscoring their deceptive quality.Additionally, human evaluations of the emails' readability and deception are conducted through questionnaires, confirming their convincing nature and the significant potential harm of the generated phishing emails.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11109v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11109v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Labeling NIDS Rules with MITRE ATT&CK Techniques: Machine Learning vs. Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Analysts in Security Operations Centers (SOCs) are often occupied with time-consuming investigations of alerts from Network Intrusion Detection Systems (NIDS).<span class='px-1 mx-1 bg-yellow-200'>Many NIDS rules lack clear explanations and associations with attack techniques, complicating the alert triage and the generation of attack hypotheses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) may be a promising technology to reduce the alert explainability gap by associating rules with attack techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span>In this paper, we investigate the ability of three prominent LLMs (ChatGPT, Claude, and Gemini) to reason about NIDS rules while labeling them with MITRE ATT&CK tactics and techniques.We discuss prompt design and present experiments performed with 973 Snort rules.Our results indicate that while LLMs provide explainable, scalable, and efficient initial mappings, traditional Machine Learning (ML) models consistently outperform them in accuracy, achieving higher precision, recall, and F1-scores.These results highlight the potential for hybrid LLM-ML approaches to enhance SOC operations and better address the evolving threat landscape.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.10978v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.10978v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Understanding the Robustness of LLM-based Evaluations under Perturbations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Traditional evaluation metrics like BLEU and ROUGE fall short when capturing the nuanced qualities of generated text, particularly when there is no single ground truth.In this paper, we explore the potential of Large Language Models (LLMs), specifically Google Gemini 1, to serve as automatic evaluators for non-standardized metrics in summarization and dialog-based tasks.We conduct experiments across multiple prompting strategies to examine how LLMs fare as quality evaluators when compared with human judgments on the SummEval and USR datasets, asking the model to generate both a score as well as a justification for the score.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we explore the robustness of the LLM evaluator by using perturbed inputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>Our findings suggest that while LLMs show promise, their alignment with human evaluators is limited, they are not robust against perturbations and significant improvements are required for their standalone use as reliable evaluators for subjective metrics.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09269v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09269v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Obfuscated Activations Bypass LLM Latent-Space Defenses
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent latent-space monitoring techniques have shown promise as defenses against LLM attacks.<span class='px-1 mx-1 bg-yellow-200'>These defenses act as scanners that seek to detect harmful activations before they lead to undesirable actions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>This prompts the question: Can models execute harmful behavior via inconspicuous latent states?Here, we study such obfuscated activations.<span class='px-1 mx-1 bg-yellow-200'>We show that state-of-the-art latent-space defenses -- including sparse autoencoders, representation probing, and latent OOD detection -- are all vulnerable to obfuscated activations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span><span class='px-1 mx-1 bg-yellow-200'>For example, against probes trained to classify harmfulness, our attacks can often reduce recall from 100% to 0% while retaining a 90% jailbreaking rate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>However, obfuscation has limits: we find that on a complex task (writing SQL code), obfuscation reduces model performance.Together, our results demonstrate that neural activations are highly malleable: we can reshape activation patterns in a variety of ways, often while preserving a network's behavior.This poses a fundamental challenge to latent-space defenses.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09565v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09565v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">HCI in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Myers-Briggs Type Indicator (MBTI) is one of the most influential personality theories reflecting individual differences in thinking, feeling, and behaving.MBTI personality detection has garnered considerable research interest and has evolved significantly over the years.However, this task tends to be overly optimistic, as it currently does not align well with the natural distribution of population personality traits.Specifically, (1) the self-reported labels in existing datasets result in incorrect labeling issues, and (2) the hard labels fail to capture the full range of population personality distributions.In this paper, we optimize the task by constructing MBTIBench, the first manually annotated high-quality MBTI personality detection dataset with soft labels, under the guidance of psychologists.As for the first challenge, MBTIBench effectively solves the incorrect labeling issues, which account for 29.58% of the data.As for the second challenge, we estimate soft labels by deriving the polarity tendency of samples.<span class='px-1 mx-1 bg-yellow-200'>The obtained soft labels confirm that there are more people with non-extreme personality traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Experimental results not only highlight the polarized predictions and biases in LLMs as key directions for future research, but also confirm that soft labels can provide more benefits to other psychological tasks than hard labels.The code and data are available at https://github.com/Personality-NLP/MbtiBench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12510v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12510v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Naturalness in LLM-Generated Utterances through Disfluency Insertion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Disfluencies are a natural feature of spontaneous human speech but are typically absent from the outputs of Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>This absence can diminish the perceived naturalness of synthesized speech, which is an important criteria when building conversational agents that aim to mimick human behaviours. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span>We show how the insertion of disfluencies can alleviate this shortcoming.The proposed approach involves (1) fine-tuning an LLM with Low-Rank Adaptation (LoRA) to incorporate various types of disfluencies into LLM-generated utterances and (2) synthesizing those utterances using a text-to-speech model that supports the generation of speech phenomena such as disfluencies.We evaluated the quality of the generated speech across two metrics: intelligibility and perceived spontaneity.<span class='px-1 mx-1 bg-yellow-200'>We demonstrate through a user study that the insertion of disfluencies significantly increase the perceived spontaneity of the generated speech. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span>This increase came, however, along with a slight reduction in intelligibility.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12710v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12710v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using LLM-Generated Draft Replies to Support Human Experts in Responding to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of Industrial AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The maritime industry requires effective communication among diverse stakeholders to address complex, safety-critical challenges.Industrial AI, including Large Language Models (LLMs), has the potential to augment human experts' workflows in this specialized domain.<span class='px-1 mx-1 bg-yellow-200'>Our case study investigated the utility of LLMs in drafting replies to stakeholder inquiries and supporting case handlers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>We conducted a preliminary study (observations and interviews), a survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding Similarity).We discover that while LLM drafts can streamline workflows, they often require significant modifications to meet the specific demands of maritime communications.Though LLMs are not yet mature enough for safety-critical applications without human oversight, they can serve as valuable augmentative tools.Final decision-making thus must remain with human experts.However, by leveraging the strengths of both humans and LLMs, fostering human-AI collaboration, industries can increase efficiency while maintaining high standards of quality and precision tailored to each case.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12732v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12732v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown promising capabilities in healthcare analysis but face several challenges like hallucinations, parroting, and bias manifestation.These challenges are exacerbated in complex, sensitive, and low-resource domains.<span class='px-1 mx-1 bg-yellow-200'>Therefore, in this work we introduce IC-AnnoMI, an expert-annotated motivational interviewing (MI) dataset built upon AnnoMI by generating in-context conversational dialogues leveraging LLMs, particularly ChatGPT. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.753</span></span>IC-AnnoMI employs targeted prompts accurately engineered through cues and tailored information, taking into account therapy style (empathy, reflection), contextual relevance, and false semantic change.<span class='px-1 mx-1 bg-yellow-200'>Subsequently, the dialogues are annotated by experts, strictly adhering to the Motivational Interviewing Skills Code (MISC), focusing on both the psychological and linguistic dimensions of MI dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span><span class='px-1 mx-1 bg-yellow-200'>We comprehensively evaluate the IC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding of domain intricacies by modeling novel classification tasks employing several classical machine learning and current state-of-the-art transformer approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span>Finally, we discuss the effects of progressive prompting strategies and the impact of augmented data in mitigating the biases manifested in IC-AnnoM.<span class='px-1 mx-1 bg-yellow-200'>Our contributions provide the MI community with not only a comprehensive dataset but also valuable insights for using LLMs in empathetic text generation for conversational therapy in supervised settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Emergence of Strategic Reasoning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As Large Language Models (LLMs) are increasingly used for a variety of complex and critical tasks, it is vital to assess their logical capabilities in strategic environments.This paper examines their ability in strategic reasoning -- the process of choosing an optimal course of action by predicting and adapting to other agents' behavior.<span class='px-1 mx-1 bg-yellow-200'>Using six LLMs, we analyze responses from play in classical games from behavioral economics (p-Beauty Contest, 11-20 Money Request Game, and Guessing Game) and evaluate their performance through hierarchical models of reasoning (level-$k$ theory and cognitive hierarchy theory). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that while LLMs show understanding of the games, the majority struggle with higher-order strategic reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span><span class='px-1 mx-1 bg-yellow-200'>Although most LLMs did demonstrate learning ability with games involving repeated interactions, they still consistently fall short of the reasoning levels demonstrated by typical behavior from human subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>The exception to these overall findings is with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning tasks -- which consistently outperforms other LLMs and human subjects.These findings highlight the challenges and pathways in advancing LLMs toward robust strategic reasoning from the perspective of behavioral economics.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13013v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13013v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span><span class='px-1 mx-1 bg-yellow-200'>This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.826</span></span><span class='px-1 mx-1 bg-yellow-200'>Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.852</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD.Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions.These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13169v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13169v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Scientific English is currently undergoing rapid change, with words like "delve," "intricate," and "underscore" appearing far more frequently than just a few years ago.It is widely assumed that scientists' use of large language models (LLMs) is responsible for such trends.We develop a formal, transferable method to characterize these linguistic changes.Application of our method yields 21 focal words whose increased occurrence in scientific abstracts is likely the result of LLM usage.<span class='px-1 mx-1 bg-yellow-200'>We then pose "the puzzle of lexical overrepresentation": WHY are such words overused by LLMs? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span>We fail to find evidence that lexical overrepresentation is caused by model architecture, algorithm choices, or training data.To assess whether reinforcement learning from human feedback (RLHF) contributes to the overuse of focal words, we undertake comparative model testing and conduct an exploratory online study.While the model testing is consistent with RLHF playing a role, our experimental results suggest that participants may be reacting differently to "delve" than to other focal words.With LLMs quickly becoming a driver of global language change, investigating these potential sources of lexical overrepresentation is important.We note that while insights into the workings of LLMs are within reach, a lack of transparency surrounding model development remains an obstacle to such research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11385v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11385v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) excel at answering questions but remain passive learners--absorbing static data without the ability to question and refine knowledge.<span class='px-1 mx-1 bg-yellow-200'>This paper explores how LLMs can transition to interactive, question-driven learning through student-teacher dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.807</span></span>We introduce INTERACT (INTEReractive Learning for Adaptive Concept Transfer), a framework in which a "student" LLM engages a "teacher" LLM through iterative inquiries to acquire knowledge across 1,347 contexts, including song lyrics, news articles, movie plots, academic papers, and images.Our experiments show that across a wide range of scenarios and LLM architectures, interactive learning consistently enhances performance, achieving up to a 25% improvement, with 'cold-start' student models matching static learning baselines in as few as five dialogue turns.Interactive setups can also mitigate the disadvantages of weaker teachers, showcasing the robustness of question-driven learning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11388v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11388v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Private Yet Social: How LLM Chatbots Support and Challenge Eating Disorder Recovery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Eating disorders (ED) are complex mental health conditions that require long-term management and support.<span class='px-1 mx-1 bg-yellow-200'>Recent advancements in large language model (LLM)-based chatbots offer the potential to assist individuals in receiving immediate support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>Yet, concerns remain about their reliability and safety in sensitive contexts such as ED.We explore the opportunities and potential harms of using LLM-based chatbots for ED recovery.We observe the interactions between 26 participants with ED and an LLM-based chatbot, WellnessBot, designed to support ED recovery, over 10 days.<span class='px-1 mx-1 bg-yellow-200'>We discovered that our participants have felt empowered in recovery by discussing ED-related stories with the chatbot, which served as a personal yet social avenue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.747</span></span>However, we also identified harmful chatbot responses, especially concerning individuals with ED, that went unnoticed partly due to participants' unquestioning trust in the chatbot's reliability.Based on these findings, we provide design implications for safe and effective LLM-based interventions in ED management.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11656v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11656v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Context.The security of critical infrastructure has been a fundamental concern since the advent of computers, and this concern has only intensified in today's cyber warfare landscape.Protecting mission-critical systems (MCSs), including essential assets like healthcare, telecommunications, and military coordination, is vital for national security.These systems require prompt and comprehensive governance to ensure their resilience, yet recent events have shown that meeting these demands is increasingly challenging.Aim.Building on prior research that demonstrated the potential of GAI, particularly Large Language Models (LLMs), in improving risk analysis tasks, we aim to explore practitioners' perspectives, specifically developers and security personnel, on using generative AI (GAI) in the governance of IT MCSs seeking to provide insights and recommendations for various stakeholders, including researchers, practitioners, and policymakers.Method.We designed a survey to collect practical experiences, concerns, and expectations of practitioners who develop and implement security solutions in the context of MCSs.Analyzing this data will help identify key trends, challenges, and opportunities for introducing GAIs in this niche domain.Conclusions and Future Works.<span class='px-1 mx-1 bg-yellow-200'>Our findings highlight that the safe use of LLMs in MCS governance requires interdisciplinary collaboration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span>Researchers should focus on designing regulation-oriented models and focus on accountability; practitioners emphasize data protection and transparency, while policymakers must establish a unified AI framework with global benchmarks to ensure ethical and secure LLMs-based MCS governance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11698v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11698v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Personalized LLM for Generating Customized Responses to the Same Query from Different Users
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Existing work on large language model (LLM) personalization assigned different responding roles to LLM, but overlooked the diversity of questioners.In this work, we propose a new form of questioner-aware LLM personalization, generating different responses even for the same query from different questioners.We design a dual-tower model architecture with a cross-questioner general encoder and a questioner-specific encoder.We further apply contrastive learning with multi-view augmentation, pulling close the dialogue representations of the same questioner, while pulling apart those of different questioners.<span class='px-1 mx-1 bg-yellow-200'>To mitigate the impact of question diversity on questioner-contrastive learning, we cluster the dialogues based on question similarity and restrict the scope of contrastive learning within each cluster. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span>We also build a multi-questioner dataset from English and Chinese scripts and WeChat records, called MQDialog, containing 173 questioners and 12 responders.Extensive evaluation with different metrics shows a significant improvement in the quality of personalized response generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11736v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11736v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rise of large language models (LLMs) has created a need for advanced benchmarking systems beyond traditional setups.To this end, we introduce QUENCH, a novel text-based English Quizzing Benchmark manually curated and transcribed from YouTube quiz videos.QUENCH possesses masked entities and rationales for the LLMs to predict via generation.At the intersection of geographical context and common sense reasoning, QUENCH helps assess world knowledge and deduction capabilities of LLMs via a zero-shot, open-domain quizzing setup.<span class='px-1 mx-1 bg-yellow-200'>We perform an extensive evaluation on 7 LLMs and 4 metrics, investigating the influence of model size, prompting style, geographical context, and gold-labeled rationale generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span>The benchmarking concludes with an error analysis to which the LLMs are prone.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11763v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11763v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CharacterBench: Benchmarking Character Customization of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Character-based dialogue (aka role-playing) enables users to freely customize characters for interaction, which often relies on LLMs, raising the need to evaluate LLMs' character customization capability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>However, existing benchmarks fail to ensure a robust evaluation as they often only involve a single character category or evaluate limited dimensions.Moreover, the sparsity of character features in responses makes feature-focused generative evaluation both ineffective and inefficient.To address these issues, we propose CharacterBench, the largest bilingual generative benchmark, with 22,859 human-annotated samples covering 3,956 characters from 25 detailed character categories.We define 11 dimensions of 6 aspects, classified as sparse and dense dimensions based on whether character features evaluated by specific dimensions manifest in each response.We enable effective and efficient evaluation by crafting tailored queries for each dimension to induce characters' responses related to specific dimensions.Further, we develop CharacterJudge model for cost-effective and stable evaluations.Experiments show its superiority over SOTA automatic judges (e.g., GPT-4) and our benchmark's potential to optimize LLMs' character customization.Our repository is at https://github.com/thu-coai/CharacterBench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11912v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11912v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula.An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support.Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them.Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning.We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM).Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM.<span class='px-1 mx-1 bg-yellow-200'>This LLM generated message recommendations for caregivers supporting their child's math practice via chat. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations.These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation.<span class='px-1 mx-1 bg-yellow-200'>We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11995v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11995v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The deployment of large language models (LLMs) in diverse applications requires a thorough understanding of their decision-making strategies and behavioral patterns.<span class='px-1 mx-1 bg-yellow-200'>As a supplement to a recent study on the behavioral Turing test, this paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.817</span></span>By benchmarking these AI chatbots, we aim to uncover and document both common and distinct behavioral patterns across a range of scenarios.<span class='px-1 mx-1 bg-yellow-200'>The findings provide valuable insights into the strategic preferences of each LLM, highlighting potential implications for their deployment in critical decision-making roles. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12362v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12362v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dual Traits in Probabilistic Reasoning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We conducted three experiments to investigate how large language models (LLMs) evaluate posterior probabilities.<span class='px-1 mx-1 bg-yellow-200'>Our results reveal the coexistence of two modes in posterior judgment among state-of-the-art models: a normative mode, which adheres to Bayes' rule, and a representative-based mode, which relies on similarity -- paralleling human System 1 and System 2 thinking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>Additionally, we observed that LLMs struggle to recall base rate information from their memory, and developing prompt engineering strategies to mitigate representative-based judgment may be challenging.We further conjecture that the dual modes of judgment may be a result of the contrastive loss function employed in reinforcement learning from human feedback.Our findings underscore the potential direction for reducing cognitive biases in LLMs and the necessity for cautious deployment of LLMs in critical areas.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11009v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11009v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NITRO: LLM Inference on Intel Laptop NPUs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have become essential tools in natural language processing, finding large usage in chatbots such as ChatGPT and Gemini, and are a central area of research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>A particular area of interest includes designing hardware specialized for these AI applications, with one such example being the neural processing unit (NPU).In 2023, Intel released the Intel Core Ultra processor with codename Meteor Lake, featuring a CPU, GPU, and NPU system-on-chip.However, official software support for the NPU through Intel's OpenVINO framework is limited to static model inference.The dynamic nature of autoregressive token generation in LLMs is therefore not supported out of the box.To address this shortcoming, we present NITRO (NPU Inference for Transformers Optimization), a Python-based framework built on top of OpenVINO to support text and chat generation on NPUs.In this paper, we discuss in detail the key modifications made to the transformer architecture to enable inference, some performance benchmarks, and future steps towards improving the package.The code repository for NITRO can be found here: https://github.com/abdelfattah-lab/nitro.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11053v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11053v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Empowering LLMs to Understand and Generate Complex Vector Graphics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions.Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives.In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics.LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs.Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks.Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information.To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected a massive dataset of more than 250k SVG data and 580k SVG-text instructions, which facilitated the adoption of the two-stage training strategy popular in LLM development.By exploring various training strategies, we developed LLM4SVG, which significantly moves beyond optimized rendering-based approaches and language-model-based baselines to achieve remarkable results in human evaluation tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11102v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11102v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) face challenges in aligning with diverse cultural values despite their remarkable performance in generation, which stems from inherent monocultural biases and difficulties in capturing nuanced cultural semantics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>Existing methods lack adaptability to unkown culture after finetuning.<span class='px-1 mx-1 bg-yellow-200'>Inspired by cultural geography across five continents, we propose Cultural Palette, a multi-agent framework for cultural alignment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span><span class='px-1 mx-1 bg-yellow-200'>We first introduce the Pentachromatic Cultural Palette Dataset synthesized using LLMs to capture diverse cultural values from social dialogues across five continents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span>Building on this, Cultural Palette integrates five continent-level alignment agents with a meta-agent using our superior Cultural MoErges alignment technique by dynamically activating relevant cultural expertise based on user prompts to adapting new culture, which outperforms other joint and merging alignment strategies in overall cultural value alignment.Each continent agent generates a cultural draft, which is then refined and self-regulated by the meta-agent to produce the final culturally aligned response.Experiments across various countries demonstrate that Cultural Palette surpasses existing baselines in cultural alignment.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11167v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11167v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Large Language Models for Active Merchant Non-player Characters
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We highlight two significant issues leading to the passivity of current merchant non-player characters (NPCs): pricing and communication.<span class='px-1 mx-1 bg-yellow-200'>While immersive interactions have been a focus, negotiations between merchant NPCs and players on item prices have not received sufficient attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>First, we define passive pricing as the limited ability of merchants to modify predefined item prices.Second, passive communication means that merchants can only interact with players in a scripted manner.To tackle these issues and create an active merchant NPC, we propose a merchant framework based on large language models (LLMs), called MART, which consists of an appraiser module and a negotiator module.We conducted two experiments to guide game developers in selecting appropriate implementations by comparing different training methods and LLM sizes.Our findings indicate that finetuning methods, such as supervised finetuning (SFT) and knowledge distillation (KD), are effective in using smaller LLMs to implement active merchant NPCs.Additionally, we found three irregular cases arising from the responses of LLMs.We expect our findings to guide developers in using LLMs for developing active merchant NPCs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11189v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11189v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Task-Oriented Dialog Systems for the Senegalese Wolof Language
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In recent years, we are seeing considerable interest in conversational agents with the rise of large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span>Although they offer considerable advantages, LLMs also present significant risks, such as hallucination, which hinder their widespread deployment in industry.Moreover, low-resource languages such as African ones are still underrepresented in these systems limiting their performance in these languages.In this paper, we illustrate a more classical approach based on modular architectures of Task-oriented Dialog Systems (ToDS) offering better control over outputs.We propose a chatbot generation engine based on the Rasa framework and a robust methodology for projecting annotations onto the Wolof language using an in-house machine translation system.After evaluating a generated chatbot trained on the Amazon Massive dataset, our Wolof Intent Classifier performs similarly to the one obtained for French, which is a resource-rich language.We also show that this approach is extensible to other low-resource languages, thanks to the intent classifier's language-agnostic pipeline, simplifying the design of chatbots in these languages.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11203v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11203v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Beyond Discrete Personas: Personality Modeling Through Journal Intensive Conversations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have significantly improved personalized conversational capabilities.However, existing datasets like Persona Chat, Synthetic Persona Chat, and Blended Skill Talk rely on static, predefined personas.<span class='px-1 mx-1 bg-yellow-200'>This approach often results in dialogues that fail to capture human personalities' fluid and evolving nature. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>To overcome these limitations, we introduce a novel dataset with around 400,000 dialogues and a framework for generating personalized conversations using long-form journal entries from Reddit.Our approach clusters journal entries for each author and filters them by selecting the most representative cluster, ensuring that the retained entries best reflect the author's personality.We further refine the data by capturing the Big Five personality traits --openness, conscientiousness, extraversion, agreeableness, and neuroticism --ensuring that dialogues authentically reflect an individual's personality.<span class='px-1 mx-1 bg-yellow-200'>Using Llama 3 70B, we generate high-quality, personality-rich dialogues grounded in these journal entries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.808</span></span><span class='px-1 mx-1 bg-yellow-200'>Fine-tuning models on this dataset leads to an 11% improvement in capturing personality traits on average, outperforming existing approaches in generating more coherent and personality-driven dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11250v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11250v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CATER: Leveraging LLM to Pioneer a Multidimensional, Reference-Independent Paradigm in Translation Quality Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper introduces the Comprehensive AI-assisted Translation Edit Ratio (CATER), a novel and fully prompt-driven framework for evaluating machine translation (MT) quality.Leveraging large language models (LLMs) via a carefully designed prompt-based protocol, CATER expands beyond traditional reference-bound metrics, offering a multidimensional, reference-independent evaluation that addresses linguistic accuracy, semantic fidelity, contextual coherence, stylistic appropriateness, and information completeness.CATER's unique advantage lies in its immediate implementability: by providing the source and target texts along with a standardized prompt, an LLM can rapidly identify errors, quantify edit effort, and produce category-level and overall scores.This approach eliminates the need for pre-computed references or domain-specific resources, enabling instant adaptation to diverse languages, genres, and user priorities through adjustable weights and prompt modifications.<span class='px-1 mx-1 bg-yellow-200'>CATER's LLM-enabled strategy supports more nuanced assessments, capturing phenomena such as subtle omissions, hallucinations, and discourse-level shifts that increasingly challenge contemporary MT systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span>By uniting the conceptual rigor of frameworks like MQM and DQF with the scalability and flexibility of LLM-based evaluation, CATER emerges as a valuable tool for researchers, developers, and professional translators worldwide.The framework and example prompts are openly available, encouraging community-driven refinement and further empirical validation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11261v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11261v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Zero-Shot Prompting Approaches for LLM-based Graphical User Interface Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Graphical user interface (GUI) prototyping represents an essential activity in the development of interactive systems, which are omnipresent today. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>GUI prototypes facilitate elicitation of requirements and help to test, evaluate, and validate ideas with users and the development team.However, creating GUI prototypes is a time-consuming process and often requires extensive resources.While existing research for automatic GUI generation focused largely on resource-intensive training and fine-tuning of LLMs, mainly for low-fidelity GUIs, we investigate the potential and effectiveness of Zero-Shot (ZS) prompting for high-fidelity GUI generation.We propose a Retrieval-Augmented GUI Generation (RAGG) approach, integrated with an LLM-based GUI retrieval re-ranking and filtering mechanism based on a large-scale GUI repository.In addition, we adapt Prompt Decomposition (PDGG) and Self-Critique (SCGG) for GUI generation.<span class='px-1 mx-1 bg-yellow-200'>To evaluate the effectiveness of the proposed ZS prompting approaches for GUI generation, we extensively evaluated the accuracy and subjective satisfaction of the generated GUI prototypes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>Our evaluation, which encompasses over 3,000 GUI annotations from over 100 crowd-workers with UI/UX experience, shows that SCGG, in contrast to PDGG and RAGG, can lead to more effective GUI generation, and provides valuable insights into the defects that are produced by the LLMs in the generated GUI prototypes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11328v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11328v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Large Language Models on Cross-Cultural Values in Connection with Training Methodology
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) closely interact with humans, and thus need an intimate understanding of the cultural values of human society. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>In this paper, we explore how open-source LLMs make judgments on diverse categories of cultural values across countries, and its relation to training methodology such as model sizes, training corpus, alignment, etc.<span class='px-1 mx-1 bg-yellow-200'>Our analysis shows that LLMs can judge socio-cultural norms similar to humans but less so on social systems and progress. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>In addition, LLMs tend to judge cultural values biased toward Western culture, which can be improved with training on the multilingual corpus.We also find that increasing model size helps a better understanding of social values, but smaller models can be enhanced by using synthetic data.<span class='px-1 mx-1 bg-yellow-200'>Our analysis reveals valuable insights into the design methodology of LLMs in connection with their understanding of cultural values. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08846v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08846v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The believable simulation of multi-user behavior is crucial for understanding complex social systems.Recently, large language models (LLMs)-based AI agents have made significant progress, enabling them to achieve human-like intelligence across various tasks.<span class='px-1 mx-1 bg-yellow-200'>However, real human societies are often dynamic and complex, involving numerous individuals engaging in multimodal interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, taking e-commerce scenarios as an example, we present LMAgent, a very large-scale and multimodal agents society based on multimodal LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>In LMAgent, besides freely chatting with friends, the agents can autonomously browse, purchase, and review products, even perform live streaming e-commerce.To simulate this complex system, we introduce a self-consistency prompting mechanism to augment agents' multimodal capabilities, resulting in significantly improved decision-making performance over the existing multi-agent system.Moreover, we propose a fast memory mechanism combined with the small-world model to enhance system efficiency, which supports more than 10,000 agent simulations in a society.Experiments on agents' behavior show that these agents achieve comparable performance to humans in behavioral indicators.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09237v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09237v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Systematic Review of Knowledge Tracing and Large Language Models in Education: Opportunities, Issues, and Future Research
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Knowledge Tracing (KT) is a research field that aims to estimate a student's knowledge state through learning interactions-a crucial component of Intelligent Tutoring Systems (ITSs).Despite significant advancements, no current KT models excel in both predictive accuracy and interpretability.Meanwhile, Large Language Models (LLMs), pre-trained on vast natural language datasets, have emerged as powerful tools with immense potential in various educational applications.<span class='px-1 mx-1 bg-yellow-200'>This systematic review explores the intersections, opportunities, and challenges of combining KT models and LLMs in educational contexts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span>The review first investigates LLM applications in education, including their adaptability to domain-specific content and ability to support personalized learning.It then examines the development and current state of KT models, from traditional to advanced approaches, aiming to uncover potential challenges that LLMs could mitigate.The core of this review focuses on integrating LLMs with KT, exploring three primary functions: addressing general concerns in KT fields, overcoming specific KT model limitations, and performing as KT models themselves.Our findings reveal that LLMs can be customized for specific educational tasks through tailor-making techniques such as in-context learning and agent-based approaches, effectively managing complex and unbalanced educational data.These models can enhance existing KT models' performance and solve cold-start problems by generating relevant features from question data.However, both current models depend heavily on structured, limited datasets, missing opportunities to use diverse educational data that could offer deeper insights into individual learners and support various educational settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09248v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09248v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>LLMs can generate human-like dialogues, yet their ability to simulate early child-adult interactions remains largely unexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span>In this paper, we examined how effectively LLMs can capture the distinctive features of child-caregiver language in interaction, using both static and interactive benchmarking methods.<span class='px-1 mx-1 bg-yellow-200'>We found that state-of-the-art LLMs like Llama 3 and GPT-4o can approximate child-caregiver dialogues at the word and utterance level, but they struggle to reproduce the child and caregiver's discursive patterns, exaggerate alignment, and fail to reach the level of diversity shown by humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>The broader goal of this work is to initiate the development of a comprehensive benchmark for LLMs in child-oriented applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09318v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09318v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Large Language Models in Social Sciences</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The Myers-Briggs Type Indicator (MBTI) is one of the most influential personality theories reflecting individual differences in thinking, feeling, and behaving. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>MBTI personality detection has garnered considerable research interest and has evolved significantly over the years.<span class='px-1 mx-1 bg-yellow-200'>However, this task tends to be overly optimistic, as it currently does not align well with the natural distribution of population personality traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.792</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, (1) the self-reported labels in existing datasets result in incorrect labeling issues, and (2) the hard labels fail to capture the full range of population personality distributions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.811</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we optimize the task by constructing MBTIBench, the first manually annotated high-quality MBTI personality detection dataset with soft labels, under the guidance of psychologists. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>As for the first challenge, MBTIBench effectively solves the incorrect labeling issues, which account for 29.58% of the data.As for the second challenge, we estimate soft labels by deriving the polarity tendency of samples.<span class='px-1 mx-1 bg-yellow-200'>The obtained soft labels confirm that there are more people with non-extreme personality traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span><span class='px-1 mx-1 bg-yellow-200'>Experimental results not only highlight the polarized predictions and biases in LLMs as key directions for future research, but also confirm that soft labels can provide more benefits to other psychological tasks than hard labels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>The code and data are available at https://github.com/Personality-NLP/MbtiBench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12510v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12510v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC).Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies.In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data.Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums.Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models.<span class='px-1 mx-1 bg-yellow-200'>Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12541v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12541v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Understanding Emotional Body Expressions via Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emotion recognition based on body movements is vital in human-computer interaction.<span class='px-1 mx-1 bg-yellow-200'>However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span>In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs).Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data.This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance.Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module.This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets.Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods.More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12581v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12581v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detecting Document-level Paraphrased Machine Generated Content: Mimicking Human Writing Style and Involving Discourse Features
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The availability of high-quality APIs for Large Language Models (LLMs) has facilitated the widespread creation of Machine-Generated Content (MGC), posing challenges such as academic plagiarism and the spread of misinformation.Existing MGC detectors often focus solely on surface-level information, overlooking implicit and structural features.<span class='px-1 mx-1 bg-yellow-200'>This makes them susceptible to deception by surface-level sentence patterns, particularly for longer texts and in texts that have been subsequently paraphrased.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span>To overcome these challenges, we introduce novel methodologies and datasets.Besides the publicly available dataset Plagbench, we developed the paraphrased Long-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts (paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by extending artifacts from their original versions.To address the challenge of detecting highly similar paraphrased texts, we propose MhBART, an encoder-decoder model designed to emulate human writing style while incorporating a novel difference score mechanism.This model outperforms strong classifier baselines and identifies deceptive sentence patterns.To better capture the structure of longer texts at document level, we propose DTransformer, a model that integrates discourse analysis through PDTB preprocessing to encode structural features.It results in substantial performance gains across both datasets -- 15.5\% absolute improvement on paraLFQA, 4\% absolute improvement on paraWP, and 1.5\% absolute improvement on M4 compared to SOTA approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12679v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12679v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                XTransplant: A Probe into the Upper Bound Performance of Multilingual Capability and Culture Adaptability in LLMs via Mutual Cross-lingual Feed-forward Transplantation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Current large language models (LLMs) often exhibit imbalances in multilingual capabilities and cultural adaptability, largely due to their English-centric pretraining data.To address this imbalance, we propose a probing method named XTransplant that explores cross-lingual latent interactions via cross-lingual feed-forward transplantation during inference stage, with the hope of enabling the model to leverage the strengths of both English and non-English languages.<span class='px-1 mx-1 bg-yellow-200'>Through extensive pilot experiments, we empirically prove that both the multilingual capabilities and cultural adaptability of LLMs hold the potential to be significantly improved by XTransplant, respectively from En -> non-En and non-En -> En, highlighting the underutilization of current LLMs' multilingual potential. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>And the patterns observed in these pilot experiments further motivate an offline scaling inference strategy, which demonstrates consistent performance improvements in multilingual and culture-aware tasks, sometimes even surpassing multilingual supervised fine-tuning.And we do hope our further analysis and discussion could help gain deeper insights into XTransplant mechanism.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12686v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12686v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Naturalness in LLM-Generated Utterances through Disfluency Insertion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Disfluencies are a natural feature of spontaneous human speech but are typically absent from the outputs of Large Language Models (LLMs).This absence can diminish the perceived naturalness of synthesized speech, which is an important criteria when building conversational agents that aim to mimick human behaviours.We show how the insertion of disfluencies can alleviate this shortcoming.The proposed approach involves (1) fine-tuning an LLM with Low-Rank Adaptation (LoRA) to incorporate various types of disfluencies into LLM-generated utterances and (2) synthesizing those utterances using a text-to-speech model that supports the generation of speech phenomena such as disfluencies.<span class='px-1 mx-1 bg-yellow-200'>We evaluated the quality of the generated speech across two metrics: intelligibility and perceived spontaneity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>We demonstrate through a user study that the insertion of disfluencies significantly increase the perceived spontaneity of the generated speech.This increase came, however, along with a slight reduction in intelligibility.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12710v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12710v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references.<span class='px-1 mx-1 bg-yellow-200'>This discrepancy undermines the reliability of traditional reference-based evaluation metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism.Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria.Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models.Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12832v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12832v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking and Understanding Compositional Relational Reasoning of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Compositional relational reasoning (CRR) is a hallmark of human intelligence, but we lack a clear understanding of whether and how existing transformer large language models (LLMs) can solve CRR tasks.To enable systematic exploration of the CRR capability of LLMs, we first propose a new synthetic benchmark called Generalized Associative Recall (GAR) by integrating and generalizing the essence of several tasks in mechanistic interpretability (MI) study in a unified framework.Evaluation shows that GAR is challenging enough for existing LLMs, revealing their fundamental deficiency in CRR.Meanwhile, it is easy enough for systematic MI study.Then, to understand how LLMs solve GAR tasks, we use attribution patching to discover the core circuits reused by Vicuna-33B across different tasks and a set of vital attention heads.<span class='px-1 mx-1 bg-yellow-200'>Intervention experiments show that the correct functioning of these heads significantly impacts task performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>Especially, we identify two classes of heads whose activations represent the abstract notion of true and false in GAR tasks respectively.They play a fundamental role in CRR across various models and tasks.The dataset and code are available at https://github.com/Caiyun-AI/GAR.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12841v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12841v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors.However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable.<span class='px-1 mx-1 bg-yellow-200'>We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span><span class='px-1 mx-1 bg-yellow-200'>However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span><span class='px-1 mx-1 bg-yellow-200'>Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., "don't cheat,"). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions.<span class='px-1 mx-1 bg-yellow-200'>To this end, we introduce a novel moral judgment approach called \textit{ClarityEthic} that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.609</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12848v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12848v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SnakModel: Lessons Learned from Training an Open Danish Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present SnakModel, a Danish large language model (LLM) based on Llama2-7B, which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M Danish instructions.<span class='px-1 mx-1 bg-yellow-200'>As best practices for creating LLMs for smaller language communities have yet to be established, we examine the effects of early modeling and training decisions on downstream performance throughout the entire training pipeline, including (1) the creation of a strictly curated corpus of Danish text from diverse sources; (2) the language modeling and instruction-tuning training process itself, including the analysis of intermediate training dynamics, and ablations across different hyperparameters; (3) an evaluation on eight language and culturally-specific tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>Across these experiments SnakModel achieves the highest overall performance, outperforming multiple contemporary Llama2-7B-based models.By making SnakModel, the majority of our pre-training corpus, and the associated code available under open licenses, we hope to foster further research and development in Danish Natural Language Processing, and establish training guidelines for languages with similar resource constraints.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12956v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12956v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown promising capabilities in healthcare analysis but face several challenges like hallucinations, parroting, and bias manifestation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span>These challenges are exacerbated in complex, sensitive, and low-resource domains.Therefore, in this work we introduce IC-AnnoMI, an expert-annotated motivational interviewing (MI) dataset built upon AnnoMI by generating in-context conversational dialogues leveraging LLMs, particularly ChatGPT.IC-AnnoMI employs targeted prompts accurately engineered through cues and tailored information, taking into account therapy style (empathy, reflection), contextual relevance, and false semantic change.<span class='px-1 mx-1 bg-yellow-200'>Subsequently, the dialogues are annotated by experts, strictly adhering to the Motivational Interviewing Skills Code (MISC), focusing on both the psychological and linguistic dimensions of MI dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span><span class='px-1 mx-1 bg-yellow-200'>We comprehensively evaluate the IC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding of domain intricacies by modeling novel classification tasks employing several classical machine learning and current state-of-the-art transformer approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span>Finally, we discuss the effects of progressive prompting strategies and the impact of augmented data in mitigating the biases manifested in IC-AnnoM.<span class='px-1 mx-1 bg-yellow-200'>Our contributions provide the MI community with not only a comprehensive dataset but also valuable insights for using LLMs in empathetic text generation for conversational therapy in supervised settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LMUnit: Fine-grained Evaluation with Natural Language Unit Tests
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge -- human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals.We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUnit, which combines multi-objective training across preferences, direct ratings, and natural language rationales.<span class='px-1 mx-1 bg-yellow-200'>Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>LMUnit achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench.These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13091v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13091v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent research, large language models (LLMs) have been increasingly used to investigate public opinions.<span class='px-1 mx-1 bg-yellow-200'>This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span><span class='px-1 mx-1 bg-yellow-200'>Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups.Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD.Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions.These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13169v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13169v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Scientific English is currently undergoing rapid change, with words like "delve," "intricate," and "underscore" appearing far more frequently than just a few years ago. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>It is widely assumed that scientists' use of large language models (LLMs) is responsible for such trends.<span class='px-1 mx-1 bg-yellow-200'>We develop a formal, transferable method to characterize these linguistic changes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>Application of our method yields 21 focal words whose increased occurrence in scientific abstracts is likely the result of LLM usage.<span class='px-1 mx-1 bg-yellow-200'>We then pose "the puzzle of lexical overrepresentation": WHY are such words overused by LLMs? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span><span class='px-1 mx-1 bg-yellow-200'>We fail to find evidence that lexical overrepresentation is caused by model architecture, algorithm choices, or training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span><span class='px-1 mx-1 bg-yellow-200'>To assess whether reinforcement learning from human feedback (RLHF) contributes to the overuse of focal words, we undertake comparative model testing and conduct an exploratory online study. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span><span class='px-1 mx-1 bg-yellow-200'>While the model testing is consistent with RLHF playing a role, our experimental results suggest that participants may be reacting differently to "delve" than to other focal words. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span><span class='px-1 mx-1 bg-yellow-200'>With LLMs quickly becoming a driver of global language change, investigating these potential sources of lexical overrepresentation is important. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>We note that while insights into the workings of LLMs are within reach, a lack of transparency surrounding model development remains an obstacle to such research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11385v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11385v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ACE-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As multimodal large language models (MLLMs) gain prominence in the medical field, the need for precise evaluation methods to assess their effectiveness has become critical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.66</span></span>While benchmarks provide a reliable means to evaluate the capabilities of MLLMs, traditional metrics like ROUGE and BLEU employed for open domain evaluation only focus on token overlap and may not align with human judgment.Although human evaluation is more reliable, it is labor-intensive, costly, and not scalable.LLM-based evaluation methods have proven promising, but to date, there is still an urgent need for open-source multimodal LLM-based evaluators in the medical field.To address this issue, we introduce ACE-$M^3$, an open-sourced \textbf{A}utomatic \textbf{C}apability \textbf{E}valuator for \textbf{M}ultimodal \textbf{M}edical \textbf{M}odels specifically designed to assess the question answering abilities of medical MLLMs.It first utilizes a branch-merge architecture to provide both detailed analysis and a concise final score based on standard medical evaluation criteria.Subsequently, a reward token-based direct preference optimization (RTDPO) strategy is incorporated to save training time without compromising performance of our model.Extensive experiments have demonstrated the effectiveness of our ACE-$M^3$ model\footnote{\url{https://huggingface.co/collections/AIUSRTMP/ace-m3-67593297ff391b93e3e5d068}} in evaluating the capabilities of medical MLLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11453v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11453v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Embodied CoT Distillation From LLM To Off-the-shelf Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices.We present DeDer, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies.In DeDer, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy.The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales.The planning-policy, guided by the rationales, can render optimized plans efficiently.In turn, DeDer allows for adopting sLMs for both policies, deployed on off-the-shelf devices.Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model.<span class='px-1 mx-1 bg-yellow-200'>Our experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DeDer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11499v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11499v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DART: An AIGT Detector using AMR of Rephrased Text
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As large language models (LLMs) generate more human-like texts, concerns about the side effects of AI-generated texts (AIGT) have grown. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span>So, researchers have developed methods for detecting AIGT.However, two challenges remain.First, the performance on detecting black-box LLMs is low, because existing models have focused on syntactic features.Second, most AIGT detectors have been tested on a single-candidate setting, which assumes that we know the origin of an AIGT and may deviate from the real-world scenario.To resolve these challenges, we propose DART, which consists of four steps: rephrasing, semantic parsing, scoring, and multiclass classification.We conducted several experiments to test the performance of DART by following previous work.The experimental result shows that DART can discriminate multiple black-box LLMs without using syntactic features and knowing the origin of AIGT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11517v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11517v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Role of Natural Language Processing Tasks in Automatic Literary Character Network Construction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The automatic extraction of character networks from literary texts is generally carried out using natural language processing (NLP) cascading pipelines.<span class='px-1 mx-1 bg-yellow-200'>While this approach is widespread, no study exists on the impact of low-level NLP tasks on their performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>In this article, we conduct such a study on a literary dataset, focusing on the role of named entity recognition (NER) and coreference resolution when extracting co-occurrence networks.To highlight the impact of these tasks' performance, we start with gold-standard annotations, progressively add uniformly distributed errors, and observe their impact in terms of character network quality.We demonstrate that NER performance depends on the tested novel and strongly affects character detection.We also show that NER-detected mentions alone miss a lot of character co-occurrences, and that coreference resolution is needed to prevent this.Finally, we present comparison points with 2 methods based on large language models (LLMs), including a fully end-to-end one, and show that these models are outperformed by traditional NLP pipelines in terms of recall.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11560v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11560v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Model (LLM)-based agents have shown promise in procedural tasks, but the potential of multimodal instructions augmented by texts and videos to assist users remains under-explored.To address this gap, we propose the Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel LLM-empowered Multimodal Procedural Planning (MPP) framework.It generates cohesive text and video procedural plans given a specified high-level objective.The main challenges are achieving textual and visual informativeness, temporal coherence, and accuracy in procedural plans.VG-TVP leverages the zero-shot reasoning capability of LLMs, the video-to-text generation ability of the video captioning models, and the text-to-video generation ability of diffusion models.VG-TVP improves the interaction between modalities by proposing a novel Fusion of Captioning (FoC) method and using Text-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B).They allow LLMs to guide the generation of visually-grounded text plans and textual-grounded video plans.To address the scarcity of datasets suitable for MPP, we have curated a new dataset called Daily-Life Task Procedural Plans (Daily-PP).<span class='px-1 mx-1 bg-yellow-200'>We conduct comprehensive experiments and benchmarks to evaluate human preferences (regarding textual and visual informativeness, temporal coherence, and plan accuracy). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>Our VG-TVP method outperforms unimodal baselines on the Daily-PP dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11621v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11621v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Private Yet Social: How LLM Chatbots Support and Challenge Eating Disorder Recovery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Eating disorders (ED) are complex mental health conditions that require long-term management and support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>Recent advancements in large language model (LLM)-based chatbots offer the potential to assist individuals in receiving immediate support.Yet, concerns remain about their reliability and safety in sensitive contexts such as ED.We explore the opportunities and potential harms of using LLM-based chatbots for ED recovery.<span class='px-1 mx-1 bg-yellow-200'>We observe the interactions between 26 participants with ED and an LLM-based chatbot, WellnessBot, designed to support ED recovery, over 10 days. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span><span class='px-1 mx-1 bg-yellow-200'>We discovered that our participants have felt empowered in recovery by discussing ED-related stories with the chatbot, which served as a personal yet social avenue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span>However, we also identified harmful chatbot responses, especially concerning individuals with ED, that went unnoticed partly due to participants' unquestioning trust in the chatbot's reliability.Based on these findings, we provide design implications for safe and effective LLM-based interventions in ED management.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11656v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11656v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs Can Simulate Standardized Patients via Agent Coevolution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Training medical personnel using standardized patients (SPs) remains a complex challenge, requiring extensive domain expertise and role-specific practice.<span class='px-1 mx-1 bg-yellow-200'>Most research on Large Language Model (LLM)-based simulated patients focuses on improving data retrieval accuracy or adjusting prompts through human feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span>However, this focus has overlooked the critical need for patient agents to learn a standardized presentation pattern that transforms data into human-like patient responses through unsupervised simulations.To address this gap, we propose EvoPatient, a novel simulated patient framework in which a patient agent and doctor agents simulate the diagnostic process through multi-turn dialogues, simultaneously gathering experience to improve the quality of both questions and answers, ultimately enabling human doctor training.Extensive experiments on various cases demonstrate that, by providing only overall SP requirements, our framework improves over existing reasoning methods by more than 10% in requirement alignment and better human preference, while achieving an optimal balance of resource consumption after evolving over 200 cases for 10 hours, with excellent generalizability.The code will be available at https://github.com/ZJUMAI/EvoPatient.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11716v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11716v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Instruction-Tuned Large Language Models to Identify Indicators of Vulnerability in Police Incident Narratives
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Objectives: Compare qualitative coding of instruction tuned large language models (IT-LLMs) against human coders in classifying the presence or absence of vulnerability in routinely collected unstructured text that describes police-public interactions.Evaluate potential bias in IT-LLM codings.<span class='px-1 mx-1 bg-yellow-200'>Methods: Analyzing publicly available text narratives of police-public interactions recorded by Boston Police Department, we provide humans and IT-LLMs with qualitative labelling codebooks and compare labels generated by both, seeking to identify situations associated with (i) mental ill health; (ii) substance misuse; (iii) alcohol dependence; and (iv) homelessness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>We explore multiple prompting strategies and model sizes, and the variability of labels generated by repeated prompts.Additionally, to explore model bias, we utilize counterfactual methods to assess the impact of two protected characteristics - race and gender - on IT-LLM classification.Results:Results demonstrate that IT-LLMs can effectively support human qualitative coding of police incident narratives.While there is some disagreement between LLM and human generated labels, IT-LLMs are highly effective at screening narratives where no vulnerabilities are present, potentially vastly reducing the requirement for human coding.<span class='px-1 mx-1 bg-yellow-200'>Counterfactual analyses demonstrate that manipulations to both gender and race of individuals described in narratives have very limited effects on IT-LLM classifications beyond those expected by chance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span>Conclusions: IT-LLMs offer effective means to augment human qualitative coding in a way that requires much lower levels of resource to analyze large unstructured datasets.Moreover, they encourage specificity in qualitative coding, promote transparency, and provide the opportunity for more standardized, replicable approaches to analyzing large free-text police data sources.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11878v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11878v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula.An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support.<span class='px-1 mx-1 bg-yellow-200'>Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning.We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM).Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM.This LLM generated message recommendations for caregivers supporting their child's math practice via chat.Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations.These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation.We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11995v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11995v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Open Source Advantage in Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning.Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today.However, they face criticism for their "black box" nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development.By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency.<span class='px-1 mx-1 bg-yellow-200'>These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017).Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains.Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources.To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI.Ethical considerations further highlight this divide.Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases.Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12004v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12004v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>WARNING:<span class='px-1 mx-1 bg-yellow-200'>This paper contains content that maybe upsetting or offensive to some readers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>Dog whistles are coded expressions with dual meanings: one intended for the general public (outgroup) and another that conveys a specific message to an intended audience (ingroup).Often, these expressions are used to convey controversial political opinions while maintaining plausible deniability and slip by content moderation filters.Identification of dog whistles relies on curated lexicons, which have trouble keeping up to date.We introduce \textbf{FETCH!}, a task for finding novel dog whistles in massive social media corpora.<span class='px-1 mx-1 bg-yellow-200'>We find that state-of-the-art systems fail to achieve meaningful results across three distinct social media case studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>We present \textbf{EarShot}, a novel system that combines the strengths of vector databases and Large Language Models (LLMs) to efficiently and effectively identify new dog whistles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12072v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12072v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary Expansion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper addresses the critical need for democratizing large language models (LLM) in the Arab world, a region that has seen slower progress in developing models comparable to state-of-the-art offerings like GPT-4 or ChatGPT 3.5, due to a predominant focus on mainstream languages (e.g., English and Chinese).One practical objective for an Arabic LLM is to utilize an Arabic-specific vocabulary for the tokenizer that could speed up decoding.However, using a different vocabulary often leads to a degradation of learned knowledge since many words are initially out-of-vocabulary (OOV) when training starts.Inspired by the vocabulary learning during Second Language (Arabic) Acquisition for humans, the released AraLLaMA employs progressive vocabulary expansion, which is implemented by a modified BPE algorithm that progressively extends the Arabic subwords in its dynamic vocabulary during training, thereby balancing the OOV ratio at every stage.<span class='px-1 mx-1 bg-yellow-200'>The ablation study demonstrated the effectiveness of Progressive Vocabulary Expansion. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>Moreover, AraLLaMA achieves decent performance comparable to the best Arabic LLMs across a variety of Arabic benchmarks.Models, training data, benchmarks, and codes will be all open-sourced.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12310v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12310v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The deployment of large language models (LLMs) in diverse applications requires a thorough understanding of their decision-making strategies and behavioral patterns.<span class='px-1 mx-1 bg-yellow-200'>As a supplement to a recent study on the behavioral Turing test, this paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>By benchmarking these AI chatbots, we aim to uncover and document both common and distinct behavioral patterns across a range of scenarios.The findings provide valuable insights into the strategic preferences of each LLM, highlighting potential implications for their deployment in critical decision-making roles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12362v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12362v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bridging the Gap: Enhancing LLM Performance for Low-Resource African Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown remarkable performance across various tasks, yet significant disparities remain for non-English languages, and especially native African languages.This paper addresses these disparities by creating approximately 1 million human-translated words of new benchmark data in 8 low-resource African languages, covering a population of over 160 million speakers of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho), Shona, Sesotho (Southern Sotho), Setswana, and Tsonga.Our benchmarks are translations of Winogrande and three sections of MMLU: college medicine, clinical knowledge, and virology.Using the translated benchmarks, we report previously unknown performance gaps between state-of-the-art (SOTA) LLMs in English and African languages.Finally, using results from over 400 fine-tuned models, we explore several methods to reduce the LLM performance gap, including high-quality dataset fine-tuning (using an LLM-as-an-Annotator), cross-lingual transfer, and cultural appropriateness adjustments.<span class='px-1 mx-1 bg-yellow-200'>Key findings include average mono-lingual improvements of 5.6% with fine-tuning (with 5.4% average mono-lingual improvements when using high-quality data over low-quality data), 2.9% average gains from cross-lingual transfer, and a 3.0% out-of-the-box performance boost on culturally appropriate questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>The publicly available benchmarks, translations, and code from this study support further research and development aimed at creating more inclusive and effective language technologies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12417v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12417v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs in Education Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Knowledge Boundary of Large Language Models: A Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although large language models (LLMs) store vast amount of knowledge in their parameters, they still have limitations in the memorization and utilization of certain knowledge, leading to undesired behaviors such as generating untruthful and inaccurate responses.<span class='px-1 mx-1 bg-yellow-200'>This highlights the critical need to understand the knowledge boundary of LLMs, a concept that remains inadequately defined in existing research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>In this survey, we propose a comprehensive definition of the LLM knowledge boundary and introduce a formalized taxonomy categorizing knowledge into four distinct types.Using this foundation, we systematically review the field through three key lenses: the motivation for studying LLM knowledge boundaries, methods for identifying these boundaries, and strategies for mitigating the challenges they present.Finally, we discuss open challenges and potential research directions in this area.We aim for this survey to offer the community a comprehensive overview, facilitate access to key issues, and inspire further advancements in LLM knowledge research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the huge number of diseases.The complexity of symptoms and the shortage of specialized doctors with relevant experience make diagnosing and treating rare diseases more challenging than common diseases.Recently, agents powered by large language models (LLMs) have demonstrated notable improvements across various domains.<span class='px-1 mx-1 bg-yellow-200'>In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical exams. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.522</span></span>However, current agent frameworks lack adaptation for real-world clinical scenarios, especially those involving the intricate demands of rare diseases.To address these challenges, we present RareAgents, the first multi-disciplinary team of LLM-based agents tailored to the complex clinical context of rare diseases.RareAgents integrates advanced planning capabilities, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model.Experimental results show that RareAgents surpasses state-of-the-art domain-specific models, GPT-4o, and existing agent frameworks in both differential diagnosis and medication recommendation for rare diseases.Furthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from MIMIC-IV, to support further advancements in this field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12475v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12475v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC).<span class='px-1 mx-1 bg-yellow-200'>Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.583</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.592</span></span>Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums.<span class='px-1 mx-1 bg-yellow-200'>Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12541v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12541v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Don't Yell at Your Robot: Physical Correction as the Collaborative Interface for Language Model Powered Robots
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present a novel approach for enhancing human-robot collaboration using physical interactions for real-time error correction of large language model (LLM) powered robots.<span class='px-1 mx-1 bg-yellow-200'>Unlike other methods that rely on verbal or text commands, the robot leverages an LLM to proactively executes 6 DoF linear Dynamical System (DS) commands using a description of the scene in natural language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span>During motion, a human can provide physical corrections, used to re-estimate the desired intention, also parameterized by linear DS.This corrected DS can be converted to natural language and used as part of the prompt to improve future LLM interactions.<span class='px-1 mx-1 bg-yellow-200'>We provide proof-of-concept result in a hybrid real+sim experiment, showcasing physical interaction as a new possibility for LLM powered human-robot interface. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12602v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12602v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program Fine-tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Program-of-Thought (PoT), which aims to use programming language instead of natural language as an intermediate step in reasoning, is an important way for LLMs to solve mathematical problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>Since different programming languages excel in different areas, it is natural to use the most suitable language for solving specific problems.However, current PoT research only focuses on single language PoT, ignoring the differences between different programming languages.Therefore, this paper proposes an multilingual program reasoning method, MultiLingPoT.This method allows the model to answer questions using multiple programming languages by fine-tuning on multilingual data.Additionally, prior and posterior hybrid methods are used to help the model select the most suitable language for each problem.Our experimental results show that the training of MultiLingPoT improves each program's mathematical reasoning by about 2.5\%.Moreover, with proper mixing, the performance of MultiLingPoT can be further improved, achieving a 6\% increase compared to the single-language PoT with the data augmentation.Resources of this paper can be found at https://github.com/Nianqi-Li/MultiLingPoT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12609v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12609v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs.However, external knowledge is often imperfect.In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses.<span class='px-1 mx-1 bg-yellow-200'>This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces.Accordingly, we propose an automated CoE discrimination approach and explore LLMs' preferences from their effectiveness, faithfulness and robustness, as well as CoE's usability in a naive Retrieval-Augmented Generation (RAG) case.<span class='px-1 mx-1 bg-yellow-200'>The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.527</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12632v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12632v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using LLM-Generated Draft Replies to Support Human Experts in Responding to Stakeholder Inquiries in Maritime Industry: A Real-World Case Study of Industrial AI
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The maritime industry requires effective communication among diverse stakeholders to address complex, safety-critical challenges.Industrial AI, including Large Language Models (LLMs), has the potential to augment human experts' workflows in this specialized domain.<span class='px-1 mx-1 bg-yellow-200'>Our case study investigated the utility of LLMs in drafting replies to stakeholder inquiries and supporting case handlers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.537</span></span>We conducted a preliminary study (observations and interviews), a survey, and a text similarity analysis (LLM-as-a-judge and Semantic Embedding Similarity).We discover that while LLM drafts can streamline workflows, they often require significant modifications to meet the specific demands of maritime communications.Though LLMs are not yet mature enough for safety-critical applications without human oversight, they can serve as valuable augmentative tools.Final decision-making thus must remain with human experts.However, by leveraging the strengths of both humans and LLMs, fostering human-AI collaboration, industries can increase efficiency while maintaining high standards of quality and precision tailored to each case.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12732v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12732v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey of Calibration Process for Black-Box LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge.While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters.Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints.<span class='px-1 mx-1 bg-yellow-200'>Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span>To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs.We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration.Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps.Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment.This is our GitHub link: https://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12767v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12767v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Question: How do Large Language Models perform on the Question Answering tasks? Answer:
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques.A common NLP-task is question-answering (QA).<span class='px-1 mx-1 bg-yellow-200'>In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.541</span></span>Since the dataset contains unanswerable questions, previous work used a double inference method.We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources.Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.   Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12893v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12893v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Emergence of Strategic Reasoning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As Large Language Models (LLMs) are increasingly used for a variety of complex and critical tasks, it is vital to assess their logical capabilities in strategic environments.This paper examines their ability in strategic reasoning -- the process of choosing an optimal course of action by predicting and adapting to other agents' behavior.Using six LLMs, we analyze responses from play in classical games from behavioral economics (p-Beauty Contest, 11-20 Money Request Game, and Guessing Game) and evaluate their performance through hierarchical models of reasoning (level-$k$ theory and cognitive hierarchy theory).Our findings reveal that while LLMs show understanding of the games, the majority struggle with higher-order strategic reasoning.<span class='px-1 mx-1 bg-yellow-200'>Although most LLMs did demonstrate learning ability with games involving repeated interactions, they still consistently fall short of the reasoning levels demonstrated by typical behavior from human subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.592</span></span>The exception to these overall findings is with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning tasks -- which consistently outperforms other LLMs and human subjects.These findings highlight the challenges and pathways in advancing LLMs toward robust strategic reasoning from the perspective of behavioral economics.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13013v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13013v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Codenames as a Benchmark for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs).Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities.Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches.<span class='px-1 mx-1 bg-yellow-200'>LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.59</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles.We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11373v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11373v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) excel at answering questions but remain passive learners--absorbing static data without the ability to question and refine knowledge.<span class='px-1 mx-1 bg-yellow-200'>This paper explores how LLMs can transition to interactive, question-driven learning through student-teacher dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.872</span></span><span class='px-1 mx-1 bg-yellow-200'>We introduce INTERACT (INTEReractive Learning for Adaptive Concept Transfer), a framework in which a "student" LLM engages a "teacher" LLM through iterative inquiries to acquire knowledge across 1,347 contexts, including song lyrics, news articles, movie plots, academic papers, and images. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments show that across a wide range of scenarios and LLM architectures, interactive learning consistently enhances performance, achieving up to a 25% improvement, with 'cold-start' student models matching static learning baselines in as few as five dialogue turns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span><span class='px-1 mx-1 bg-yellow-200'>Interactive setups can also mitigate the disadvantages of weaker teachers, showcasing the robustness of question-driven learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11388v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11388v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output.Such an ability is well-suited for and often optimized by preference learning.However, existing methods often directly sample multiple independent responses from the model when creating preference pairs.<span class='px-1 mx-1 bg-yellow-200'>Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span>In light of this, we introduce SPaR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions.By playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations.Our experiments show that a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities.Furthermore, SPaR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B.We also identify how inference scaling in tree search would impact model performance.Our code and data are publicly available at https://github.com/thu-coai/SPaR.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11605v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11605v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Model (LLM)-based agents have shown promise in procedural tasks, but the potential of multimodal instructions augmented by texts and videos to assist users remains under-explored.To address this gap, we propose the Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel LLM-empowered Multimodal Procedural Planning (MPP) framework.It generates cohesive text and video procedural plans given a specified high-level objective.The main challenges are achieving textual and visual informativeness, temporal coherence, and accuracy in procedural plans.VG-TVP leverages the zero-shot reasoning capability of LLMs, the video-to-text generation ability of the video captioning models, and the text-to-video generation ability of diffusion models.VG-TVP improves the interaction between modalities by proposing a novel Fusion of Captioning (FoC) method and using Text-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B).<span class='px-1 mx-1 bg-yellow-200'>They allow LLMs to guide the generation of visually-grounded text plans and textual-grounded video plans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>To address the scarcity of datasets suitable for MPP, we have curated a new dataset called Daily-Life Task Procedural Plans (Daily-PP).We conduct comprehensive experiments and benchmarks to evaluate human preferences (regarding textual and visual informativeness, temporal coherence, and plan accuracy).Our VG-TVP method outperforms unimodal baselines on the Daily-PP dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11621v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11621v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Private Yet Social: How LLM Chatbots Support and Challenge Eating Disorder Recovery
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Eating disorders (ED) are complex mental health conditions that require long-term management and support.Recent advancements in large language model (LLM)-based chatbots offer the potential to assist individuals in receiving immediate support.Yet, concerns remain about their reliability and safety in sensitive contexts such as ED.<span class='px-1 mx-1 bg-yellow-200'>We explore the opportunities and potential harms of using LLM-based chatbots for ED recovery. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span><span class='px-1 mx-1 bg-yellow-200'>We observe the interactions between 26 participants with ED and an LLM-based chatbot, WellnessBot, designed to support ED recovery, over 10 days. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>We discovered that our participants have felt empowered in recovery by discussing ED-related stories with the chatbot, which served as a personal yet social avenue.However, we also identified harmful chatbot responses, especially concerning individuals with ED, that went unnoticed partly due to participants' unquestioning trust in the chatbot's reliability.<span class='px-1 mx-1 bg-yellow-200'>Based on these findings, we provide design implications for safe and effective LLM-based interventions in ED management. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11656v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11656v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Context.The security of critical infrastructure has been a fundamental concern since the advent of computers, and this concern has only intensified in today's cyber warfare landscape.Protecting mission-critical systems (MCSs), including essential assets like healthcare, telecommunications, and military coordination, is vital for national security.These systems require prompt and comprehensive governance to ensure their resilience, yet recent events have shown that meeting these demands is increasingly challenging.Aim.Building on prior research that demonstrated the potential of GAI, particularly Large Language Models (LLMs), in improving risk analysis tasks, we aim to explore practitioners' perspectives, specifically developers and security personnel, on using generative AI (GAI) in the governance of IT MCSs seeking to provide insights and recommendations for various stakeholders, including researchers, practitioners, and policymakers.Method.We designed a survey to collect practical experiences, concerns, and expectations of practitioners who develop and implement security solutions in the context of MCSs.Analyzing this data will help identify key trends, challenges, and opportunities for introducing GAIs in this niche domain.Conclusions and Future Works.<span class='px-1 mx-1 bg-yellow-200'>Our findings highlight that the safe use of LLMs in MCS governance requires interdisciplinary collaboration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span>Researchers should focus on designing regulation-oriented models and focus on accountability; practitioners emphasize data protection and transparency, while policymakers must establish a unified AI framework with global benchmarks to ensure ethical and secure LLMs-based MCS governance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11698v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11698v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CoinMath: Harnessing the Power of Coding Instruction for Math LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have shown strong performance in solving mathematical problems, with code-based solutions proving particularly effective.<span class='px-1 mx-1 bg-yellow-200'>However, the best practice to leverage coding instruction data to enhance mathematical reasoning remains underexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span><span class='px-1 mx-1 bg-yellow-200'>This study investigates three key questions: (1) How do different coding styles of mathematical code-based rationales impact LLMs' learning performance? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.582</span></span>(2) Can general-domain coding instructions improve performance?<span class='px-1 mx-1 bg-yellow-200'>(3) How does integrating textual rationales with code-based ones during training enhance mathematical reasoning abilities? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.571</span></span>Our findings reveal that code-based rationales with concise comments, descriptive naming, and hardcoded solutions are beneficial, while improvements from general-domain coding instructions and textual rationales are relatively minor.Based on these insights, we propose CoinMath, a learning strategy designed to enhance mathematical reasoning by diversifying the coding styles of code-based rationales.CoinMath generates a variety of code-based rationales incorporating concise comments, descriptive naming conventions, and hardcoded solutions.Experimental results demonstrate that CoinMath significantly outperforms its baseline model, MAmmoTH, one of the SOTA math LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11699v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11699v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Personalized LLM for Generating Customized Responses to the Same Query from Different Users
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Existing work on large language model (LLM) personalization assigned different responding roles to LLM, but overlooked the diversity of questioners.In this work, we propose a new form of questioner-aware LLM personalization, generating different responses even for the same query from different questioners.We design a dual-tower model architecture with a cross-questioner general encoder and a questioner-specific encoder.We further apply contrastive learning with multi-view augmentation, pulling close the dialogue representations of the same questioner, while pulling apart those of different questioners.<span class='px-1 mx-1 bg-yellow-200'>To mitigate the impact of question diversity on questioner-contrastive learning, we cluster the dialogues based on question similarity and restrict the scope of contrastive learning within each cluster. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.559</span></span>We also build a multi-questioner dataset from English and Chinese scripts and WeChat records, called MQDialog, containing 173 questioners and 12 responders.Extensive evaluation with different metrics shows a significant improvement in the quality of personalized response generation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11736v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11736v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                QUENCH: Measuring the gap between Indic and Non-Indic Contextual General Reasoning in LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rise of large language models (LLMs) has created a need for advanced benchmarking systems beyond traditional setups.To this end, we introduce QUENCH, a novel text-based English Quizzing Benchmark manually curated and transcribed from YouTube quiz videos.QUENCH possesses masked entities and rationales for the LLMs to predict via generation.<span class='px-1 mx-1 bg-yellow-200'>At the intersection of geographical context and common sense reasoning, QUENCH helps assess world knowledge and deduction capabilities of LLMs via a zero-shot, open-domain quizzing setup. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span>We perform an extensive evaluation on 7 LLMs and 4 metrics, investigating the influence of model size, prompting style, geographical context, and gold-labeled rationale generation.The benchmarking concludes with an error analysis to which the LLMs are prone.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11763v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11763v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper we look at the ability of recent large language models (LLMs) at solving mathematical problems in combinatorics.<span class='px-1 mx-1 bg-yellow-200'>We compare models LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and undergraduates with prior experience in mathematical olympiads. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>To facilitate these comparisons we introduce the Combi-Puzzles dataset, which contains 125 problem variants based on 25 combinatorial reasoning problems.Each problem is presented in one of five distinct forms, created by systematically manipulating the problem statements through adversarial additions, numeric parameter changes, and linguistic obfuscation.Our variations preserve the mathematical core and are designed to measure the generalisability of LLM problem-solving abilities, while also increasing confidence that problems are submitted to LLMs in forms that have not been seen as training instances.We found that a model based on GPT-4 outperformed all other models in producing correct responses, and performed significantly better in the mathematical variation of the problems than humans.We also found that modifications to problem statements significantly impact the LLM's performance, while human performance remains unaffected.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11908v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11908v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.532</span></span>As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant.This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs).<span class='px-1 mx-1 bg-yellow-200'>We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.59</span></span>We categorize the field into three dimensions: benchmarks, methodologies, and challenges.<span class='px-1 mx-1 bg-yellow-200'>In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.577</span></span>Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities.This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11936v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11936v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with a GAN-Inspired Approach to Synthetic Dataset Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Consider the problem: ``If one man and one woman can produce one child in one year, how many children will be produced by one woman and three men in 0.5 years?"Current large language models (LLMs) such as GPT-4o, GPT-o1-preview, and Gemini Flash frequently answer "0.5," which does not make sense.While these models sometimes acknowledge the unrealistic nature of the question, in many cases (8 out of 10 trials), they provide the nonsensical answer of "0.5 child."Additionally, temporal variation has been observed: if an LLM answers correctly once (by recognizing the faulty nature of the question), subsequent responses are more likely to also reflect this understanding.However, this is inconsistent.   These types of questions have motivated us to develop a dataset of science questions, SciFaultyQA, where the questions themselves are intentionally faulty.<span class='px-1 mx-1 bg-yellow-200'>We observed that LLMs often proceed to answer these flawed questions without recognizing their inherent issues, producing results that are logically or scientifically invalid. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.529</span></span>By analyzing such patterns, we developed a novel method for generating synthetic datasets to evaluate and benchmark the performance of various LLMs in identifying these flawed questions.We have also developed novel approaches to reduce the errors.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11988v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11988v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.513</span></span><span class='px-1 mx-1 bg-yellow-200'>Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span><span class='px-1 mx-1 bg-yellow-200'>An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span><span class='px-1 mx-1 bg-yellow-200'>Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.53</span></span><span class='px-1 mx-1 bg-yellow-200'>Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM).<span class='px-1 mx-1 bg-yellow-200'>Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span><span class='px-1 mx-1 bg-yellow-200'>This LLM generated message recommendations for caregivers supporting their child's math practice via chat. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span><span class='px-1 mx-1 bg-yellow-200'>Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span><span class='px-1 mx-1 bg-yellow-200'>These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.531</span></span><span class='px-1 mx-1 bg-yellow-200'>We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.72</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11995v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11995v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite their remarkable success, large language models (LLMs) have shown limited ability on applied tasks such as vulnerability detection.<span class='px-1 mx-1 bg-yellow-200'>We investigate various prompting strategies for vulnerability detection and, as part of this exploration, propose a prompting strategy that integrates natural language descriptions of vulnerabilities with a contrastive chain-of-thought reasoning approach, augmented using contrastive samples from a synthetic dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span><span class='px-1 mx-1 bg-yellow-200'>Our study highlights the potential of LLMs to detect vulnerabilities by integrating natural language descriptions, contrastive reasoning, and synthetic examples into a comprehensive prompting framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.54</span></span>Our results show that this approach can enhance LLM understanding of vulnerabilities.On a high-quality vulnerability detection dataset such as SVEN, our prompting strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%, 11%, and 14%, respectively.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12039v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12039v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Visual Instruction Tuning with 500x Fewer Parameters through Modality Linear Representation-Steering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multimodal Large Language Models (MLLMs) have significantly advanced visual tasks by integrating visual representations into large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>The textual modality, inherited from LLMs, equips MLLMs with abilities like instruction following and in-context learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span>In contrast, the visual modality enhances performance in downstream tasks by leveraging rich semantic content, spatial information, and grounding capabilities.These intrinsic modalities work synergistically across various visual tasks.Our research initially reveals a persistent imbalance between these modalities, with text often dominating output generation during visual instruction tuning.This imbalance occurs when using both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods.We then found that re-balancing these modalities can significantly reduce the number of trainable parameters required, inspiring a direction for further optimizing visual instruction tuning.We introduce Modality Linear Representation-Steering (MoReS) to achieve the goal.MoReS effectively re-balances the intrinsic modalities throughout the model, where the key idea is to steer visual representations through linear transformations in the visual subspace across each model layer.To validate our solution, we composed LLaVA Steering, a suite of models integrated with the proposed MoReS method.Evaluation results show that the composed LLaVA Steering models require, on average, 500 times fewer trainable parameters than LoRA needs while still achieving comparable performance across three visual benchmarks and eight visual question-answering tasks.Last, we present the LLaVA Steering Factory, an in-house developed platform that enables researchers to quickly customize various MLLMs with component-based architecture for seamlessly integrating state-of-the-art models, and evaluate their intrinsic modality imbalance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12359v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12359v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The deployment of large language models (LLMs) in diverse applications requires a thorough understanding of their decision-making strategies and behavioral patterns.<span class='px-1 mx-1 bg-yellow-200'>As a supplement to a recent study on the behavioral Turing test, this paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span>By benchmarking these AI chatbots, we aim to uncover and document both common and distinct behavioral patterns across a range of scenarios.The findings provide valuable insights into the strategic preferences of each LLM, highlighting potential implications for their deployment in critical decision-making roles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12362v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12362v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs as Recommender Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph Reasoning for Cold-start Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Knowledge Graphs (KGs) represent relationships between entities in a graph structure and have been widely studied as promising tools for realizing recommendations that consider the accurate content information of items. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span><span class='px-1 mx-1 bg-yellow-200'>However, traditional KG-based recommendation methods face fundamental challenges: insufficient consideration of temporal information and poor performance in cold-start scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.801</span></span><span class='px-1 mx-1 bg-yellow-200'>On the other hand, Large Language Models (LLMs) can be considered databases with a wealth of knowledge learned from the web data, and they have recently gained attention due to their potential application as recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span><span class='px-1 mx-1 bg-yellow-200'>Although approaches that treat LLMs as recommendation systems can leverage LLMs' high recommendation literacy, their input token limitations make it impractical to consider the entire recommendation domain dataset and result in scalability issues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR).Our main idea is to treat LLMs as reasoners that output intuitive exploration strategies for KGs.<span class='px-1 mx-1 bg-yellow-200'>To integrate the knowledge of LLMs and KGs, we trained a recommendation agent through reinforcement learning using a reward function that integrates different recommendation strategies, including LLM's intuition and KG embeddings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span><span class='px-1 mx-1 bg-yellow-200'>By incorporating temporal awareness through prompt engineering and generating textual representations of user preferences from limited interactions, LIKR can improve recommendation performance in cold-start scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.758</span></span>Furthermore, LIKR can avoid scalability issues by using KGs to represent recommendation domain datasets and limiting the LLM's output to KG exploration strategies.<span class='px-1 mx-1 bg-yellow-200'>Experiments on real-world datasets demonstrate that our model outperforms state-of-the-art recommendation methods in cold-start sequential recommendation scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12464v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12464v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey on Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Different from most conventional recommendation problems, sequential recommendation focuses on learning users' preferences by exploiting the internal order and dependency among the interacted items, which has received significant attention from both researchers and practitioners. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.821</span></span>In recent years, we have witnessed great progress and achievements in this field, necessitating a new survey.<span class='px-1 mx-1 bg-yellow-200'>In this survey, we study the SR problem from a new perspective (i.e., the construction of an item's properties), and summarize the most recent techniques used in sequential recommendation such as pure ID-based SR, SR with side information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR and data-augmented SR. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, we introduce some frontier research topics in sequential recommendation, e.g., open-domain SR, data-centric SR, could-edge collaborative SR, continuous SR, SR for good, and explainable SR. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span>We believe that our survey could be served as a valuable roadmap for readers in this field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12770v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12770v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based MOE Architecture
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the increasing availability of multimodal data, many fields urgently require advanced architectures capable of effectively integrating these diverse data sources to address specific problems.<span class='px-1 mx-1 bg-yellow-200'>This study proposes a hybrid recommendation model that combines the Mixture of Experts (MOE) framework with large language models to enhance the performance of recommendation systems in the healthcare domain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>We built a small dataset for recommending healthy food based on patient descriptions and evaluated the model's performance on several key metrics, including Precision, Recall, NDCG, and MAP@5.<span class='px-1 mx-1 bg-yellow-200'>The experimental results show that the hybrid model outperforms the baseline models, which use MOE or large language models individually, in terms of both accuracy and personalized recommendation effectiveness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span><span class='px-1 mx-1 bg-yellow-200'>The paper finds image data provided relatively limited improvement in the performance of the personalized recommendation system, particularly in addressing the cold start problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span>Then, the issue of reclassification of images also affected the recommendation results, especially when dealing with low-quality images or changes in the appearance of items, leading to suboptimal performance.<span class='px-1 mx-1 bg-yellow-200'>The findings provide valuable insights into the development of powerful, scalable, and high-performance recommendation systems, advancing the application of personalized recommendation technologies in real-world domains such as healthcare. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11557v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11557v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics.Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula.An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support.Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them.Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning.<span class='px-1 mx-1 bg-yellow-200'>We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span>Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM.This LLM generated message recommendations for caregivers supporting their child's math practice via chat.Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations.These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation.We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11995v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11995v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RecSys Arena: Pair-wise Recommender System Evaluation with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Evaluating the quality of recommender systems is critical for algorithm design and optimization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span>Most evaluation methods are computed based on offline metrics for quick algorithm evolution, since online experiments are usually risky and time-consuming.However, offline evaluation usually cannot fully reflect users' preference for the outcome of different recommendation algorithms, and the results may not be consistent with online A/B test.Moreover, many offline metrics such as AUC do not offer sufficient information for comparing the subtle differences between two competitive recommender systems in different aspects, which may lead to substantial performance differences in long-term online serving.<span class='px-1 mx-1 bg-yellow-200'>Fortunately, due to the strong commonsense knowledge and role-play capability of large language models (LLMs), it is possible to obtain simulated user feedback on offline recommendation results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span><span class='px-1 mx-1 bg-yellow-200'>Motivated by the idea of LLM Chatbot Arena, in this paper we present the idea of RecSys Arena, where the recommendation results given by two different recommender systems in each session are evaluated by an LLM judger to obtain fine-grained evaluation feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>More specifically, for each sample we use LLM to generate a user profile description based on user behavior history or off-the-shelf profile features, which is used to guide LLM to play the role of this user and evaluate the relative preference for two recommendation results generated by different models.<span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments on two recommendation datasets in different scenarios, we demonstrate that many different LLMs not only provide general evaluation results that are highly consistent with canonical offline metrics, but also provide rich insight in many subjective aspects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.807</span></span>Moreover, it can better distinguish different algorithms with comparable performance in terms of AUC and nDCG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11068v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11068v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MOPI-HFRS: A Multi-objective Personalized Health-aware Food Recommendation System with LLM-enhanced Interpretation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The prevalence of unhealthy eating habits has become an increasingly concerning issue in the United States.However, major food recommendation platforms (e.g., Yelp) continue to prioritize users' dietary preferences over the healthiness of their choices.<span class='px-1 mx-1 bg-yellow-200'>Although efforts have been made to develop health-aware food recommendation systems, the personalization of such systems based on users' specific health conditions remains under-explored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>In addition, few research focus on the interpretability of these systems, which hinders users from assessing the reliability of recommendations and impedes the practical deployment of these systems.<span class='px-1 mx-1 bg-yellow-200'>In response to this gap, we first establish two large-scale personalized health-aware food recommendation benchmarks at the first attempt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span><span class='px-1 mx-1 bg-yellow-200'>We then develop a novel framework, Multi-Objective Personalized Interpretable Health-aware Food Recommendation System (MOPI-HFRS), which provides food recommendations by jointly optimizing the three objectives: user preference, personalized healthiness and nutritional diversity, along with an large language model (LLM)-enhanced reasoning module to promote healthy dietary knowledge through the interpretation of recommended results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>Specifically, this holistic graph learning framework first utilizes two structure learning and a structure pooling modules to leverage both descriptive features and health data.Then it employs Pareto optimization to achieve designed multi-facet objectives.Finally, to further promote the healthy dietary knowledge and awareness, we exploit an LLM by utilizing knowledge-infusion, prompting the LLMs with knowledge obtained from the recommendation model for interpretation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08847v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08847v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SPRec: Leveraging Self-Play to Debias Preference Alignment for Large Language Model-based Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have attracted significant attention in recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span><span class='px-1 mx-1 bg-yellow-200'>Current LLM-based recommender systems primarily rely on supervised fine-tuning (SFT) to train the model for recommendation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>However, relying solely on positive samples limits the model's ability to align with user satisfaction and expectations.<span class='px-1 mx-1 bg-yellow-200'>To address this, researchers have introduced Direct Preference Optimization (DPO), which explicitly aligns recommendations with user preferences using offline preference ranking data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span>Despite its advantages, our theoretical analysis reveals that DPO inherently biases the model towards a few items, exacerbating the filter bubble issue and ultimately degrading user experience.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose SPRec, a novel self-play recommendation framework designed to mitigate over-recommendation and improve fairness without requiring additional data or manual intervention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>In each self-play iteration, the model undergoes an SFT step followed by a DPO step, treating offline interaction data as positive samples and the predicted outputs from the previous iteration as negative samples.This effectively re-weights the DPO loss function using the model's logits, adaptively suppressing biased items.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on multiple real-world datasets demonstrate SPRec's effectiveness in enhancing recommendation accuracy and addressing fairness concerns. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09243v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09243v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AltFS: Agency-light Feature Selection with Large Language Models in Deep Recommender Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Feature selection is crucial in recommender systems for improving model efficiency and predictive performance.Traditional methods rely on agency models, such as decision trees or neural networks, to estimate feature importance.However, this approach is inherently limited, as the agency models may fail to learn effectively in all scenarios due to suboptimal training conditions (e.g., feature collinearity, high-dimensional sparsity, and data insufficiency).In this paper, we propose AltFS, an Agency-light Feature Selection method for deep recommender systems.AltFS integrates semantic reasoning from Large Language Models (LLMs) with task-specific learning from agency models.Initially, LLMs will generate a semantic ranking of feature importance, which is then refined by an agency model, combining world knowledge with task-specific insights.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments on three public datasets from real-world recommender platforms demonstrate the effectiveness of AltFS. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>Our code is publicly available for reproducibility.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08516v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08516v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Preference Discerning with LLM-Enhanced Generative Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Sequential recommendation systems aim to provide personalized recommendations for users based on their interaction history. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span>To achieve this, they often incorporate auxiliary information, such as textual descriptions of items and auxiliary tasks, like predicting user preferences and intent.Despite numerous efforts to enhance these models, they still suffer from limited personalization.To address this issue, we propose a new paradigm, which we term preference discerning.<span class='px-1 mx-1 bg-yellow-200'>In preference dscerning, we explicitly condition a generative sequential recommendation system on user preferences within its context. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>To this end, we generate user preferences using Large Language Models (LLMs) based on user reviews and item-specific data.<span class='px-1 mx-1 bg-yellow-200'>To evaluate preference discerning capabilities of sequential recommendation systems, we introduce a novel benchmark that provides a holistic evaluation across various scenarios, including preference steering and sentiment following. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span>We assess current state-of-the-art methods using our benchmark and show that they struggle to accurately discern user preferences.Therefore, we propose a new method named Mender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce $\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods and achieves state-of-the-art performance on our benchmark.<span class='px-1 mx-1 bg-yellow-200'>Our results show that Mender can be effectively guided by human preferences even though they have not been observed during training, paving the way toward more personalized sequential recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span>We will open-source the code and benchmarks upon publication.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08604v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08604v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                IntellectSeeker: A Personalized Literature Management System with the Probabilistic Model and Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Faced with the burgeoning volume of academic literature, researchers often need help with uncertain article quality and mismatches in term searches using traditional academic engines.We introduce IntellectSeeker, an innovative and personalized intelligent academic literature management platform to address these challenges.This platform integrates a Large Language Model (LLM)--based semantic enhancement bot with a sophisticated probability model to personalize and streamline literature searches.We adopted the GPT-3.5-turbo model to transform everyday language into professional academic terms across various scenarios using multiple rounds of few-shot learning.This adaptation mainly benefits academic newcomers, effectively bridging the gap between general inquiries and academic terminology.The probabilistic model intelligently filters academic articles to align closely with the specific interests of users, which are derived from explicit needs and behavioral patterns.Moreover, IntellectSeeker incorporates an advanced recommendation system and text compression tools.<span class='px-1 mx-1 bg-yellow-200'>These features enable intelligent article recommendations based on user interactions and present search results through concise one-line summaries and innovative word cloud visualizations, significantly enhancing research efficiency and user experience. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>IntellectSeeker offers academic researchers a highly customizable literature management solution with exceptional search precision and matching capabilities.The code can be found here: https://github.com/LuckyBian/ISY5001</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07213v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07213v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PRECISE: Pre-training Sequential Recommenders with Collaborative and Semantic Information
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Real-world recommendation systems commonly offer diverse content scenarios for users to interact with. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span><span class='px-1 mx-1 bg-yellow-200'>Considering the enormous number of users in industrial platforms, it is infeasible to utilize a single unified recommendation model to meet the requirements of all scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span><span class='px-1 mx-1 bg-yellow-200'>Usually, separate recommendation pipelines are established for each distinct scenario. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>This practice leads to challenges in comprehensively grasping users' interests.Recent research endeavors have been made to tackle this problem by pre-training models to encapsulate the overall interests of users.<span class='px-1 mx-1 bg-yellow-200'>Traditional pre-trained recommendation models mainly capture user interests by leveraging collaborative signals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span>Nevertheless, a prevalent drawback of these systems is their incapacity to handle long-tail items and cold-start scenarios.With the recent advent of large language models, there has been a significant increase in research efforts focused on exploiting LLMs to extract semantic information for users and items.<span class='px-1 mx-1 bg-yellow-200'>However, text-based recommendations highly rely on elaborate feature engineering and frequently fail to capture collaborative similarities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.764</span></span><span class='px-1 mx-1 bg-yellow-200'>To overcome these limitations, we propose a novel pre-training framework for sequential recommendation, termed PRECISE. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>This framework combines collaborative signals with semantic information.Moreover, PRECISE employs a learning framework that initially models users' comprehensive interests across all recommendation scenarios and subsequently concentrates on the specific interests of target-scene behaviors.We demonstrate that PRECISE precisely captures the entire range of user interests and effectively transfers them to the target interests.Empirical findings reveal that the PRECISE framework attains outstanding performance on both public and industrial datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.06308v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.06308v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-09</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bridging Conversational and Collaborative Signals for Conversational Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Conversational recommendation systems (CRS) leverage contextual information from conversations to generate recommendations but often struggle due to a lack of collaborative filtering (CF) signals, which capture user-item interaction patterns essential for accurate recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span>We introduce Reddit-ML32M, a dataset that links reddit conversations with interactions on MovieLens 32M, to enrich item representations by leveraging collaborative knowledge and addressing interaction sparsity in conversational datasets.<span class='px-1 mx-1 bg-yellow-200'>We propose an LLM-based framework that uses Reddit-ML32M to align LLM-generated recommendations with CF embeddings, refining rankings for better performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate our framework against three sets of baselines: CF-based recommenders using only interactions from CRS tasks, traditional CRS models, and LLM-based methods relying on conversational context without item representations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.805</span></span>Our approach achieves consistent improvements, including a 12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the best-performing baseline that relies on conversational context but lacks collaborative item representations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.06949v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.06949v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing High-order Interaction Awareness in LLM-based Recommender Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have demonstrated prominent reasoning capabilities in recommendation tasks by transforming them into text-generation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>However, existing approaches either disregard or ineffectively model the user-item high-order interactions.<span class='px-1 mx-1 bg-yellow-200'>To this end, this paper presents an enhanced LLM-based recommender (ELMRec). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span><span class='px-1 mx-1 bg-yellow-200'>We enhance whole-word embeddings to substantially enhance LLMs' interpretation of graph-constructed interactions for recommendations, without requiring graph pre-training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span><span class='px-1 mx-1 bg-yellow-200'>This finding may inspire endeavors to incorporate rich knowledge graphs into LLM-based recommenders via whole-word embedding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.803</span></span><span class='px-1 mx-1 bg-yellow-200'>We also found that LLMs often recommend items based on users' earlier interactions rather than recent ones, and present a reranking solution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.822</span></span><span class='px-1 mx-1 bg-yellow-200'>Our ELMRec outperforms state-of-the-art (SOTA) methods in both direct and sequential recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.19979v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.19979v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mitigating Propensity Bias of Large Language Models for Recommender Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid development of Large Language Models (LLMs) creates new opportunities for recommender systems, especially by exploiting the side information (e.g., descriptions and analyses of items) generated by these models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>However, aligning this side information with collaborative information from historical interactions poses significant challenges.The inherent biases within LLMs can skew recommendations, resulting in distorted and potentially unfair user experiences.On the other hand, propensity bias causes side information to be aligned in such a way that it often tends to represent all inputs in a low-dimensional subspace, leading to a phenomenon known as dimensional collapse, which severely restricts the recommender system's ability to capture user preferences and behaviours.To address these issues, we introduce a novel framework named Counterfactual LLM Recommendation (CLLMR).Specifically, we propose a spectrum-based side information encoder that implicitly embeds structural information from historical interactions into the side information representation, thereby circumventing the risk of dimension collapse.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, our CLLMR approach explores the causal relationships inherent in LLM-based recommender systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span>By leveraging counterfactual inference, we counteract the biases introduced by LLMs.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate that our CLLMR approach consistently enhances the performance of various recommender models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.842</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.20052v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.20052v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-09-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PersonalLLM: Tailoring LLMs to Individual Preferences
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As LLMs become capable of complex tasks, there is growing potential for personalized interactions tailored to the subtle and idiosyncratic preferences of the user. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>We present a public benchmark, PersonalLLM, focusing on adapting LLMs to provide maximal benefits for a particular user.Departing from existing alignment benchmarks that implicitly assume uniform preferences, we curate open-ended prompts paired with many high-quality answers over which users would be expected to display heterogeneous latent preferences.Instead of persona-prompting LLMs based on high-level attributes (e.g., user's race or response length), which yields homogeneous preferences relative to humans, we develop a method that can simulate a large user base with diverse preferences from a set of pre-trained reward models.Our dataset and generated personalities offer an innovative testbed for developing personalization algorithms that grapple with continual data sparsity--few relevant feedback from the particular user--by leveraging historical data from other (similar) users.We explore basic in-context learning and meta-learning baselines to illustrate the utility of PersonalLLM and highlight the need for future methodological development.Our dataset is available at https://huggingface.co/datasets/namkoong-lab/PersonalLLM</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2409.20296v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2409.20296v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Production workflows for LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Truthful Text Sanitization Guided by Inference Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The purpose of text sanitization is to rewrite those text spans in a document that may directly or indirectly identify an individual, to ensure they no longer disclose personal information.<span class='px-1 mx-1 bg-yellow-200'>Text sanitization must strike a balance between preventing the leakage of personal information (privacy protection) while also retaining as much of the document's original content as possible (utility preservation). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.342</span></span>We present an automated text sanitization strategy based on generalizations, which are more abstract (but still informative) terms that subsume the semantic content of the original text spans.<span class='px-1 mx-1 bg-yellow-200'>The approach relies on instruction-tuned large language models (LLMs) and is divided into two stages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.33</span></span><span class='px-1 mx-1 bg-yellow-200'>The LLM is first applied to obtain truth-preserving replacement candidates and rank them according to their abstraction level. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.56</span></span><span class='px-1 mx-1 bg-yellow-200'>Those candidates are then evaluated for their ability to protect privacy by conducting inference attacks with the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.395</span></span>Finally, the system selects the most informative replacement shown to be resistant to those attacks.<span class='px-1 mx-1 bg-yellow-200'>As a consequence of this two-stage process, the chosen replacements effectively balance utility and privacy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.581</span></span><span class='px-1 mx-1 bg-yellow-200'>We also present novel metrics to automatically evaluate these two aspects without the need to manually annotate data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.327</span></span><span class='px-1 mx-1 bg-yellow-200'>Empirical results on the Text Anonymization Benchmark show that the proposed approach leads to enhanced utility, with only a marginal increase in the risk of re-identifying protected individuals compared to fully suppressing the original information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.452</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, the selected replacements are shown to be more truth-preserving and abstractive than previous methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.435</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FineGates: LLMs Finetuning with Compression using Stochastic Gates
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs), with billions of parameters, present significant challenges for full finetuning due to the high computational demands, memory requirements, and impracticality of many real-world applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span><span class='px-1 mx-1 bg-yellow-200'>When faced with limited computational resources or small datasets, updating all model parameters can often result in overfitting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, lightweight finetuning techniques have been proposed, like learning low-rank adapter layers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span><span class='px-1 mx-1 bg-yellow-200'>These methods aim to train only a few additional parameters combined with the base model, which remains frozen, reducing resource usage and mitigating overfitting risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we propose an adaptor model based on stochastic gates that simultaneously sparsify the frozen base model with task-specific adaptation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span><span class='px-1 mx-1 bg-yellow-200'>Our method comes with a small number of trainable parameters and allows us to speed up the base model inference with competitive accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.483</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate it in additional variants by equipping it with additional low-rank parameters and comparing it to several recent baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.301</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that the proposed method improves the finetuned model accuracy comparatively to the several baselines and allows the removal of up to 20-40\% without significant accuracy loss. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.406</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12951v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12951v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SnakModel: Lessons Learned from Training an Open Danish Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We present SnakModel, a Danish large language model (LLM) based on Llama2-7B, which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M Danish instructions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.385</span></span>As best practices for creating LLMs for smaller language communities have yet to be established, we examine the effects of early modeling and training decisions on downstream performance throughout the entire training pipeline, including (1) the creation of a strictly curated corpus of Danish text from diverse sources; (2) the language modeling and instruction-tuning training process itself, including the analysis of intermediate training dynamics, and ablations across different hyperparameters; (3) an evaluation on eight language and culturally-specific tasks.Across these experiments SnakModel achieves the highest overall performance, outperforming multiple contemporary Llama2-7B-based models.By making SnakModel, the majority of our pre-training corpus, and the associated code available under open licenses, we hope to foster further research and development in Danish Natural Language Processing, and establish training guidelines for languages with similar resource constraints.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12956v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12956v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adaptations of AI models for querying the LandMatrix database in natural language
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Land Matrix initiative (https://landmatrix.org) and its global observatory aim to provide reliable data on large-scale land acquisitions to inform debates and actions in sectors such as agriculture, extraction, or energy in low- and middle-income countries.Although these data are recognized in the academic world, they remain underutilized in public policy, mainly due to the complexity of access and exploitation, which requires technical expertise and a good understanding of the database schema.   The objective of this work is to simplify access to data from different database systems.The methods proposed in this article are evaluated using data from the Land Matrix.<span class='px-1 mx-1 bg-yellow-200'>This work presents various comparisons of Large Language Models (LLMs) as well as combinations of LLM adaptations (Prompt Engineering, RAG, Agents) to query different database systems (GraphQL and REST queries). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.343</span></span>The experiments are reproducible, and a demonstration is available online: https://github.com/tetis-nlp/landmatrix-graphql-python.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12961v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12961v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown promising capabilities in healthcare analysis but face several challenges like hallucinations, parroting, and bias manifestation.These challenges are exacerbated in complex, sensitive, and low-resource domains.<span class='px-1 mx-1 bg-yellow-200'>Therefore, in this work we introduce IC-AnnoMI, an expert-annotated motivational interviewing (MI) dataset built upon AnnoMI by generating in-context conversational dialogues leveraging LLMs, particularly ChatGPT. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.337</span></span>IC-AnnoMI employs targeted prompts accurately engineered through cues and tailored information, taking into account therapy style (empathy, reflection), contextual relevance, and false semantic change.Subsequently, the dialogues are annotated by experts, strictly adhering to the Motivational Interviewing Skills Code (MISC), focusing on both the psychological and linguistic dimensions of MI dialogues.<span class='px-1 mx-1 bg-yellow-200'>We comprehensively evaluate the IC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding of domain intricacies by modeling novel classification tasks employing several classical machine learning and current state-of-the-art transformer approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.329</span></span>Finally, we discuss the effects of progressive prompting strategies and the impact of augmented data in mitigating the biases manifested in IC-AnnoM.Our contributions provide the MI community with not only a comprehensive dataset but also valuable insights for using LLMs in empathetic text generation for conversational therapy in supervised settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Emergence of Strategic Reasoning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As Large Language Models (LLMs) are increasingly used for a variety of complex and critical tasks, it is vital to assess their logical capabilities in strategic environments.This paper examines their ability in strategic reasoning -- the process of choosing an optimal course of action by predicting and adapting to other agents' behavior.Using six LLMs, we analyze responses from play in classical games from behavioral economics (p-Beauty Contest, 11-20 Money Request Game, and Guessing Game) and evaluate their performance through hierarchical models of reasoning (level-$k$ theory and cognitive hierarchy theory).Our findings reveal that while LLMs show understanding of the games, the majority struggle with higher-order strategic reasoning.Although most LLMs did demonstrate learning ability with games involving repeated interactions, they still consistently fall short of the reasoning levels demonstrated by typical behavior from human subjects.The exception to these overall findings is with OpenAI's GPT-o1 -- specifically trained to solve complex reasoning tasks -- which consistently outperforms other LLMs and human subjects.<span class='px-1 mx-1 bg-yellow-200'>These findings highlight the challenges and pathways in advancing LLMs toward robust strategic reasoning from the perspective of behavioral economics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.328</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13013v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13013v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.439</span></span><span class='px-1 mx-1 bg-yellow-200'>Our benchmark is characterized by its multi-dimensional evaluation framework, including (1) a matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios; (2) a multi-dimensional evaluation data generation approach, which combines GPT-4-based automatic generation and human annotation, achieving an 87.47\% acceptance ratio in human evaluations on generated instances; (3) a multi-stage evaluation system that evaluates both retrieval and generation performance, result in a comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.34</span></span>Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains.We open source the code of our benchmark in \href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13018v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13018v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LMUnit: Fine-grained Evaluation with Natural Language Unit Tests
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge -- human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals.We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUnit, which combines multi-objective training across preferences, direct ratings, and natural language rationales.Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows.<span class='px-1 mx-1 bg-yellow-200'>LMUnit achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.448</span></span>These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13091v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13091v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Evaluation plays a crucial role in the advancement of information retrieval (IR) models.However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently.<span class='px-1 mx-1 bg-yellow-200'>To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.341</span></span>AIR-Bench is distinguished by three key features: 1) Automated.<span class='px-1 mx-1 bg-yellow-200'>The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.43</span></span>2) Heterogeneous.The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages.3) Dynamic.The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers.<span class='px-1 mx-1 bg-yellow-200'>We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.351</span></span>Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models.The resources in AIR-Bench are publicly available at https://github.com/AIR-Bench/AIR-Bench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13102v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13102v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI PERSONA: Towards Life-long Personalization of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce the task of life-long personalization of large language models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.548</span></span><span class='px-1 mx-1 bg-yellow-200'>While recent mainstream efforts in the LLM community mainly focus on scaling data and compute for improved capabilities of LLMs, we argue that it is also very important to enable LLM systems, or language agents, to continuously adapt to the diverse and ever-changing profiles of every distinct user and provide up-to-date personalized assistance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span><span class='px-1 mx-1 bg-yellow-200'>We provide a clear task formulation and introduce a simple, general, effective, and scalable framework for life-long personalization of LLM systems and language agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.573</span></span><span class='px-1 mx-1 bg-yellow-200'>To facilitate future research on LLM personalization, we also introduce methods to synthesize realistic benchmarks and robust evaluation metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.53</span></span><span class='px-1 mx-1 bg-yellow-200'>We will release all codes and data for building and benchmarking life-long personalized LLM systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13103v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13103v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Are Your LLMs Capable of Stable Reasoning?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks.However, a significant discrepancy persists between benchmark performances and real-world applications.We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial.This work makes two key contributions.<span class='px-1 mx-1 bg-yellow-200'>First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.305</span></span><span class='px-1 mx-1 bg-yellow-200'>Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.579</span></span><span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.583</span></span>Our findings reveal substantial room for improvement in LLMs' "realistic" reasoning capabilities, highlighting the need for more robust evaluation methods.The benchmark and detailed results are available at: https://github.com/open-compass/GPassK.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the success of large language models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.301</span></span><span class='px-1 mx-1 bg-yellow-200'>However, they maintain additional moving average states throughout training, which results in memory requirements several times greater than the model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.364</span></span><span class='px-1 mx-1 bg-yellow-200'>This overhead imposes constraints on scalability and computational efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span><span class='px-1 mx-1 bg-yellow-200'>On the other hand, while stochastic gradient descent (SGD) is optimal in terms of memory efficiency, their capability in LLM training is limited (Zhao et al., 2024b).    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.346</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this dilemma, we show that pre-processing SGD is sufficient to reach Adam-level performance on LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.39</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we propose to preprocess the instantaneous stochastic gradients with two simple operators: $\mathtt{GradNorm}$ and $\mathtt{GradWhitening}$. $\mathtt{GradNorm}$ stabilizes gradient distributions, and $\mathtt{GradWhitening}$ counteracts the local curvature of the loss landscape, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.445</span></span><span class='px-1 mx-1 bg-yellow-200'>This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any accumulative state variables. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.481</span></span><span class='px-1 mx-1 bg-yellow-200'>Empirically, SWAN has the same memory footprint as SGD, achieving $\approx 50\%$ reduction on total end-to-end memory compared to Adam. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.317</span></span>In language modeling tasks, SWAN demonstrates the same or even a substantial improvement over Adam.<span class='px-1 mx-1 bg-yellow-200'>Specifically, when pre-training the LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation perplexity in less than half tokens seen. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.41</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13148v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13148v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Organizations seeking to utilize Large Language Models (LLMs) for knowledge querying and analysis often encounter challenges in maintaining an LLM fine-tuned on targeted, up-to-date information that keeps answers relevant and grounded. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.514</span></span>Retrieval Augmented Generation (RAG) has quickly become a feasible solution for organizations looking to overcome the challenges of maintaining proprietary models and to help reduce LLM hallucinations in their query responses.<span class='px-1 mx-1 bg-yellow-200'>However, RAG comes with its own issues regarding scaling data pipelines across tiered-access and disparate data sources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.434</span></span><span class='px-1 mx-1 bg-yellow-200'>In many scenarios, it is necessary to query beyond a single data silo to provide richer and more relevant context for an LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.51</span></span><span class='px-1 mx-1 bg-yellow-200'>Analyzing data sources within and across organizational trust boundaries is often limited by complex data-sharing policies that prohibit centralized data storage, therefore, inhibit the fast and effective setup and scaling of RAG solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.39</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce Confidential Computing (CC) techniques as a solution for secure Federated Retrieval Augmented Generation (FedRAG). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span><span class='px-1 mx-1 bg-yellow-200'>Our proposed Confidential FedRAG system (C-FedRAG) enables secure connection and scaling of a RAG workflows across a decentralized network of data providers by ensuring context confidentiality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>We also demonstrate how to implement a C-FedRAG system using the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and MIRAGE benchmarking dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13163v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13163v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DnDScore: Decontextualization and Decomposition for Factuality Verification in Long-Form Text Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The decompose-then-verify strategy for verification of Large Language Model (LLM) generations decomposes claims that are then independently verified.Decontextualization augments text (claims) to ensure it can be verified outside of the original context, enabling reliable verification.While decomposition and decontextualization have been explored independently, their interactions in a complete system have not been investigated.Their conflicting purposes can create tensions: decomposition isolates atomic facts while decontextualization inserts relevant information.Furthermore, a decontextualized subclaim presents a challenge to the verification step: what part of the augmented text should be verified as it now contains multiple atomic facts?We conduct an evaluation of different decomposition, decontextualization, and verification strategies and find that the choice of strategy matters in the resulting factuality scores.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we introduce DnDScore, a decontextualization aware verification method which validates subclaims in the context of contextual information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.317</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13175v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13175v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLM Model Architectures and Training Techniques</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Survey of Calibration Process for Black-Box LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge.<span class='px-1 mx-1 bg-yellow-200'>While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.403</span></span><span class='px-1 mx-1 bg-yellow-200'>Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.427</span></span><span class='px-1 mx-1 bg-yellow-200'>Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.43</span></span><span class='px-1 mx-1 bg-yellow-200'>To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.503</span></span>We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration.<span class='px-1 mx-1 bg-yellow-200'>Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.457</span></span>Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment.<span class='px-1 mx-1 bg-yellow-200'>This is our GitHub link: https://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.476</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12767v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12767v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Retrieval-augmented generation (RAG) improves the service quality of large language models by retrieving relevant documents from credible literature and integrating them into the context of the user query.Recently, the rise of the cloud RAG service has made it possible for users to query relevant documents conveniently.However, directly sending queries to the cloud brings potential privacy leakage.In this paper, we are the first to formally define the privacy-preserving cloud RAG service to protect the user query and propose RemoteRAG as a solution regarding privacy, efficiency, and accuracy.For privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage of the user query and the leakage inferred from relevant documents.For efficiency, we limit the search range from the total documents to a small number of selected documents related to a perturbed embedding generated from $(n,\epsilon)$-DistanceDP, so that computation and communication costs required for privacy protection significantly decrease.For accuracy, we ensure that the small range includes target documents related to the user query with detailed theoretical analysis.<span class='px-1 mx-1 bg-yellow-200'>Experimental results also demonstrate that RemoteRAG can resist existing embedding inversion attack methods while achieving no loss in retrieval under various settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.418</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, RemoteRAG is efficient, incurring only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$ GB with the non-optimized privacy-preserving scheme) when retrieving from a total of $10^6$ documents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.43</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12775v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12775v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Activating Distributed Visual Region within LLMs for Efficient and Effective Vision-Language Training and Inference
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Vision-Language Models (LVLMs) typically learn visual capacity through visual instruction tuning, involving updates to both a projector and their LLM backbones.Drawing inspiration from the concept of visual region in the human brain, we investigate the existence of an analogous \textit{visual region} within LLMs that functions as a cognitive core, and explore the possibility of efficient training of LVLMs via selective layers tuning.We use Bunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B for validation across a range of visual and textual tasks.<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that selectively updating 25\% of LLMs layers, when sparsely and uniformly distributed, can preserve nearly 99\% of visual performance while maintaining or enhancing textual task results, and also effectively reducing training time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.409</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on this targeted training approach, we further propose a novel visual region-based pruning paradigm, removing non-critical layers outside the visual region, which can achieve minimal performance loss. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.545</span></span>This study offers an effective and efficient strategy for LVLM training and inference by activating a layer-wise visual region within LLMs, which is consistently effective across different models and parameter scales.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12785v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12785v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references.<span class='px-1 mx-1 bg-yellow-200'>This discrepancy undermines the reliability of traditional reference-based evaluation metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.484</span></span>In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism.Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria.Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models.Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12832v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12832v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FocusChat: Text-guided Long Video Understanding via Spatiotemporal Information Filtering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, multi-modal large language models have made significant progress.However, visual information lacking of guidance from the user's intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos.To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the user's prompt.In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively.<span class='px-1 mx-1 bg-yellow-200'>The two branches are combined using the Spatial-Temporal Filtering Module (STFM). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.435</span></span><span class='px-1 mx-1 bg-yellow-200'>STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the user's query. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.414</span></span><span class='px-1 mx-1 bg-yellow-200'>It lowers the essential number of visual tokens inputted into the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.444</span></span>FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied.It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12833v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12833v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions In A Multi-Modal Jungle
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce Hive -- a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.Hive operates over sets of models and, upon receiving natural language instructions (i.e. user queries), schedules and executes explainable plans of atomic actions.These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints.<span class='px-1 mx-1 bg-yellow-200'>Notably, Hive handles tasks that involve multi-modal inputs and outputs, enabling it to handle complex, real-world queries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.424</span></span>Our system is capable of planning complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations.We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems.<span class='px-1 mx-1 bg-yellow-200'>Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.473</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12839v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12839v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking and Understanding Compositional Relational Reasoning of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Compositional relational reasoning (CRR) is a hallmark of human intelligence, but we lack a clear understanding of whether and how existing transformer large language models (LLMs) can solve CRR tasks.To enable systematic exploration of the CRR capability of LLMs, we first propose a new synthetic benchmark called Generalized Associative Recall (GAR) by integrating and generalizing the essence of several tasks in mechanistic interpretability (MI) study in a unified framework.<span class='px-1 mx-1 bg-yellow-200'>Evaluation shows that GAR is challenging enough for existing LLMs, revealing their fundamental deficiency in CRR. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.44</span></span>Meanwhile, it is easy enough for systematic MI study.Then, to understand how LLMs solve GAR tasks, we use attribution patching to discover the core circuits reused by Vicuna-33B across different tasks and a set of vital attention heads.<span class='px-1 mx-1 bg-yellow-200'>Intervention experiments show that the correct functioning of these heads significantly impacts task performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.422</span></span>Especially, we identify two classes of heads whose activations represent the abstract notion of true and false in GAR tasks respectively.They play a fundamental role in CRR across various models and tasks.The dataset and code are available at https://github.com/Caiyun-AI/GAR.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12841v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12841v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Selective Shot Learning for Code Explanation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Code explanation plays a crucial role in the software engineering domain, aiding developers in grasping code functionality efficiently.Recent work shows that the performance of LLMs for code explanation improves in a few-shot setting, especially when the few-shot examples are selected intelligently.State-of-the-art approaches for such Selective Shot Learning (SSL) include token-based and embedding-based methods.<span class='px-1 mx-1 bg-yellow-200'>However, these SSL approaches have been evaluated on proprietary LLMs, without much exploration on open-source Code-LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.41</span></span><span class='px-1 mx-1 bg-yellow-200'>Additionally, these methods lack consideration for programming language syntax. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.412</span></span>To bridge these gaps, we present a comparative study and propose a novel SSL method (SSL_ner) that utilizes entity information for few-shot example selection.We present several insights and show the effectiveness of SSL_ner approach over state-of-the-art methods across two datasets.To the best of our knowledge, this is the first systematic benchmarking of open-source Code-LLMs while assessing the performances of the various few-shot examples selection approaches for the code explanation task.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12852v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12852v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Alignment, endowing a pre-trained Large language model (LLM) with the ability to follow instructions, is crucial for its real-world applications.<span class='px-1 mx-1 bg-yellow-200'>Conventional supervised fine-tuning (SFT) methods formalize it as causal language modeling typically with a cross-entropy objective, requiring a large amount of high-quality instruction-response pairs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>However, the quality of widely used SFT datasets can not be guaranteed due to the high cost and intensive labor for the creation and maintenance in practice.To overcome the limitations associated with the quality of SFT datasets, we introduce a novel \textbf{p}reference-\textbf{o}riented supervised \textbf{f}ine-\textbf{t}uning approach, namely PoFT.The intuition is to boost SFT by imposing a particular preference: \textit{favoring the target model over aligned LLMs on the same SFT data.}This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.Extensive experiments are conducted, and the results validate the effectiveness of the proposed method.<span class='px-1 mx-1 bg-yellow-200'>PoFT achieves stable and consistent improvements over the SFT baselines across different training datasets and base models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.436</span></span><span class='px-1 mx-1 bg-yellow-200'>Moreover, we prove that PoFT can be integrated with existing SFT data filtering methods to achieve better performance, and further improved by following preference optimization procedures, such as DPO. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12865v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12865v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Question: How do Large Language Models perform on the Question Answering tasks? Answer:
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques.A common NLP-task is question-answering (QA).In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique.Since the dataset contains unanswerable questions, previous work used a double inference method.We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources.Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.   <span class='px-1 mx-1 bg-yellow-200'>Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.47</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12893v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12893v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural Language Descriptions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Piping and Instrumentation Diagrams (P&IDs) are foundational to the design, construction, and operation of workflows in the engineering and process industries.<span class='px-1 mx-1 bg-yellow-200'>However, their manual creation is often labor-intensive, error-prone, and lacks robust mechanisms for error detection and correction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.491</span></span>While recent advancements in Generative AI, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), have demonstrated significant potential across various domains, their application in automating generation of engineering workflows remains underexplored.In this work, we introduce a novel copilot for automating the generation of P&IDs from natural language descriptions.Leveraging a multi-step agentic workflow, our copilot provides a structured and iterative approach to diagram creation directly from Natural Language prompts.We demonstrate the feasibility of the generation process by evaluating the soundness and completeness of the workflow, and show improved results compared to vanilla zero-shot and few-shot generation approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12898v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12898v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Truthful Text Sanitization Guided by Inference Attacks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The purpose of text sanitization is to rewrite those text spans in a document that may directly or indirectly identify an individual, to ensure they no longer disclose personal information.Text sanitization must strike a balance between preventing the leakage of personal information (privacy protection) while also retaining as much of the document's original content as possible (utility preservation).We present an automated text sanitization strategy based on generalizations, which are more abstract (but still informative) terms that subsume the semantic content of the original text spans.The approach relies on instruction-tuned large language models (LLMs) and is divided into two stages.The LLM is first applied to obtain truth-preserving replacement candidates and rank them according to their abstraction level.Those candidates are then evaluated for their ability to protect privacy by conducting inference attacks with the LLM.<span class='px-1 mx-1 bg-yellow-200'>Finally, the system selects the most informative replacement shown to be resistant to those attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.42</span></span><span class='px-1 mx-1 bg-yellow-200'>As a consequence of this two-stage process, the chosen replacements effectively balance utility and privacy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span><span class='px-1 mx-1 bg-yellow-200'>We also present novel metrics to automatically evaluate these two aspects without the need to manually annotate data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.471</span></span>Empirical results on the Text Anonymization Benchmark show that the proposed approach leads to enhanced utility, with only a marginal increase in the risk of re-identifying protected individuals compared to fully suppressing the original information.Furthermore, the selected replacements are shown to be more truth-preserving and abstractive than previous methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FineGates: LLMs Finetuning with Compression using Stochastic Gates
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs), with billions of parameters, present significant challenges for full finetuning due to the high computational demands, memory requirements, and impracticality of many real-world applications.<span class='px-1 mx-1 bg-yellow-200'>When faced with limited computational resources or small datasets, updating all model parameters can often result in overfitting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.494</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, lightweight finetuning techniques have been proposed, like learning low-rank adapter layers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span>These methods aim to train only a few additional parameters combined with the base model, which remains frozen, reducing resource usage and mitigating overfitting risks.<span class='px-1 mx-1 bg-yellow-200'>In this work, we propose an adaptor model based on stochastic gates that simultaneously sparsify the frozen base model with task-specific adaptation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.548</span></span>Our method comes with a small number of trainable parameters and allows us to speed up the base model inference with competitive accuracy.<span class='px-1 mx-1 bg-yellow-200'>We evaluate it in additional variants by equipping it with additional low-rank parameters and comparing it to several recent baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.442</span></span>Our results show that the proposed method improves the finetuned model accuracy comparatively to the several baselines and allows the removal of up to 20-40\% without significant accuracy loss.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12951v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12951v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown promising capabilities in healthcare analysis but face several challenges like hallucinations, parroting, and bias manifestation.<span class='px-1 mx-1 bg-yellow-200'>These challenges are exacerbated in complex, sensitive, and low-resource domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>Therefore, in this work we introduce IC-AnnoMI, an expert-annotated motivational interviewing (MI) dataset built upon AnnoMI by generating in-context conversational dialogues leveraging LLMs, particularly ChatGPT.IC-AnnoMI employs targeted prompts accurately engineered through cues and tailored information, taking into account therapy style (empathy, reflection), contextual relevance, and false semantic change.Subsequently, the dialogues are annotated by experts, strictly adhering to the Motivational Interviewing Skills Code (MISC), focusing on both the psychological and linguistic dimensions of MI dialogues.We comprehensively evaluate the IC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding of domain intricacies by modeling novel classification tasks employing several classical machine learning and current state-of-the-art transformer approaches.Finally, we discuss the effects of progressive prompting strategies and the impact of augmented data in mitigating the biases manifested in IC-AnnoM.Our contributions provide the MI community with not only a comprehensive dataset but also valuable insights for using LLMs in empathetic text generation for conversational therapy in supervised settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LMUnit: Fine-grained Evaluation with Natural Language Unit Tests
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge -- human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.423</span></span>We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUnit, which combines multi-objective training across preferences, direct ratings, and natural language rationales.Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows.LMUnit achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench.These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13091v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13091v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Evaluation plays a crucial role in the advancement of information retrieval (IR) models.However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently.To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench).AIR-Bench is distinguished by three key features: 1) Automated.<span class='px-1 mx-1 bg-yellow-200'>The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.411</span></span>2) Heterogeneous.<span class='px-1 mx-1 bg-yellow-200'>The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.415</span></span>3) Dynamic.<span class='px-1 mx-1 bg-yellow-200'>The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.401</span></span>We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora.Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models.The resources in AIR-Bench are publicly available at https://github.com/AIR-Bench/AIR-Bench.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13102v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13102v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI PERSONA: Towards Life-long Personalization of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this work, we introduce the task of life-long personalization of large language models.While recent mainstream efforts in the LLM community mainly focus on scaling data and compute for improved capabilities of LLMs, we argue that it is also very important to enable LLM systems, or language agents, to continuously adapt to the diverse and ever-changing profiles of every distinct user and provide up-to-date personalized assistance.We provide a clear task formulation and introduce a simple, general, effective, and scalable framework for life-long personalization of LLM systems and language agents.<span class='px-1 mx-1 bg-yellow-200'>To facilitate future research on LLM personalization, we also introduce methods to synthesize realistic benchmarks and robust evaluation metrics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.419</span></span>We will release all codes and data for building and benchmarking life-long personalized LLM systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13103v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13103v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Are Your LLMs Capable of Stable Reasoning?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks.<span class='px-1 mx-1 bg-yellow-200'>However, a significant discrepancy persists between benchmark performances and real-world applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.473</span></span>We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial.This work makes two key contributions.First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability.<span class='px-1 mx-1 bg-yellow-200'>Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.453</span></span><span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.559</span></span>Our findings reveal substantial room for improvement in LLMs' "realistic" reasoning capabilities, highlighting the need for more robust evaluation methods.<span class='px-1 mx-1 bg-yellow-200'>The benchmark and detailed results are available at: https://github.com/open-compass/GPassK. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.42</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the success of large language models.<span class='px-1 mx-1 bg-yellow-200'>However, they maintain additional moving average states throughout training, which results in memory requirements several times greater than the model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.417</span></span><span class='px-1 mx-1 bg-yellow-200'>This overhead imposes constraints on scalability and computational efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.517</span></span>On the other hand, while stochastic gradient descent (SGD) is optimal in terms of memory efficiency, their capability in LLM training is limited (Zhao et al., 2024b).   <span class='px-1 mx-1 bg-yellow-200'>To address this dilemma, we show that pre-processing SGD is sufficient to reach Adam-level performance on LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.499</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we propose to preprocess the instantaneous stochastic gradients with two simple operators: $\mathtt{GradNorm}$ and $\mathtt{GradWhitening}$. $\mathtt{GradNorm}$ stabilizes gradient distributions, and $\mathtt{GradWhitening}$ counteracts the local curvature of the loss landscape, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.472</span></span>This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any accumulative state variables.<span class='px-1 mx-1 bg-yellow-200'>Empirically, SWAN has the same memory footprint as SGD, achieving $\approx 50\%$ reduction on total end-to-end memory compared to Adam. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.514</span></span>In language modeling tasks, SWAN demonstrates the same or even a substantial improvement over Adam.Specifically, when pre-training the LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation perplexity in less than half tokens seen.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13148v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13148v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Organizations seeking to utilize Large Language Models (LLMs) for knowledge querying and analysis often encounter challenges in maintaining an LLM fine-tuned on targeted, up-to-date information that keeps answers relevant and grounded.Retrieval Augmented Generation (RAG) has quickly become a feasible solution for organizations looking to overcome the challenges of maintaining proprietary models and to help reduce LLM hallucinations in their query responses.<span class='px-1 mx-1 bg-yellow-200'>However, RAG comes with its own issues regarding scaling data pipelines across tiered-access and disparate data sources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.569</span></span>In many scenarios, it is necessary to query beyond a single data silo to provide richer and more relevant context for an LLM.Analyzing data sources within and across organizational trust boundaries is often limited by complex data-sharing policies that prohibit centralized data storage, therefore, inhibit the fast and effective setup and scaling of RAG solutions.In this paper, we introduce Confidential Computing (CC) techniques as a solution for secure Federated Retrieval Augmented Generation (FedRAG).<span class='px-1 mx-1 bg-yellow-200'>Our proposed Confidential FedRAG system (C-FedRAG) enables secure connection and scaling of a RAG workflows across a decentralized network of data providers by ensuring context confidentiality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.464</span></span><span class='px-1 mx-1 bg-yellow-200'>We also demonstrate how to implement a C-FedRAG system using the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and MIRAGE benchmarking dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.471</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13163v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13163v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the integration of large language models (LLMs), embodied agents have strong capabilities to execute complicated instructions in natural language, paving a way for the potential deployment of embodied robots.However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in real world.To study this issue, we present SafeAgentBench -- a new benchmark for safety-aware task planning of embodied LLM agents.SafeAgentBench includes: (1) a new dataset with 750 tasks, covering 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 8 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives.Experimental results show that the best-performing baseline gets 69% success rate for safe tasks, but only 5% rejection rate for hazardous tasks, indicating significant safety risks.<span class='px-1 mx-1 bg-yellow-200'>More details and codes are available at https://github.com/shengyin1224/SafeAgentBench. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.444</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.13178v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.13178v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Programming applications of LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Generating Move Smart Contracts based on Concepts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The growing adoption of formal verification for smart contracts has spurred the development of new verifiable languages like Move.<span class='px-1 mx-1 bg-yellow-200'>However, the limited availability of training data for these languages hinders effective code generation by large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper presents ConMover, a novel framework that enhances LLM-based code generation for Move by leveraging a knowledge graph of Move concepts and a small set of verified code examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.87</span></span>ConMover integrates concept retrieval, planning, coding, and debugging agents in an iterative process to refine generated code.Evaluations with various open-source LLMs demonstrate substantial accuracy improvements over baseline models.<span class='px-1 mx-1 bg-yellow-200'>These results underscore ConMover's potential to address low-resource code generation challenges, bridging the gap between natural language descriptions and reliable smart contract development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12513v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12513v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Seed-CTS: Unleashing the Power of Tree Search for Superior Performance in Competitive Coding Tasks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Competition-level code generation tasks pose significant challenges for current state-of-the-art large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span>For example, on the LiveCodeBench-Hard dataset, models such as O1-Mini and O1-Preview achieve pass@1 rates of only 0.366 and 0.143, respectively.<span class='px-1 mx-1 bg-yellow-200'>While tree search techniques have proven effective in domains like mathematics and general coding, their potential in competition-level code generation remains under-explored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.798</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we propose a novel token-level tree search method specifically designed for code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.863</span></span>Leveraging Qwen2.5-Coder-32B-Instruct, our approach achieves a pass rate of 0.305 on LiveCodeBench-Hard, surpassing the pass@100 performance of GPT4o-0513 (0.245).Furthermore, by integrating Chain-of-Thought (CoT) prompting, we improve our method's performance to 0.351, approaching O1-Mini's pass@1 rate.To ensure reproducibility, we report the average number of generations required per problem by our tree search method on the test set.<span class='px-1 mx-1 bg-yellow-200'>Our findings underscore the potential of tree search to significantly enhance performance on competition-level code generation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.839</span></span><span class='px-1 mx-1 bg-yellow-200'>This opens up new possibilities for large-scale synthesis of challenging code problems supervised fine-tuning (SFT) data, advancing competition-level code generation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.728</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12544v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12544v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program Fine-tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Program-of-Thought (PoT), which aims to use programming language instead of natural language as an intermediate step in reasoning, is an important way for LLMs to solve mathematical problems.<span class='px-1 mx-1 bg-yellow-200'>Since different programming languages excel in different areas, it is natural to use the most suitable language for solving specific problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>However, current PoT research only focuses on single language PoT, ignoring the differences between different programming languages.Therefore, this paper proposes an multilingual program reasoning method, MultiLingPoT.This method allows the model to answer questions using multiple programming languages by fine-tuning on multilingual data.Additionally, prior and posterior hybrid methods are used to help the model select the most suitable language for each problem.Our experimental results show that the training of MultiLingPoT improves each program's mathematical reasoning by about 2.5\%.Moreover, with proper mixing, the performance of MultiLingPoT can be further improved, achieving a 6\% increase compared to the single-language PoT with the data augmentation.Resources of this paper can be found at https://github.com/Nianqi-Li/MultiLingPoT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12609v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12609v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Selective Shot Learning for Code Explanation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code explanation plays a crucial role in the software engineering domain, aiding developers in grasping code functionality efficiently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span><span class='px-1 mx-1 bg-yellow-200'>Recent work shows that the performance of LLMs for code explanation improves in a few-shot setting, especially when the few-shot examples are selected intelligently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.79</span></span>State-of-the-art approaches for such Selective Shot Learning (SSL) include token-based and embedding-based methods.However, these SSL approaches have been evaluated on proprietary LLMs, without much exploration on open-source Code-LLMs.Additionally, these methods lack consideration for programming language syntax.To bridge these gaps, we present a comparative study and propose a novel SSL method (SSL_ner) that utilizes entity information for few-shot example selection.We present several insights and show the effectiveness of SSL_ner approach over state-of-the-art methods across two datasets.<span class='px-1 mx-1 bg-yellow-200'>To the best of our knowledge, this is the first systematic benchmarking of open-source Code-LLMs while assessing the performances of the various few-shot examples selection approaches for the code explanation task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.758</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12852v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12852v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Codenames as a Benchmark for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities.Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches.LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges.We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups.Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles.We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11373v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11373v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CoinMath: Harnessing the Power of Coding Instruction for Math LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown strong performance in solving mathematical problems, with code-based solutions proving particularly effective. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.865</span></span>However, the best practice to leverage coding instruction data to enhance mathematical reasoning remains underexplored.<span class='px-1 mx-1 bg-yellow-200'>This study investigates three key questions: (1) How do different coding styles of mathematical code-based rationales impact LLMs' learning performance? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.673</span></span>(2) Can general-domain coding instructions improve performance?(3) How does integrating textual rationales with code-based ones during training enhance mathematical reasoning abilities?<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that code-based rationales with concise comments, descriptive naming, and hardcoded solutions are beneficial, while improvements from general-domain coding instructions and textual rationales are relatively minor. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>Based on these insights, we propose CoinMath, a learning strategy designed to enhance mathematical reasoning by diversifying the coding styles of code-based rationales.CoinMath generates a variety of code-based rationales incorporating concise comments, descriptive naming conventions, and hardcoded solutions.Experimental results demonstrate that CoinMath significantly outperforms its baseline model, MAmmoTH, one of the SOTA math LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11699v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11699v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Precise Length Control in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span>Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail.In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length.Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response termination length.Fine-tuning with LDPE allows the model to learn to terminate responses coherently at the desired length, achieving mean token errors of less than 3 tokens.We also introduce Max New Tokens++, an extension that enables flexible upper-bound length control, rather than an exact target.Experimental results on tasks such as question answering and document summarization demonstrate that our method enables precise length control without compromising response quality.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11937v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11937v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ExecRepoBench: Multi-level Executable Code Completion Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code completion has become an essential tool for daily software development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.811</span></span>Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets.<span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.825</span></span>ExecRepoBench includes 1.2K samples from active Python repositories.<span class='px-1 mx-1 bg-yellow-200'>Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.747</span></span>Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model.Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages.The deployment of \ourmethod{} can be used as a high-performance, local service for programming development\footnote{\url{https://execrepobench.github.io/}}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11990v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11990v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-16</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present BioRAGent, an interactive web-based retrieval-augmented generation (RAG) system for biomedical question answering.<span class='px-1 mx-1 bg-yellow-200'>The system uses large language models (LLMs) for query expansion, snippet extraction, and answer generation while maintaining transparency through citation links to the source documents and displaying generated queries for further editing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>Building on our successful participation in the BioASQ 2024 challenge, we demonstrate how few-shot learning with LLMs can be effectively applied for a professional search setting.The system supports both direct short paragraph style responses and responses with inline citations.Our demo is available online, and the source code is publicly accessible through GitHub.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.12358v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.12358v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PromptV: Leveraging LLM-powered Multi-Agent Prompting for High-quality Verilog Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in agentic LLMs have demonstrated remarkable automated Verilog code generation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>However, existing approaches either demand substantial computational resources or rely on LLM-assisted single-agent prompt learning techniques, which we observe for the first time has a degeneration issue - characterized by deteriorating generative performance and diminished error detection and correction capabilities.<span class='px-1 mx-1 bg-yellow-200'>This paper proposes a novel multi-agent prompt learning framework to address these limitations and enhance code generation quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>We show for the first time that multi-agent architectures can effectively mitigate the degeneration risk while improving code error correction capabilities, resulting in higher-quality Verilog code generation.Experimental results show that the proposed method could achieve 96.4% and 96.5% pass@10 scores on VerilogEval Machine and Human benchmarks, respectively while attaining 100% Syntax and 99.9% Functionality pass@5 metrics on the RTLLM benchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11014v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11014v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                NITRO: LLM Inference on Intel Laptop NPUs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have become essential tools in natural language processing, finding large usage in chatbots such as ChatGPT and Gemini, and are a central area of research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>A particular area of interest includes designing hardware specialized for these AI applications, with one such example being the neural processing unit (NPU).In 2023, Intel released the Intel Core Ultra processor with codename Meteor Lake, featuring a CPU, GPU, and NPU system-on-chip.However, official software support for the NPU through Intel's OpenVINO framework is limited to static model inference.The dynamic nature of autoregressive token generation in LLMs is therefore not supported out of the box.To address this shortcoming, we present NITRO (NPU Inference for Transformers Optimization), a Python-based framework built on top of OpenVINO to support text and chat generation on NPUs.In this paper, we discuss in detail the key modifications made to the transformer architecture to enable inference, some performance benchmarks, and future steps towards improving the package.The code repository for NITRO can be found here: https://github.com/abdelfattah-lab/nitro.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11053v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11053v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AD-LLM: Benchmarking Large Language Models for Anomaly Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring.Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity.<span class='px-1 mx-1 bg-yellow-200'>Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span>This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection.We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models.Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging.Based on these results, we outline six future research directions on LLMs for AD.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.11142v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.11142v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Kajal: Extracting Grammar of a Source Code Using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Understanding and extracting the grammar of a domain-specific language (DSL) is crucial for various software engineering tasks; however, manually creating these grammars is time-intensive and error-prone.<span class='px-1 mx-1 bg-yellow-200'>This paper presents Kajal, a novel approach that automatically infers grammar from DSL code snippets by leveraging Large Language Models (LLMs) through prompt engineering and few-shot learning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>Kajal dynamically constructs input prompts, using contextual information to guide the LLM in generating the corresponding grammars, which are iteratively refined through a feedback-driven approach.Our experiments show that Kajal achieves 60% accuracy with few-shot learning and 45% without it, demonstrating the significant impact of few-shot learning on the tool's effectiveness.This approach offers a promising solution for automating DSL grammar extraction, and future work will explore using smaller, open-source LLMs and testing on larger datasets to further validate Kajal's performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08842v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08842v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Oversight in Action: Experiences with Instructor-Moderated LLM Responses in an Online Discussion Forum
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The integration of large language models (LLMs) into computing education offers many potential benefits to student learning, and several novel pedagogical approaches have been reported in the literature. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span>However LLMs also present challenges, one of the most commonly cited being that of student over-reliance.This challenge is compounded by the fact that LLMs are always available to provide instant help and solutions to students, which can undermine their ability to independently solve problems and diagnose and resolve errors.Providing instructor oversight of LLM-generated content can mitigate this problem, however it is often not practical in real-time learning contexts.Online class discussion forums, which are widely used in computing education, present an opportunity for exploring instructor oversight because they operate asynchronously.Unlike real-time interactions, the discussion forum format aligns with the expectation that responses may take time, making oversight not only feasible but also pedagogically appropriate.In this practitioner paper, we present the design, deployment, and evaluation of a `bot' module that is controlled by the instructor, and integrated into an online discussion forum.The bot assists the instructor by generating draft responses to student questions, which are reviewed, modified, and approved before release.Key features include the ability to leverage course materials, access archived discussions, and publish responses anonymously to encourage open participation.<span class='px-1 mx-1 bg-yellow-200'>We report our experiences using this tool in a 12-week second-year software engineering course on object-oriented programming. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>Instructor feedback confirmed the tool successfully alleviated workload but highlighted a need for improvement in handling complex, context-dependent queries.We report the features that were viewed as most beneficial, and suggest avenues for future exploration.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.09048v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.09048v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Scalable, Validated Code Translation of Entire Projects using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) show promise in code translation due to their ability to generate idiomatic code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span><span class='px-1 mx-1 bg-yellow-200'>However, a significant limitation when using LLMs for code translation is scalability: existing works have shown a drop in translation success rates for code exceeding around 100 lines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span>We overcome this limitation by developing a modular approach to translation, where we partition the code into small code fragments which can be translated independently and semantically validated (that is, checking I/O equivalence).When this approach is applied naively, we discover that LLMs are unreliable when translating features of the source language that do not have a direct mapping to the target language, and that the LLM often gets stuck in repair loops when attempting to fix errors.To address these issues, we introduce two key concepts: (1) feature mapping, which integrates predefined translation rules with LLM-based translation to guide the LLM in navigating subtle language differences and producing semantically accurate code; and (2) type-compatibility, which facilitates localized checks at the function signature level to detect errors early, thereby narrowing the scope of potential repairs.We apply our approach to translating real-world Go codebases to Rust, demonstrating that we can consistently generate reliable Rust translations for projects up to 6,600 lines of code and 369 functions, with an average of 73% of functions successfully validated for I/O equivalence, considerably higher than any existing work.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08035v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08035v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Quantifying the benefits of code hints for refactoring deprecated Java APIs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>When done manually, refactoring legacy code in order to eliminate uses of deprecated APIs is an error-prone and time-consuming process.In this paper, we investigate to which degree refactorings for deprecated Java APIs can be automated, and quantify the benefit of Javadoc code hints for this task.To this end, we build a symbolic and a neural engine for the automatic refactoring of deprecated APIs.<span class='px-1 mx-1 bg-yellow-200'>The former is based on type-directed and component-based program synthesis, whereas the latter uses LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>We applied our engines to refactor the deprecated methods in the Oracle JDK 15.Our experiments show that code hints are enabling for the automation of this task: even the worst engine correctly refactors 71% of the tasks with code hints, which drops to at best 14% on tasks without.Adding more code hints to Javadoc can hence boost the refactoring of code that uses deprecated APIs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08041v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08041v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ContextModule: Improving Code Completion via Repository-level Contextual Information
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have demonstrated impressive capabilities in code completion tasks, where they assist developers by predicting and generating new code in real-time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.903</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing LLM-based code completion systems primarily rely on the immediate context of the file being edited, often missing valuable repository-level information, user behaviour and edit history that could improve suggestion accuracy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>Additionally, challenges such as efficiently retrieving relevant code snippets from large repositories, incorporating user behavior, and balancing accuracy with low-latency requirements in production environments remain unresolved.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose ContextModule, a framework designed to enhance LLM-based code completion by retrieving and integrating three types of contextual information from the repository: user behavior-based code, similar code snippets, and critical symbol definitions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.809</span></span><span class='px-1 mx-1 bg-yellow-200'>By capturing user interactions across files and leveraging repository-wide static analysis, ContextModule improves the relevance and precision of generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.778</span></span>We implement performance optimizations, such as index caching, to ensure the system meets the latency constraints of real-world coding environments.Experimental results and industrial practise demonstrate that ContextModule significantly improves code completion accuracy and user acceptance rates.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08063v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08063v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DialogAgent: An Auto-engagement Agent for Code Question Answering Data Production
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have become increasingly integral to enhancing developer productivity, particularly in code generation, comprehension, and repair tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.911</span></span>However, fine-tuning these models with high-quality, real-world data is challenging due to privacy concerns and the lack of accessible, labeled datasets.In this paper, we present DialogAgent, an automated tool for generating synthetic training data that closely mimics real developer interactions within Integrated Development Environments (IDEs).DialogAgent enables the production of diverse, high-fidelity query-response pairs by simulating multi-turn dialogues and contextual behaviors observed in real-world programming scenarios.The tool significantly reduces the reliance on manual data generation, increasing efficiency by 4.8 times compared to traditional methods.<span class='px-1 mx-1 bg-yellow-200'>Our experiments and online deployment demonstrate substantial improvements in model performance for code-related question-answering tasks: the acceptance rate of responses generated by our in-house model is improved by 33%, after training on synthesized data generated by DialogAgent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08069v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08069v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You See Is Not Always What You Get: An Empirical Study of Code Comprehension by Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering domain, covering numerous tasks such as code generation and comprehension. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.885</span></span>While the benefit of LLMs for coding task is well noted, it is perceived that LLMs are vulnerable to adversarial attacks.In this paper, we study the specific LLM vulnerability to imperceptible character attacks, a type of prompt-injection attack that uses special characters to befuddle an LLM whilst keeping the attack hidden to human eyes.We devise four categories of attacks and investigate their effects on the performance outcomes of tasks relating to code analysis and code comprehension.Two generations of ChatGPT are included to evaluate the impact of advancements made to contemporary models.Our experimental design consisted of comparing perturbed and unperturbed code snippets and evaluating two performance outcomes, which are model confidence using log probabilities of response, and correctness of response.We conclude that earlier version of ChatGPT exhibits a strong negative linear correlation between the amount of perturbation and the performance outcomes, while the recent ChatGPT presents a strong negative correlation between the presence of perturbation and performance outcomes, but no valid correlational relationship between perturbation budget and performance outcomes.We anticipate this work contributes to an in-depth understanding of leveraging LLMs for coding tasks.It is suggested future research should delve into how to create LLMs that can return a correct response even if the prompt exhibits perturbations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08098v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08098v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unseen Horizons: Unveiling the Real Capability of LLM Code Generation Beyond the Familiar
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recently, large language models (LLMs) have shown strong potential in code generation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.93</span></span>However, there are still gaps before they can be fully applied in actual software development processes.<span class='px-1 mx-1 bg-yellow-200'>Accurately assessing the code generation capabilities of large language models has become an important basis for evaluating and improving the models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.905</span></span>Some existing works have constructed datasets to evaluate the capabilities of these models.However, the current evaluation process may encounter the illusion of "Specialist in Familiarity", primarily due to three gaps: the exposure of target code, case timeliness, and dependency availability.The fundamental reason for these gaps is that the code in current datasets may have been extensively exposed and exercised during the training phase, and due to the continuous training and development of LLM, their timeliness has been severely compromised.The key to solve the problem is to, as much as possible, evaluate the LLMs using code that they have not encountered before.Thus, the fundamental idea in this paper is to draw on the concept of code obfuscation, changing code at different levels while ensuring the functionality and output.To this end, we build a code-obfuscation based benchmark OBFUSEVAL.We first collect 1,354 raw cases from five real-world projects, including function description and code.Then we use three-level strategy (symbol, structure and semantic) to obfuscate descriptions, code and context dependencies.We evaluate four LLMs on OBFU- SEVAL and compared the effectiveness of different obfuscation strategy.We use official test suites of these projects to evaluate the generated code.The results show that after obfuscation, the average decrease ratio of test pass rate can up to 62.5%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08109v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08109v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Code LLMs: A Taxonomy-based Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks and have recently expanded their impact to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span>This taxonomy-based survey provides a comprehensive analysis of LLMs in the NL-PL domain, investigating how these models are utilized in coding tasks and examining their methodologies, architectures, and training processes.We propose a taxonomy-based framework that categorizes relevant concepts, providing a unified classification system to facilitate a deeper understanding of this rapidly evolving field.This survey offers insights into the current state and future directions of LLMs in coding tasks, including their applications and limitations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08291v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08291v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can We Generate Visual Programs Without Prompting LLMs?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Visual programming prompts LLMs (large language mod-els) to generate executable code for visual tasks like visual question answering (VQA).Prompt-based methods are difficult to improve while also being unreliable and costly in both time and money.Our goal is to develop an efficient visual programming system without 1) using prompt-based LLMs at inference time and 2) a large set of program and answer annotations.<span class='px-1 mx-1 bg-yellow-200'>We develop a synthetic data augmentation approach and alternative program generation method based on decoupling programs into higher-level skills called templates and the corresponding arguments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span>Our results show that with data augmentation, prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive with state-of-the art models with the added benefit of much faster inference</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.08564v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.08564v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConceptSearch: Towards Efficient Program Search Using LLMs for Abstraction and Reasoning Corpus (ARC)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to artificial intelligence, demanding broad generalization and few-shot learning capabilities that remain elusive for current deep learning methods, including large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>While LLMs excel in program synthesis, their direct application to ARC yields limited success. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, we introduce ConceptSearch, a novel function-search algorithm that leverages LLMs for program generation and employs a concept-based scoring method to guide the search efficiently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Unlike simplistic pixel-based metrics like Hamming distance, ConceptSearch evaluates programs on their ability to capture the underlying transformation concept reflected in the input-output examples.We explore three scoring functions: Hamming distance, a CNN-based scoring function, and an LLM-based natural language scoring function.Experimental results demonstrate the effectiveness of ConceptSearch, achieving a significant performance improvement over direct prompting with GPT-4.Moreover, our novel concept-based scoring exhibits up to 30% greater efficiency compared to Hamming distance, measured in terms of the number of iterations required to reach the correct solution.<span class='px-1 mx-1 bg-yellow-200'>These findings highlight the potential of LLM-driven program search when integrated with concept-based guidance for tackling challenging generalization problems like ARC. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07322v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07322v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Coding Spot: Understanding Parametric Contributions to LLM Coding Performance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have demonstrated notable proficiency in both code generation and comprehension across multiple programming languages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.932</span></span>However, the mechanisms underlying this proficiency remain underexplored, particularly with respect to whether distinct programming languages are processed independently or within a shared parametric region.Drawing an analogy to the specialized regions of the brain responsible for distinct cognitive functions, we introduce the concept of Coding Spot, a specialized parametric region within LLMs that facilitates coding capabilities.Our findings identify this Coding Spot and show that targeted modifications to this subset significantly affect performance on coding tasks, while largely preserving non-coding functionalities.This compartmentalization mirrors the functional specialization observed in cognitive neuroscience, where specific brain regions are dedicated to distinct tasks, suggesting that LLMs may similarly employ specialized parameter regions for different knowledge domains.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07113v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07113v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConceptSearch: Towards Efficient Program Search Using LLMs for Abstraction and Reasoning Corpus (ARC)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to artificial intelligence, demanding broad generalization and few-shot learning capabilities that remain elusive for current deep learning methods, including large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>While LLMs excel in program synthesis, their direct application to ARC yields limited success. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>To address this, we introduce ConceptSearch, a novel function-search algorithm that leverages LLMs for program generation and employs a concept-based scoring method to guide the search efficiently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Unlike simplistic pixel-based metrics like Hamming distance, ConceptSearch evaluates programs on their ability to capture the underlying transformation concept reflected in the input-output examples.We explore three scoring functions: Hamming distance, a CNN-based scoring function, and an LLM-based natural language scoring function.Experimental results demonstrate the effectiveness of ConceptSearch, achieving a significant performance improvement over direct prompting with GPT-4.Moreover, our novel concept-based scoring exhibits up to 30% greater efficiency compared to Hamming distance, measured in terms of the number of iterations required to reach the correct solution.<span class='px-1 mx-1 bg-yellow-200'>These findings highlight the potential of LLM-driven program search when integrated with concept-based guidance for tackling challenging generalization problems like ARC. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>Code: https://github.com/kksinghal/concept-search</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07322v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07322v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Beyond Search Engines: Can Large Language Models Improve Curriculum Development?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While Online Learning is growing and becoming widespread, the associated curricula often suffer from a lack of coverage and outdated content.In this regard, a key question is how to dynamically define the topics that must be covered to thoroughly learn a subject (e.g., a course).<span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) are considered candidates that can be used to address curriculum development challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span>Therefore, we developed a framework and a novel dataset, built on YouTube, to evaluate LLMs' performance when it comes to generating learning topics for specific courses.The experiment was conducted across over 100 courses and nearly 7,000 YouTube playlists in various subject areas.Our results indicate that GPT-4 can produce more accurate topics for the given courses than extracted topics from YouTube video playlists in terms of BERTScore</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07422v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07422v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches, which combine high-level symbolic plan with low-level motion planning.<span class='px-1 mx-1 bg-yellow-200'>Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which struggles in adapting to dynamic environments and complex task contexts.To address these limitations, this work proposes a novel ontology-driven prompt-tuning framework that employs knowledge-based reasoning to refine and expand user prompts with task contextual reasoning and knowledge-based environment state descriptions.Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans.The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement.The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments, and the generation of semantically correct task plans.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07493v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07493v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-12-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MAGE: A Multi-Agent Engine for Automated RTL Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The automatic generation of RTL code (e.g., Verilog) through natural language instructions has emerged as a promising direction with the advancement of large language models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span>However, producing RTL code that is both syntactically and functionally correct remains a significant challenge.Existing single-LLM-agent approaches face substantial limitations because they must navigate between various programming languages and handle intricate generation, verification, and modification tasks.To address these challenges, this paper introduces MAGE, the first open-source multi-agent AI system designed for robust and accurate Verilog RTL code generation.We propose a novel high-temperature RTL candidate sampling and debugging system that effectively explores the space of code candidates and significantly improves the quality of the candidates.Furthermore, we design a novel Verilog-state checkpoint checking mechanism that enables early detection of functional errors and delivers precise feedback for targeted fixes, significantly enhancing the functional correctness of the generated RTL code.MAGE achieves a 95.7% rate of syntactic and functional correctness code generation on VerilogEval-Human 2 benchmark, surpassing the state-of-the-art Claude-3.5-sonnet by 23.3 %, demonstrating a robust and reliable approach for AI-driven RTL design workflows.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2412.07822v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2412.07822v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
  function extractGSUrl(url) {
            var regex = /\/([^/]+?)(?:v\d+)?$/;
            var matches = url.match(regex);

            if (matches) {
                var endOfUrl = matches[1];
                var prefix = "https://scholar.google.com/scholar_lookup?arxiv_id=";
                var finalUrl = prefix + endOfUrl;
                console.log("Final URL: " + finalUrl);
                
                // You can open the final URL in a new tab/window
                window.open(finalUrl, "_blank");
            } else {
                console.log("No match found");
            }
        };
</script>
</body>
</html>