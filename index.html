<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Ryan's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-08-16.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by running a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <p class="text-sm text-gray-500 pt-2">This project was originally created by <a href="https://koaning.io/">Vincent Warmerdam</a>, modifying his original <a href="https://koaning.github.io/arxiv-frontpage/">frontpage</a> for different paper categories.</p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Prompt Engineering in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Large Language Model-based Speech Recognition by Contextualization for Rare and Ambiguous Words
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We develop a large language model (LLM) based automatic speech recognition (ASR) system that can be contextualized by providing keywords as prior information in text prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>We adopt decoder-only architecture and use our in-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by Japanese and English texts as the decoder.We adopt a pre-trained Whisper encoder as an audio encoder, and the audio embeddings from the audio encoder are projected to the text embedding space by an adapter layer and concatenated with text embeddings converted from text prompts to form inputs to the decoder.By providing keywords as prior information in the text prompts, we can contextualize our LLM-based ASR system without modifying the model architecture to transcribe ambiguous words in the input audio accurately.Experimental results demonstrate that providing keywords to the decoder can significantly improve the recognition performance of rare and ambiguous words.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08027v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08027v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although Large Language Models(LLMs) can generate coherent and contextually relevant text, they often struggle to recognise the intent behind the human user's query.Natural Language Understanding (NLU) models, however, interpret the purpose and key information of user's input to enable responsive interactions.Existing NLU models generally map individual utterances to a dual-level semantic frame, involving sentence-level intent and word-level slot labels.However, real-life conversations primarily consist of multi-turn conversations, involving the interpretation of complex and extended dialogues.Researchers encounter challenges addressing all facets of multi-turn dialogue conversations using a unified single NLU model.This paper introduces a novel approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge distillation for multi-turn NLU.To achieve this, we construct distinct teachers for varying levels of conversation knowledge, namely, sentence-level intent detection, word-level slot filling, and conversation-level domain classification.<span class='px-1 mx-1 bg-yellow-200'>These teachers are then fine-tuned to acquire specific knowledge of their designated levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span><span class='px-1 mx-1 bg-yellow-200'>A multi-teacher loss is proposed to facilitate the combination of these multi-level teachers, guiding a student model in multi-turn dialogue tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>The experimental results demonstrate the efficacy of our model in improving the overall multi-turn conversation understanding, showcasing the potential for advancements in NLU models through the incorporation of multi-level dialogue knowledge distillation techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08144v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08144v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EmBARDiment: an Embodied AI Agent for Productivity in XR
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios.However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query.We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment.<span class='px-1 mx-1 bg-yellow-200'>This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08158v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08158v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models Prompting With Episodic Memory
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly into the prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span><span class='px-1 mx-1 bg-yellow-200'>Despite the growing interest in optimizing prompts with few-shot examples, existing methods for prompt optimization are often resource-intensive or perform inadequately. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt optimization technique that is simple, efficient, and demonstrates strong generalization capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.854</span></span><span class='px-1 mx-1 bg-yellow-200'>We approach prompt optimization as a Reinforcement Learning (RL) challenge, using episodic memory to archive combinations of input data, permutations of few-shot examples, and the rewards observed during training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span>In the testing phase, we optimize the sequence of examples for each test query by selecting the sequence that yields the highest total rewards from the top-k most similar training examples in the episodic memory.Our results show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over 5.3% in various text classification tasks.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, our approach adapts well to broader language understanding tasks, consistently outperforming conventional heuristic methods for ordering examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07465v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07465v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson Planning in Ugandan Secondary Schools. Prototype Quality Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas.Research identifies several problems, including low quality or absent teacher lesson planning.As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened.Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks.<span class='px-1 mx-1 bg-yellow-200'>This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>Methods: The prototype was created using Cohere LLM and Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website.Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level.<span class='px-1 mx-1 bg-yellow-200'>Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.   Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to "very good lesson plan".None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic.In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07542v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07542v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The complexity and heterogeneity of data in many real-world applications pose significant challenges for traditional machine learning and signal processing techniques.For instance, in medicine, effective analysis of diverse physiological signals is crucial for patient monitoring and clinical decision-making and yet highly challenging.We introduce MedTsLLM, a general multimodal large language model (LLM) framework that effectively integrates time series data and rich contextual information in the form of text to analyze physiological signals, performing three tasks with clinical relevance: semantic segmentation, boundary detection, and anomaly detection in time series.These critical tasks enable deeper analysis of physiological signals and can provide actionable insights for clinicians.We utilize a reprogramming layer to align embeddings of time series patches with a pretrained LLM's embedding space and make effective use of raw time series, in conjunction with textual context.Given the multivariate nature of medical datasets, we develop methods to handle multiple covariates.<span class='px-1 mx-1 bg-yellow-200'>We additionally tailor the text prompt to include patient-specific information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>Our model outperforms state-of-the-art baselines, including deep learning models, other LLMs, and clinical methods across multiple medical domains, specifically electrocardiograms and respiratory waveforms.MedTsLLM presents a promising step towards harnessing the power of LLMs for medical time series analysis that can elevate data-driven tools for clinicians and improve patient outcomes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07773v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07773v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Decision to Action in Surgical Autonomy: Multi-Modal Large Language Models for Robot-Assisted Blood Suction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rise of Large Language Models (LLMs) has impacted research in robotics and automation.While progress has been made in integrating LLMs into general robotics tasks, a noticeable void persists in their adoption in more specific domains such as surgery, where critical factors such as reasoning, explainability, and safety are paramount.Achieving autonomy in robotic surgery, which entails the ability to reason and adapt to changes in the environment, remains a significant challenge.In this work, we propose a multi-modal LLM integration in robot-assisted surgery for autonomous blood suction.The reasoning and prioritization are delegated to the higher-level task-planning LLM, and the motion planning and execution are handled by the lower-level deep reinforcement learning model, creating a distributed agency between the two components.As surgical operations are highly dynamic and may encounter unforeseen circumstances, blood clots and active bleeding were introduced to influence decision-making.<span class='px-1 mx-1 bg-yellow-200'>Results showed that using a multi-modal LLM as a higher-level reasoning unit can account for these surgical complexities to achieve a level of reasoning previously unattainable in robot-assisted surgeries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>These findings demonstrate the potential of multi-modal LLMs to significantly enhance contextual understanding and decision-making in robotic-assisted surgeries, marking a step toward autonomous surgical systems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07806v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07806v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Biomedical Event Extraction via Structure-aware Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Biomedical Event Extraction (BEE) is a critical task that involves modeling complex relationships between fine-grained entities in biomedical text data.However, most existing BEE models rely on classification methods that neglect the label semantics and argument dependency structure within the data.To address these limitations, we propose GenBEE, a generative model enhanced with a structure-aware prefix for biomedical event extraction.<span class='px-1 mx-1 bg-yellow-200'>GenBEE constructs event prompts that leverage knowledge distilled from large language models (LLMs), thereby incorporating both label semantics and argument dependency relationships. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span>Additionally, GenBEE introduces a structural prefix learning module that generates structure-aware prefixes with structural prompts, enriching the generation process with structural features.Extensive experiments on three benchmark datasets demonstrate the effectiveness of GenBEE and it achieves state-of-the-art performance on the MLEE and GE11 datasets.Furthermore, our analysis shows that the structural prefixes effectively bridge the gap between structural prompts and the representation space of generative models, enabling better integration of event structural information.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06583v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06583v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Tuning as User Inherent Profile Inference Machine
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have exhibited significant promise in recommender systems by empowering user profiles with their extensive world knowledge and superior reasoning capabilities.However, LLMs face challenges like unstable instruction compliance, modality gaps, and high inference latency, leading to textual noise and limiting their effectiveness in recommender systems.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we propose UserIP-Tuning, which uses prompt-tuning to infer user profiles. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>It integrates the causal relationship between user profiles and behavior sequences into LLMs' prompts.<span class='px-1 mx-1 bg-yellow-200'>And employs expectation maximization to infer the embedded latent profile, minimizing textual noise by fixing the prompt template. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>Furthermore, A profile quantization codebook bridges the modality gap by categorizing profile embeddings into collaborative IDs, which are pre-stored for online deployment.This improves time efficiency and reduces memory usage.Experiments on four public datasets show that UserIP-Tuning outperforms state-of-the-art recommendation algorithms.Additional tests and case studies confirm its effectiveness, robustness, and transferability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06577v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06577v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                An Event Structure-aware Generative Model for Biomedical Event Extraction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Biomedical Event Extraction (BEE) is a challenging task that involves modeling complex relationships between fine-grained entities in biomedical text.Most existing BEE models rely on classification methods that ignore label semantics and argument dependencies in the data.Although generative models that use prompts are increasingly being used for event extraction, they face two main challenges: creating effective prompts for the biomedical domain and dealing with events with complex structures in the text.To address these limitations, we propose GenBEE, a generative model enhanced with structure-aware prefixes for biomedical event extraction.<span class='px-1 mx-1 bg-yellow-200'>GenBEE constructs event prompts that leverage knowledge distilled from large language models (LLMs), thereby incorporating both label semantics and argument dependency relationships. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span>Additionally, GenBEE introduces a structural prefix learning module that generates structure-aware prefixes with structural prompts, enriching the generation process with structural features.Extensive experiments on three benchmark datasets demonstrate the effectiveness of GenBEE and it achieves state-of-the-art performance on the MLEE and GE11 datasets.Moreover, our analysis shows that the structural prefixes effectively bridge the gap between structural prompts and the representation space of generative models, enabling better integration of event structural information.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06583v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06583v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pragmatic inference of scalar implicature by LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This study investigates how Large Language Models (LLMs), particularly BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic inference of scalar implicature, such as some. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>Two sets of experiments were conducted using cosine similarity and next sentence/token prediction as experimental methods.The results in experiment 1 showed that, both models interpret some as pragmatic implicature not all in the absence of context, aligning with human language processing.In experiment 2, in which Question Under Discussion (QUD) was presented as a contextual cue, BERT showed consistent performance regardless of types of QUDs, while GPT-2 encountered processing difficulties since a certain type of QUD required pragmatic inference for implicature.The findings revealed that, in terms of theoretical approaches, BERT inherently incorporates pragmatic implicature not all within the term some, adhering to Default model (Levinson, 2000).In contrast, GPT-2 seems to encounter processing difficulties in inferring pragmatic implicature within context, consistent with Context-driven model (Sperber and Wilson, 2002).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06673v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06673v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Unlock the Power of Frozen LLMs in Knowledge Graph Completion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Classical knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs).Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, which is ideal for mitigating the limitations of previous methods.Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results.In this work, we aim to leverage LLMs for KGC effectively and efficiently.<span class='px-1 mx-1 bg-yellow-200'>We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC.We also generate entity descriptions with subgraph sampling on KGs, reducing the ambiguity of triplets and enriching the knowledge representation.Extensive experiments on standard benchmarks showcase the efficiency and effectiveness of our approach.We outperform classical KGC methods on most datasets and match the performance of fine-tuned LLMs.Additionally, compared to fine-tuned LLMs, we boost GPU memory efficiency by \textbf{$188\times$} and speed up training+inference by \textbf{$13.48\times$}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06787v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06787v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HLSPilot: LLM-based High-Level Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have catalyzed an upsurge in automatic code generation, garnering significant attention for register transfer level (RTL) code generation.Despite the potential of RTL code generation with natural language, it remains error-prone and limited to relatively small modules because of the substantial semantic gap between natural language expressions and hardware design intent.In response to the limitations, we propose a methodology that reduces the semantic gaps by utilizing C/C++ for generating hardware designs via High-Level Synthesis (HLS) tools.Basically, we build a set of C-to-HLS optimization strategies catering to various code patterns, such as nested loops and local arrays.<span class='px-1 mx-1 bg-yellow-200'>Then, we apply these strategies to sequential C/C++ code through in-context learning, which provides the LLMs with exemplary C/C++ to HLS prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>With this approach, HLS designs can be generated effectively.Since LLMs still face problems in determining the optimized pragma parameters precisely, we have a design space exploration (DSE) tool integrated for pragma parameter tuning.Furthermore, we also employ profiling tools to pinpoint the performance bottlenecks within a program and selectively convert bottleneck components to HLS code for hardware acceleration.By combining the LLM-based profiling, C/C++ to HLS translation, and DSE, we have established HLSPilot, the first LLM-enabled high-level synthesis framework, which can fully automate the high-level application acceleration on hybrid CPU-FPGA architectures.According to our experiments on real-world application benchmarks, HLSPilot achieve comparable performance in general and can even outperform manually crafted counterparts, thereby underscoring the substantial promise of LLM-assisted hardware designs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06810v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06810v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Language Models for Emotion and Behavior Analysis in Education
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences.Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues.<span class='px-1 mx-1 bg-yellow-200'>This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.875</span></span><span class='px-1 mx-1 bg-yellow-200'>Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span><span class='px-1 mx-1 bg-yellow-200'>We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding.<span class='px-1 mx-1 bg-yellow-200'>This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.839</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As large language models (LLMs) continue to scale, their enhanced performance often proves insufficient for solving domain-specific tasks.Systematically analyzing their failures and effectively enhancing their performance remain significant challenges.This paper introduces the Re-TASK framework, a novel theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space Theory.The Re-TASK framework provides a systematic methodology to deepen our understanding, evaluation, and enhancement of LLMs for domain-specific tasks.It explores the interplay among an LLM's capabilities, the knowledge it processes, and the skills it applies, elucidating how these elements are interconnected and impact task performance.Our application of the Re-TASK framework reveals that many failures in domain-specific tasks can be attributed to insufficient knowledge or inadequate skill adaptation.With this insight, we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we identify key capability items associated with tasks and employ a deliberately designed prompting strategy to enhance task performance, thereby reducing the need for extensive fine-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Alternatively, we fine-tune the LLM using capability-specific instructions, further validating the efficacy of our framework.Experimental results confirm the framework's effectiveness, demonstrating substantial improvements in both the performance and applicability of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06904v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06904v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds.We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment.Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits.Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses.<span class='px-1 mx-1 bg-yellow-200'>In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.803</span></span>These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06929v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06929v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "You still have to study" -- On the Security of LLM generated code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We witness an increasing usage of AI-assistants even for routine (classroom) programming tasks.<span class='px-1 mx-1 bg-yellow-200'>However, the code generated on basis of a so called "prompt" by the programmer does not always meet accepted security standards. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.851</span></span>On the one hand, this may be due to lack of best-practice examples in the training data.<span class='px-1 mx-1 bg-yellow-200'>On the other hand, the actual quality of the programmers prompt appears to influence whether generated code contains weaknesses or not. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>In this paper we analyse 4 major LLMs with respect to the security of generated code.We do this on basis of a case study for the Python and Javascript language, using the MITRE CWE catalogue as the guiding security definition.<span class='px-1 mx-1 bg-yellow-200'>Our results show that using different prompting techniques, some LLMs initially generate 65% code which is deemed insecure by a trained security engineer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>On the other hand almost all analysed LLMs will eventually generate code being close to 100% secure with increasing manual guidance of a skilled engineer.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07106v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07106v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span>Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation.Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes.To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm.Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks.We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search.In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search.We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07199v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07199v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Reason? A Characterization via 3-SAT
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) are said to possess advanced reasoning abilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.713</span></span><span class='px-1 mx-1 bg-yellow-200'>However, some skepticism exists as recent works show how LLMs often bypass true reasoning using shortcuts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>Current methods for assessing the reasoning abilities of LLMs typically rely on open-source benchmarks that may be overrepresented in LLM training data, potentially skewing performance.We instead provide a computational theory perspective of reasoning, using 3-SAT -- the prototypical NP-complete problem that lies at the core of logical reasoning and constraint satisfaction tasks.By examining the phase transitions in 3-SAT, we empirically characterize the reasoning abilities of LLMs and show how they vary with the inherent hardness of the problems.Our experimental evidence shows that LLMs cannot perform true reasoning, as is required for solving 3-SAT problems.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07215v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07215v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions.But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns.This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host.We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues.Unlike traditional knowledge distillation, where the "student" model learns directly from the "teacher" model's responses via fine-tuning, our interpretable "strategy" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios.<span class='px-1 mx-1 bg-yellow-200'>This method alternates between a "scenario generation" step and a "strategies for improvement" step, creating a customized library of scenarios and optimized strategies for automated prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span>The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters.In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set.The method's interpretabilty helps safeguard against potential harms through human audit.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Most vulnerability detection studies focus on datasets of vulnerabilities in C/C++ code, offering limited language diversity.Thus, the effectiveness of deep learning methods, including large language models (LLMs), in detecting software vulnerabilities beyond these languages is still largely unexplored.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we evaluate the effectiveness of LLMs in detecting and classifying Common Weakness Enumerations (CWE) using different prompt and role strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>Our experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5- Turbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro) and five programming languages: Python, C, C++, Java, and JavaScript.We compiled a multi-language vulnerability dataset from different sources, to ensure representativeness.Our results showed that GPT-4o achieves the highest vulnerability detection and CWE classification scores using a few-shot setting.Aside from the quantitative results of our study, we developed a library called CODEGUARDIAN integrated with VSCode which enables developers to perform LLM-assisted real-time vulnerability analysis in real-world security scenarios.We have evaluated CODEGUARDIAN with a user study involving 22 developers from the industry.Our study showed that, by using CODEGUARDIAN, developers are more accurate and faster at detecting vulnerabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06428v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06428v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chain-of-Strategy Planning with LLMs: Aligning the Generation of Psychotherapy Dialogue with Strategy in Motivational Interviewing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in large language models (LLMs) have shown promise in generating psychotherapeutic dialogues, especially in Motivational Interviewing (MI).However, how to employ strategies, a set of motivational interviewing (MI) skills, to generate therapeutic-adherent conversations with explainability is underexplored.<span class='px-1 mx-1 bg-yellow-200'>We propose an approach called strategy-aware dialogue generation with Chain-of-Strategy (CoS) planning, which first predicts MI strategies as reasoning and utilizes these strategies to guide the subsequent dialogue generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span>It brings the potential for controllable and explainable generation in psychotherapy by aligning the generated MI dialogues with therapeutic strategies.Extensive experiments including automatic and human evaluations are conducted to validate the effectiveness of the MI strategy.Our findings demonstrate the potential of LLMs in producing strategically aligned dialogues and suggest directions for practical applications in psychotherapeutic settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06527v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06527v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span><span class='px-1 mx-1 bg-yellow-200'>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.869</span></span><span class='px-1 mx-1 bg-yellow-200'>The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span><span class='px-1 mx-1 bg-yellow-200'>The second method involves creating natural language prompts on top of existing Arabic NLP datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.854</span></span>We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TRIZ-GPT: An LLM-augmented method for problem-solving
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>TRIZ, the Theory of Inventive Problem Solving, is derived from a comprehensive analysis of patents across various domains, offering a framework and practical tools for problem-solving.Despite its potential to foster innovative solutions, the complexity and abstractness of TRIZ methodology often make its acquisition and application challenging.This often requires users to have a deep understanding of the theory, as well as substantial practical experience and knowledge across various disciplines.The advent of Large Language Models (LLMs) presents an opportunity to address these challenges by leveraging their extensive knowledge bases and reasoning capabilities for innovative solution generation within TRIZ-based problem-solving process.This study explores and evaluates the application of LLMs within the TRIZ-based problem-solving process.The construction of TRIZ case collections establishes a solid empirical foundation for our experiments and offers valuable resources to the TRIZ community.<span class='px-1 mx-1 bg-yellow-200'>A specifically designed workflow, utilizing step-by-step reasoning and evaluation-validated prompt strategies, effectively transforms concrete problems into TRIZ problems and finally generates inventive solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span>Finally, we present a case study in mechanical engineering field that highlights the practical application of this LLM-augmented method.It showcases GPT-4's ability to generate solutions that closely resonate with original solutions and suggests more implementation mechanisms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05897v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05897v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Med42-v2: A Suite of Clinical LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Med42-v2 introduces a suite of clinical large language models (LLMs) designed to address the limitations of generic models in healthcare settings.These models are built on Llama3 architecture and fine-tuned using specialized clinical data.<span class='px-1 mx-1 bg-yellow-200'>They underwent multi-stage preference alignment to effectively respond to natural prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.742</span></span>While generic models are often preference-aligned to avoid answering clinical queries as a precaution, Med42-v2 is specifically trained to overcome this limitation, enabling its use in clinical settings.Med42-v2 models demonstrate superior performance compared to the original Llama3 models in both 8B and 70B parameter configurations and GPT-4 across various medical benchmarks.These LLMs are developed to understand clinical queries, perform reasoning tasks, and provide valuable assistance in clinical environments.The models are now publicly available at \href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06142v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06142v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Animate, or Inanimate, That is the Question for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding.Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information.Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do?<span class='px-1 mx-1 bg-yellow-200'>We then propose a systematic analysis via prompting approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts.Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies.Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06332v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06332v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conversational Prompt Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompts are how humans communicate with LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.892</span></span><span class='px-1 mx-1 bg-yellow-200'>Informative prompts are essential for guiding LLMs to produce the desired output. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span><span class='px-1 mx-1 bg-yellow-200'>However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.906</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span><span class='px-1 mx-1 bg-yellow-200'>CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction.Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs.<span class='px-1 mx-1 bg-yellow-200'>The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span><span class='px-1 mx-1 bg-yellow-200'>The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04560v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04560v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Robustness Tools in LLM Safety</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images.Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos.One major contributing factor is the absence of datasets in the surgical field.In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far.To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos.The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services.<span class='px-1 mx-1 bg-yellow-200'>It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span>We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks.We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos.We will release our code, model, and the instruction-tuning dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Large Language Model based Personal Information Extraction and Countermeasures
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatically extracting personal information--such as name, phone number, and email address--from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing.Traditional methods--such as regular expression, keyword search, and entity detection--achieve limited success at such personal information extraction.In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures.Towards this goal, we present a framework for LLM-based extraction attacks; collect three datasets including a synthetic dataset generated by GPT-4 and two real-world datasets with manually labeled 8 categories of personal information; introduce a novel mitigation strategy based on \emph{prompt injection}; and systematically benchmark LLM-based attacks and countermeasures using 10 LLMs and our 3 datasets.<span class='px-1 mx-1 bg-yellow-200'>Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms conventional methods at such extraction; and prompt injection can mitigate such risk to a large extent and outperforms conventional countermeasures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>Our code and data are available at: \url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07291v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07291v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Enhanced Static Analysis for Precise Identification of Vulnerable OSS Versions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature.<span class='px-1 mx-1 bg-yellow-200'>However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span>Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules.<span class='px-1 mx-1 bg-yellow-200'>They then use syntactic-level code clone detection to identify the vulnerable versions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection.<span class='px-1 mx-1 bg-yellow-200'>This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>Vercation combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches.<span class='px-1 mx-1 bg-yellow-200'>It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation.On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods.<span class='px-1 mx-1 bg-yellow-200'>More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07321v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07321v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Usefulness of data flow diagrams and large language models for security threat validation: a registered report
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well.<span class='px-1 mx-1 bg-yellow-200'>Threat analysis and risk assessment are used to identify security threats for new or refactored systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis.Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats.We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material.In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design.Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07537v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07537v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction.One field benefiting greatly from these advancements is cybersecurity.In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols.This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems.The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research.The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field.The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles.The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more.<span class='px-1 mx-1 bg-yellow-200'>Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07583v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07583v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI).<span class='px-1 mx-1 bg-yellow-200'>However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span>Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path.To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system.First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval.WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods.Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process.Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates.Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07611v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07611v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness.Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation.Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation.Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods.These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios.Moreover, previous studies do not approach the problem at a suitable scale for real-life applications.<span class='px-1 mx-1 bg-yellow-200'>Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>To address these gaps, we have developed an approach for generating and evaluating more real-life complexity test suites.Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment.In this work, we present \textsc{AgoneTest}: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites.Starting from a state-of-the-art dataset (i.e., \textsc{Methods2Test}), we built a new dataset for comparing human-written tests with those generated by LLMs.Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07846v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07846v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large language models can consistently generate high-quality content for election disinformation operations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advances in large language models have raised concerns about their potential use in generating compelling election disinformation at scale.This study presents a two-part investigation into the capabilities of LLMs to automate stages of an election disinformation operation.<span class='px-1 mx-1 bg-yellow-200'>First, we introduce DisElect, a novel evaluation dataset designed to measure LLM compliance with instructions to generate content for an election disinformation operation in localised UK context, containing 2,200 malicious prompts and 50 benign prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span><span class='px-1 mx-1 bg-yellow-200'>Using DisElect, we test 13 LLMs and find that most models broadly comply with these requests; we also find that the few models which refuse malicious prompts also refuse benign election-related prompts, and are more likely to refuse to generate content from a right-wing perspective. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>Secondly, we conduct a series of experiments (N=2,340) to assess the "humanness" of LLMs: the extent to which disinformation operation content generated by an LLM is able to pass as human-written.<span class='px-1 mx-1 bg-yellow-200'>Our experiments suggest that almost all LLMs tested released since 2022 produce election disinformation operation content indiscernible by human evaluators over 50% of the time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>Notably, we observe that multiple models achieve above-human levels of humanness.Taken together, these findings suggest that current LLMs can be used to generate high-quality content for election disinformation operations, even in hyperlocalised scenarios, at far lower costs than traditional methods, and offer researchers and policymakers an empirical benchmark for the measurement and evaluation of these capabilities in current and future models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06731v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06731v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience.Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services.However, the privacy consequences associated with these services and their third-party plugins are not well understood.Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span>Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services.At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier.<span class='px-1 mx-1 bg-yellow-200'>We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07004v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07004v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "You still have to study" -- On the Security of LLM generated code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We witness an increasing usage of AI-assistants even for routine (classroom) programming tasks.<span class='px-1 mx-1 bg-yellow-200'>However, the code generated on basis of a so called "prompt" by the programmer does not always meet accepted security standards. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>On the one hand, this may be due to lack of best-practice examples in the training data.<span class='px-1 mx-1 bg-yellow-200'>On the other hand, the actual quality of the programmers prompt appears to influence whether generated code contains weaknesses or not. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper we analyse 4 major LLMs with respect to the security of generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>We do this on basis of a case study for the Python and Javascript language, using the MITRE CWE catalogue as the guiding security definition.<span class='px-1 mx-1 bg-yellow-200'>Our results show that using different prompting techniques, some LLMs initially generate 65% code which is deemed insecure by a trained security engineer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.786</span></span><span class='px-1 mx-1 bg-yellow-200'>On the other hand almost all analysed LLMs will eventually generate code being close to 100% secure with increasing manual guidance of a skilled engineer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.713</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07106v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07106v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions.But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns.This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host.We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues.Unlike traditional knowledge distillation, where the "student" model learns directly from the "teacher" model's responses via fine-tuning, our interpretable "strategy" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios.This method alternates between a "scenario generation" step and a "strategies for improvement" step, creating a customized library of scenarios and optimized strategies for automated prompting.The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters.In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set.<span class='px-1 mx-1 bg-yellow-200'>The method's interpretabilty helps safeguard against potential harms through human audit. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Most vulnerability detection studies focus on datasets of vulnerabilities in C/C++ code, offering limited language diversity.<span class='px-1 mx-1 bg-yellow-200'>Thus, the effectiveness of deep learning methods, including large language models (LLMs), in detecting software vulnerabilities beyond these languages is still largely unexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>In this paper, we evaluate the effectiveness of LLMs in detecting and classifying Common Weakness Enumerations (CWE) using different prompt and role strategies.Our experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5- Turbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro) and five programming languages: Python, C, C++, Java, and JavaScript.We compiled a multi-language vulnerability dataset from different sources, to ensure representativeness.Our results showed that GPT-4o achieves the highest vulnerability detection and CWE classification scores using a few-shot setting.<span class='px-1 mx-1 bg-yellow-200'>Aside from the quantitative results of our study, we developed a library called CODEGUARDIAN integrated with VSCode which enables developers to perform LLM-assisted real-time vulnerability analysis in real-world security scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>We have evaluated CODEGUARDIAN with a user study involving 22 developers from the industry.<span class='px-1 mx-1 bg-yellow-200'>Our study showed that, by using CODEGUARDIAN, developers are more accurate and faster at detecting vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06428v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06428v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps).Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic.<span class='px-1 mx-1 bg-yellow-200'>However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp.Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps.This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis.Finally, HYPERION reports the inconsistency based on predefined detection patterns.The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies.We also implement HYPERION to analyze 835 real-world DApps.The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06037v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06037v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Effects of Steering Latent Representation for Large Language Model Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span>Despite its high performance, the underlying cause and explanation remain underexplored.In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses.Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers.Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks.Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs.To resolve this drawback, we propose Adaptive RMU -- a simple yet effective alternative method that makes unlearning effective with most layers.Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we introduce PhishLang, an open-source, lightweight Large Language Model (LLM) specifically designed for phishing website detection through contextual analysis of the website.Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats and deep learning models that are computationally intensive, our model utilizes the advanced language processing capabilities of LLMs to learn granular features that are characteristic of phishing attacks.Furthermore, PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning tools, while being significantly faster and less resource-intensive.<span class='px-1 mx-1 bg-yellow-200'>Over a 3.5-month testing period, PhishLang successfully identified approximately 26K phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to aid current detection measures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span><span class='px-1 mx-1 bg-yellow-200'>We also evaluate PhishLang against several realistic adversarial attacks and develop six patches that make it very robust against such threats. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>Furthermore, we integrate PhishLang with GPT-3.5<span class='px-1 mx-1 bg-yellow-200'>Turbo to create \textit{explainable blocklisting} - warnings that provide users with contextual information about different features that led to a website being marked as phishing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>Finally, we have open-sourced the PhishLang framework and developed a Chromium-based browser extension and URL scanner website, which implement explainable warnings for end-users.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05667v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05667v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses.This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the required skills surpassing their capabilities.<span class='px-1 mx-1 bg-yellow-200'>We first systematically conceptualize infeasible tasks for LLMs, providing formal definitions and categorizations that cover a spectrum of related hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.777</span></span>We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to test multiple LLMs' abilities on task feasibility.Furthermore, we explore the potential of training enhancements to increase LLMs' refusal capabilities with fine-tuning.Experiments validate the effectiveness of our methods, offering promising directions for refining the operational boundaries of LLMs in real applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05873v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05873v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains.However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare).Moreover, certain application-specific scenarios may require fine-tuning these models on private data.<span class='px-1 mx-1 bg-yellow-200'>This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline.These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training.Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs.This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learning to Rewrite: Generalized LLM-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) can be abused at scale to create non-factual content and spread disinformation.Detecting LLM-generated content is essential to mitigate these risks, but current classifiers often fail to generalize in open-world contexts.<span class='px-1 mx-1 bg-yellow-200'>Prior work shows that LLMs tend to rewrite LLM-generated content less frequently, which can be used for detection and naturally generalizes to unforeseen data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>However, we find that the rewriting edit distance between human and LLM content can be indistinguishable across domains, leading to detection failures.We propose training an LLM to rewrite input text, producing minimal edits for LLM-generated content and more edits for human-written text, deriving a distinguishable and generalizable edit distance difference across different domains.Experiments on text from 21 independent domains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that our classifier outperforms the state-of-the-art zero-shot classifier by up to 20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score.Our work suggests that LLM can effectively detect machine-generated text if they are trained properly.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04237v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04237v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs).Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated).<span class='px-1 mx-1 bg-yellow-200'>This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>In this paper, we present $\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished.Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation.This might be useful in some domains like education, where any LLM intervention is usually prohibited.Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains.LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New.The video describing our system is available at https://youtu.be/E8eT_bE7k8c.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04284v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04284v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Explainable Network Intrusion Detection using Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents.However, their applicability to threat detection problems remains unclear.This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explainability.Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS.Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments.Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge.Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS.<span class='px-1 mx-1 bg-yellow-200'>Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span>Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04342v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04342v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Competitive debate is a comprehensive and complex computational argumentation task.<span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) encounter hallucinations and lack competitiveness in this task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate.Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents (Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary.To comprehensively evaluate framework performance, we construct the Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions.We recruite ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans.The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking.Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans.Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As diverse linguistic communities and users adopt large language models (LLMs), assessing their safety across languages becomes critical.<span class='px-1 mx-1 bg-yellow-200'>Despite ongoing efforts to make LLMs safe, they can still be made to behave unsafely with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span>Research on LLM safety and jailbreaking, however, has so far mostly focused on English, limiting our understanding of LLM safety in other languages.We contribute towards closing this gap by investigating the effectiveness of many-shot jailbreaking, where models are prompted with unsafe demonstrations to induce unsafe behaviour, in Italian.To enable our analysis, we create a new dataset of unsafe Italian question-answer pairs.With this dataset, we identify clear safety vulnerabilities in four families of open-weight LLMs.<span class='px-1 mx-1 bg-yellow-200'>We find that the models exhibit unsafe behaviors even when prompted with few unsafe demonstrations, and -- more alarmingly -- that this tendency rapidly escalates with more demonstrations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04522v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04522v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learning Fine-Grained Grounded Citations for Attributed Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span><span class='px-1 mx-1 bg-yellow-200'>Attributed LLMs, which augment generated text with in-line citations, have shown potential in mitigating hallucinations and improving verifiability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning.Furthermore, the practice of citing only coarse document identifiers makes it challenging for users to perform fine-grained verification.In this work, we introduce FRONT, a training framework designed to teach LLMs to generate Fine-Grained Grounded Citations.By grounding model outputs in fine-grained supporting quotes, these quotes guide the generation of grounded and consistent responses, not only improving citation quality but also facilitating fine-grained verification.Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in generating superior grounded responses and highly supportive citations.With LLaMA-2-7B, the framework significantly outperforms all the baselines, achieving an average of 14.21% improvement in citation quality across all datasets, even surpassing ChatGPT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04568v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04568v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence.These models can process multi-modal prompts, enabling them to generate more context-aware responses.However, this integration is not without challenges.<span class='px-1 mx-1 bg-yellow-200'>One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span>These tasks require precise and reliable responses to ensure safe and effective operation.Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously.<span class='px-1 mx-1 bg-yellow-200'>For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03515v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03515v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RepoMasterEval: Evaluating Code Completion via Real-World Repositories
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical.However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model.By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block.These limitations makes the evaluation poorly align with the practical scenarios of code completion tools.In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories.Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites.<span class='px-1 mx-1 bg-yellow-200'>To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios.The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03519v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03519v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EnJa: Ensemble Jailbreak on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As Large Language Models (LLMs) are increasingly being deployed in safety-critical applications, their vulnerability to potential jailbreaks -- malicious prompts that can disable the safety mechanism of LLMs -- has attracted growing research attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span><span class='px-1 mx-1 bg-yellow-200'>While alignment methods have been proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can still be jailbroken by carefully crafted malicious prompts, producing content that violates policy regulations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span>Existing jailbreak attacks on LLMs can be categorized into prompt-level methods which make up stories/logic to circumvent safety alignment and token-level attack methods which leverage gradient methods to find adversarial tokens.In this work, we introduce the concept of Ensemble Jailbreak and explore methods that can integrate prompt-level and token-level jailbreak into a more powerful hybrid jailbreak attack.Specifically, we propose a novel EnJa attack to hide harmful instructions using prompt-level jailbreak, boost the attack success rate using a gradient-based attack, and connect the two types of jailbreak attacks via a template-based connector.We evaluate the effectiveness of EnJa on several aligned models and show that it achieves a state-of-the-art attack success rate with fewer queries and is much stronger than any individual jailbreak.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03603v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03603v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>WalledEval is a comprehensive AI safety testing toolkit designed to evaluate large language models (LLMs).It accommodates a diverse range of models, including both open-weight and API-based ones, and features over 35 safety benchmarks covering areas such as multilingual safety, exaggerated safety, and prompt injections.The framework supports both LLM and judge benchmarking, and incorporates custom mutators to test safety against various text-style mutations such as future tense and paraphrasing.<span class='px-1 mx-1 bg-yellow-200'>Additionally, WalledEval introduces WalledGuard, a new, small and performant content moderation tool, and SGXSTest, a benchmark for assessing exaggerated safety in cultural contexts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>We make WalledEval publicly available at https://github.com/walledai/walledevalA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03837v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03837v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization.These complex tasks require AI to continuously summarize and improve.Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference.This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing.We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy.Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example.Note that as the memories continue to grow, this cumulative improvement may be substantial.<span class='px-1 mx-1 bg-yellow-200'>The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03841v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03841v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) can help writers build story worlds by generating world elements, such as factions, characters, and locations.However, making sense of many generated elements can be overwhelming.Moreover, if the user wants to precisely control aspects of generated elements that are difficult to specify verbally, prompting alone may be insufficient.We introduce Patchview, a customizable LLM-powered system that visually aids worldbuilding by allowing users to interact with story concepts and elements through the physical metaphor of magnets and dust.Elements in Patchview are visually dragged closer to concepts with high relevance, facilitating sensemaking.The user can also steer the generation with verbally elusive concepts by indicating the desired position of the element between concepts.<span class='px-1 mx-1 bg-yellow-200'>When the user disagrees with the LLM's visualization and generation, they can correct those by repositioning the element. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>These corrections can be used to align the LLM's future behaviors to the user's perception.With a user study, we show that Patchview supports the sensemaking of world elements and steering of element generation, facilitating exploration during the worldbuilding process.Patchview provides insights on how customizable visual representation can help sensemake, steer, and align generative AI model behaviors with the user's intentions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring RAG-based Vulnerability Augmentation with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Detecting vulnerabilities is a crucial task for maintaining the integrity, availability, and security of software systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span><span class='px-1 mx-1 bg-yellow-200'>Utilizing DL-based models for vulnerability detection has become commonplace in recent years. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span>However, such deep learning-based vulnerability detectors (DLVD) suffer from a shortage of sizable datasets to train effectively.Data augmentation can potentially alleviate the shortage of data, but augmenting vulnerable code is challenging and requires designing a generative solution that maintains vulnerability.Hence, the work on generating vulnerable code samples has been limited and previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities.Lately, large language models (LLMs) are being used for solving various code generation and comprehension tasks and have shown inspiring results, especially when fused with retrieval augmented generation (RAG).In this study, we explore three different strategies to augment vulnerabilities both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension.We conducted an extensive evaluation of our proposed approach on three vulnerability datasets and three DLVD models, using two LLMs.Our results show that our injection-based clustering-enhanced RAG method beats the baseline setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling (ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with 15K generated vulnerable samples.Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04125v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04125v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Security Challenges in LLM Development</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KGV: Integrating Large Language Models with Knowledge Graphs for Cyber Threat Intelligence Credibility Assessment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Cyber threat intelligence is a critical tool that many organizations and individuals use to protect themselves from sophisticated, organized, persistent, and weaponized cyber attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>However, few studies have focused on the quality assessment of threat intelligence provided by intelligence platforms, and this work still requires manual analysis by cybersecurity experts.In this paper, we propose a knowledge graph-based verifier, a novel Cyber Threat Intelligence (CTI) quality assessment framework that combines knowledge graphs and Large Language Models (LLMs).Our approach introduces LLMs to automatically extract OSCTI key claims to be verified and utilizes a knowledge graph consisting of paragraphs for fact-checking.This method differs from the traditional way of constructing complex knowledge graphs with entities as nodes.By constructing knowledge graphs with paragraphs as nodes and semantic similarity as edges, it effectively enhances the semantic understanding ability of the model and simplifies labeling requirements.Additionally, to fill the gap in the research field, we created and made public the first dataset for threat intelligence assessment from heterogeneous sources.To the best of our knowledge, this work is the first to create a dataset on threat intelligence reliability verification, providing a reference for future research.Experimental results show that KGV (Knowledge Graph Verifier) significantly improves the performance of LLMs in intelligence quality assessment.Compared with traditional methods, we reduce a large amount of data annotation while the model still exhibits strong reasoning capabilities.<span class='px-1 mx-1 bg-yellow-200'>Finally, our method can achieve XXX accuracy in network threat assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08088v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08088v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) exhibit high inference latency due to their autoregressive decoding nature.While the draft head in speculative decoding mitigates this issue, its full potential remains unexplored.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an orthogonal approach to the draft head. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>By transforming the conventional single-layer draft head into a multi-layer architecture and incorporating adversarial learning into the traditional supervised training, KOALA significantly improves the accuracy of the draft head in predicting subsequent tokens, thus more closely mirroring the functionality of LLMs.Although this improvement comes at the cost of slightly increased drafting overhead, KOALA substantially unlocks the draft head's potential, greatly enhancing speculative decoding.We conducted comprehensive evaluations of KOALA, including both autoregressive and non-autoregressive draft heads across various tasks, demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is 10.57%-14.09% faster than the original draft heads.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08146v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08146v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Seeing and Understanding: Bridging Vision with Chemical Knowledge Via ChemVLM
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this technical report, we propose ChemVLM, the first open-source multimodal large language model dedicated to the fields of chemistry, designed to address the incompatibility between chemical image understanding and text analysis.Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as the foundational large model, endowing our model with robust capabilities in understanding and utilizing chemical text knowledge.Additionally, we employ InternVIT-6B as a powerful image encoder.We have curated high-quality data from the chemical domain, including molecules, reaction formulas, and chemistry examination data, and compiled these into a bilingual multimodal question-answering dataset.We test the performance of our model on multiple open-source benchmarks and three custom evaluation sets.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that our model achieves excellent performance, securing state-of-the-art results in five out of six involved tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.639</span></span>Our model can be found at https://huggingface.co/AI4Chem/ChemVLM-26B.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07246v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07246v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Large Language Model based Personal Information Extraction and Countermeasures
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatically extracting personal information--such as name, phone number, and email address--from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing.Traditional methods--such as regular expression, keyword search, and entity detection--achieve limited success at such personal information extraction.In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures.<span class='px-1 mx-1 bg-yellow-200'>Towards this goal, we present a framework for LLM-based extraction attacks; collect three datasets including a synthetic dataset generated by GPT-4 and two real-world datasets with manually labeled 8 categories of personal information; introduce a novel mitigation strategy based on \emph{prompt injection}; and systematically benchmark LLM-based attacks and countermeasures using 10 LLMs and our 3 datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.794</span></span><span class='px-1 mx-1 bg-yellow-200'>Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms conventional methods at such extraction; and prompt injection can mitigate such risk to a large extent and outperforms conventional countermeasures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>Our code and data are available at: \url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07291v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07291v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Enhanced Static Analysis for Precise Identification of Vulnerable OSS Versions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature.<span class='px-1 mx-1 bg-yellow-200'>However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.851</span></span><span class='px-1 mx-1 bg-yellow-200'>Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span><span class='px-1 mx-1 bg-yellow-200'>They then use syntactic-level code clone detection to identify the vulnerable versions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span><span class='px-1 mx-1 bg-yellow-200'>These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span><span class='px-1 mx-1 bg-yellow-200'>Vercation combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code.<span class='px-1 mx-1 bg-yellow-200'>We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span><span class='px-1 mx-1 bg-yellow-200'>We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span>On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods.More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07321v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07321v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learning-based Models for Vulnerability Detection: An Extensive Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Though many deep learning-based models have made great progress in vulnerability detection, we have no good understanding of these models, which limits the further advancement of model capability, understanding of the mechanism of model detection, and efficiency and safety of practical application of models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span>In this paper, we extensively and comprehensively investigate two types of state-of-the-art learning-based approaches (sequence-based and graph-based) by conducting experiments on a recently built large-scale dataset.We investigate seven research questions from five dimensions, namely model capabilities, model interpretation, model stability, ease of use of model, and model economy.We experimentally demonstrate the priority of sequence-based models and the limited abilities of both LLM (ChatGPT) and graph-based models.<span class='px-1 mx-1 bg-yellow-200'>We explore the types of vulnerability that learning-based models skilled in and reveal the instability of the models though the input is subtlely semantical-equivalently changed. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.85</span></span>We empirically explain what the models have learned.We summarize the pre-processing as well as requirements for easily using the models.Finally, we initially induce the vital information for economically and safely practical usage of these models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07526v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07526v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Usefulness of data flow diagrams and large language models for security threat validation: a registered report
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span><span class='px-1 mx-1 bg-yellow-200'>Threat analysis and risk assessment are used to identify security threats for new or refactored systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span>Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis.<span class='px-1 mx-1 bg-yellow-200'>Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material.In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design.Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07537v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07537v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large language models can consistently generate high-quality content for election disinformation operations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advances in large language models have raised concerns about their potential use in generating compelling election disinformation at scale.This study presents a two-part investigation into the capabilities of LLMs to automate stages of an election disinformation operation.First, we introduce DisElect, a novel evaluation dataset designed to measure LLM compliance with instructions to generate content for an election disinformation operation in localised UK context, containing 2,200 malicious prompts and 50 benign prompts.<span class='px-1 mx-1 bg-yellow-200'>Using DisElect, we test 13 LLMs and find that most models broadly comply with these requests; we also find that the few models which refuse malicious prompts also refuse benign election-related prompts, and are more likely to refuse to generate content from a right-wing perspective. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>Secondly, we conduct a series of experiments (N=2,340) to assess the "humanness" of LLMs: the extent to which disinformation operation content generated by an LLM is able to pass as human-written.Our experiments suggest that almost all LLMs tested released since 2022 produce election disinformation operation content indiscernible by human evaluators over 50% of the time.Notably, we observe that multiple models achieve above-human levels of humanness.Taken together, these findings suggest that current LLMs can be used to generate high-quality content for election disinformation operations, even in hyperlocalised scenarios, at far lower costs than traditional methods, and offer researchers and policymakers an empirical benchmark for the measurement and evaluation of these capabilities in current and future models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06731v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06731v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience.Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services.However, the privacy consequences associated with these services and their third-party plugins are not well understood.Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services.At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier.We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07004v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07004v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "You still have to study" -- On the Security of LLM generated code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We witness an increasing usage of AI-assistants even for routine (classroom) programming tasks.<span class='px-1 mx-1 bg-yellow-200'>However, the code generated on basis of a so called "prompt" by the programmer does not always meet accepted security standards. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>On the one hand, this may be due to lack of best-practice examples in the training data.<span class='px-1 mx-1 bg-yellow-200'>On the other hand, the actual quality of the programmers prompt appears to influence whether generated code contains weaknesses or not. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper we analyse 4 major LLMs with respect to the security of generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span><span class='px-1 mx-1 bg-yellow-200'>We do this on basis of a case study for the Python and Javascript language, using the MITRE CWE catalogue as the guiding security definition. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that using different prompting techniques, some LLMs initially generate 65% code which is deemed insecure by a trained security engineer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span><span class='px-1 mx-1 bg-yellow-200'>On the other hand almost all analysed LLMs will eventually generate code being close to 100% secure with increasing manual guidance of a skilled engineer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.711</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07106v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07106v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Most vulnerability detection studies focus on datasets of vulnerabilities in C/C++ code, offering limited language diversity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span><span class='px-1 mx-1 bg-yellow-200'>Thus, the effectiveness of deep learning methods, including large language models (LLMs), in detecting software vulnerabilities beyond these languages is still largely unexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span>In this paper, we evaluate the effectiveness of LLMs in detecting and classifying Common Weakness Enumerations (CWE) using different prompt and role strategies.Our experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5- Turbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro) and five programming languages: Python, C, C++, Java, and JavaScript.<span class='px-1 mx-1 bg-yellow-200'>We compiled a multi-language vulnerability dataset from different sources, to ensure representativeness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results showed that GPT-4o achieves the highest vulnerability detection and CWE classification scores using a few-shot setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span><span class='px-1 mx-1 bg-yellow-200'>Aside from the quantitative results of our study, we developed a library called CODEGUARDIAN integrated with VSCode which enables developers to perform LLM-assisted real-time vulnerability analysis in real-world security scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span>We have evaluated CODEGUARDIAN with a user study involving 22 developers from the industry.<span class='px-1 mx-1 bg-yellow-200'>Our study showed that, by using CODEGUARDIAN, developers are more accurate and faster at detecting vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.796</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06428v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06428v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multimodal Large Language Models for Phishing Webpage Detection and Identification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>To address the challenging problem of detecting phishing webpages, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms.Among these, brand-based phishing detection that uses models from Computer Vision to detect if a given webpage is imitating a well-known brand has received widespread attention.However, such models are costly and difficult to maintain, as they need to be retrained with labeled dataset that has to be regularly and continuously collected.Besides, they also need to maintain a good reference list of well-known websites and related meta-data for effective performance.   In this work, we take steps to study the efficacy of large language models (LLMs), in particular the multimodal LLMs, in detecting phishing webpages.Given that the LLMs are pretrained on a large corpus of data, we aim to make use of their understanding of different aspects of a webpage (logo, theme, favicon, etc.) to identify the brand of a given webpage and compare the identified brand with the domain name in the URL to detect a phishing attack.We propose a two-phase system employing LLMs in both phases: the first phase focuses on brand identification, while the second verifies the domain.We carry out comprehensive evaluations on a newly collected dataset.Our experiments show that the LLM-based system achieves a high detection rate at high precision; importantly, it also provides interpretable evidence for the decisions.<span class='px-1 mx-1 bg-yellow-200'>Our system also performs significantly better than a state-of-the-art brand-based phishing detection system while demonstrating robustness against two known adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05941v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05941v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On Effects of Steering Latent Representation for Large Language Model Unlearning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Representation Misdirection for Unlearning (RMU), which steers model representation in the intermediate layer to a target random representation, is an effective method for large language model (LLM) unlearning.Despite its high performance, the underlying cause and explanation remain underexplored.In this paper, we first theoretically demonstrate that steering forget representations in the intermediate layer reduces token confidence, causing LLMs to generate wrong or nonsense responses.Second, we investigate how the coefficient influences the alignment of forget-sample representations with the random direction and hint at the optimal coefficient values for effective unlearning across different network layers.<span class='px-1 mx-1 bg-yellow-200'>Third, we show that RMU unlearned models are robust against adversarial jailbreak attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.833</span></span>Last, our empirical analysis shows that RMU is less effective when applied to the middle and later layers in LLMs.To resolve this drawback, we propose Adaptive RMU -- a simple yet effective alternative method that makes unlearning effective with most layers.Extensive experiments demonstrate that Adaptive RMU significantly improves the unlearning performance compared to prior art while incurring no additional computational cost.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the constantly evolving field of cybersecurity, it is imperative for analysts to stay abreast of the latest attack trends and pertinent information that aids in the investigation and attribution of cyber-attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we introduce the first question-answering (QA) model and its application that provides information to the cybersecurity experts about cyber-attacks investigations and attribution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>Our QA model is based on Retrieval Augmented Generation (RAG) techniques together with a Large Language Model (LLM) and provides answers to the users' queries based on either our knowledge base (KB) that contains curated information about cyber-attacks investigations and attribution or on outside resources provided by the users.We have tested and evaluated our QA model with various types of questions, including KB-based, metadata-based, specific documents from the KB, and external sources-based questions.We compared the answers for KB-based questions with those from OpenAI's GPT-3.5 and the latest GPT-4o LLMs.Our proposed QA model outperforms OpenAI's GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models, which is critical for cyber-attack investigation and attribution.Additionally, our analysis showed that when the RAG QA model is given few-shot examples rather than zero-shot instructions, it generates better answers compared to cases where no examples are supplied in addition to the query.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06272v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06272v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we introduce PhishLang, an open-source, lightweight Large Language Model (LLM) specifically designed for phishing website detection through contextual analysis of the website.<span class='px-1 mx-1 bg-yellow-200'>Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats and deep learning models that are computationally intensive, our model utilizes the advanced language processing capabilities of LLMs to learn granular features that are characteristic of phishing attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span>Furthermore, PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning tools, while being significantly faster and less resource-intensive.Over a 3.5-month testing period, PhishLang successfully identified approximately 26K phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to aid current detection measures.<span class='px-1 mx-1 bg-yellow-200'>We also evaluate PhishLang against several realistic adversarial attacks and develop six patches that make it very robust against such threats. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.877</span></span>Furthermore, we integrate PhishLang with GPT-3.5<span class='px-1 mx-1 bg-yellow-200'>Turbo to create \textit{explainable blocklisting} - warnings that provide users with contextual information about different features that led to a website being marked as phishing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span><span class='px-1 mx-1 bg-yellow-200'>Finally, we have open-sourced the PhishLang framework and developed a Chromium-based browser extension and URL scanner website, which implement explainable warnings for end-users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.613</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05667v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05667v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Retriever Augmented Large Language Models for Attack Graph Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the complexity of modern systems increases, so does the importance of assessing their security posture through effective vulnerability management and threat modeling techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span><span class='px-1 mx-1 bg-yellow-200'>One powerful tool in the arsenal of cybersecurity professionals is the attack graph, a representation of all potential attack paths within a system that an adversary might exploit to achieve a certain objective. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>Traditional methods of generating attack graphs involve expert knowledge, manual curation, and computational algorithms that might not cover the entire threat landscape due to the ever-evolving nature of vulnerabilities and exploits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper explores the approach of leveraging large language models (LLMs), such as ChatGPT, to automate the generation of attack graphs by intelligently chaining Common Vulnerabilities and Exposures (CVEs) based on their preconditions and effects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.845</span></span><span class='px-1 mx-1 bg-yellow-200'>It also shows how to utilize LLMs to create attack graphs from threat reports. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05855v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05855v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Based Robust Product Classification in Commerce and Compliance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Product classification is a crucial task in international trade, as compliance regulations are verified and taxes and duties are applied based on product categories.Manual classification of products is time-consuming and error-prone, and the sheer volume of products imported and exported renders the manual process infeasible.Consequently, e-commerce platforms and enterprises involved in international trade have turned to automatic product classification using machine learning.However, current approaches do not consider the real-world challenges associated with product classification, such as very abbreviated and incomplete product descriptions.In addition, recent advancements in generative Large Language Models (LLMs) and their reasoning capabilities are mainly untapped in product classification and e-commerce.In this research, we explore the real-life challenges of industrial classification and we propose data perturbations that allow for realistic data simulation.Furthermore, we employ LLM-based product classification to improve the robustness of the prediction in presence of incomplete data.Our research shows that LLMs with in-context learning outperform the supervised approaches in the clean-data scenario.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we illustrate that LLMs are significantly more robust than the supervised approaches when data attacks are present. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-10</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains.However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare).Moreover, certain application-specific scenarios may require fine-tuning these models on private data.<span class='px-1 mx-1 bg-yellow-200'>This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span><span class='px-1 mx-1 bg-yellow-200'>We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.699</span></span>These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training.Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs.This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the increasing demand for practical applications of Large Language Models (LLMs), many attention-efficient models have been developed to balance performance and computational cost.<span class='px-1 mx-1 bg-yellow-200'>However, the adversarial robustness of these models remains under-explored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we design a framework to investigate the trade-off between efficiency, performance, and adversarial robustness of LLMs by comparing three prominent models with varying levels of complexity and efficiency -- Transformer++, Gated Linear Attention (GLA) Transformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span><span class='px-1 mx-1 bg-yellow-200'>The AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to challenge model robustness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span><span class='px-1 mx-1 bg-yellow-200'>Our results show that while the GLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate higher efficiency and either superior or comparative robustness on AdvGLUE tasks compared to Transformer++ across different attack levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span><span class='px-1 mx-1 bg-yellow-200'>These findings highlight the potential of simplified architectures to achieve a compelling balance between efficiency, performance, and adversarial robustness, offering valuable insights for applications where resource constraints and resilience to adversarial attacks are critical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04585v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04585v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">HCI in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates.With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt.<span class='px-1 mx-1 bg-yellow-200'>This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span><span class='px-1 mx-1 bg-yellow-200'>We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs.Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS.Our work not only enhances understanding of stigma's manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD.Code and data will be made publicly available upon acceptance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07873v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07873v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Assessing Language Models' Worldview for Fiction Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The use of Large Language Models (LLMs) has become ubiquitous, with abundant applications in computational creativity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span>One such application is fictional story generation.Fiction is a narrative that occurs in a story world that is slightly different than ours.With LLMs becoming writing partners, we question how suitable they are to generate fiction.This study investigates the ability of LLMs to maintain a state of world essential to generate fiction.Through a series of questions to nine LLMs, we find that only two models exhibit consistent worldview, while the rest are self-conflicting.Subsequent analysis of stories generated by four models revealed a strikingly uniform narrative pattern.This uniformity across models further suggests a lack of `state' necessary for fiction.We highlight the limitations of current LLMs in fiction writing and advocate for future research to test and create story worlds for LLMs to reside in.All code, dataset, and the generated responses can be found in https://github.com/tanny411/llm-reliability-and-consistency-evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07904v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07904v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The performance of ChatGPT\copyright{} and other LLMs has improved tremendously, and in online environments, they are increasingly likely to be used in a wide variety of situations, such as ChatBot on web pages, call center operations using voice interaction, and dialogue functions using agents.In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots.In this multimodal dialogue, mutual emotion recognition between the AI and the user will become important.So far, there have been methods for expressing emotions on the part of the AI agent or for recognizing them using textual or voice information of the user's utterances, but methods for AI agents to recognize emotions from the user's facial expressions have not been studied.<span class='px-1 mx-1 bg-yellow-200'>In this study, we examined whether or not LLM-based AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.873</span></span><span class='px-1 mx-1 bg-yellow-200'>The results confirmed that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07982v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07982v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools.This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry.<span class='px-1 mx-1 bg-yellow-200'>To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software.Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality.Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework.The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input.<span class='px-1 mx-1 bg-yellow-200'>Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08054v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08054v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although Large Language Models(LLMs) can generate coherent and contextually relevant text, they often struggle to recognise the intent behind the human user's query.Natural Language Understanding (NLU) models, however, interpret the purpose and key information of user's input to enable responsive interactions.Existing NLU models generally map individual utterances to a dual-level semantic frame, involving sentence-level intent and word-level slot labels.<span class='px-1 mx-1 bg-yellow-200'>However, real-life conversations primarily consist of multi-turn conversations, involving the interpretation of complex and extended dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>Researchers encounter challenges addressing all facets of multi-turn dialogue conversations using a unified single NLU model.This paper introduces a novel approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge distillation for multi-turn NLU.To achieve this, we construct distinct teachers for varying levels of conversation knowledge, namely, sentence-level intent detection, word-level slot filling, and conversation-level domain classification.These teachers are then fine-tuned to acquire specific knowledge of their designated levels.A multi-teacher loss is proposed to facilitate the combination of these multi-level teachers, guiding a student model in multi-turn dialogue tasks.The experimental results demonstrate the efficacy of our model in improving the overall multi-turn conversation understanding, showcasing the potential for advancements in NLU models through the incorporation of multi-level dialogue knowledge distillation techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08144v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08144v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EmBARDiment: an Embodied AI Agent for Productivity in XR
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios.However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query.We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment.This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.<span class='px-1 mx-1 bg-yellow-200'>Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.769</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08158v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08158v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span><span class='px-1 mx-1 bg-yellow-200'>Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data.Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting.Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text).The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment.Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential.Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07313v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07313v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LPU: A Latency-Optimized and Highly Scalable Processor for Large Language Model Inference
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The explosive arrival of OpenAI's ChatGPT has fueled the globalization of large language model (LLM), which consists of billions of pretrained parameters that embodies the aspects of syntax and semantics. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>HyperAccel introduces latency processing unit (LPU), a latency-optimized and highly scalable processor architecture for the acceleration of LLM inference.LPU perfectly balances the memory bandwidth and compute logic with streamlined dataflow to maximize performance and efficiency.LPU is equipped with expandable synchronization link (ESL) that hides data synchronization latency between multiple LPUs.HyperDex complements LPU as an intuitive software framework to run LLM applications.LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and 66B model, respectively, which is 2.09x and 1.37x faster than the GPU.LPU, synthesized using Samsung 4nm process, has total area of 0.824 mm2 and power consumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy efficiency over NVIDIA H100 and L4 servers, respectively.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07326v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07326v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech.However, these models often exhibit biases due to the nature of their training data.Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases.<span class='px-1 mx-1 bg-yellow-200'>This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases.Our experiments reveal significant insights into their performance and bias levels.The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07665v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07665v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are known for their remarkable ability to generate synthesized 'knowledge', such as text documents, music, images, etc.<span class='px-1 mx-1 bg-yellow-200'>However, there is a huge gap between LLM's and human capabilities for understanding abstract concepts and reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>We discuss these issues in a larger philosophical context of human knowledge acquisition and the Turing test.In addition, we illustrate the limitations of LLMs by analyzing GPT-4 responses to questions ranging from science and math to common sense reasoning.These examples show that GPT-4 can often imitate human reasoning, even though it lacks understanding.However, LLM responses are synthesized from a large LLM model trained on all available data.In contrast, human understanding is based on a small number of abstract concepts.Based on this distinction, we discuss the impact of LLMs on acquisition of human knowledge and education.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06598v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06598v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study on Bar Charts with Varying Layouts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span><span class='px-1 mx-1 bg-yellow-200'>Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study.<span class='px-1 mx-1 bg-yellow-200'>We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span>In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies.We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways.In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings.Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans.<span class='px-1 mx-1 bg-yellow-200'>In Experiment 3, we examined the effect of chart context and data on LLM takeaways. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span><span class='px-1 mx-1 bg-yellow-200'>We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span><span class='px-1 mx-1 bg-yellow-200'>Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06837v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06837v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DracoGPT: Extracting Visualization Design Preferences from Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices.However, if they fail to do so, they might provide unreliable visualization recommendations.<span class='px-1 mx-1 bg-yellow-200'>What visualization design preferences, then, have LLMs learned? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span><span class='px-1 mx-1 bg-yellow-200'>We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span>To assess varied tasks, we develop two pipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to either rank or recommend visual encoding specifications.We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research.We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints.Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments.Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06845v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06845v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Language Models for Emotion and Behavior Analysis in Education
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span>Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues.This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students.<span class='px-1 mx-1 bg-yellow-200'>Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span><span class='px-1 mx-1 bg-yellow-200'>We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span>Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding.<span class='px-1 mx-1 bg-yellow-200'>This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span><span class='px-1 mx-1 bg-yellow-200'>We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits.Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses.In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance.These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06929v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06929v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLMs can Schedule
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The job shop scheduling problem (JSSP) remains a significant hurdle in optimizing production processes.This challenge involves efficiently allocating jobs to a limited number of machines while minimizing factors like total processing time or job delays.<span class='px-1 mx-1 bg-yellow-200'>While recent advancements in artificial intelligence have yielded promising solutions, such as reinforcement learning and graph neural networks, this paper explores the potential of Large Language Models (LLMs) for JSSP. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span>We introduce the very first supervised 120k dataset specifically designed to train LLMs for JSSP.Surprisingly, our findings demonstrate that LLM-based scheduling can achieve performance comparable to other neural approaches.Furthermore, we propose a sampling method that enhances the effectiveness of LLMs in tackling JSSP.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06993v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06993v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.666</span></span>Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation.Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes.To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm.Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks.We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search.In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search.We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07199v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07199v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Neural embedding of beliefs reveals the role of relative dissonance in human decision-making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Beliefs serve as the foundation for human cognition and decision-making.They guide individuals in deriving meaning from their lives, shaping their behaviors, and forming social connections.Therefore, a model that encapsulates beliefs and their interrelationships is crucial for quantitatively studying the influence of beliefs on our actions.<span class='px-1 mx-1 bg-yellow-200'>Despite its importance, research on the interplay between human beliefs has often been limited to a small set of beliefs pertaining to specific issues, with a heavy reliance on surveys or experiments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span>Here, we propose a method for extracting nuanced relations between thousands of beliefs by leveraging large-scale user participation data from an online debate platform and mapping these beliefs to an embedding space using a fine-tuned large language model (LLM).This belief embedding space effectively encapsulates the interconnectedness of diverse beliefs as well as polarization across various social issues.We discover that the positions within this belief space predict new beliefs of individuals.Furthermore, we find that the relative distance between one's existing beliefs and new beliefs can serve as a quantitative estimate of cognitive dissonance, allowing us to predict new beliefs.<span class='px-1 mx-1 bg-yellow-200'>Our study highlights how modern LLMs, when combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human belief formation and decision-making processes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07237v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07237v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions.But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns.This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host.<span class='px-1 mx-1 bg-yellow-200'>We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>Unlike traditional knowledge distillation, where the "student" model learns directly from the "teacher" model's responses via fine-tuning, our interpretable "strategy" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios.This method alternates between a "scenario generation" step and a "strategies for improvement" step, creating a customized library of scenarios and optimized strategies for automated prompting.The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters.In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set.The method's interpretabilty helps safeguard against potential harms through human audit.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chain-of-Strategy Planning with LLMs: Aligning the Generation of Psychotherapy Dialogue with Strategy in Motivational Interviewing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in large language models (LLMs) have shown promise in generating psychotherapeutic dialogues, especially in Motivational Interviewing (MI). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.873</span></span><span class='px-1 mx-1 bg-yellow-200'>However, how to employ strategies, a set of motivational interviewing (MI) skills, to generate therapeutic-adherent conversations with explainability is underexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span><span class='px-1 mx-1 bg-yellow-200'>We propose an approach called strategy-aware dialogue generation with Chain-of-Strategy (CoS) planning, which first predicts MI strategies as reasoning and utilizes these strategies to guide the subsequent dialogue generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span><span class='px-1 mx-1 bg-yellow-200'>It brings the potential for controllable and explainable generation in psychotherapy by aligning the generated MI dialogues with therapeutic strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>Extensive experiments including automatic and human evaluations are conducted to validate the effectiveness of the MI strategy.<span class='px-1 mx-1 bg-yellow-200'>Our findings demonstrate the potential of LLMs in producing strategically aligned dialogues and suggest directions for practical applications in psychotherapeutic settings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06527v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06527v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) and conversational assistants necessitates dynamic, scalable, and configurable conversational datasets for training and evaluation.These datasets must accommodate diverse user interaction modes, including text and voice, each presenting unique modeling challenges.Knowledge Graphs (KGs), with their structured and evolving nature, offer an ideal foundation for current and precise knowledge.Although human-curated KG-based conversational datasets exist, they struggle to keep pace with the rapidly changing user information needs.We present ConvKGYarn, a scalable method for generating up-to-date and configurable conversational KGQA datasets.<span class='px-1 mx-1 bg-yellow-200'>Qualitative psychometric analyses confirm our method can generate high-quality datasets rivaling a popular conversational KGQA dataset while offering it at scale and covering a wide range of human-interaction configurations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.887</span></span>We showcase its utility by testing LLMs on diverse conversations - exploring model behavior on conversational KGQA sets with different configurations grounded in the same KG fact set.Our results highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate parametric knowledge of LLMs, thus offering a robust solution to the constantly evolving landscape of conversational assistants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05948v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05948v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Animate, or Inanimate, That is the Question for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The cognitive essence of humans is deeply intertwined with the concept of animacy, which plays an essential role in shaping their memory, vision, and multi-layered language understanding.Although animacy appears in language via nuanced constraints on verbs and adjectives, it is also learned and refined through extralinguistic information.Similarly, we assume that the LLMs' limited abilities to understand natural language when processing animacy are motivated by the fact that these models are trained exclusively on text.   Hence, the question this paper aims to answer arises: can LLMs, in their digital wisdom, process animacy in a similar way to what humans would do?We then propose a systematic analysis via prompting approaches.In particular, we probe different LLMs by prompting them using animate, inanimate, usual, and stranger contexts.<span class='px-1 mx-1 bg-yellow-200'>Results reveal that, although LLMs have been trained predominantly on textual data, they exhibit human-like behavior when faced with typical animate and inanimate entities in alignment with earlier studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Hence, LLMs can adapt to understand unconventional situations by recognizing oddities as animated without needing to interface with unspoken cognitive triggers humans rely on to break down animations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06332v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06332v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Recognizing Emotion Regulation Strategies from Human Behavior with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Human emotions are often not expressed directly, but regulated according to internal processes and social display rules. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span>For affective computing systems, an understanding of how users regulate their emotions can be highly useful, for example to provide feedback in job interview training, or in psychotherapeutic scenarios.However, at present no method to automatically classify different emotion regulation strategies in a cross-user scenario exists.At the same time, recent studies showed that instruction-tuned Large Language Models (LLMs) can reach impressive performance across a variety of affect recognition tasks such as categorical emotion recognition or sentiment analysis.<span class='px-1 mx-1 bg-yellow-200'>While these results are promising, it remains unclear to what extent the representational power of LLMs can be utilized in the more subtle task of classifying users' internal emotion regulation strategy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>To close this gap, we make use of the recently introduced \textsc{Deep} corpus for modeling the social display of the emotion shame, where each point in time is annotated with one of seven different emotion regulation classes.We fine-tune Llama2-7B as well as the recently introduced Gemma model using Low-rank Optimization on prompts generated from different sources of information on the \textsc{Deep} corpus.These include verbal and nonverbal behavior, person factors, as well as the results of an in-depth interview after the interaction.<span class='px-1 mx-1 bg-yellow-200'>Our results show, that a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation strategy with high accuracy (0.84) without needing access to data from post-interaction interviews. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span><span class='px-1 mx-1 bg-yellow-200'>This represents a significant improvement over previous approaches based on Bayesian Networks and highlights the importance of modeling verbal behavior in emotion regulation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04420v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04420v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Competitive debate is a comprehensive and complex computational argumentation task.Large Language Models (LLMs) encounter hallucinations and lack competitiveness in this task.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span><span class='px-1 mx-1 bg-yellow-200'>Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents (Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span>These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary.To comprehensively evaluate framework performance, we construct the Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions.<span class='px-1 mx-1 bg-yellow-200'>We recruite ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking.Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans.Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding.However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information.<span class='px-1 mx-1 bg-yellow-200'>In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.746</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Conversational Prompt Engineering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Prompts are how humans communicate with LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span>Informative prompts are essential for guiding LLMs to produce the desired output.However, prompt engineering is often tedious and time-consuming, requiring significant expertise, limiting its widespread use.We propose Conversational Prompt Engineering (CPE), a user-friendly tool that helps users create personalized prompts for their specific tasks.CPE uses a chat model to briefly interact with users, helping them articulate their output preferences and integrating these into the prompt.The process includes two main stages: first, the model uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction.Then, the model shares the outputs generated by the instruction and uses user feedback to further refine the instruction and the outputs.The final result is a few-shot prompt, where the outputs approved by the user serve as few-shot examples.A user study on summarization tasks demonstrates the value of CPE in creating personalized, high-performing prompts.The results suggest that the zero-shot prompt obtained is comparable to its - much longer - few-shot counterpart, indicating significant savings in scenarios involving repetitive tasks with large text volumes.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04560v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04560v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Large Language Models in Social Sciences</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Stigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.837</span></span><span class='px-1 mx-1 bg-yellow-200'>With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span><span class='px-1 mx-1 bg-yellow-200'>This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.864</span></span><span class='px-1 mx-1 bg-yellow-200'>We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.886</span></span><span class='px-1 mx-1 bg-yellow-200'>Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.729</span></span><span class='px-1 mx-1 bg-yellow-200'>Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.821</span></span>Our work not only enhances understanding of stigma's manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD.Code and data will be made publicly available upon acceptance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07873v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07873v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The doctor will polygraph you now: ethical concerns with AI for fact-checking patients
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Clinical artificial intelligence (AI) methods have been proposed for predicting social behaviors which could be reasonably understood from patient-reported data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.889</span></span>This raises ethical concerns about respect, privacy, and patient awareness/control over how their health data is used.<span class='px-1 mx-1 bg-yellow-200'>Ethical concerns surrounding clinical AI systems for social behavior verification were divided into three main categories: (1) the use of patient data retrospectively without informed consent for the specific task of verification, (2) the potential for inaccuracies or biases within such systems, and (3) the impact on trust in patient-provider relationships with the introduction of automated AI systems for fact-checking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span>Additionally, this report showed the simulated misuse of a verification system and identified a potential LLM bias against patient-reported information in favor of multimodal data, published literature, and the outputs of other AI methods (i.e., AI self-trust).Finally, recommendations were presented for mitigating the risk that AI verification systems will cause harm to patients or undermine the purpose of the healthcare system.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07896v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07896v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The performance of ChatGPT\copyright{} and other LLMs has improved tremendously, and in online environments, they are increasingly likely to be used in a wide variety of situations, such as ChatBot on web pages, call center operations using voice interaction, and dialogue functions using agents.In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots.In this multimodal dialogue, mutual emotion recognition between the AI and the user will become important.So far, there have been methods for expressing emotions on the part of the AI agent or for recognizing them using textual or voice information of the user's utterances, but methods for AI agents to recognize emotions from the user's facial expressions have not been studied.In this study, we examined whether or not LLM-based AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts.<span class='px-1 mx-1 bg-yellow-200'>The results confirmed that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07982v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07982v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Confidence-weighted integration of human and machine judgments for superior decision-making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have emerged as powerful tools in various domains.<span class='px-1 mx-1 bg-yellow-200'>Recent studies have shown that LLMs can surpass humans in certain tasks, such as predicting the outcomes of neuroscience studies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span><span class='px-1 mx-1 bg-yellow-200'>What role does this leave for humans in the overall decision process? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span><span class='px-1 mx-1 bg-yellow-200'>One possibility is that humans, despite performing worse than LLMs, can still add value when teamed with them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>A human and machine team can surpass each individual teammate when team members' confidence is well-calibrated and team members diverge in which tasks they find difficult (i.e., calibration and diversity are needed).We simplified and extended a Bayesian approach to combining judgments using a logistic regression framework that integrates confidence-weighted judgments for any number of team members.<span class='px-1 mx-1 bg-yellow-200'>Using this straightforward method, we demonstrated in a neuroscience forecasting task that, even when humans were inferior to LLMs, their combination with one or more LLMs consistently improved team performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span>Our hope is that this simple and effective strategy for integrating the judgments of humans and machines will lead to productive collaborations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08083v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08083v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.606</span></span>However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning.Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS).This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures.By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS.Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08210v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08210v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While various approaches have recently been studied for bias identification, little is known about how implicit language that does not explicitly convey a viewpoint affects bias amplification in large language models.<span class='px-1 mx-1 bg-yellow-200'>To examine the severity of bias toward a view, we evaluated the performance of two downstream tasks where the implicit and explicit knowledge of social groups were used. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.793</span></span>First, we present a stress test evaluation by using a biased model in edge cases of excessive bias scenarios.<span class='px-1 mx-1 bg-yellow-200'>Then, we evaluate how LLMs calibrate linguistically in response to both implicit and explicit opinions when they are aligned with conflicting viewpoints. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Our findings reveal a discrepancy in LLM performance in identifying implicit and explicit opinions, with a general tendency of bias toward explicit opinions of opposing stances.Moreover, the bias-aligned models generate more cautious responses using uncertainty phrases compared to the unaligned (zero-shot) base models.<span class='px-1 mx-1 bg-yellow-200'>The direct, incautious responses of the unaligned models suggest a need for further refinement of decisiveness by incorporating uncertainty markers to enhance their reliability, especially on socially nuanced topics with high subjectivity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span><span class='px-1 mx-1 bg-yellow-200'>Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.827</span></span>However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data.Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting.<span class='px-1 mx-1 bg-yellow-200'>Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span><span class='px-1 mx-1 bg-yellow-200'>The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential.Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07313v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07313v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span>However, the winning response and the losing response within pairwise data are generated isolatedly, leading to weak correlations between them as well as suboptimal alignment performance.To address this issue, we propose an effective framework named BMC, for bridging and modeling correlations in pairwise data.Firstly, we increase the consistency and informativeness of the pairwise preference signals by targeted modifications, synthesizing a pseudo winning response through improving the losing response based on the winning response.Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations.Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training.Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO.Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07471v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07471v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Development of a Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide.<span class='px-1 mx-1 bg-yellow-200'>While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span>This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain.The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator.It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist.The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system.Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management.By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes.This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07531v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07531v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Warning: This paper may contain texts with uncomfortable content.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.767</span></span>Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech.However, these models often exhibit biases due to the nature of their training data.Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases.<span class='px-1 mx-1 bg-yellow-200'>This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.899</span></span><span class='px-1 mx-1 bg-yellow-200'>By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>Our experiments reveal significant insights into their performance and bias levels.<span class='px-1 mx-1 bg-yellow-200'>The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.761</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07665v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07665v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Social Debiasing for Fair Multi-modal LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities.<span class='px-1 mx-1 bg-yellow-200'>However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.822</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper addresses the issue of social biases in MLLMs by i) <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.849</span></span><span class='px-1 mx-1 bg-yellow-200'>Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span><span class='px-1 mx-1 bg-yellow-200'>ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases.<span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06569v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06569v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are known for their remarkable ability to generate synthesized 'knowledge', such as text documents, music, images, etc.However, there is a huge gap between LLM's and human capabilities for understanding abstract concepts and reasoning.We discuss these issues in a larger philosophical context of human knowledge acquisition and the Turing test.In addition, we illustrate the limitations of LLMs by analyzing GPT-4 responses to questions ranging from science and math to common sense reasoning.These examples show that GPT-4 can often imitate human reasoning, even though it lacks understanding.However, LLM responses are synthesized from a large LLM model trained on all available data.In contrast, human understanding is based on a small number of abstract concepts.<span class='px-1 mx-1 bg-yellow-200'>Based on this distinction, we discuss the impact of LLMs on acquisition of human knowledge and education. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.608</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06598v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06598v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pragmatic inference of scalar implicature by LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This study investigates how Large Language Models (LLMs), particularly BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic inference of scalar implicature, such as some.Two sets of experiments were conducted using cosine similarity and next sentence/token prediction as experimental methods.<span class='px-1 mx-1 bg-yellow-200'>The results in experiment 1 showed that, both models interpret some as pragmatic implicature not all in the absence of context, aligning with human language processing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span>In experiment 2, in which Question Under Discussion (QUD) was presented as a contextual cue, BERT showed consistent performance regardless of types of QUDs, while GPT-2 encountered processing difficulties since a certain type of QUD required pragmatic inference for implicature.The findings revealed that, in terms of theoretical approaches, BERT inherently incorporates pragmatic implicature not all within the term some, adhering to Default model (Levinson, 2000).In contrast, GPT-2 seems to encounter processing difficulties in inferring pragmatic implicature within context, consistent with Context-driven model (Sperber and Wilson, 2002).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06673v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06673v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large language models can consistently generate high-quality content for election disinformation operations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Advances in large language models have raised concerns about their potential use in generating compelling election disinformation at scale. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>This study presents a two-part investigation into the capabilities of LLMs to automate stages of an election disinformation operation.First, we introduce DisElect, a novel evaluation dataset designed to measure LLM compliance with instructions to generate content for an election disinformation operation in localised UK context, containing 2,200 malicious prompts and 50 benign prompts.Using DisElect, we test 13 LLMs and find that most models broadly comply with these requests; we also find that the few models which refuse malicious prompts also refuse benign election-related prompts, and are more likely to refuse to generate content from a right-wing perspective.<span class='px-1 mx-1 bg-yellow-200'>Secondly, we conduct a series of experiments (N=2,340) to assess the "humanness" of LLMs: the extent to which disinformation operation content generated by an LLM is able to pass as human-written. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>Our experiments suggest that almost all LLMs tested released since 2022 produce election disinformation operation content indiscernible by human evaluators over 50% of the time.Notably, we observe that multiple models achieve above-human levels of humanness.Taken together, these findings suggest that current LLMs can be used to generate high-quality content for election disinformation operations, even in hyperlocalised scenarios, at far lower costs than traditional methods, and offer researchers and policymakers an empirical benchmark for the measurement and evaluation of these capabilities in current and future models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06731v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06731v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Research Quality with Large Language Models: An Analysis of ChatGPT's Effectiveness with Different Settings and Inputs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Evaluating the quality of academic journal articles is a time consuming but critical task for national research evaluation exercises, appointments and promotion.It is therefore important to investigate whether Large Language Models (LLMs) can play a role in this process.This article assesses which ChatGPT inputs (full text without tables, figures and references; title and abstract; title only) produce better quality score estimates, and the extent to which scores are affected by ChatGPT models and system prompts.<span class='px-1 mx-1 bg-yellow-200'>The results show that the optimal input is the article title and abstract, with average ChatGPT scores based on these (30 iterations on a dataset of 51 papers) correlating at 0.67 with human scores, the highest ever reported. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>ChatGPT 4o is slightly better than 3.5-turbo (0.66), and 4o-mini (0.66).The results suggest that article full texts might confuse LLM research quality evaluations, even though complex system instructions for the task are more effective than simple ones.Thus, whilst abstracts contain insufficient information for a thorough assessment of rigour, they may contain strong pointers about originality and significance.Finally, linear regression can be used to convert the model scores into the human scale scores, which is 31% more accurate than guessing.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06752v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06752v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study on Bar Charts with Varying Layouts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways?<span class='px-1 mx-1 bg-yellow-200'>Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study.We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked.In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies.We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways.In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings.<span class='px-1 mx-1 bg-yellow-200'>Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span>In Experiment 3, we examined the effect of chart context and data on LLM takeaways.We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout.<span class='px-1 mx-1 bg-yellow-200'>Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06837v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06837v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Language Models for Emotion and Behavior Analysis in Education
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues.This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students.Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution.We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting.Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding.<span class='px-1 mx-1 bg-yellow-200'>This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.703</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds.<span class='px-1 mx-1 bg-yellow-200'>We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.805</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span><span class='px-1 mx-1 bg-yellow-200'>Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance.<span class='px-1 mx-1 bg-yellow-200'>These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06929v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06929v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Neural embedding of beliefs reveals the role of relative dissonance in human decision-making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Beliefs serve as the foundation for human cognition and decision-making. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>They guide individuals in deriving meaning from their lives, shaping their behaviors, and forming social connections.Therefore, a model that encapsulates beliefs and their interrelationships is crucial for quantitatively studying the influence of beliefs on our actions.<span class='px-1 mx-1 bg-yellow-200'>Despite its importance, research on the interplay between human beliefs has often been limited to a small set of beliefs pertaining to specific issues, with a heavy reliance on surveys or experiments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.795</span></span>Here, we propose a method for extracting nuanced relations between thousands of beliefs by leveraging large-scale user participation data from an online debate platform and mapping these beliefs to an embedding space using a fine-tuned large language model (LLM).This belief embedding space effectively encapsulates the interconnectedness of diverse beliefs as well as polarization across various social issues.We discover that the positions within this belief space predict new beliefs of individuals.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we find that the relative distance between one's existing beliefs and new beliefs can serve as a quantitative estimate of cognitive dissonance, allowing us to predict new beliefs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span><span class='px-1 mx-1 bg-yellow-200'>Our study highlights how modern LLMs, when combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human belief formation and decision-making processes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07237v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07237v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs in Education Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Training Large Language Models (LLMs) incurs substantial data-related costs, motivating the development of data-efficient training methods through optimised data ordering and selection.Human-inspired learning strategies, such as curriculum learning, offer possibilities for efficient training by organising data according to common human learning practices.Despite evidence that fine-tuning with curriculum learning improves the performance of LLMs for natural language understanding tasks, its effectiveness is typically assessed using a single model.<span class='px-1 mx-1 bg-yellow-200'>In this work, we extend previous research by evaluating both curriculum-based and non-curriculum-based learning strategies across multiple LLMs, using human-defined and automated data labels for medical question answering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>Our results indicate a moderate impact of using human-inspired learning strategies for fine-tuning LLMs, with maximum accuracy gains of 1.77% per model and 1.81% per dataset.Crucially, we demonstrate that the effectiveness of these strategies varies significantly across different model-dataset combinations, emphasising that the benefits of a specific human-inspired strategy for fine-tuning LLMs do not generalise.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we find evidence that curriculum learning using LLM-defined question difficulty outperforms human-defined difficulty, highlighting the potential of using model-generated measures for optimal curriculum design. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.56</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07888v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07888v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios.<span class='px-1 mx-1 bg-yellow-200'>While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>This is because the robot needs to locate the target object for manipulation within the physical workspace.To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models.For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image.Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline.This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks.The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories.Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks.This indicates its potential to generalize to scenarios beyond the tabletop.More information and video results are available here: https://star-uu-wang.github.io/Polaris/</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07975v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07975v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images.Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos.One major contributing factor is the absence of datasets in the surgical field.In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far.<span class='px-1 mx-1 bg-yellow-200'>To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span>The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services.It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data.We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks.<span class='px-1 mx-1 bg-yellow-200'>We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span>We will release our code, model, and the instruction-tuning dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The performance of ChatGPT\copyright{} and other LLMs has improved tremendously, and in online environments, they are increasingly likely to be used in a wide variety of situations, such as ChatBot on web pages, call center operations using voice interaction, and dialogue functions using agents.In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots.In this multimodal dialogue, mutual emotion recognition between the AI and the user will become important.So far, there have been methods for expressing emotions on the part of the AI agent or for recognizing them using textual or voice information of the user's utterances, but methods for AI agents to recognize emotions from the user's facial expressions have not been studied.<span class='px-1 mx-1 bg-yellow-200'>In this study, we examined whether or not LLM-based AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.509</span></span>The results confirmed that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07982v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07982v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools.This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry.To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions.This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software.Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality.Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework.The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input.<span class='px-1 mx-1 bg-yellow-200'>Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08054v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08054v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have showcased exceptional ability in causal reasoning from textual information.However, will these causalities remain straightforward for Vision Large Language Models (VLLMs) when only visual hints are provided?Motivated by this, we propose a novel Multimodal Causal Reasoning benchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect relationship when solely relying on visual cues such as action, appearance, clothing, and environment.Specifically, we introduce a prompt-driven image synthesis approach to create siamese images with embedded semantic causality and visual cues, which can effectively evaluate VLLMs' causal reasoning capabilities.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we develop tailored metrics from multiple perspectives, including image-level match, phrase-level understanding, and sentence-level explanation, to comprehensively assess VLLMs' comprehension abilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.552</span></span>Our extensive experiments reveal that the current state-of-the-art VLLMs are not as skilled at multimodal causal reasoning as we might have hoped.Furthermore, we perform a comprehensive analysis to understand these models' shortcomings from different views and suggest directions for future research.We hope MuCR can serve as a valuable resource and foundational benchmark in multimodal causal reasoning research.The project is available at: https://github.com/Zhiyuan-Li-John/MuCR</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08105v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08105v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although Large Language Models(LLMs) can generate coherent and contextually relevant text, they often struggle to recognise the intent behind the human user's query.Natural Language Understanding (NLU) models, however, interpret the purpose and key information of user's input to enable responsive interactions.Existing NLU models generally map individual utterances to a dual-level semantic frame, involving sentence-level intent and word-level slot labels.However, real-life conversations primarily consist of multi-turn conversations, involving the interpretation of complex and extended dialogues.Researchers encounter challenges addressing all facets of multi-turn dialogue conversations using a unified single NLU model.This paper introduces a novel approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge distillation for multi-turn NLU.<span class='px-1 mx-1 bg-yellow-200'>To achieve this, we construct distinct teachers for varying levels of conversation knowledge, namely, sentence-level intent detection, word-level slot filling, and conversation-level domain classification. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.528</span></span><span class='px-1 mx-1 bg-yellow-200'>These teachers are then fine-tuned to acquire specific knowledge of their designated levels. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>A multi-teacher loss is proposed to facilitate the combination of these multi-level teachers, guiding a student model in multi-turn dialogue tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span><span class='px-1 mx-1 bg-yellow-200'>The experimental results demonstrate the efficacy of our model in improving the overall multi-turn conversation understanding, showcasing the potential for advancements in NLU models through the incorporation of multi-level dialogue knowledge distillation techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.52</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08144v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08144v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in AI have been significantly driven by the capabilities of large language models (LLMs) to solve complex problems in ways that resemble human thinking.<span class='px-1 mx-1 bg-yellow-200'>However, there is an ongoing debate about the extent to which LLMs are capable of actual reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS).This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures.By viewing LLMs as abstract machines that process information through a natural language interface, we examine the conditions under which it is possible to compute suitable approximations of PN and PS.<span class='px-1 mx-1 bg-yellow-200'>Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.545</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08210v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08210v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Understand Symbolic Graphics Programs?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training.We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data.<span class='px-1 mx-1 bg-yellow-200'>LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.572</span></span>Unlike conventional programs, symbolic graphics programs can be translated to graphics content.<span class='px-1 mx-1 bg-yellow-200'>Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment.<span class='px-1 mx-1 bg-yellow-200'>To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.589</span></span>We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs.This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts.We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs.We find that this task distinguishes existing LLMs and models considered good at reasoning perform better.Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability.Specifically, we query GPT4-o with questions and images generated by symbolic programs.Such data are then used to finetune an LLM.We also find that SIT data can improve the general instruction following ability of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08313v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08313v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data.However, the winning response and the losing response within pairwise data are generated isolatedly, leading to weak correlations between them as well as suboptimal alignment performance.To address this issue, we propose an effective framework named BMC, for bridging and modeling correlations in pairwise data.Firstly, we increase the consistency and informativeness of the pairwise preference signals by targeted modifications, synthesizing a pseudo winning response through improving the losing response based on the winning response.Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations.Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training.<span class='px-1 mx-1 bg-yellow-200'>Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.594</span></span>Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07471v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07471v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Development of a Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide.While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making.This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain.<span class='px-1 mx-1 bg-yellow-200'>The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.542</span></span>It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist.The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system.Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management.By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes.This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07531v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07531v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Usefulness of data flow diagrams and large language models for security threat validation: a registered report
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well.Threat analysis and risk assessment are used to identify security threats for new or refactored systems.Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis.Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats.<span class='px-1 mx-1 bg-yellow-200'>We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span><span class='px-1 mx-1 bg-yellow-200'>In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.59</span></span>Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07537v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07537v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson Planning in Ugandan Secondary Schools. Prototype Quality Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span><span class='px-1 mx-1 bg-yellow-200'>Research identifies several problems, including low quality or absent teacher lesson planning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span><span class='px-1 mx-1 bg-yellow-200'>As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span><span class='px-1 mx-1 bg-yellow-200'>Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.575</span></span><span class='px-1 mx-1 bg-yellow-200'>This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.577</span></span>Methods: The prototype was created using Cohere LLM and Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website.<span class='px-1 mx-1 bg-yellow-200'>Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.548</span></span><span class='px-1 mx-1 bg-yellow-200'>Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span><span class='px-1 mx-1 bg-yellow-200'>The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span><span class='px-1 mx-1 bg-yellow-200'>Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to "very good lesson plan". <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span><span class='px-1 mx-1 bg-yellow-200'>None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07542v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07542v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are known for their remarkable ability to generate synthesized 'knowledge', such as text documents, music, images, etc.<span class='px-1 mx-1 bg-yellow-200'>However, there is a huge gap between LLM's and human capabilities for understanding abstract concepts and reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.53</span></span><span class='px-1 mx-1 bg-yellow-200'>We discuss these issues in a larger philosophical context of human knowledge acquisition and the Turing test. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span><span class='px-1 mx-1 bg-yellow-200'>In addition, we illustrate the limitations of LLMs by analyzing GPT-4 responses to questions ranging from science and math to common sense reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>These examples show that GPT-4 can often imitate human reasoning, even though it lacks understanding.However, LLM responses are synthesized from a large LLM model trained on all available data.In contrast, human understanding is based on a small number of abstract concepts.<span class='px-1 mx-1 bg-yellow-200'>Based on this distinction, we discuss the impact of LLMs on acquisition of human knowledge and education. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06598v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06598v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although large language models (LLMs) are capable of performing various tasks, they still suffer from producing plausible but incorrect responses.To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not.However, most uncertainty quantification methods have been evaluated on questions requiring a single clear answer, ignoring the existence of data uncertainty that arises from irreducible randomness.Instead, these methods only consider model uncertainty, which arises from a lack of knowledge.In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty.Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs.Our findings show that entropy and consistency-based methods estimate the model uncertainty well even under data uncertainty, while other methods for white- and black-box LLMs struggle depending on the tasks.<span class='px-1 mx-1 bg-yellow-200'>Additionally, methods designed for white-box LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge queries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.52</span></span>We believe our observations will pave the way for future work on uncertainty quantification in realistic setting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06816v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06816v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study on Bar Charts with Varying Layouts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways?Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts.In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study.We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked.In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies.We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways.In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings.Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans.<span class='px-1 mx-1 bg-yellow-200'>In Experiment 3, we examined the effect of chart context and data on LLM takeaways. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout.Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06837v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06837v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DracoGPT: Extracting Visualization Design Preferences from Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices.However, if they fail to do so, they might provide unreliable visualization recommendations.<span class='px-1 mx-1 bg-yellow-200'>What visualization design preferences, then, have LLMs learned? <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.545</span></span>We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs.To assess varied tasks, we develop two pipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to either rank or recommend visual encoding specifications.We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research.We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints.Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments.Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06845v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06845v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Causal Agent based on Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have achieved significant success across various domains.However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively.Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately.Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data.This lack of causal reasoning capability limits the development of LLMs.To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems.The causal agent comprises tools, memory, and reasoning modules.In the tools module, the causal agent applies causal methods to align tabular data with natural language.In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools.In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs.To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level.<span class='px-1 mx-1 bg-yellow-200'>We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%.For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06849v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06849v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Language Models for Emotion and Behavior Analysis in Education
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.591</span></span>Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues.<span class='px-1 mx-1 bg-yellow-200'>This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution.<span class='px-1 mx-1 bg-yellow-200'>We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.514</span></span>Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding.<span class='px-1 mx-1 bg-yellow-200'>This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06874v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06874v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SceneGPT: A Language Model for 3D Scene Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes.In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training.<span class='px-1 mx-1 bg-yellow-200'>The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.555</span></span>The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning.We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06926v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06926v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                "You still have to study" -- On the Security of LLM generated code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We witness an increasing usage of AI-assistants even for routine (classroom) programming tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span>However, the code generated on basis of a so called "prompt" by the programmer does not always meet accepted security standards.On the one hand, this may be due to lack of best-practice examples in the training data.On the other hand, the actual quality of the programmers prompt appears to influence whether generated code contains weaknesses or not.In this paper we analyse 4 major LLMs with respect to the security of generated code.We do this on basis of a case study for the Python and Javascript language, using the MITRE CWE catalogue as the guiding security definition.Our results show that using different prompting techniques, some LLMs initially generate 65% code which is deemed insecure by a trained security engineer.On the other hand almost all analysed LLMs will eventually generate code being close to 100% secure with increasing manual guidance of a skilled engineer.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07106v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07106v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Reason? A Characterization via 3-SAT
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) are said to possess advanced reasoning abilities.However, some skepticism exists as recent works show how LLMs often bypass true reasoning using shortcuts.Current methods for assessing the reasoning abilities of LLMs typically rely on open-source benchmarks that may be overrepresented in LLM training data, potentially skewing performance.We instead provide a computational theory perspective of reasoning, using 3-SAT -- the prototypical NP-complete problem that lies at the core of logical reasoning and constraint satisfaction tasks.By examining the phase transitions in 3-SAT, we empirically characterize the reasoning abilities of LLMs and show how they vary with the inherent hardness of the problems.<span class='px-1 mx-1 bg-yellow-200'>Our experimental evidence shows that LLMs cannot perform true reasoning, as is required for solving 3-SAT problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.544</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07215v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07215v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior performance in complex human-like interactions.But they are costly, or too large for edge devices such as smartphones and harder to self-host, leading to security and privacy concerns.This paper introduces a novel interpretable knowledge distillation approach to enhance the performance of smaller, more economical LLMs that firms can self-host.We study this problem in the context of building a customer service agent aimed at achieving high customer satisfaction through goal-oriented dialogues.Unlike traditional knowledge distillation, where the "student" model learns directly from the "teacher" model's responses via fine-tuning, our interpretable "strategy" teaching approach involves the teacher providing strategies to improve the student's performance in various scenarios.This method alternates between a "scenario generation" step and a "strategies for improvement" step, creating a customized library of scenarios and optimized strategies for automated prompting.<span class='px-1 mx-1 bg-yellow-200'>The method requires only black-box access to both student and teacher models; hence it can be used without manipulating model parameters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.569</span></span>In our customer service application, the method improves performance, and the learned strategies are transferable to other LLMs and scenarios beyond the training set.The method's interpretabilty helps safeguard against potential harms through human audit.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.593</span></span>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Med42-v2: A Suite of Clinical LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Med42-v2 introduces a suite of clinical large language models (LLMs) designed to address the limitations of generic models in healthcare settings.These models are built on Llama3 architecture and fine-tuned using specialized clinical data.They underwent multi-stage preference alignment to effectively respond to natural prompts.While generic models are often preference-aligned to avoid answering clinical queries as a precaution, Med42-v2 is specifically trained to overcome this limitation, enabling its use in clinical settings.Med42-v2 models demonstrate superior performance compared to the original Llama3 models in both 8B and 70B parameter configurations and GPT-4 across various medical benchmarks.<span class='px-1 mx-1 bg-yellow-200'>These LLMs are developed to understand clinical queries, perform reasoning tasks, and provide valuable assistance in clinical environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>The models are now publicly available at \href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06142v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06142v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities.<span class='px-1 mx-1 bg-yellow-200'>There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span>In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans.We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning?(2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context?(3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement?Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT).Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06318v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06318v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs as Recommender Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Sequential recommendation systems fundamentally rely on users' historical interaction sequences, which are often contaminated by noisy interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span>Identifying these noisy interactions accurately without additional information is particularly difficult due to the lack of explicit supervisory signals to denote noise.Large Language Models (LLMs), equipped with extensive open knowledge and semantic reasoning abilities, present a promising avenue to bridge this information gap.<span class='px-1 mx-1 bg-yellow-200'>However, employing LLMs for denoising in sequential recommendation introduces notable challenges: 1) Direct application of pretrained LLMs may not be competent for the denoising task, frequently generating nonsensical responses; 2) Even after fine-tuning, the reliability of LLM outputs remains questionable, especially given the complexity of the task and th inherent hallucinatory issue of LLMs.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span><span class='px-1 mx-1 bg-yellow-200'>To tackle these challenges, we propose LLM4DSR, a tailored approach for denoising sequential recommendation using LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>We constructed a self-supervised fine-tuning task to activate LLMs' capabilities to identify noisy items and suggest replacements.Furthermore, we developed an uncertainty estimation module that ensures only high-confidence responses are utilized for sequence corrections.<span class='px-1 mx-1 bg-yellow-200'>Remarkably, LLM4DSR is model-agnostic, allowing the corrected sequences to be flexibly applied across various recommendation models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments validate the superiority of LLM4DSR over existing methods across three datasets and three recommendation backbones. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08208v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08208v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span>Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment.<span class='px-1 mx-1 bg-yellow-200'>In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.632</span></span>Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved.Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models.Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization.Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer.Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks.Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08231v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08231v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for LLM-Based Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Sequential recommender system (SRS) predicts the next items that users may prefer based on user historical interaction sequences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span>Inspired by the rise of large language models (LLMs) in various AI applications, there is a surge of work on LLM-based SRS.Despite their attractive performance, existing LLM-based SRS still exhibit some limitations, including neglecting intra-item relations, ignoring long-term collaborative knowledge and using inflexible architecture designs for adaption.To alleviate these issues, we propose an LLM-based SRS named MixRec.Built on top of coarse-grained adaption for capturing inter-item relations, MixRec is further enhanced with (1) context masking that models intra-item relations to help LLM better understand token and item semantics in the context of SRS, (2) collaborative knowledge injection that helps LLM incorporate long-term collaborative knowledge, and (3) a dynamic adaptive mixture-of-experts design that can flexibly choose expert architectures based on Bayesian optimization to better incorporate different sequential information.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate that MixRec can effectively handle sequential recommendation in a dynamic and adaptive manner. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.867</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07427v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07427v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Prompt Tuning as User Inherent Profile Inference Machine
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have exhibited significant promise in recommender systems by empowering user profiles with their extensive world knowledge and superior reasoning capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.801</span></span>However, LLMs face challenges like unstable instruction compliance, modality gaps, and high inference latency, leading to textual noise and limiting their effectiveness in recommender systems.To address these challenges, we propose UserIP-Tuning, which uses prompt-tuning to infer user profiles.It integrates the causal relationship between user profiles and behavior sequences into LLMs' prompts.And employs expectation maximization to infer the embedded latent profile, minimizing textual noise by fixing the prompt template.Furthermore, A profile quantization codebook bridges the modality gap by categorizing profile embeddings into collaborative IDs, which are pre-stored for online deployment.This improves time efficiency and reduces memory usage.<span class='px-1 mx-1 bg-yellow-200'>Experiments on four public datasets show that UserIP-Tuning outperforms state-of-the-art recommendation algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Additional tests and case studies confirm its effectiveness, robustness, and transferability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06577v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06577v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DracoGPT: Extracting Visualization Design Preferences from Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices.However, if they fail to do so, they might provide unreliable visualization recommendations.What visualization design preferences, then, have LLMs learned?We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs.To assess varied tasks, we develop two pipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to either rank or recommend visual encoding specifications.We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research.We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints.<span class='px-1 mx-1 bg-yellow-200'>Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06845v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06845v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities.<span class='px-1 mx-1 bg-yellow-200'>To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles.It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions.<span class='px-1 mx-1 bg-yellow-200'>This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.736</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06276v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06276v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recently, increasing attention has been paid to LLM-based recommender systems, but their deployment is still under exploration in the industry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span>Most deployments utilize LLMs as feature enhancers, generating augmentation knowledge in the offline stage.<span class='px-1 mx-1 bg-yellow-200'>However, in recommendation scenarios, involving numerous users and items, even offline generation with LLMs consumes considerable time and resources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>This generation inefficiency stems from the autoregressive nature of LLMs, and a promising direction for acceleration is speculative decoding, a Draft-then-Verify paradigm that increases the number of generated tokens per decoding step.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we first identify that recommendation knowledge generation is suitable for retrieval-based speculative decoding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span>Then, we discern two characteristics: (1) extensive items and users in RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for text generated by LLMs.<span class='px-1 mx-1 bg-yellow-200'>Based on the above insights, we propose a Decoding Acceleration Framework for LLM-based Recommendation (dubbed DARE), with Customized Retrieval Pool to improve retrieval efficiency and Relaxed Verification to increase the acceptance rate of draft tokens, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>Extensive experiments demonstrate that DARE achieves a 3-5x speedup and is compatible with various frameworks and backbone LLMs.DARE has also been deployed to online advertising scenarios within a large-scale commercial environment, achieving a 3.45x speedup while maintaining the downstream performance.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05676v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05676v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMREC: LLM Based Multi-Modal Recommender System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span><span class='px-1 mx-1 bg-yellow-200'>This surge in content presents unique challenges for designing effective recommender systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.766</span></span>Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences.<span class='px-1 mx-1 bg-yellow-200'>This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.832</span></span><span class='px-1 mx-1 bg-yellow-200'>The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span><span class='px-1 mx-1 bg-yellow-200'>The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.866</span></span>The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model.Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information.<span class='px-1 mx-1 bg-yellow-200'>This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04211v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04211v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.81</span></span><span class='px-1 mx-1 bg-yellow-200'>Current mainstream efforts mainly center around injecting personalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, leading to suboptimal performance.(2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths.(3) Existing approaches aren't scalable for large datasets due to training efficiency constraints.Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space.To address these problems, we propose RecLoRA.This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.749</span></span>Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03533v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03533v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Model Aided QoS Prediction for Service Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have seen rapid improvement in the recent years, and are used in a wider range of applications.After being trained on large text corpus, LLMs obtain the capability of extracting rich features from textual data.Such capability is potentially useful for the web service recommendation task, where the web users and services have intrinsic attributes that can be described using natural language sentences and are useful for recommendation.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we explore the possibility and practicality of using LLMs for web service recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span>We propose the large language model aided QoS prediction (llmQoS) model, which use LLMs to extract useful information from attributes of web users and services via descriptive sentences.This information is then used in combination with the QoS values of historical interactions of users and services, to predict QoS values for any given user-service pair.Our proposed model is shown to overcome the data sparsity issue for QoS prediction.We show that on the WSDream dataset, llmQoS outperforms comparable baseline models consistently.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02223v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02223v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-30</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The rapid evolution of large language models (LLMs) has opened up new possibilities for applications such as context-driven product recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.697</span></span>However, the effectiveness of these models in this context is heavily reliant on their comprehensive understanding of the product inventory.This paper presents a novel approach to equipping LLMs with product knowledge by training them to respond contextually to synthetic search queries that include product IDs.We delve into an extensive analysis of this method, evaluating its effectiveness, outlining its benefits, and highlighting its constraints.<span class='px-1 mx-1 bg-yellow-200'>The paper also discusses the potential improvements and future directions for this approach, providing a comprehensive understanding of the role of LLMs in product recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.20856v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.20856v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-25</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The recent emergence of Large Language Models (LLMs) has heralded a new era of human-AI interaction.These sophisticated models, exemplified by Chat-GPT and its successors, have exhibited remarkable capabilities in language understanding.However, as these LLMs have undergone exponential growth, a crucial dimension that remains understudied is the personalization of these models.Large foundation models such as GPT-3 etc. focus on creating a universal model that serves a broad range of tasks and users.This approach emphasizes the model's generalization capabilities, treating users as a collective rather than as distinct individuals.While practical for many common applications, this one-size-fits-all approach often fails to address the rich tapestry of human diversity and individual needs.To explore this issue we introduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP models for user personalization.\datasetname{} consists of a series of user-centered tasks containing diverse and individualized expressions where the preferences of users can potentially differ for the same input.<span class='px-1 mx-1 bg-yellow-200'>Using PEFT-U, we explore the challenge of efficiently personalizing LLMs to accommodate user-specific preferences in the context of diverse user-centered tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.18078v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.18078v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-24</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reinforced Prompt Personalization for Recommendation with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>However, existing research predominantly concentrates on task-wise prompting, developing fixed prompt templates composed of four patterns (i.e., role-playing, history records, reasoning guidance, and output format) and applying them to all users for a given task.Although convenient, task-wise prompting overlooks individual user differences, leading to potential mismatches in capturing user preferences.To address it, we introduce the concept of instance-wise prompting to personalize discrete prompts for individual users and propose Reinforced Prompt Personalization (RPP) to optimize the four patterns in prompts using multi-agent reinforcement learning (MARL).To boost efficiency, RPP formulates prompt personalization as selecting optimal sentences holistically across the four patterns, rather than optimizing word-by-word.<span class='px-1 mx-1 bg-yellow-200'>To ensure the quality of prompts, RPP meticulously crafts diverse expressions for each of the four patterns, considering multiple analytical perspectives for specific recommendation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.635</span></span>In addition to RPP, our proposal of RPP+ aims to enhance the scalability of action space by dynamically refining actions with LLMs throughout the iterative process.We evaluate the effectiveness of RPP/RPP+ in ranking tasks over various datasets.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate the superiority of RPP/RPP+ over traditional recommender models, few-shot methods, and other prompt-based methods, underscoring the significance of instance-wise prompting for LLMs in recommendation tasks and validating the effectiveness of RPP/RPP+. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.86</span></span>Our code is available at https://github.com/maowenyu-11/RPP.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.17115v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.17115v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-23</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                How to Leverage Personal Textual Knowledge for Personalized Conversational Information Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Personalized conversational information retrieval (CIR) combines conversational and personalizable elements to satisfy various users' complex information needs through multi-turn interaction based on their backgrounds. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>The key promise is that the personal textual knowledge base (PTKB) can improve the CIR effectiveness because the retrieval results can be more related to the user's background.However, PTKB is noisy: not every piece of knowledge in PTKB is relevant to the specific query at hand.In this paper, we explore and test several ways to select knowledge from PTKB and use it for query reformulation by using a large language model (LLM).The experimental results show the PTKB might not always improve the search results when used alone, but LLM can help generate a more appropriate personalized query when high-quality guidance is provided.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.16192v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.16192v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Medication Recommendation with LLM Text Representation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Most of the existing medication recommendation models are predicted with only structured data such as medical codes, with the remaining other large amount of unstructured or semi-structured data underutilization.<span class='px-1 mx-1 bg-yellow-200'>To increase the utilization effectively, we proposed a method of enhancing medication recommendation with Large Language Model (LLM) text representation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.628</span></span>LLM harnesses powerful language understanding and generation capabilities, enabling the extraction of information from complex and lengthy unstructured data such as clinical notes which contain complex terminology.<span class='px-1 mx-1 bg-yellow-200'>This method can be applied to several existing base models we selected and improve medication recommendation performance with the combination representation of text and medical codes experiments on two different datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>LLM text representation alone can even demonstrate a comparable ability to the medical code representation alone.<span class='px-1 mx-1 bg-yellow-200'>Overall, this is a general method that can be applied to other models for improved recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.10453v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.10453v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantic Understanding and Data Imputation using Large Language Model to Accelerate Recommendation System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper aims to address the challenge of sparse and missing data in recommendation systems, a significant hurdle in the age of big data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span>Traditional imputation methods struggle to capture complex relationships within the data.<span class='px-1 mx-1 bg-yellow-200'>We propose a novel approach that fine-tune Large Language Model (LLM) and use it impute missing data for recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>LLM which is trained on vast amounts of text, is able to understand complex relationship among data and intelligently fill in missing information.This enriched data is then used by the recommendation system to generate more accurate and personalized suggestions, ultimately enhancing the user experience.<span class='px-1 mx-1 bg-yellow-200'>We evaluate our LLM-based imputation method across various tasks within the recommendation system domain, including single classification, multi-classification, and regression compared to traditional data imputation methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span><span class='px-1 mx-1 bg-yellow-200'>By demonstrating the superiority of LLM imputation over traditional methods, we establish its potential for improving recommendation system performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.10078v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.10078v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-07-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recommender systems (RS) are vital for managing information overload and delivering personalized content, responding to users' diverse information needs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span><span class='px-1 mx-1 bg-yellow-200'>The emergence of large language models (LLMs) offers a new horizon for redefining recommender systems with vast general knowledge and reasoning capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span><span class='px-1 mx-1 bg-yellow-200'>Standing across this LLM era, we aim to integrate recommender systems into a broader picture, and pave the way for more comprehensive solutions for future research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.812</span></span><span class='px-1 mx-1 bg-yellow-200'>Therefore, we first offer a comprehensive overview of the technical progression of recommender systems, particularly focusing on language foundation models and their applications in recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.818</span></span><span class='px-1 mx-1 bg-yellow-200'>We identify two evolution paths of modern recommender systems -- via list-wise recommendation and conversational recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.658</span></span>These two paths finally converge at LLM agents with superior capabilities of long-term memory, reflection, and tool intelligence.<span class='px-1 mx-1 bg-yellow-200'>Along these two paths, we point out that the information effectiveness of the recommendation is increased, while the user's acquisition cost is decreased. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span><span class='px-1 mx-1 bg-yellow-200'>Technical features, research methodologies, and inherent challenges for each milestone along the path are carefully investigated -- from traditional list-wise recommendation to LLM-enhanced recommendation to recommendation with LLM agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span>Finally, we highlight several unresolved challenges crucial for the development of future personalization technologies and interfaces and discuss the future prospects.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2407.10081v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2407.10081v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Production workflows for LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FuseChat: Knowledge Fusion of Chat Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, it incurs substantial costs and may lead to redundancy in competencies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span><span class='px-1 mx-1 bg-yellow-200'>Knowledge fusion aims to integrate existing LLMs of diverse architectures and capabilities into a more potent LLM through lightweight continual training, thereby reducing the need for costly LLM development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.437</span></span>In this work, we propose a new framework for the knowledge fusion of chat LLMs through two main stages, resulting in FuseChat.Firstly, we conduct pairwise knowledge fusion on source chat LLMs of varying structures and scales to create multiple target LLMs with identical structure and size via lightweight fine-tuning.<span class='px-1 mx-1 bg-yellow-200'>During this process, a statistics-based token alignment approach is introduced as the cornerstone for fusing LLMs with different structures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>Secondly, we merge these target LLMs within the parameter space, where we propose a novel method for determining the merging coefficients based on the magnitude of parameter updates before and after fine-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.453</span></span>We implement and validate FuseChat using six prominent chat LLMs with diverse architectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha, NH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and Qwen-1.5-Chat-72B. Experimental results on two instruction-following benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of FuseChat-7B over baselines of various sizes.<span class='px-1 mx-1 bg-yellow-200'>Our model is even comparable to the larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.409</span></span><span class='px-1 mx-1 bg-yellow-200'>Our code, model weights, and data are public at \url{https://github.com/fanqiwan/FuseAI}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07990v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07990v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Large Language Model-based Speech Recognition by Contextualization for Rare and Ambiguous Words
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We develop a large language model (LLM) based automatic speech recognition (ASR) system that can be contextualized by providing keywords as prior information in text prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.328</span></span><span class='px-1 mx-1 bg-yellow-200'>We adopt decoder-only architecture and use our in-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by Japanese and English texts as the decoder. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.574</span></span><span class='px-1 mx-1 bg-yellow-200'>We adopt a pre-trained Whisper encoder as an audio encoder, and the audio embeddings from the audio encoder are projected to the text embedding space by an adapter layer and concatenated with text embeddings converted from text prompts to form inputs to the decoder. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.326</span></span><span class='px-1 mx-1 bg-yellow-200'>By providing keywords as prior information in the text prompts, we can contextualize our LLM-based ASR system without modifying the model architecture to transcribe ambiguous words in the input audio accurately. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.358</span></span>Experimental results demonstrate that providing keywords to the decoder can significantly improve the recognition performance of rare and ambiguous words.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08027v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08027v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools.This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry.To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions.<span class='px-1 mx-1 bg-yellow-200'>This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.417</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.331</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.309</span></span>The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input.Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08054v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08054v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have achieved significant advancements, however, the common learning paradigm treats LLMs as passive information repositories, neglecting their potential for active learning and alignment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.392</span></span><span class='px-1 mx-1 bg-yellow-200'>Some approaches train LLMs using their own generated synthetic data, exploring the possibility of active alignment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.487</span></span>However, there is still a huge gap between these one-time alignment methods and the continuous automatic alignment of humans.In this paper, we introduce \textbf{I-SHEEP}, an \textbf{I}terative \textbf{S}elf-En\textbf{H}anc\textbf{E}m\textbf{E}nt \textbf{P}aradigm.<span class='px-1 mx-1 bg-yellow-200'>This human-like paradigm enables LLMs to \textbf{continuously self-align from scratch with nothing}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.33</span></span><span class='px-1 mx-1 bg-yellow-200'>Compared to the one-time alignment method Dromedary \cite{sun2023principledriven}, which refers to the first iteration in this paper, I-SHEEP can significantly enhance capacities on both Qwen and Llama models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.388</span></span><span class='px-1 mx-1 bg-yellow-200'>I-SHEEP achieves a maximum relative improvement of 78.2\% in the Alpaca Eval, 24.0\% in the MT Bench, and an absolute increase of 8.88\% in the IFEval accuracy over subsequent iterations in Qwen-1.5 72B model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.315</span></span><span class='px-1 mx-1 bg-yellow-200'>Additionally, I-SHEEP surpasses the base model in various standard benchmark generation tasks, achieving an average improvement of 24.77\% in code generation tasks, 12.04\% in TrivialQA, and 20.29\% in SQuAD. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.41</span></span>We also provide new insights based on the experiment results.Our codes, datasets, and models are available at \textbf{https://anonymous.4open.science/r/I-SHEEP}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08072v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08072v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Confidence-weighted integration of human and machine judgments for superior decision-making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have emerged as powerful tools in various domains. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.323</span></span>Recent studies have shown that LLMs can surpass humans in certain tasks, such as predicting the outcomes of neuroscience studies.What role does this leave for humans in the overall decision process?<span class='px-1 mx-1 bg-yellow-200'>One possibility is that humans, despite performing worse than LLMs, can still add value when teamed with them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.322</span></span>A human and machine team can surpass each individual teammate when team members' confidence is well-calibrated and team members diverge in which tasks they find difficult (i.e., calibration and diversity are needed).<span class='px-1 mx-1 bg-yellow-200'>We simplified and extended a Bayesian approach to combining judgments using a logistic regression framework that integrates confidence-weighted judgments for any number of team members. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.396</span></span>Using this straightforward method, we demonstrated in a neuroscience forecasting task that, even when humans were inferior to LLMs, their combination with one or more LLMs consistently improved team performance.Our hope is that this simple and effective strategy for integrating the judgments of humans and machines will lead to productive collaborations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08083v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08083v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KGV: Integrating Large Language Models with Knowledge Graphs for Cyber Threat Intelligence Credibility Assessment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Cyber threat intelligence is a critical tool that many organizations and individuals use to protect themselves from sophisticated, organized, persistent, and weaponized cyber attacks.However, few studies have focused on the quality assessment of threat intelligence provided by intelligence platforms, and this work still requires manual analysis by cybersecurity experts.In this paper, we propose a knowledge graph-based verifier, a novel Cyber Threat Intelligence (CTI) quality assessment framework that combines knowledge graphs and Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Our approach introduces LLMs to automatically extract OSCTI key claims to be verified and utilizes a knowledge graph consisting of paragraphs for fact-checking. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.38</span></span>This method differs from the traditional way of constructing complex knowledge graphs with entities as nodes.By constructing knowledge graphs with paragraphs as nodes and semantic similarity as edges, it effectively enhances the semantic understanding ability of the model and simplifies labeling requirements.Additionally, to fill the gap in the research field, we created and made public the first dataset for threat intelligence assessment from heterogeneous sources.To the best of our knowledge, this work is the first to create a dataset on threat intelligence reliability verification, providing a reference for future research.Experimental results show that KGV (Knowledge Graph Verifier) significantly improves the performance of LLMs in intelligence quality assessment.Compared with traditional methods, we reduce a large amount of data annotation while the model still exhibits strong reasoning capabilities.Finally, our method can achieve XXX accuracy in network threat assessment.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08088v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08088v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although Large Language Models(LLMs) can generate coherent and contextually relevant text, they often struggle to recognise the intent behind the human user's query.<span class='px-1 mx-1 bg-yellow-200'>Natural Language Understanding (NLU) models, however, interpret the purpose and key information of user's input to enable responsive interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.365</span></span>Existing NLU models generally map individual utterances to a dual-level semantic frame, involving sentence-level intent and word-level slot labels.However, real-life conversations primarily consist of multi-turn conversations, involving the interpretation of complex and extended dialogues.Researchers encounter challenges addressing all facets of multi-turn dialogue conversations using a unified single NLU model.<span class='px-1 mx-1 bg-yellow-200'>This paper introduces a novel approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.393</span></span>To achieve this, we construct distinct teachers for varying levels of conversation knowledge, namely, sentence-level intent detection, word-level slot filling, and conversation-level domain classification.These teachers are then fine-tuned to acquire specific knowledge of their designated levels.A multi-teacher loss is proposed to facilitate the combination of these multi-level teachers, guiding a student model in multi-turn dialogue tasks.The experimental results demonstrate the efficacy of our model in improving the overall multi-turn conversation understanding, showcasing the potential for advancements in NLU models through the incorporation of multi-level dialogue knowledge distillation techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08144v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08144v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) exhibit high inference latency due to their autoregressive decoding nature.While the draft head in speculative decoding mitigates this issue, its full potential remains unexplored.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an orthogonal approach to the draft head. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.416</span></span><span class='px-1 mx-1 bg-yellow-200'>By transforming the conventional single-layer draft head into a multi-layer architecture and incorporating adversarial learning into the traditional supervised training, KOALA significantly improves the accuracy of the draft head in predicting subsequent tokens, thus more closely mirroring the functionality of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.387</span></span>Although this improvement comes at the cost of slightly increased drafting overhead, KOALA substantially unlocks the draft head's potential, greatly enhancing speculative decoding.We conducted comprehensive evaluations of KOALA, including both autoregressive and non-autoregressive draft heads across various tasks, demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is 10.57%-14.09% faster than the original draft heads.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08146v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08146v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                P/D-Serve: Serving Disaggregated Large Language Model at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Serving disaggregated large language models (LLMs) over tens of thousands of xPU devices (GPUs or NPUs) with reliable performance faces multiple challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.736</span></span>1) Ignoring the diversity (various prefixes and tidal requests), treating all the prompts in a mixed pool is inadequate.<span class='px-1 mx-1 bg-yellow-200'>To facilitate the similarity per scenario and minimize the inner mismatch on P/D (prefill and decoding) processing, fine-grained organization is required, dynamically adjusting P/D ratios for better performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.421</span></span><span class='px-1 mx-1 bg-yellow-200'>2) Due to inaccurate estimation on workload (queue status or maintained connections), the global scheduler easily incurs unnecessary timeouts in prefill. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.433</span></span>3) Block-fixed device-to-device (D2D) KVCache transfer over cluster-level RDMA (remote direct memory access) fails to achieve desired D2D utilization as expected.<span class='px-1 mx-1 bg-yellow-200'>To overcome previous problems, this paper proposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps (machine learning operations), which models end-to-end (E2E) P/D performance and enables: 1) fine-grained P/D organization, mapping the service with RoCE (RDMA over converged ethernet) as needed, to facilitate similar processing and dynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for idle prefill, decoupling the scheduler from regular inaccurate reports and local queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer via optimized D2D access. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span><span class='px-1 mx-1 bg-yellow-200'>P/D-Serve is implemented upon Ascend and MindSpore, has been deployed over tens of thousands of NPUs for more than eight months in commercial use, and further achieves 60\%, 42\% and 46\% improvements on E2E throughput, time-to-first-token (TTFT) SLO (service level objective) and D2D transfer time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span><span class='px-1 mx-1 bg-yellow-200'>As the E2E system with optimizations, P/D-Serve achieves 6.7x increase on throughput, compared with aggregated LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.564</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EmBARDiment: an Embodied AI Agent for Productivity in XR
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.499</span></span>However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query.We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment.This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08158v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08158v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Long-horizon planning is hindered by challenges such as uncertainty accumulation, computational complexity, delayed rewards and incomplete information.This work proposes an approach to exploit the task hierarchy from human instructions to facilitate multi-robot planning.Using Large Language Models (LLMs), we propose a two-step approach to translate multi-sentence instructions into a structured language, Hierarchical Linear Temporal Logic (LTL), which serves as a formal representation for planning.Initially, LLMs transform the instructions into a hierarchical representation defined as Hierarchical Task Tree, capturing the logical and temporal relations among tasks.<span class='px-1 mx-1 bg-yellow-200'>Following this, a domain-specific fine-tuning of LLM translates sub-tasks of each task into flat LTL formulas, aggregating them to form hierarchical LTL specifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.433</span></span>These specifications are then leveraged for planning using off-the-shelf planners.Our framework not only bridges the gap between instructions and algorithmic planning but also showcases the potential of LLMs in harnessing hierarchical reasoning to automate multi-robot task planning.Through evaluations in both simulation and real-world experiments involving human participants, we demonstrate that our method can handle more complex instructions compared to existing methods.The results indicate that our approach achieves higher success rates and lower costs in multi-robot task allocation and plan generation.Demos videos are available at https://youtu.be/7WOrDKxIMIs .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08188v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08188v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Sequential recommendation systems fundamentally rely on users' historical interaction sequences, which are often contaminated by noisy interactions.Identifying these noisy interactions accurately without additional information is particularly difficult due to the lack of explicit supervisory signals to denote noise.Large Language Models (LLMs), equipped with extensive open knowledge and semantic reasoning abilities, present a promising avenue to bridge this information gap.However, employing LLMs for denoising in sequential recommendation introduces notable challenges: 1) Direct application of pretrained LLMs may not be competent for the denoising task, frequently generating nonsensical responses; 2) Even after fine-tuning, the reliability of LLM outputs remains questionable, especially given the complexity of the task and th inherent hallucinatory issue of LLMs.   To tackle these challenges, we propose LLM4DSR, a tailored approach for denoising sequential recommendation using LLMs.<span class='px-1 mx-1 bg-yellow-200'>We constructed a self-supervised fine-tuning task to activate LLMs' capabilities to identify noisy items and suggest replacements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.331</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, we developed an uncertainty estimation module that ensures only high-confidence responses are utilized for sequence corrections. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.394</span></span>Remarkably, LLM4DSR is model-agnostic, allowing the corrected sequences to be flexibly applied across various recommendation models.Extensive experiments validate the superiority of LLM4DSR over existing methods across three datasets and three recommendation backbones.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08208v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08208v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While various approaches have recently been studied for bias identification, little is known about how implicit language that does not explicitly convey a viewpoint affects bias amplification in large language models.To examine the severity of bias toward a view, we evaluated the performance of two downstream tasks where the implicit and explicit knowledge of social groups were used.First, we present a stress test evaluation by using a biased model in edge cases of excessive bias scenarios.Then, we evaluate how LLMs calibrate linguistically in response to both implicit and explicit opinions when they are aligned with conflicting viewpoints.Our findings reveal a discrepancy in LLM performance in identifying implicit and explicit opinions, with a general tendency of bias toward explicit opinions of opposing stances.<span class='px-1 mx-1 bg-yellow-200'>Moreover, the bias-aligned models generate more cautious responses using uncertainty phrases compared to the unaligned (zero-shot) base models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.336</span></span>The direct, incautious responses of the unaligned models suggest a need for further refinement of decisiveness by incorporating uncertainty markers to enhance their reliability, especially on socially nuanced topics with high subjectivity.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data.<span class='px-1 mx-1 bg-yellow-200'>However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.437</span></span>In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance.<span class='px-1 mx-1 bg-yellow-200'>Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.4</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08217v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08217v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems.Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment.<span class='px-1 mx-1 bg-yellow-200'>In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.314</span></span>Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved.<span class='px-1 mx-1 bg-yellow-200'>Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.474</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.311</span></span>Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer.Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks.Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08231v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08231v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HELP: Hierarchical Embeddings-based Log Parsing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Logs are a first-hand source of information for software maintenance and failure diagnosis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.306</span></span><span class='px-1 mx-1 bg-yellow-200'>Log parsing, which converts semi-structured log messages into structured templates, is a prerequisite for automated log analysis tasks such as anomaly detection, troubleshooting, and root cause analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.49</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing log parsers fail in real-world systems for three main reasons. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.412</span></span><span class='px-1 mx-1 bg-yellow-200'>First, traditional heuristics-based parsers require handcrafted features and domain knowledge, which are difficult to generalize at scale. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.437</span></span><span class='px-1 mx-1 bg-yellow-200'>Second, existing large language model-based parsers rely on periodic offline processing, limiting their effectiveness in real-time use cases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span><span class='px-1 mx-1 bg-yellow-200'>Third, existing online parsing algorithms are susceptible to log drift, where slight log changes create false positives that drown out real anomalies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.58</span></span>To address these challenges, we propose HELP, a Hierarchical Embeddings-based Log Parser.<span class='px-1 mx-1 bg-yellow-200'>HELP is the first online semantic-based parser to leverage LLMs for performant and cost-effective log parsing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span><span class='px-1 mx-1 bg-yellow-200'>We achieve this through a novel hierarchical embeddings module, which fine-tunes a text embedding model to cluster logs before parsing, reducing querying costs by multiple orders of magnitude. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.345</span></span><span class='px-1 mx-1 bg-yellow-200'>To combat log drift, we also develop an iterative rebalancing module, which periodically updates existing log groupings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.463</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate HELP extensively on 14 public large-scale datasets, showing that HELP achieves significantly higher F1-weighted grouping and parsing accuracy than current state-of-the-art online log parsers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span>We also implement HELP into Iudex's production observability platform, confirming HELP's practicality in a production environment.<span class='px-1 mx-1 bg-yellow-200'>Our results show that HELP is effective and efficient for high-throughput real-world log parsing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08300v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08300v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Understand Symbolic Graphics Programs?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training.We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data.LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs?Unlike conventional programs, symbolic graphics programs can be translated to graphics content.Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content.This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment.To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content.We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs.This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts.We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs.We find that this task distinguishes existing LLMs and models considered good at reasoning perform better.Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we query GPT4-o with questions and images generated by symbolic programs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.344</span></span>Such data are then used to finetune an LLM.We also find that SIT data can improve the general instruction following ability of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08313v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08313v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLM Model Architectures and Training Techniques</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Fine-tuning Large Language Models with Human-inspired Learning Strategies in Medical Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Training Large Language Models (LLMs) incurs substantial data-related costs, motivating the development of data-efficient training methods through optimised data ordering and selection.Human-inspired learning strategies, such as curriculum learning, offer possibilities for efficient training by organising data according to common human learning practices.<span class='px-1 mx-1 bg-yellow-200'>Despite evidence that fine-tuning with curriculum learning improves the performance of LLMs for natural language understanding tasks, its effectiveness is typically assessed using a single model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.429</span></span>In this work, we extend previous research by evaluating both curriculum-based and non-curriculum-based learning strategies across multiple LLMs, using human-defined and automated data labels for medical question answering.<span class='px-1 mx-1 bg-yellow-200'>Our results indicate a moderate impact of using human-inspired learning strategies for fine-tuning LLMs, with maximum accuracy gains of 1.77% per model and 1.81% per dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.498</span></span><span class='px-1 mx-1 bg-yellow-200'>Crucially, we demonstrate that the effectiveness of these strategies varies significantly across different model-dataset combinations, emphasising that the benefits of a specific human-inspired strategy for fine-tuning LLMs do not generalise. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.405</span></span>Additionally, we find evidence that curriculum learning using LLM-defined question difficulty outperforms human-defined difficulty, highlighting the potential of using model-generated measures for optimal curriculum design.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07888v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07888v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates the task of the open-ended interactive robotic manipulation on table-top scenarios.While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment.This is because the robot needs to locate the target object for manipulation within the physical workspace.To this end, we introduce an interactive robotic manipulation framework called Polaris, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models.For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image.Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline.This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks.<span class='px-1 mx-1 bg-yellow-200'>The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.406</span></span>Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks.This indicates its potential to generalize to scenarios beyond the tabletop.More information and video results are available here: https://star-uu-wang.github.io/Polaris/</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07975v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07975v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Surgical Video Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images.Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos.One major contributing factor is the absence of datasets in the surgical field.In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far.To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos.<span class='px-1 mx-1 bg-yellow-200'>The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.572</span></span>It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data.We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks.We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos.<span class='px-1 mx-1 bg-yellow-200'>We will release our code, model, and the instruction-tuning dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.532</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07981v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07981v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancements in Large Language Models (LLMs) have led to significant improvements in various natural language processing tasks.However, the evaluation of LLMs' legal knowledge, particularly in non-English languages such as Arabic, remains under-explored.To address this gap, we introduce ArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal knowledge of LLMs.Inspired by the MMLU and LegalBench datasets, ArabLegalEval consists of multiple tasks sourced from Saudi legal documents and synthesized questions.In this work, we aim to analyze the capabilities required to solve legal problems in Arabic and benchmark the performance of state-of-the-art LLMs.<span class='px-1 mx-1 bg-yellow-200'>We explore the impact of in-context learning and investigate various evaluation methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.474</span></span>Additionally, we explore workflows for generating questions with automatic validation to enhance the dataset's quality.<span class='px-1 mx-1 bg-yellow-200'>We benchmark multilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.498</span></span>We also share our methodology for creating the dataset and validation, which can be generalized to other domains.We hope to accelerate AI research in the Arabic Legal domain by releasing the ArabLegalEval dataset and code: https://github.com/Thiqah/ArabLegalEval</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07983v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07983v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FuseChat: Knowledge Fusion of Chat Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, it incurs substantial costs and may lead to redundancy in competencies.Knowledge fusion aims to integrate existing LLMs of diverse architectures and capabilities into a more potent LLM through lightweight continual training, thereby reducing the need for costly LLM development.In this work, we propose a new framework for the knowledge fusion of chat LLMs through two main stages, resulting in FuseChat.Firstly, we conduct pairwise knowledge fusion on source chat LLMs of varying structures and scales to create multiple target LLMs with identical structure and size via lightweight fine-tuning.<span class='px-1 mx-1 bg-yellow-200'>During this process, a statistics-based token alignment approach is introduced as the cornerstone for fusing LLMs with different structures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.468</span></span><span class='px-1 mx-1 bg-yellow-200'>Secondly, we merge these target LLMs within the parameter space, where we propose a novel method for determining the merging coefficients based on the magnitude of parameter updates before and after fine-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span><span class='px-1 mx-1 bg-yellow-200'>We implement and validate FuseChat using six prominent chat LLMs with diverse architectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha, NH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and Qwen-1.5-Chat-72B. Experimental results on two instruction-following benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of FuseChat-7B over baselines of various sizes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.419</span></span>Our model is even comparable to the larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.Our code, model weights, and data are public at \url{https://github.com/fanqiwan/FuseAI}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07990v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07990v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools.This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry.To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions.This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.416</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.55</span></span>The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input.Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08054v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08054v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Confidence-weighted integration of human and machine judgments for superior decision-making
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have emerged as powerful tools in various domains.Recent studies have shown that LLMs can surpass humans in certain tasks, such as predicting the outcomes of neuroscience studies.What role does this leave for humans in the overall decision process?<span class='px-1 mx-1 bg-yellow-200'>One possibility is that humans, despite performing worse than LLMs, can still add value when teamed with them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.448</span></span>A human and machine team can surpass each individual teammate when team members' confidence is well-calibrated and team members diverge in which tasks they find difficult (i.e., calibration and diversity are needed).We simplified and extended a Bayesian approach to combining judgments using a logistic regression framework that integrates confidence-weighted judgments for any number of team members.Using this straightforward method, we demonstrated in a neuroscience forecasting task that, even when humans were inferior to LLMs, their combination with one or more LLMs consistently improved team performance.Our hope is that this simple and effective strategy for integrating the judgments of humans and machines will lead to productive collaborations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08083v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08083v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although Large Language Models(LLMs) can generate coherent and contextually relevant text, they often struggle to recognise the intent behind the human user's query.Natural Language Understanding (NLU) models, however, interpret the purpose and key information of user's input to enable responsive interactions.Existing NLU models generally map individual utterances to a dual-level semantic frame, involving sentence-level intent and word-level slot labels.However, real-life conversations primarily consist of multi-turn conversations, involving the interpretation of complex and extended dialogues.Researchers encounter challenges addressing all facets of multi-turn dialogue conversations using a unified single NLU model.<span class='px-1 mx-1 bg-yellow-200'>This paper introduces a novel approach, MIDAS, leveraging a multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.531</span></span>To achieve this, we construct distinct teachers for varying levels of conversation knowledge, namely, sentence-level intent detection, word-level slot filling, and conversation-level domain classification.These teachers are then fine-tuned to acquire specific knowledge of their designated levels.<span class='px-1 mx-1 bg-yellow-200'>A multi-teacher loss is proposed to facilitate the combination of these multi-level teachers, guiding a student model in multi-turn dialogue tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.418</span></span>The experimental results demonstrate the efficacy of our model in improving the overall multi-turn conversation understanding, showcasing the potential for advancements in NLU models through the incorporation of multi-level dialogue knowledge distillation techniques.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08144v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08144v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) exhibit high inference latency due to their autoregressive decoding nature.<span class='px-1 mx-1 bg-yellow-200'>While the draft head in speculative decoding mitigates this issue, its full potential remains unexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.41</span></span>In this paper, we introduce KOALA (K-layer Optimized Adversarial Learning Architecture), an orthogonal approach to the draft head.By transforming the conventional single-layer draft head into a multi-layer architecture and incorporating adversarial learning into the traditional supervised training, KOALA significantly improves the accuracy of the draft head in predicting subsequent tokens, thus more closely mirroring the functionality of LLMs.Although this improvement comes at the cost of slightly increased drafting overhead, KOALA substantially unlocks the draft head's potential, greatly enhancing speculative decoding.<span class='px-1 mx-1 bg-yellow-200'>We conducted comprehensive evaluations of KOALA, including both autoregressive and non-autoregressive draft heads across various tasks, demonstrating a latency speedup ratio improvement of 0.24x-0.41x, which is 10.57%-14.09% faster than the original draft heads. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.418</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08146v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08146v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                P/D-Serve: Serving Disaggregated Large Language Model at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Serving disaggregated large language models (LLMs) over tens of thousands of xPU devices (GPUs or NPUs) with reliable performance faces multiple challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.582</span></span><span class='px-1 mx-1 bg-yellow-200'>1) Ignoring the diversity (various prefixes and tidal requests), treating all the prompts in a mixed pool is inadequate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.496</span></span><span class='px-1 mx-1 bg-yellow-200'>To facilitate the similarity per scenario and minimize the inner mismatch on P/D (prefill and decoding) processing, fine-grained organization is required, dynamically adjusting P/D ratios for better performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.454</span></span><span class='px-1 mx-1 bg-yellow-200'>2) Due to inaccurate estimation on workload (queue status or maintained connections), the global scheduler easily incurs unnecessary timeouts in prefill. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span><span class='px-1 mx-1 bg-yellow-200'>3) Block-fixed device-to-device (D2D) KVCache transfer over cluster-level RDMA (remote direct memory access) fails to achieve desired D2D utilization as expected. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.529</span></span><span class='px-1 mx-1 bg-yellow-200'>To overcome previous problems, this paper proposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps (machine learning operations), which models end-to-end (E2E) P/D performance and enables: 1) fine-grained P/D organization, mapping the service with RoCE (RDMA over converged ethernet) as needed, to facilitate similar processing and dynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for idle prefill, decoupling the scheduler from regular inaccurate reports and local queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer via optimized D2D access. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span><span class='px-1 mx-1 bg-yellow-200'>P/D-Serve is implemented upon Ascend and MindSpore, has been deployed over tens of thousands of NPUs for more than eight months in commercial use, and further achieves 60\%, 42\% and 46\% improvements on E2E throughput, time-to-first-token (TTFT) SLO (service level objective) and D2D transfer time. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span><span class='px-1 mx-1 bg-yellow-200'>As the E2E system with optimizations, P/D-Serve achieves 6.7x increase on throughput, compared with aggregated LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.534</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08147v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08147v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EmBARDiment: an Embodied AI Agent for Productivity in XR
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>XR devices running chat-bots powered by Large Language Models (LLMs) have tremendous potential as always-on agents that can enable much better productivity scenarios.However, screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query.<span class='px-1 mx-1 bg-yellow-200'>We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.427</span></span>This minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.Our user studies demonstrate the imminent feasibility and transformative potential of our approach to streamline user interaction in XR with chat-bots, while offering insights for the design of future XR-embodied LLM agents.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08158v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08158v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Scaling Up Natural Language Understanding for Multi-Robots Through the Lens of Hierarchy
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Long-horizon planning is hindered by challenges such as uncertainty accumulation, computational complexity, delayed rewards and incomplete information.This work proposes an approach to exploit the task hierarchy from human instructions to facilitate multi-robot planning.Using Large Language Models (LLMs), we propose a two-step approach to translate multi-sentence instructions into a structured language, Hierarchical Linear Temporal Logic (LTL), which serves as a formal representation for planning.Initially, LLMs transform the instructions into a hierarchical representation defined as Hierarchical Task Tree, capturing the logical and temporal relations among tasks.<span class='px-1 mx-1 bg-yellow-200'>Following this, a domain-specific fine-tuning of LLM translates sub-tasks of each task into flat LTL formulas, aggregating them to form hierarchical LTL specifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.467</span></span>These specifications are then leveraged for planning using off-the-shelf planners.Our framework not only bridges the gap between instructions and algorithmic planning but also showcases the potential of LLMs in harnessing hierarchical reasoning to automate multi-robot task planning.Through evaluations in both simulation and real-world experiments involving human participants, we demonstrate that our method can handle more complex instructions compared to existing methods.The results indicate that our approach achieves higher success rates and lower costs in multi-robot task allocation and plan generation.Demos videos are available at https://youtu.be/7WOrDKxIMIs .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08188v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08188v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Sequential recommendation systems fundamentally rely on users' historical interaction sequences, which are often contaminated by noisy interactions.Identifying these noisy interactions accurately without additional information is particularly difficult due to the lack of explicit supervisory signals to denote noise.Large Language Models (LLMs), equipped with extensive open knowledge and semantic reasoning abilities, present a promising avenue to bridge this information gap.However, employing LLMs for denoising in sequential recommendation introduces notable challenges: 1) Direct application of pretrained LLMs may not be competent for the denoising task, frequently generating nonsensical responses; 2) Even after fine-tuning, the reliability of LLM outputs remains questionable, especially given the complexity of the task and th inherent hallucinatory issue of LLMs.   To tackle these challenges, we propose LLM4DSR, a tailored approach for denoising sequential recommendation using LLMs.<span class='px-1 mx-1 bg-yellow-200'>We constructed a self-supervised fine-tuning task to activate LLMs' capabilities to identify noisy items and suggest replacements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.472</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, we developed an uncertainty estimation module that ensures only high-confidence responses are utilized for sequence corrections. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.419</span></span>Remarkably, LLM4DSR is model-agnostic, allowing the corrected sequences to be flexibly applied across various recommendation models.Extensive experiments validate the superiority of LLM4DSR over existing methods across three datasets and three recommendation backbones.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08208v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08208v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>While various approaches have recently been studied for bias identification, little is known about how implicit language that does not explicitly convey a viewpoint affects bias amplification in large language models.To examine the severity of bias toward a view, we evaluated the performance of two downstream tasks where the implicit and explicit knowledge of social groups were used.<span class='px-1 mx-1 bg-yellow-200'>First, we present a stress test evaluation by using a biased model in edge cases of excessive bias scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span>Then, we evaluate how LLMs calibrate linguistically in response to both implicit and explicit opinions when they are aligned with conflicting viewpoints.Our findings reveal a discrepancy in LLM performance in identifying implicit and explicit opinions, with a general tendency of bias toward explicit opinions of opposing stances.Moreover, the bias-aligned models generate more cautious responses using uncertainty phrases compared to the unaligned (zero-shot) base models.The direct, incautious responses of the unaligned models suggest a need for further refinement of decisiveness by incorporating uncertainty markers to enhance their reliability, especially on socially nuanced topics with high subjectivity.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08212v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08212v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data.<span class='px-1 mx-1 bg-yellow-200'>However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.54</span></span><span class='px-1 mx-1 bg-yellow-200'>In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.479</span></span><span class='px-1 mx-1 bg-yellow-200'>Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.42</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08217v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08217v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems.Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment.In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem.Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved.<span class='px-1 mx-1 bg-yellow-200'>Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.493</span></span>Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization.Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer.Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks.Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08231v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08231v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HELP: Hierarchical Embeddings-based Log Parsing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Logs are a first-hand source of information for software maintenance and failure diagnosis.<span class='px-1 mx-1 bg-yellow-200'>Log parsing, which converts semi-structured log messages into structured templates, is a prerequisite for automated log analysis tasks such as anomaly detection, troubleshooting, and root cause analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.435</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing log parsers fail in real-world systems for three main reasons. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.418</span></span>First, traditional heuristics-based parsers require handcrafted features and domain knowledge, which are difficult to generalize at scale.Second, existing large language model-based parsers rely on periodic offline processing, limiting their effectiveness in real-time use cases.Third, existing online parsing algorithms are susceptible to log drift, where slight log changes create false positives that drown out real anomalies.To address these challenges, we propose HELP, a Hierarchical Embeddings-based Log Parser.<span class='px-1 mx-1 bg-yellow-200'>HELP is the first online semantic-based parser to leverage LLMs for performant and cost-effective log parsing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.426</span></span>We achieve this through a novel hierarchical embeddings module, which fine-tunes a text embedding model to cluster logs before parsing, reducing querying costs by multiple orders of magnitude.<span class='px-1 mx-1 bg-yellow-200'>To combat log drift, we also develop an iterative rebalancing module, which periodically updates existing log groupings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.565</span></span>We evaluate HELP extensively on 14 public large-scale datasets, showing that HELP achieves significantly higher F1-weighted grouping and parsing accuracy than current state-of-the-art online log parsers.<span class='px-1 mx-1 bg-yellow-200'>We also implement HELP into Iudex's production observability platform, confirming HELP's practicality in a production environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.425</span></span>Our results show that HELP is effective and efficient for high-throughput real-world log parsing.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08300v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08300v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Large Language Models Understand Symbolic Graphics Programs?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Assessing the capabilities of large language models (LLMs) is often challenging, in part, because it is hard to find tasks to which they have not been exposed during training.We take one step to address this challenge by turning to a new task: focusing on symbolic graphics programs, which are a popular representation for graphics content that procedurally generates visual data.LLMs have shown exciting promise towards program synthesis, but do they understand symbolic graphics programs?Unlike conventional programs, symbolic graphics programs can be translated to graphics content.Here, we characterize an LLM's understanding of symbolic programs in terms of their ability to answer questions related to the graphics content.This task is challenging as the questions are difficult to answer from the symbolic programs alone -- yet, they would be easy to answer from the corresponding graphics content as we verify through a human experiment.To understand symbolic programs, LLMs may need to possess the ability to imagine how the corresponding graphics content would look without directly accessing the rendered visual content.We use this task to evaluate LLMs by creating a large benchmark for the semantic understanding of symbolic graphics programs.<span class='px-1 mx-1 bg-yellow-200'>This benchmark is built via program-graphics correspondence, hence requiring minimal human efforts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.443</span></span>We evaluate current LLMs on our benchmark to elucidate a preliminary assessment of their ability to reason about visual scenes from programs.We find that this task distinguishes existing LLMs and models considered good at reasoning perform better.<span class='px-1 mx-1 bg-yellow-200'>Lastly, we introduce Symbolic Instruction Tuning (SIT) to improve this ability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.607</span></span>Specifically, we query GPT4-o with questions and images generated by symbolic programs.Such data are then used to finetune an LLM.<span class='px-1 mx-1 bg-yellow-200'>We also find that SIT data can improve the general instruction following ability of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.426</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08313v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08313v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness.<span class='px-1 mx-1 bg-yellow-200'>Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.429</span></span>Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation.Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods.<span class='px-1 mx-1 bg-yellow-200'>These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.413</span></span>Moreover, previous studies do not approach the problem at a suitable scale for real-life applications.<span class='px-1 mx-1 bg-yellow-200'>Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.419</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these gaps, we have developed an approach for generating and evaluating more real-life complexity test suites. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.478</span></span><span class='px-1 mx-1 bg-yellow-200'>Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.491</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we present \textsc{AgoneTest}: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.44</span></span>Starting from a state-of-the-art dataset (i.e., \textsc{Methods2Test}), we built a new dataset for comparing human-written tests with those generated by LLMs.<span class='px-1 mx-1 bg-yellow-200'>Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.45</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07846v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07846v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Programming applications of LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Assessing Language Models' Worldview for Fiction Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The use of Large Language Models (LLMs) has become ubiquitous, with abundant applications in computational creativity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>One such application is fictional story generation.Fiction is a narrative that occurs in a story world that is slightly different than ours.With LLMs becoming writing partners, we question how suitable they are to generate fiction.This study investigates the ability of LLMs to maintain a state of world essential to generate fiction.Through a series of questions to nine LLMs, we find that only two models exhibit consistent worldview, while the rest are self-conflicting.Subsequent analysis of stories generated by four models revealed a strikingly uniform narrative pattern.This uniformity across models further suggests a lack of `state' necessary for fiction.We highlight the limitations of current LLMs in fiction writing and advocate for future research to test and create story worlds for LLMs to reside in.All code, dataset, and the generated responses can be found in https://github.com/tanny411/llm-reliability-and-consistency-evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07904v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07904v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-15</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools.This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry.To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions.This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool's APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software.Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality.Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework.The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input.<span class='px-1 mx-1 bg-yellow-200'>Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.642</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.08054v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.08054v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-Enhanced Static Analysis for Precise Identification of Vulnerable OSS Versions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative development model and cost-effective nature.However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities.Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules.They then use syntactic-level code clone detection to identify the vulnerable versions.These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection.This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++.<span class='px-1 mx-1 bg-yellow-200'>Vercation combines program slicing with a Large Language Model (LLM) to identify vulnerability-relevant code from vulnerability patches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code.<span class='px-1 mx-1 bg-yellow-200'>We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation.On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods.More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07321v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07321v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness.Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation.<span class='px-1 mx-1 bg-yellow-200'>Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.872</span></span><span class='px-1 mx-1 bg-yellow-200'>Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.707</span></span>These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios.Moreover, previous studies do not approach the problem at a suitable scale for real-life applications.Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency.To address these gaps, we have developed an approach for generating and evaluating more real-life complexity test suites.<span class='px-1 mx-1 bg-yellow-200'>Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>In this work, we present \textsc{AgoneTest}: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites.Starting from a state-of-the-art dataset (i.e., \textsc{Methods2Test}), we built a new dataset for comparing human-written tests with those generated by LLMs.Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07846v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07846v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HLSPilot: LLM-based High-Level Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have catalyzed an upsurge in automatic code generation, garnering significant attention for register transfer level (RTL) code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.856</span></span>Despite the potential of RTL code generation with natural language, it remains error-prone and limited to relatively small modules because of the substantial semantic gap between natural language expressions and hardware design intent.In response to the limitations, we propose a methodology that reduces the semantic gaps by utilizing C/C++ for generating hardware designs via High-Level Synthesis (HLS) tools.Basically, we build a set of C-to-HLS optimization strategies catering to various code patterns, such as nested loops and local arrays.Then, we apply these strategies to sequential C/C++ code through in-context learning, which provides the LLMs with exemplary C/C++ to HLS prompts.With this approach, HLS designs can be generated effectively.Since LLMs still face problems in determining the optimized pragma parameters precisely, we have a design space exploration (DSE) tool integrated for pragma parameter tuning.Furthermore, we also employ profiling tools to pinpoint the performance bottlenecks within a program and selectively convert bottleneck components to HLS code for hardware acceleration.By combining the LLM-based profiling, C/C++ to HLS translation, and DSE, we have established HLSPilot, the first LLM-enabled high-level synthesis framework, which can fully automate the high-level application acceleration on hybrid CPU-FPGA architectures.According to our experiments on real-world application benchmarks, HLSPilot achieve comparable performance in general and can even outperform manually crafted counterparts, thereby underscoring the substantial promise of LLM-assisted hardware designs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06810v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06810v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.78</span></span>The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others.To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise.DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving.Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin.For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions.Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite.Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.07060v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.07060v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Most vulnerability detection studies focus on datasets of vulnerabilities in C/C++ code, offering limited language diversity.Thus, the effectiveness of deep learning methods, including large language models (LLMs), in detecting software vulnerabilities beyond these languages is still largely unexplored.In this paper, we evaluate the effectiveness of LLMs in detecting and classifying Common Weakness Enumerations (CWE) using different prompt and role strategies.Our experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5- Turbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro) and five programming languages: Python, C, C++, Java, and JavaScript.We compiled a multi-language vulnerability dataset from different sources, to ensure representativeness.Our results showed that GPT-4o achieves the highest vulnerability detection and CWE classification scores using a few-shot setting.Aside from the quantitative results of our study, we developed a library called CODEGUARDIAN integrated with VSCode which enables developers to perform LLM-assisted real-time vulnerability analysis in real-world security scenarios.<span class='px-1 mx-1 bg-yellow-200'>We have evaluated CODEGUARDIAN with a user study involving 22 developers from the industry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.693</span></span>Our study showed that, by using CODEGUARDIAN, developers are more accurate and faster at detecting vulnerabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06428v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06428v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating Language Models for Efficient Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.841</span></span>Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics.DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation.DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions.To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score.As a proof of concept, we use DPE to create EvalPerf, a benchmark with 121 performance-challenging coding tasks.Our comprehensive evaluation draws interesting findings on the efficiency impact of model sizes, instruction tuning, and prompting.For example, while the scaling law fails to account for code efficiency, general instruction tuning benefits both code correctness and efficiency.We also evaluate the evaluation by examining the effectiveness of DPE, showing that EvalPerf is reliable and convenient to use even across platforms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06450v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06450v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Creating Arabic LLM Prompts at Scale
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The debut of chatGPT and BARD has popularized instruction following text generation using LLMs, where a user can interrogate an LLM using natural language requests and obtain natural language answers that matches their requests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span>Training LLMs to respond in this manner requires a large number of worked out examples of user requests (aka prompts) with corresponding gold responses.In this paper, we introduce two methods for creating such prompts for Arabic cheaply and quickly.The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only.The second method involves creating natural language prompts on top of existing Arabic NLP datasets.Using these two methods we were able to create more than 67.4 million Arabic prompts that cover a variety of tasks including summarization, headline generation, grammar checking, open/closed question answering, creative writing, etc.We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05882v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05882v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps).Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic.However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness.In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>Finally, HYPERION reports the inconsistency based on predefined detection patterns.The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies.We also implement HYPERION to analyze 835 real-world DApps.The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.06037v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.06037v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Code generation has been greatly enhanced by the profound advancements in Large Language Models (LLMs) recently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.953</span></span><span class='px-1 mx-1 bg-yellow-200'>Nevertheless, such LLM-based code generation approaches still struggle to generate error-free code in a few tries when faced with complex problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>To address this, the prevailing strategy is to sample a huge number of candidate programs, with the hope of any one in them could work.However, users of code generation systems usually expect to find a correct program by reviewing or testing only a small number of code candidates.Otherwise, the system would be unhelpful.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose Top Pass, a code ranking approach that identifies potential correct solutions from a large number of candidates. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span>Top Pass directly optimizes the pass@k loss function, enhancing the quality at the top of the candidate list.This enables the user to find the correct solution within as few tries as possible.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on four benchmarks indicate that our Top Pass method enhances the usability of code generation models by producing better ranking results, particularly achieving a 32.9\% relative improvement in pass@1 on CodeContests when compared to the state-of-the-art ranking method. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.861</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05715v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05715v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Supporting Software Maintenance with Dynamically Generated Document Hierarchies
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software documentation supports a broad set of software maintenance tasks; however, creating and maintaining high-quality, multi-level software documentation can be incredibly time-consuming and therefore many code bases suffer from a lack of adequate documentation.<span class='px-1 mx-1 bg-yellow-200'>We address this problem through presenting HGEN, a fully automated pipeline that leverages LLMs to transform source code through a series of six stages into a well-organized hierarchy of formatted documents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>We evaluate HGEN both quantitatively and qualitatively.First, we use it to generate documentation for three diverse projects, and engage key developers in comparing the quality of the generated documentation against their own previously produced manually-crafted documentation.We then pilot HGEN in nine different industrial projects using diverse datasets provided by each project.We collect feedback from project stakeholders, and analyze it using an inductive approach to identify recurring themes.Results show that HGEN produces artifact hierarchies similar in quality to manually constructed documentation, with much higher coverage of the core concepts than the baseline approach.Stakeholder feedback highlights HGEN's commercial impact potential as a tool for accelerating code comprehension and maintenance tasks.Results and associated supplemental materials can be found at https://zenodo.org/records/11403244</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.05829v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.05829v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Semantic-Enhanced Indirect Call Analysis with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In contemporary software development, the widespread use of indirect calls to achieve dynamic features poses challenges in constructing precise control flow graphs (CFGs), which further impacts the performance of downstream static analysis tasks.To tackle this issue, various types of indirect call analyzers have been proposed.However, they do not fully leverage the semantic information of the program, limiting their effectiveness in real-world scenarios.To address these issues, this paper proposes Semantic-Enhanced Analysis (SEA), a new approach to enhance the effectiveness of indirect call analysis.Our fundamental insight is that for common programming practices, indirect calls often exhibit semantic similarity with their invoked targets.This semantic alignment serves as a supportive mechanism for static analysis techniques in filtering out false targets.<span class='px-1 mx-1 bg-yellow-200'>Notably, contemporary large language models (LLMs) are trained on extensive code corpora, encompassing tasks such as code summarization, making them well-suited for semantic analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span>Specifically, SEA leverages LLMs to generate natural language summaries of both indirect calls and target functions from multiple perspectives.Through further analysis of these summaries, SEA can determine their suitability as caller-callee pairs.Experimental results demonstrate that SEA can significantly enhance existing static analysis methods by producing more precise target sets for indirect calls.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04344v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04344v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators.<span class='px-1 mx-1 bg-yellow-200'>Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population.Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels.In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy.We use advanced prompting techniques with varying complexity for AEQG.We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions.Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered.We also show that automated evaluation is not on par with human evaluation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04394v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04394v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for cross-language code clone detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the involvement of multiple programming languages in modern software development, cross-lingual code clone detection has gained traction with the software engineering community. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.762</span></span>Numerous studies have explored this topic, proposing various promising approaches.Inspired by the significant advances in machine learning in recent years, particularly Large Language Models (LLMs), which have demonstrated their ability to tackle various tasks, this paper revisits cross-lingual code clone detection.   We investigate the capabilities of four (04) LLMs and eight (08) prompts for the identification of cross-lingual code clones.Additionally, we evaluate a pre-trained embedding model to assess the effectiveness of the generated representations for classifying clone and non-clone pairs.Both studies (based on LLMs and Embedding models) are evaluated using two widely used cross-lingual datasets, XLCoST and CodeNet.Our results show that LLMs can achieve high F1 scores, up to 0.98, for straightforward programming examples (e.g., from XLCoST).<span class='px-1 mx-1 bg-yellow-200'>However, they not only perform less well on programs associated with complex programming challenges but also do not necessarily understand the meaning of code clones in a cross-lingual setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span><span class='px-1 mx-1 bg-yellow-200'>We show that embedding models used to represent code fragments from different programming languages in the same representation space enable the training of a basic classifier that outperforms all LLMs by ~2 and ~24 percentage points on the XLCoST and CodeNet datasets, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.735</span></span>This finding suggests that, despite the apparent capabilities of LLMs, embeddings provided by embedding models offer suitable representations to achieve state-of-the-art performance in cross-lingual code clone detection.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04430v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04430v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                What You Need is What You Get: Theory of Mind for an LLM-Based Code Understanding Assistant
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>A growing number of tools have used Large Language Models (LLMs) to support developers' code understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.934</span></span>However, developers still face several barriers to using such tools, including challenges in describing their intent in natural language, interpreting the tool outcome, and refining an effective prompt to obtain useful information.In this study, we designed an LLM-based conversational assistant that provides a personalized interaction based on inferred user mental state (e.g., background knowledge and experience).We evaluate the approach in a within-subject study with fourteen novices to capture their perceptions and preferences.<span class='px-1 mx-1 bg-yellow-200'>Our results provide insights for researchers and tool builders who want to create or improve LLM-based conversational assistants to support novices in code understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.868</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04477v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04477v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-08</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have exhibited remarkable proficiency across a diverse array of natural language processing (NLP) tasks.However, adapting LLMs to downstream applications typically necessitates computationally intensive and memory-demanding fine-tuning procedures.To mitigate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead.While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data.In this work, we introduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance.BA-LoRA incorporates three distinct regularization terms: (1) consistency regularizer, (2) diversity regularizer, and (3) singular vector decomposition regularizer.These regularizers collectively aim to improve the generative models' consistency, diversity, and generalization capabilities during the fine-tuning process.<span class='px-1 mx-1 bg-yellow-200'>Through extensive experiments on a variety of natural language understanding (NLU) and natural language generation (NLG) tasks, employing prominent LLMs such as LLaMA, Mistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of LoRA and its state-of-the-art variants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>Moreover, our method effectively mitigates the deleterious effects of pre-training bias, leading to more reliable and robust model outputs.The code is available at https://github.com/cyp-jlu-ai/BA-LoRA.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04556v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04556v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Harnessing the Power of LLMs in Source Code Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Software vulnerabilities, caused by unintentional flaws in source code, are a primary root cause of cyberattacks.<span class='px-1 mx-1 bg-yellow-200'>Static analysis of source code has been widely used to detect these unintentional defects introduced by software developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>Large Language Models (LLMs) have demonstrated human-like conversational abilities due to their capacity to capture complex patterns in sequential data, such as natural languages.In this paper, we harness LLMs' capabilities to analyze source code and detect known vulnerabilities.To ensure the proposed vulnerability detection method is universal across multiple programming languages, we convert source code to LLVM IR and train LLMs on these intermediate representations.We conduct extensive experiments on various LLM architectures and compare their accuracy.Our comprehensive experiments on real-world and synthetic codes from NVD and SARD demonstrate high accuracy in identifying source code vulnerabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03489v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03489v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RepoMasterEval: Evaluating Code Completion via Real-World Repositories
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span><span class='px-1 mx-1 bg-yellow-200'>However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block.These limitations makes the evaluation poorly align with the practical scenarios of code completion tools.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.83</span></span>Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites.To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score.Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios.The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice.<span class='px-1 mx-1 bg-yellow-200'>Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03519v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03519v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring the extent of similarities in software failures across industries using LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid evolution of software development necessitates enhanced safety measures.Extracting information about software failures from companies is becoming increasingly more available through news articles.   This research utilizes the Failure Analysis Investigation with LLMs (FAIL) model to extract industry-specific information.Although the FAIL model's database is rich in information, it could benefit from further categorization and industry-specific insights to further assist software engineers.   In previous work news articles were collected from reputable sources and categorized by incidents inside a database.Prompt engineering and Large Language Models (LLMs) were then applied to extract relevant information regarding the software failure.This research extends these methods by categorizing articles into specific domains and types of software failures.The results are visually represented through graphs.   The analysis shows that throughout the database some software failures occur significantly more often in specific industries.This categorization provides a valuable resource for software engineers and companies to identify and address common failures.   <span class='px-1 mx-1 bg-yellow-200'>This research highlights the synergy between software engineering and Large Language Models (LLMs) to automate and enhance the analysis of software failures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.718</span></span>By transforming data from the database into an industry specific model, we provide a valuable resource that can be used to identify common vulnerabilities, predict potential risks, and implement proactive measures for preventing software failures.Leveraging the power of the current FAIL database and data visualization, we aim to provide an avenue for safer and more secure software in the future.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03528v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03528v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span>These complex tasks require AI to continuously summarize and improve.Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference.This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing.We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differentiation, and design the MaxMind model for SOTG accordingly.To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the MaxMind philosophy.Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example.Note that as the memories continue to grow, this cumulative improvement may be substantial.The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer.These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03841v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03841v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.87</span></span><span class='px-1 mx-1 bg-yellow-200'>This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks.Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications.To mitigate these limitations, we introduce \framework, a system that integrates LLM agents with graph database interfaces extracted from code repositories.By leveraging the structural properties of graph databases and the flexibility of the graph query language, \framework enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation.We assess \framework using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench.Additionally, we develop five real-world coding applications.With a unified graph database schema, \framework demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering.Our application demo: https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03910v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03910v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring RAG-based Vulnerability Augmentation with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Detecting vulnerabilities is a crucial task for maintaining the integrity, availability, and security of software systems.Utilizing DL-based models for vulnerability detection has become commonplace in recent years.However, such deep learning-based vulnerability detectors (DLVD) suffer from a shortage of sizable datasets to train effectively.Data augmentation can potentially alleviate the shortage of data, but augmenting vulnerable code is challenging and requires designing a generative solution that maintains vulnerability.Hence, the work on generating vulnerable code samples has been limited and previous works have only focused on generating samples that contain single statements or specific types of vulnerabilities.<span class='px-1 mx-1 bg-yellow-200'>Lately, large language models (LLMs) are being used for solving various code generation and comprehension tasks and have shown inspiring results, especially when fused with retrieval augmented generation (RAG). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.881</span></span>In this study, we explore three different strategies to augment vulnerabilities both single and multi-statement vulnerabilities, with LLMs, namely Mutation, Injection, and Extension.We conducted an extensive evaluation of our proposed approach on three vulnerability datasets and three DLVD models, using two LLMs.Our results show that our injection-based clustering-enhanced RAG method beats the baseline setting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling (ROS) by 30.80\%, 27.48\%, 27.93\%, and 15.41\% in f1-score with 5K generated vulnerable samples on average, and 53.84\%, 54.10\%, 69.90\%, and 40.93\% with 15K generated vulnerable samples.Our approach demonstrates its feasibility for large-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.04125v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.04125v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                TestART: Improving LLM-based Unit Test via Co-evolution of Automated Generation and Repair Iteration
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Unit test is crucial for detecting bugs in individual program units but consumes time and effort.<span class='px-1 mx-1 bg-yellow-200'>The existing automated unit test generation methods are mainly based on search-based software testing (SBST) and language models to liberate developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span><span class='px-1 mx-1 bg-yellow-200'>Recently, large language models (LLMs) have demonstrated remarkable reasoning and generation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>However, several problems limit their ability to generate high-quality test cases: (1) LLMs may generate invalid test cases under insufficient context, resulting in compilation errors; (2) Lack of test and coverage feedback information may cause runtime errors and low coverage rates.(3) The repetitive suppression problem causes LLMs to get stuck into the repetition loop of self-repair or re-generation attempts.In this paper, we propose TestART, a novel unit test generation method that leverages the strengths of LLMs while overcoming the limitations mentioned.TestART improves LLM-based unit test via co-evolution of automated generation and repair iteration.TestART leverages the template-based repair technique to fix bugs in LLM-generated test cases, using prompt injection to guide the next-step automated generation and avoid repetition suppression.Furthermore, TestART extracts coverage information from the passed test cases and utilizes it as testing feedback to enhance the sufficiency of the final test case.This synergy between generation and repair elevates the quality, effectiveness, and readability of the produced test cases significantly beyond previous methods.In comparative experiments, the pass rate of TestART-generated test cases is 78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and the same ChatGPT-3.5-based method ChatUniTest.It also achieves an impressive line coverage rate of 90.96% on the focal methods that passed the test, exceeding EvoSuite by 3.4%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03095v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03095v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Translation Performance of Large Language Models Based on Euas-20
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap.Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges.Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03119v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03119v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs).We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM).We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset.The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency.<span class='px-1 mx-1 bg-yellow-200'>All the developed code is publicly available on a GitHub repository <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.736</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.03127v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.03127v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have shown great potential in code-related tasks, yet open-source models lag behind their closed-source counterparts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.865</span></span>To bridge this performance gap, existing methods generate vast amounts of synthetic data for fine-tuning, leading to inefficiencies in training.Motivated by the need for more effective and efficient training, we propose the Code Adaptive Compute-efficient Tuning (CodeACT) framework.CodeACT introduces the Complexity and Diversity Aware Sampling (CDAS) method to select high-quality training data based on complexity and diversity, and the Dynamic Pack padding strategy to reduce computational resource usage by minimizing padding tokens during training.Experimental results demonstrate that CodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data, achieves an 8.6% performance increase on HumanEval, reduces training time by 78%, and decreases peak GPU memory usage by 27%.<span class='px-1 mx-1 bg-yellow-200'>These findings underscore CodeACT's ability to enhance the performance and efficiency of open-source models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.741</span></span><span class='px-1 mx-1 bg-yellow-200'>By optimizing both the data selection and training processes, CodeACT offers a comprehensive approach to improving the capabilities of open-source LLMs while significantly reducing computational requirements, addressing the dual challenges of data quality and training efficiency, and paving the way for more resource-efficient and performant models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02193v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02193v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Evaluating the Performance of Large Language Models for SDG Mapping (Technical Report)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The use of large language models (LLMs) is expanding rapidly, and open-source versions are becoming available, offering users safer and more adaptable options. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>These models enable users to protect data privacy by eliminating the need to provide data to third parties and can be customized for specific tasks.In this study, we compare the performance of various language models on the Sustainable Development Goal (SDG) mapping task, using the output of GPT-4o as the baseline.The selected open-source models for comparison include Mixtral, LLaMA 2, LLaMA 3, Gemma, and Qwen2.Additionally, GPT-4o-mini, a more specialized version of GPT-4o, was included to extend the comparison.Given the multi-label nature of the SDG mapping task, we employed metrics such as F1 score, precision, and recall with micro-averaging to evaluate different aspects of the models' performance.These metrics are derived from the confusion matrix to ensure a comprehensive evaluation.We provide a clear observation and analysis of each model's performance by plotting curves based on F1 score, precision, and recall at different thresholds.According to the results of this experiment, LLaMA 2 and Gemma still have significant room for improvement.The other four models do not exhibit particularly large differences in performance.The outputs from all seven models are available on Zenodo: https://doi.org/10.5281/zenodo.12789375.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02201v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02201v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite advances in text-to-3D generation methods, generation of multi-object arrangements remains challenging.Current methods exhibit failures in generating physically plausible arrangements that respect the provided text description.We present SceneMotifCoder (SMC), an example-driven framework for generating 3D object arrangements through visual program learning.<span class='px-1 mx-1 bg-yellow-200'>SMC leverages large language models (LLMs) and program synthesis to overcome these challenges by learning visual programs from example arrangements. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>These programs are generalized into compact, editable meta-programs.When combined with 3D object retrieval and geometry-aware optimization, they can be used to create object arrangements varying in arrangement structure and contained objects.Our experiments show that SMC generates high-quality arrangements using meta-programs learned from few examples.Evaluation results demonstrates that object arrangements generated by SMC better conform to user-specified text descriptions and are more physically plausible when compared with state-of-the-art text-to-3D generation and layout methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02211v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02211v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                SpecRover: Code Intent Extraction via LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Autonomous program improvement typically involves automatically producing bug fixes and feature additions.<span class='px-1 mx-1 bg-yellow-200'>Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches.In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent.Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior.The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches.Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover.In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover.Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite.The production of explanation by SpecRover allows for a better "signal" to be given to the developer, on when the suggested patches can be accepted with confidence.SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02232v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02232v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.<span class='px-1 mx-1 bg-yellow-200'>LLMs have achieved remarkable success in areas including code generation and vulnerability detection. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>However, they also exhibit numerous limitations and shortcomings.LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents.It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain.In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering.In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance.We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering.We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02479v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02479v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A First Look at License Compliance Capability of LLMs in Code Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.902</span></span>However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production.This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code.To establish this benchmark, we conduct an empirical study to identify a reasonable standard for "striking similarity" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code.Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs.<span class='px-1 mx-1 bg-yellow-200'>Using LiCoEval, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.649</span></span>Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses.These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks.Our study provides a foundation for future research and development to improve license compliance in AI-assisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02487v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02487v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-08-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context Conquers Parameters: Outperforming Proprietary LLM in Commit Message Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Commit messages provide descriptions of the modifications made in a commit using natural language, making them crucial for software maintenance and evolution.<span class='px-1 mx-1 bg-yellow-200'>Recent developments in Large Language Models (LLMs) have led to their use in generating high-quality commit messages, such as the Omniscient Message Generator (OMG). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>This method employs GPT-4 to produce state-of-the-art commit messages.However, the use of proprietary LLMs like GPT-4 in coding tasks raises privacy and sustainability concerns, which may hinder their industrial adoption.Considering that open-source LLMs have achieved competitive performance in developer tasks such as compiler validation, this study investigates whether they can be used to generate commit messages that are comparable with OMG.Our experiments show that an open-source LLM can generate commit messages that are comparable to those produced by OMG.In addition, through a series of contextual refinements, we propose lOcal MessagE GenerAtor (OMEGA) , a CMG approach that uses a 4-bit quantized 8B open-source LLM.OMEGA produces state-of-the-art commit messages, surpassing the performance of GPT-4 in practitioners' preference.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2408.02502v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2408.02502v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
  function extractGSUrl(url) {
            var regex = /\/([^/]+?)(?:v\d+)?$/;
            var matches = url.match(regex);

            if (matches) {
                var endOfUrl = matches[1];
                var prefix = "https://scholar.google.com/scholar_lookup?arxiv_id=";
                var finalUrl = prefix + endOfUrl;
                console.log("Final URL: " + finalUrl);
                
                // You can open the final URL in a new tab/window
                window.open(finalUrl, "_blank");
            } else {
                console.log("No match found");
            }
        };
</script>
</body>
</html>