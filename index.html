<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="https://cdn.tailwindcss.com"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body>
  <div class="relative mx-auto h-full max-w-2xl text-md">
    <table class="table-auto">
      <tbody>
        <tr>
          <td></td>
          <td>
            <h1 class="text-4xl pt-4 font-bold"><span class="underline">Ryan's</span> Arxiv FrontPage</h1>
            <br>
            <p>Generated on 2024-03-25.</p><br/>
            <p class="text-sm text-gray-500 pt-2">This frontpage is made by scraping arxiv and by running a sentence-model that detects if the abstract describes a paper about a topic of interest. One cool feature: it all pretty much runs via Github Actions. </p>
            <p class="text-sm text-gray-500 pt-2">This project was originally created by <a href="https://koaning.io/">Vincent Warmerdam</a>, modifying his original <a href="https://koaning.github.io/arxiv-frontpage/">frontpage</a> for different paper categories.</p>
            <br>
          </td>
        </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Prompt Engineering in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the recent Large Language Models(LLM's) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM's in conversation-based tutoring systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.714</span></span><span class='px-1 mx-1 bg-yellow-200'>However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.736</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.828</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>The results substantiate that our system's framework facilitates personalization, with particular emphasis on the elements constituting student modeling.A web demo of our system is available at http://rlearning-its.com.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14071v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14071v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation.Detecting DeepFakes is currently solved with programmed machine learning algorithms.In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection.<span class='px-1 mx-1 bg-yellow-200'>We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.602</span></span>This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming.We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14077v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14077v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.<span class='px-1 mx-1 bg-yellow-200'>We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.844</span></span><span class='px-1 mx-1 bg-yellow-200'>Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.905</span></span><span class='px-1 mx-1 bg-yellow-200'>We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization.It can also serve as a reference for studies in other fields.We will release CHARM at https://github.com/opendatalab/CHARM .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions.The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants.<span class='px-1 mx-1 bg-yellow-200'>We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span>The peer conversation transcripts were analyzed using thematic analysis.We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded.<span class='px-1 mx-1 bg-yellow-200'>As participants, they foster children's creative thinking but may not consistently provide timely feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>These findings highlight potential design improvements and considerations for peer agents in both roles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14227v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14227v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents.Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance.<span class='px-1 mx-1 bg-yellow-200'>Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate.To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs).By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model.Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14252v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14252v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-based Extraction of Contradictions from Patents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products.However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents.Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods.Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT.They are, for example, used for dense retrieval, question answering or summarization and key concept extraction.A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions.Succeeding rule-based approaches, finetuned BERT-like language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction.While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models.<span class='px-1 mx-1 bg-yellow-200'>This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on Prompt Engineering using a generative Large Language Model (LLM), namely OpenAI's GPT-4. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span><span class='px-1 mx-1 bg-yellow-200'>Contradiction detection, sentence extraction, contradiction summarization, parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single prompt using the LangChain framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>Our results show that "off-the-shelf" GPT-4 is a serious alternative to existing approaches.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14258v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14258v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.906</span></span><span class='px-1 mx-1 bg-yellow-200'>Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span><span class='px-1 mx-1 bg-yellow-200'>In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.901</span></span>CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification.We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset.We call the resulting model ChainLM.<span class='px-1 mx-1 bg-yellow-200'>To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.708</span></span>In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance.We release our dataset and code at https://github.com/RUCAIBox/ChainLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14312v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14312v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring the Potential of Large Language Models in Graph Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification.However, the abilities of LLMs for graph generation remain unexplored in the literature.Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging.In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments.Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation.Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation.<span class='px-1 mx-1 bg-yellow-200'>We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.779</span></span>Besides, LLMs show potential in generating molecules with specific properties.These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14358v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14358v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems.However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models.<span class='px-1 mx-1 bg-yellow-200'>To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.789</span></span>However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information.In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs.Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities.In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples.Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT.Analysis shows that our method could preserve the model's general task performance on AlpacaEval.Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14399v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14399v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command.<span class='px-1 mx-1 bg-yellow-200'>To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform.Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM).Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM.Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%.Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14438v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14438v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science.While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores.<span class='px-1 mx-1 bg-yellow-200'>Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.627</span></span><span class='px-1 mx-1 bg-yellow-200'>Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14565v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14565v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched.In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain.<span class='px-1 mx-1 bg-yellow-200'>We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions.We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14578v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14578v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span>Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data.<span class='px-1 mx-1 bg-yellow-200'>However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct.The central role is an ActRe prompting agent, which explains the reason for an arbitrary action.When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales.Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action.In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training.Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement.We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2.In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds.In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts.<span class='px-1 mx-1 bg-yellow-200'>A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14589v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14589v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have recently made significant advances in code generation through the 'Chain-of-Thought' prompting technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.881</span></span>This technique empowers the model to autonomously devise "solution plans" to tackle intricate programming challenges, thereby improving its performance in code generation.Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities.Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation.Consequently, there arises a compelling need for transferring LLMs' code generation reasoning abilities to the smaller models.In this paper, we propose the CodePLAN framework, which aims to transfer LLMs' reasoning capabilities to smaller models through distillation.We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of the smaller model.To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies.Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model's code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13271v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13271v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the explosive growth of multi-modal information on the Internet, unimodal search cannot satisfy the requirement of Internet applications.Text-image retrieval research is needed to realize high-quality and efficient retrieval between different modalities.Existing text-image retrieval research is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K), in which the query utterance is rigid and unnatural (i.e. verbosity and formality).To overcome the shortcoming, we construct a new Compact and Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image retrieval task considering multiple query content and style, including compact and fine-grained entity-relation corpus.<span class='px-1 mx-1 bg-yellow-200'>We propose a novel query-enhanced text-image retrieval method using prompt engineering based on LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span>Experiments show that our proposed Flickr30-CFQ reveals the insufficiency of existing vision-language datasets in realistic text-image tasks.Our LLM-based Query-enhanced method applied on different existing text-image retrieval models improves query understanding performance both on public dataset and our challenge set Flickr30-CFQ with over 0.9% and 2.4% respectively.Our project can be available anonymously in https://sites.google.com/view/Flickr30K-cfq.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13317v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13317v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code.However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.<span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications.CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets.It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality and reliability of LLMs in generating intricate code.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13583v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13583v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.663</span></span>While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice.<span class='px-1 mx-1 bg-yellow-200'>To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span><span class='px-1 mx-1 bg-yellow-200'>The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span>This approach enables large language models to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding.<span class='px-1 mx-1 bg-yellow-200'>Experiments on real-world datasets can prove the effectiveness and flexibility of our prompting method with multiple state-of-the-art LLMs over existing prompting baselines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.836</span></span>We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying LLMs for psychotherapy behavior understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13786v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13786v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We demonstrate experimental results with LLMs that address robotics action planning problems.Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes.In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies.<span class='px-1 mx-1 bg-yellow-200'>Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.899</span></span>Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13801v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13801v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ICE: Interactive 3D Game Character Editing via Dialogue
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters.However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification.In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process.In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span>To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner.Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results.Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE.Project page: https://iceedit.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12667v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12667v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pragmatic Competence Evaluation of Large Language Models for Korean
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean.We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options.Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.743</span></span>Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12675v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12675v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Instructing Large Language Models to Identify and Ignore Irrelevant Conditions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.<span class='px-1 mx-1 bg-yellow-200'>Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.911</span></span>However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.Then it prompts LLMs to verify the irrelevant conditions.Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.<span class='px-1 mx-1 bg-yellow-200'>Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement.We conduct extensive experiments on eight MWP datasets.I$^3$C can be combined with any CoT prompting methods to improve the performance of solving MWPs.<span class='px-1 mx-1 bg-yellow-200'>Notably, with GPT-3.5-Turbo and I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span>Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12744v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12744v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Epistemology of Language Models: Do Language Models Have Holistic Knowledge?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates the inherent knowledge in language models from the perspective of epistemological holism.The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism.These characteristics suggest that core knowledge, such as general scientific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise.<span class='px-1 mx-1 bg-yellow-200'>To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>In the abduction task, the language models explained situations while avoiding revising the core knowledge.However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12862v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12862v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities.<span class='px-1 mx-1 bg-yellow-200'>Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span>HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner.The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop.This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness.Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12884v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12884v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.This lends to it a quality of creativity which can be empowering in the early stages of design.<span class='px-1 mx-1 bg-yellow-200'>In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.<span class='px-1 mx-1 bg-yellow-200'>Participants typically prompted in a straightforward manner with concise instructions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.79</span></span>We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.<span class='px-1 mx-1 bg-yellow-200'>Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatic text summarization (ATS) is an emerging technology to assist clinicians in providing continuous and coordinated care.This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>We developed prompt-tuning algorithms to instruct generative LLMs to summarize clinical text. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span><span class='px-1 mx-1 bg-yellow-200'>We examined the prompt-tuning strategies, the size of soft prompts, and the few-short learning ability of GatorTronGPT, a generative clinical LLM developed using 277 billion clinical and general English words with up to 20 billion parameters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.732</span></span>We compared GatorTronGPT with a previous solution based on fine-tuning of a widely used T5 model, using a clinical benchmark dataset MTS-DIALOG.The experimental results show that the GatorTronGPT- 20B model achieved the best performance on all evaluation metrics.The proposed solution has a low computing cost as the LLM parameters are not updated during prompt-tuning.This study demonstrates the efficiency of generative clinical LLMs for clinical ATS through prompt tuning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13089v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13089v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study of Vulnerability Repair in JavaScript Programs with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, JavaScript has become the most widely used programming language, especially in web development.However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications.<span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs.<span class='px-1 mx-1 bg-yellow-200'>We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13193v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13193v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Robustness Tools in LLM Safety</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatic detection of multimodal misinformation has gained a widespread attention recently.However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored.Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation.To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline.This pipeline consists of a visual information processing module and an evidence retrieval module.<span class='px-1 mx-1 bg-yellow-200'>Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments.The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14171v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14171v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-role Consensus through LLMs Discussions for Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.<span class='px-1 mx-1 bg-yellow-200'>To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14274v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14274v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Blockchain Security: A Systematic Literature Review
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS).Several recent studies are exploring LLMs applied to BS.However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security.To fill this gap, we conduct a literature review on LLM4BS.   <span class='px-1 mx-1 bg-yellow-200'>As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span>Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security.<span class='px-1 mx-1 bg-yellow-200'>We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks.Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14280v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14280v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare.Despite their potential benefits, researchers have underscored various ethical implications.While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare.Electronic databases and preprint servers were queried using a comprehensive search strategy.Studies were screened and extracted following a modified rapid review approach.Methodological quality was assessed using a hybrid approach.For 53 records, a meta-aggregative synthesis was performed.Four fields of applications emerged and testify to a vivid exploration phase.Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility.However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy.<span class='px-1 mx-1 bg-yellow-200'>A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>A recurrent plea for ethical guidance and human oversight is evident.Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications.This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14473v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14473v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched.In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain.<span class='px-1 mx-1 bg-yellow-200'>We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span>We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions.We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14578v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14578v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent progress in large language models (LLMs) has led to their widespread adoption in various domains.However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations.Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature.In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases.Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions.To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models.Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13213v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13213v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have recently made significant advances in code generation through the 'Chain-of-Thought' prompting technique.This technique empowers the model to autonomously devise "solution plans" to tackle intricate programming challenges, thereby improving its performance in code generation.<span class='px-1 mx-1 bg-yellow-200'>Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>Consequently, there arises a compelling need for transferring LLMs' code generation reasoning abilities to the smaller models.In this paper, we propose the CodePLAN framework, which aims to transfer LLMs' reasoning capabilities to smaller models through distillation.We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of the smaller model.To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies.Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model's code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13271v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13271v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid integration of Large Language Models (LLMs) across diverse sectors has marked a transformative era, showcasing remarkable capabilities in text generation and problem-solving tasks.However, this technological advancement is accompanied by significant risks and vulnerabilities.<span class='px-1 mx-1 bg-yellow-200'>Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span>Compounding the issue, organisations are deploying LLM-integrated systems without understanding the severity of potential consequences.Existing studies by OWASP and MITRE offer a general overview of threats and vulnerabilities but lack a method for directly and succinctly analysing the risks for security practitioners, developers, and key decision-makers who are working with this novel technology.To address this gap, we propose a risk assessment process using tools like the OWASP risk rating methodology which is used for traditional systems.We conduct scenario analysis to identify potential threat agents and map the dependent system components against vulnerability factors.Through this analysis, we assess the likelihood of a cyberattack.Subsequently, we conduct a thorough impact analysis to derive a comprehensive threat matrix.We also map threats against three key stakeholder groups: developers engaged in model fine-tuning, application developers utilizing third-party APIs, and end users.The proposed threat matrix provides a holistic evaluation of LLM-related risks, enabling stakeholders to make informed decisions for effective mitigation strategies.Our outlined process serves as an actionable and comprehensive tool for security practitioners, offering insights for resource management and enhancing the overall system security.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13309v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13309v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BadEdit: Backdooring large language models by model editing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs).To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework.<span class='px-1 mx-1 bg-yellow-200'>BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.7</span></span><span class='px-1 mx-1 bg-yellow-200'>It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span>(2) Efficiency:<span class='px-1 mx-1 bg-yellow-200'>BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span><span class='px-1 mx-1 bg-yellow-200'>(3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>(4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning.<span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100\% success rate while maintaining the model's performance on benign inputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13355v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13355v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content.<span class='px-1 mx-1 bg-yellow-200'>They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable.To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos.We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to different visual states of longer videos, and 2) Temporal Attention Regularization (TAR) - a regularization technique to refine the temporal attention units of the pre-trained T2V diffusion models, which enables control over the video dynamics.We experimentally showcase the superiority of the proposed approach in generating longer, visually appealing videos over existing open-sourced T2V models.We additionally analyze the temporal attention maps realized with and without VSTAR, demonstrating the importance of applying our method to mitigate neglect of the desired visual change over time.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13501v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13501v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Despite the remarkable performance of video-based large language models (LLMs), their adversarial threat remains unexplored.To fill this gap, we propose the first adversarial attack tailored for video-based LLMs by crafting flow-based multi-modal adversarial perturbations on a small fraction of frames within a video, dubbed FMM-Attack.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that our attack can effectively induce video-based LLMs to generate incorrect answers when videos are added with imperceptible adversarial perturbations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span><span class='px-1 mx-1 bg-yellow-200'>Intriguingly, our FMM-Attack can also induce garbling in the model output, prompting video-based LLMs to hallucinate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>Overall, our observations inspire a further understanding of multi-modal robustness and safety-related feature alignment across different modalities, which is of great importance for various large multi-modal models.Our code is available at https://github.com/THU-Kingmin/FMM-Attack.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13507v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13507v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains.<span class='px-1 mx-1 bg-yellow-200'>However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span>Current mitigation strategies, while effective, are not resilient under adversarial attacks.<span class='px-1 mx-1 bg-yellow-200'>This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation.Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks.The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13031v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13031v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources.Recently, Large Language Models (LLMs) have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, reasoning, and utilizing tools.However, LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions.This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance.To alleviate this issue, we propose the Discriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism.Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to judge whether the generated answers are off-topic.<span class='px-1 mx-1 bg-yellow-200'>In cases where an off-topic answer is detected, the Corrector performs step-wise revisions along the reversed reasoning chain (Re-Compose->Re-Solve->Re-Decompose) until the final answer becomes on-topic. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>Experimental results on the HotpotQA and 2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving the performance in Exact Match (EM) by nearly 3% compared to the baseline method without the Dr3 mechanism.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12393v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12393v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances.Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation.Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12415v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12415v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the effectiveness of Large Language Models for GitHub Workflows
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., YAML files with a list of jobs.However, engineering valid workflows is tedious.<span class='px-1 mx-1 bg-yellow-200'>They are also prone to severe security issues, which can result in supply chain vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.677</span></span>Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in various software development tasks.However, GitHub workflows differ from regular programs in both structure and semantics.We perform the first comprehensive study to understand the effectiveness of LLMs on five workflow-related tasks with different levels of prompts.We curated a set of $\sim$400K workflows and generated prompts with varying detail.We also fine-tuned LLMs on GitHub workflow tasks.Our evaluation of three state-of-the-art LLMs and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12446v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12446v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP).Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations.Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations.<span class='px-1 mx-1 bg-yellow-200'>These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span><span class='px-1 mx-1 bg-yellow-200'>This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span><span class='px-1 mx-1 bg-yellow-200'>Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12503v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12503v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering.Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress.However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes.Also, they can not integrate textual information such as financial news or reports.Meanwhile, large language models (LLMs) have remarkable textual understanding and generation ability.<span class='px-1 mx-1 bg-yellow-200'>But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLMs still suffer from hallucinations and are unable to keep up with the latest information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.829</span></span>To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data.It has a positive impact on training LLMs for completing financial analysis.We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques.Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12582v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12582v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Instructing Large Language Models to Identify and Ignore Irrelevant Conditions
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs.However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.<span class='px-1 mx-1 bg-yellow-200'>Then it prompts LLMs to verify the irrelevant conditions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot reasoning.We develop I$^3$C-Select that selects the most confusing problems based on the semantic relevance measurement.We conduct extensive experiments on eight MWP datasets.I$^3$C can be combined with any CoT prompting methods to improve the performance of solving MWPs.Notably, with GPT-3.5-Turbo and I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1.Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12744v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12744v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents.How to integrate agent ability into general LLMs becomes a crucial and urgent problem.<span class='px-1 mx-1 bg-yellow-200'>This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation datasets.<span class='px-1 mx-1 bg-yellow-200'>With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span>Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs.The code will be available at https://github.com/InternLM/Agent-FLAN.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12881v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12881v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Systemic property dispossession from minority groups has often been carried out in the name of technological progress.In this paper, we identify evidence that the current paradigm of large language models (LLMs) likely continues this long history.Examining common LLM training datasets, we find that a disproportionate amount of content authored by Jewish Americans is used for training without their consent.The degree of over-representation ranges from around 2x to around 6.5x.<span class='px-1 mx-1 bg-yellow-200'>Given that LLMs may substitute for the paid labor of those who produced their training data, they have the potential to cause even more substantial and disproportionate economic harm to Jewish Americans in the coming years. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a "canary in the coal mine" that highlights deep structural concerns about the current LLM paradigm whose harms could soon affect nearly everyone.We discuss the implications of these results for the policymakers thinking about how to regulate LLMs as well as for those in the AI field who are working to advance LLMs.Our findings stress the importance of working together towards alternative LLM paradigms that avoid both disparate impacts and widespread societal harms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13073v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13073v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study of Vulnerability Repair in JavaScript Programs with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, JavaScript has become the most widely used programming language, especially in web development.<span class='px-1 mx-1 bg-yellow-200'>However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing.In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs.<span class='px-1 mx-1 bg-yellow-200'>We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13193v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13193v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Robots That Know When They Need Help: Affordance-Based Uncertainty for Large Language Model Planners
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) showcase many desirable traits for intelligent and helpful robots.<span class='px-1 mx-1 bg-yellow-200'>However, they are also known to hallucinate predictions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span><span class='px-1 mx-1 bg-yellow-200'>This issue is exacerbated in consumer robotics where LLM hallucinations may result in robots confidently executing plans that are contrary to user goals, relying more frequently on human assistance, or preventing the robot from asking for help at all. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.889</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we present LAP, a novel approach for utilizing off-the-shelf LLM's, alongside scene and object Affordances, in robotic Planners that minimize harmful hallucinations and know when to ask for help. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>Our key finding is that calculating and leveraging a scene affordance score, a measure of whether a given action is possible in the provided scene, helps to mitigate hallucinations in LLM predictions and better align the LLM's confidence measure with the probability of success.We specifically propose and test three different affordance scores, which can be used independently or in tandem to improve performance across different use cases.The most successful of these individual scores involves prompting an LLM to determine if a given action is possible and safe in the given scene and uses the LLM's response to compute the score.Through experiments in both simulation and the real world, on tasks with a variety of ambiguities, we show that LAP significantly increases success rate and decreases the amount of human intervention required relative to prior art.For example, in our real-world testing paradigm, LAP decreases the human help rate of previous methods by over 33% at a success rate of 70%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13198v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13198v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Context-aware LLM-based Safe Control Against Latent Risks
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>It is challenging for autonomous control systems to perform complex tasks in the presence of latent risks.Motivated by this challenge, this paper proposes an integrated framework that involves Large Language Models (LLMs), stochastic gradient descent (SGD), and optimization-based control.In the first phrase, the proposed framework breaks down complex tasks into a sequence of smaller subtasks, whose specifications account for contextual information and latent risks.In the second phase, these subtasks and their parameters are refined through a dual process involving LLMs and SGD.<span class='px-1 mx-1 bg-yellow-200'>LLMs are used to generate rough guesses and failure explanations, and SGD is used to fine-tune parameters. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>The proposed framework is tested using simulated case studies of robots and vehicles.The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11863v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11863v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success.<span class='px-1 mx-1 bg-yellow-200'>However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.858</span></span>In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction.We introduce an environmental feedback-based self-correction method called ERASER.Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary.Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1.Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs.By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11886v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11886v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Investigating Markers and Drivers of Gender Bias in Machine Translations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Implicit gender bias in Large Language Models (LLMs) is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases.However, some LLMs use heuristics or post-processing to mask such bias, making investigation difficult.<span class='px-1 mx-1 bg-yellow-200'>Here, we examine bias in LLMss via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>Each statement starts with 'she', and is translated first into a 'genderless' intermediate language then back into English; we then examine pronoun-choice in the back-translated texts.We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated translations, avoiding the over-interpretation of individual pronouns, apparent in earlier work; (3) by investigating sentence features that drive bias; (4) and by comparing results from three time-lapsed datasets to establish the reproducibility of the approach.We found that some languages display similar patterns of pronoun use, falling into three loose groups, but that patterns vary between groups; this underlines the need to work with multiple languages.We also identify the main verb appearing in a sentence as a likely significant driver of implied gender in the translations.Moreover, we see a good level of replicability in the results, and establish that our variation metric proves robust despite an obvious change in the behaviour of the DeepL translation API during the course of the study.These results show that the back-translation method can provide further insights into bias in language models.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11896v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11896v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>They are designed to bypass safeguards and elicit prohibited outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations.This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs.It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator.This modular framework enables researchers to easily construct attacks from combinations of novel and existing components.So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs.<span class='px-1 mx-1 bg-yellow-200'>Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57% and 33%, respectively.We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12171v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12171v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.701</span></span><span class='px-1 mx-1 bg-yellow-200'>Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.62</span></span>Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results.The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.   We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and zero-shot-role-play-Chain of Thought (CoT) prompting techniques for ChatGPT.We studied 5,115 npm packages (of which 2,180 are malicious) and performed a baseline comparison of the GPT-3 and GPT-4 models with a static analysis tool.Our findings showed promising results for GPT models with low misclassification alert rates.Our baseline comparison demonstrates a notable improvement over static analysis in precision scores above 25% and F1 scores above 15%.We attained precision and F1 scores of 91% and 94%, respectively, for the GPT-3 model.Overall, GPT-4 demonstrates superior performance in precision (99%) and F1 (97%) scores, while GPT-3 presents a cost-effective balance between performance and expenditure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12196v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12196v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large language models in 6G security: challenges and opportunities
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid integration of Generative AI (GenAI) and Large Language Models (LLMs) in sectors such as education and healthcare have marked a significant advancement in technology.However, this growth has also led to a largely unexplored aspect: their security vulnerabilities.As the ecosystem that includes both offline and online models, various tools, browser plugins, and third-party applications continues to expand, it significantly widens the attack surface, thereby escalating the potential for security breaches.<span class='px-1 mx-1 bg-yellow-200'>These expansions in the 6G and beyond landscape provide new avenues for adversaries to manipulate LLMs for malicious purposes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span><span class='px-1 mx-1 bg-yellow-200'>We focus on the security aspects of LLMs from the viewpoint of potential adversaries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>We aim to dissect their objectives and methodologies, providing an in-depth analysis of known security weaknesses.This will include the development of a comprehensive threat taxonomy, categorizing various adversary behaviors.Also, our research will concentrate on how LLMs can be integrated into cybersecurity efforts by defense teams, also known as blue teams.We will explore the potential synergy between LLMs and blockchain technology, and how this combination could lead to the development of next-generation, fully autonomous security solutions.This approach aims to establish a unified cybersecurity strategy across the entire computing continuum, enhancing overall digital security infrastructure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts.Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design.<span class='px-1 mx-1 bg-yellow-200'>However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers.This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts.For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template.<span class='px-1 mx-1 bg-yellow-200'>For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.604</span></span>For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script.To evaluate the effectiveness of our data augmentation method, we finetune Llama2-13B and Llama2-7B models using the dataset generated by our augmentation framework.The results demonstrate a significant improvement in the Verilog generation tasks with LLMs.<span class='px-1 mx-1 bg-yellow-200'>Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.656</span></span><span class='px-1 mx-1 bg-yellow-200'>Our 13B model (ChipGPT-FT) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11202v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11202v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Security Challenges in LLM Development</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-role Consensus through LLMs Discussions for Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.824</span></span>Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.<span class='px-1 mx-1 bg-yellow-200'>To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.712</span></span>Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14274v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14274v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Blockchain Security: A Systematic Literature Review
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span>Several recent studies are exploring LLMs applied to BS.However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security.To fill this gap, we conduct a literature review on LLM4BS.   <span class='px-1 mx-1 bg-yellow-200'>As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.731</span></span><span class='px-1 mx-1 bg-yellow-200'>Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span><span class='px-1 mx-1 bg-yellow-200'>We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.756</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span>Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14280v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14280v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detoxifying Large Language Models via Knowledge Editing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.88</span></span>We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance.Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance.We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent progress in large language models (LLMs) has led to their widespread adoption in various domains.However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations.<span class='px-1 mx-1 bg-yellow-200'>Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span><span class='px-1 mx-1 bg-yellow-200'>Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span>As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span><span class='px-1 mx-1 bg-yellow-200'>Using the case of Llama 2 as an example, we illustrate how LLMs' safety responses can still encode harmful assumptions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span>To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models.Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to quality-of-service harms for marginalized populations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13213v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13213v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid integration of Large Language Models (LLMs) across diverse sectors has marked a transformative era, showcasing remarkable capabilities in text generation and problem-solving tasks.<span class='px-1 mx-1 bg-yellow-200'>However, this technological advancement is accompanied by significant risks and vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span><span class='px-1 mx-1 bg-yellow-200'>Despite ongoing security enhancements, attackers persistently exploit these weaknesses, casting doubts on the overall trustworthiness of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.857</span></span>Compounding the issue, organisations are deploying LLM-integrated systems without understanding the severity of potential consequences.<span class='px-1 mx-1 bg-yellow-200'>Existing studies by OWASP and MITRE offer a general overview of threats and vulnerabilities but lack a method for directly and succinctly analysing the risks for security practitioners, developers, and key decision-makers who are working with this novel technology. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.852</span></span>To address this gap, we propose a risk assessment process using tools like the OWASP risk rating methodology which is used for traditional systems.<span class='px-1 mx-1 bg-yellow-200'>We conduct scenario analysis to identify potential threat agents and map the dependent system components against vulnerability factors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span><span class='px-1 mx-1 bg-yellow-200'>Through this analysis, we assess the likelihood of a cyberattack. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>Subsequently, we conduct a thorough impact analysis to derive a comprehensive threat matrix.<span class='px-1 mx-1 bg-yellow-200'>We also map threats against three key stakeholder groups: developers engaged in model fine-tuning, application developers utilizing third-party APIs, and end users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span><span class='px-1 mx-1 bg-yellow-200'>The proposed threat matrix provides a holistic evaluation of LLM-related risks, enabling stakeholders to make informed decisions for effective mitigation strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.65</span></span><span class='px-1 mx-1 bg-yellow-200'>Our outlined process serves as an actionable and comprehensive tool for security practitioners, offering insights for resource management and enhancing the overall system security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13309v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13309v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                BadEdit: Backdooring large language models by model editing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.8</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.847</span></span><span class='px-1 mx-1 bg-yellow-200'>BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span><span class='px-1 mx-1 bg-yellow-200'>It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.716</span></span>(2) Efficiency:BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption.(3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised.<span class='px-1 mx-1 bg-yellow-200'>(4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.648</span></span><span class='px-1 mx-1 bg-yellow-200'>Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100\% success rate while maintaining the model's performance on benign inputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.886</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13355v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13355v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Despite the remarkable performance of video-based large language models (LLMs), their adversarial threat remains unexplored. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.835</span></span><span class='px-1 mx-1 bg-yellow-200'>To fill this gap, we propose the first adversarial attack tailored for video-based LLMs by crafting flow-based multi-modal adversarial perturbations on a small fraction of frames within a video, dubbed FMM-Attack. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that our attack can effectively induce video-based LLMs to generate incorrect answers when videos are added with imperceptible adversarial perturbations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.864</span></span>Intriguingly, our FMM-Attack can also induce garbling in the model output, prompting video-based LLMs to hallucinate.Overall, our observations inspire a further understanding of multi-modal robustness and safety-related feature alignment across different modalities, which is of great importance for various large multi-modal models.Our code is available at https://github.com/THU-Kingmin/FMM-Attack.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13507v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13507v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains.<span class='px-1 mx-1 bg-yellow-200'>However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span><span class='px-1 mx-1 bg-yellow-200'>Current mitigation strategies, while effective, are not resilient under adversarial attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.816</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span>By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation.<span class='px-1 mx-1 bg-yellow-200'>Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.84</span></span>The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13031v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13031v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content.Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech.<span class='px-1 mx-1 bg-yellow-200'>Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span>To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design.Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable.Our comprehensive evaluation on a variety of social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the effectiveness of Large Language Models for GitHub Workflows
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., YAML files with a list of jobs.However, engineering valid workflows is tedious.<span class='px-1 mx-1 bg-yellow-200'>They are also prone to severe security issues, which can result in supply chain vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in various software development tasks.However, GitHub workflows differ from regular programs in both structure and semantics.We perform the first comprehensive study to understand the effectiveness of LLMs on five workflow-related tasks with different levels of prompts.We curated a set of $\sim$400K workflows and generated prompts with varying detail.We also fine-tuned LLMs on GitHub workflow tasks.Our evaluation of three state-of-the-art LLMs and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12446v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12446v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP).Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations.<span class='px-1 mx-1 bg-yellow-200'>Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span><span class='px-1 mx-1 bg-yellow-200'>These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.817</span></span><span class='px-1 mx-1 bg-yellow-200'>This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.858</span></span><span class='px-1 mx-1 bg-yellow-200'>Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.665</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12503v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12503v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study of Vulnerability Repair in JavaScript Programs with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent years, JavaScript has become the most widely used programming language, especially in web development.<span class='px-1 mx-1 bg-yellow-200'>However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.854</span></span>Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing.<span class='px-1 mx-1 bg-yellow-200'>In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.805</span></span><span class='px-1 mx-1 bg-yellow-200'>We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13193v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13193v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large language models in 6G security: challenges and opportunities
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid integration of Generative AI (GenAI) and Large Language Models (LLMs) in sectors such as education and healthcare have marked a significant advancement in technology.<span class='px-1 mx-1 bg-yellow-200'>However, this growth has also led to a largely unexplored aspect: their security vulnerabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.764</span></span><span class='px-1 mx-1 bg-yellow-200'>As the ecosystem that includes both offline and online models, various tools, browser plugins, and third-party applications continues to expand, it significantly widens the attack surface, thereby escalating the potential for security breaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.757</span></span><span class='px-1 mx-1 bg-yellow-200'>These expansions in the 6G and beyond landscape provide new avenues for adversaries to manipulate LLMs for malicious purposes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.659</span></span><span class='px-1 mx-1 bg-yellow-200'>We focus on the security aspects of LLMs from the viewpoint of potential adversaries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.885</span></span><span class='px-1 mx-1 bg-yellow-200'>We aim to dissect their objectives and methodologies, providing an in-depth analysis of known security weaknesses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span><span class='px-1 mx-1 bg-yellow-200'>This will include the development of a comprehensive threat taxonomy, categorizing various adversary behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span>Also, our research will concentrate on how LLMs can be integrated into cybersecurity efforts by defense teams, also known as blue teams.<span class='px-1 mx-1 bg-yellow-200'>We will explore the potential synergy between LLMs and blockchain technology, and how this combination could lead to the development of next-generation, fully autonomous security solutions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>This approach aims to establish a unified cybersecurity strategy across the entire computing continuum, enhancing overall digital security infrastructure.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12239v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12239v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">HCI in Large Language Models</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought.<span class='px-1 mx-1 bg-yellow-200'>Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.68</span></span>We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance.We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization.It can also serve as a reference for studies in other fields.We will release CHARM at https://github.com/opendatalab/CHARM .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions.<span class='px-1 mx-1 bg-yellow-200'>The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.894</span></span><span class='px-1 mx-1 bg-yellow-200'>We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span><span class='px-1 mx-1 bg-yellow-200'>The peer conversation transcripts were analyzed using thematic analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.886</span></span>We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded.As participants, they foster children's creative thinking but may not consistently provide timely feedback.These findings highlight potential design improvements and considerations for peer agents in both roles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14227v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14227v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring the Potential of Large Language Models in Graph Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification.However, the abilities of LLMs for graph generation remain unexplored in the literature.Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging.In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments.Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation.Our evaluations demonstrate that LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation tasks, including rule-based and distribution-based generation.We also observe that popular prompting methods, such as few-shot and chain-of-thought prompting, do not consistently enhance performance.Besides, LLMs show potential in generating molecules with specific properties.<span class='px-1 mx-1 bg-yellow-200'>These findings may serve as foundations for designing good LLMs based models for graph generation and provide valuable insights and further research. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14358v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14358v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages.<span class='px-1 mx-1 bg-yellow-200'>However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting.<span class='px-1 mx-1 bg-yellow-200'>We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent.<span class='px-1 mx-1 bg-yellow-200'>We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31).<span class='px-1 mx-1 bg-yellow-200'>Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.74</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14380v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14380v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Alternative Solutions: Large Language Models Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications.This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics.These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more.<span class='px-1 mx-1 bg-yellow-200'>Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span><span class='px-1 mx-1 bg-yellow-200'>A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms.Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs.Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature.By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories.This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14469v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14469v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have recently made significant advances in code generation through the 'Chain-of-Thought' prompting technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span>This technique empowers the model to autonomously devise "solution plans" to tackle intricate programming challenges, thereby improving its performance in code generation.Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities.Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation.Consequently, there arises a compelling need for transferring LLMs' code generation reasoning abilities to the smaller models.In this paper, we propose the CodePLAN framework, which aims to transfer LLMs' reasoning capabilities to smaller models through distillation.We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of the smaller model.To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies.Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model's code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13271v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13271v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In human-centered design, developing a comprehensive and in-depth understanding of user experiences, i.e., empathic understanding, is paramount for designing products that truly meet human needs.Nevertheless, accurately comprehending the real underlying mental states of a large human population remains a significant challenge today.<span class='px-1 mx-1 bg-yellow-200'>This difficulty mainly arises from the trade-off between depth and scale of user experience research: gaining in-depth insights from a small group of users does not easily scale to a larger population, and vice versa. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.615</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper investigates the use of Large Language Models (LLMs) for performing mental inference tasks, specifically inferring users' underlying goals and fundamental psychological needs (FPNs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.63</span></span>Baseline and benchmark datasets were collected from human users and designers to develop an empathic accuracy metric for measuring the mental inference performance of LLMs.The empathic accuracy of inferring goals and FPNs of different LLMs with varied zero-shot prompt engineering techniques are experimented against that of human designers.Experimental results suggest that LLMs can infer and understand the underlying goals and FPNs of users with performance comparable to that of human designers, suggesting a promising avenue for enhancing the scalability of empathic design approaches through the integration of advanced artificial intelligence technologies.<span class='px-1 mx-1 bg-yellow-200'>This work has the potential to significantly augment the toolkit available to designers during human-centered design, enabling the development of both large-scale and in-depth understanding of users' experiences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.646</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13301v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13301v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience.<span class='px-1 mx-1 bg-yellow-200'>However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span>In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation.Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender.The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling.Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying user preferences from heterogeneous interaction behaviors.In order to integrate the merits of the SR model and the supplemental LLM recommender, we design a twostage training paradigm.The first stage is personalized preference alignment, which aims to align the preference representations from both components, thereby enhancing the semantics of the SR model.The second stage is recommendation-oriented fine-tuning, in which the alignment-enhanced SR model is fine-tuned according to specific objectives.Extensive experiments in both video and comment recommendation tasks demonstrate the effectiveness of LSVCR.Additionally, online A/B testing on the KuaiShou platform verifies the actual benefits brought by our approach.In particular, we achieve a significant overall gain of 4.13% in comment watch time.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13574v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13574v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance.<span class='px-1 mx-1 bg-yellow-200'>We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.775</span></span><span class='px-1 mx-1 bg-yellow-200'>We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.845</span></span><span class='px-1 mx-1 bg-yellow-200'>Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span><span class='px-1 mx-1 bg-yellow-200'>The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13592v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13592v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span><span class='px-1 mx-1 bg-yellow-200'>While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.75</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.769</span></span>The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances.We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs.We find that agents excelling in individual level does not imply their proficiency in group level.Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.869</span></span>The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13679v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13679v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.768</span></span>While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice.<span class='px-1 mx-1 bg-yellow-200'>To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.808</span></span><span class='px-1 mx-1 bg-yellow-200'>The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>This approach enables large language models to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding.Experiments on real-world datasets can prove the effectiveness and flexibility of our prompting method with multiple state-of-the-art LLMs over existing prompting baselines.<span class='px-1 mx-1 bg-yellow-200'>We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying LLMs for psychotherapy behavior understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.668</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13786v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13786v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Characteristic AI Agents via Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems.<span class='px-1 mx-1 bg-yellow-200'>Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span><span class='px-1 mx-1 bg-yellow-200'>While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.793</span></span>Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings.Current investigations have primarily focused on act on roles with simple profiles.In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics.A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play.With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings.In addition, we devise a set of automatic metrics for quantitative performance evaluation.The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents.The benchmark is available at https://github.com/nuaa-nlp/Character100.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12368v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12368v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems.<span class='px-1 mx-1 bg-yellow-200'>Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.726</span></span>Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret.In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches.Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples.The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12388v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12388v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Embodied LLM Agents Learn to Cooperate in Organized Teams
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks.<span class='px-1 mx-1 bg-yellow-200'>LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span>However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation.<span class='px-1 mx-1 bg-yellow-200'>Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span><span class='px-1 mx-1 bg-yellow-200'>Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12482v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12482v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pragmatic Competence Evaluation of Large Language Models for Korean
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.<span class='px-1 mx-1 bg-yellow-200'>Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span><span class='px-1 mx-1 bg-yellow-200'>We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.704</span></span>Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4.Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12675v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12675v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advances in visual reasoning (VR), particularly with the aid of Large Vision-Language Models (VLMs), show promise but require access to large-scale datasets and face challenges such as high computational costs and limited generalization capabilities.<span class='px-1 mx-1 bg-yellow-200'>Compositional visual reasoning approaches have emerged as effective strategies; however, they heavily rely on the commonsense knowledge encoded in Large Language Models (LLMs) to perform planning, reasoning, or both, without considering the effect of their decisions on the visual reasoning process, which can lead to errors or failed procedures. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.629</span></span>To address these challenges, we introduce HYDRA, a multi-stage dynamic compositional visual reasoning framework designed for reliable and incrementally progressive general reasoning.HYDRA integrates three essential modules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive controller, and a reasoner.The planner and reasoner modules utilize an LLM to generate instruction samples and executable code from the selected instruction, respectively, while the RL agent dynamically interacts with these modules, making high-level decisions on selection of the best instruction sample given information from the historical state stored through a feedback loop.This adaptable design enables HYDRA to adjust its actions based on previous feedback received during the reasoning process, leading to more reliable reasoning outputs and ultimately enhancing its overall effectiveness.Our framework demonstrates state-of-the-art performance in various VR tasks on four different widely-used datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12884v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12884v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.This lends to it a quality of creativity which can be empowering in the early stages of design.<span class='px-1 mx-1 bg-yellow-200'>In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span>We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.Participants typically prompted in a straightforward manner with concise instructions.We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.<span class='px-1 mx-1 bg-yellow-200'>Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span>Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates.The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information.Unique to our approach is the integration of both scene-level and ego-centric 3D information.This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization.Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the scene.Our experiments with Scene-LLM demonstrate its strong capabilities in dense captioning, question answering, and interactive planning.We believe Scene-LLM advances the field of 3D visual understanding and reasoning, offering new possibilities for sophisticated agent interactions in indoor settings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11401v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11401v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) demonstrate superior performance in generative scenarios and have attracted widespread attention.<span class='px-1 mx-1 bg-yellow-200'>Among them, stylized dialogue generation is essential in the context of LLMs for building intelligent and engaging dialogue agent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>However the ability of LLMs is data-driven and limited by data bias, leading to poor performance on specific tasks.In particular, stylized dialogue generation suffers from a severe lack of supervised data.Furthermore, although many prompt-based methods have been proposed to accomplish specific tasks, their performance in complex real-world scenarios involving a wide variety of dialog styles further enhancement.In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control.<span class='px-1 mx-1 bg-yellow-200'>Based on this, we propose the stylized dialogue framework StyleChat via recitation-augmented memory strategy and multi-task style learning strategy to promote generalization ability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span>To evaluate the effectiveness of our approach, we created a test benchmark that included both a generation task and a choice task to comprehensively evaluate trained models and assess whether styles and preferences are remembered and understood.Experimental results show that our proposed framework StyleChat outperforms all the baselines and helps to break the style boundary of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11439v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11439v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent.In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references.<span class='px-1 mx-1 bg-yellow-200'>Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span>We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions.Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data.To the best of our knowledge, ours is the first LLM-driven approach capable of generating "human-like" instructions in a platform-agnostic manner, without requiring any form of training.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11487v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11487v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Rhymes and poems are a powerful medium for transmitting cultural norms and societal roles.However, the pervasive existence of gender stereotypes in these works perpetuates biased perceptions and limits the scope of individuals' identities.Past works have shown that stereotyping and prejudice emerge in early childhood, and developmental research on causal mechanisms is critical for understanding and controlling stereotyping and prejudice.This work contributes by gathering a dataset of rhymes and poems to identify gender stereotypes and propose a model with 97\% accuracy to identify gender bias.<span class='px-1 mx-1 bg-yellow-200'>Gender stereotypes were rectified using a Large Language Model (LLM) and its effectiveness was evaluated in a comparative survey against human educator rectifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>To summarize, this work highlights the pervasive nature of gender stereotypes in literary works and reveals the potential of LLMs to rectify gender stereotypes.<span class='px-1 mx-1 bg-yellow-200'>This study raises awareness and promotes inclusivity within artistic expressions, making a significant contribution to the discourse on gender equality. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11752v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11752v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Metaphor Understanding Challenge Dataset for LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.671</span></span>Metaphor understanding is therefore an essential task for large language models (LLMs).We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs.The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases.The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity.All apt and inapt paraphrases were manually annotated.<span class='px-1 mx-1 bg-yellow-200'>The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span>Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs.The dataset is freely accessible at https://github.com/xiaoyuisrain/metaphor-understanding-challenge.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11810v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11810v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span>Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive.Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at?We propose EnvGen, a novel framework to address this question.First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel.Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.).Next, we train a small RL agent in a mixture of the original and LLM-generated environments.Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance.We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments.We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster.We show qualitatively how the LLM adapts training environments to help improve RL agents' weaker skills over time.Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls.Lastly, we present detailed ablation studies for our design choices.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12014v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12014v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years.Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks.This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models.The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks.In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics and sources of both charts and textual inputs.Modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance.Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance.Challenges and future directions are addressed in a dedicated section, highlighting issues such as domain-specific charts, lack of efforts in evaluation, and agent-oriented settings.This survey paper serves to provide valuable insights and directions for future research in chart understanding leveraging large foundation models.The studies mentioned in this paper, along with emerging new research, will be continually updated at: https://github.com/khuangaf/Awesome-Chart-Understanding.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12027v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12027v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Large Language Models in Social Sciences</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought.<span class='px-1 mx-1 bg-yellow-200'>Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.616</span></span>We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance.We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization.It can also serve as a reference for studies in other fields.We will release CHARM at https://github.com/opendatalab/CHARM .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions.<span class='px-1 mx-1 bg-yellow-200'>The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.667</span></span>We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem.<span class='px-1 mx-1 bg-yellow-200'>The peer conversation transcripts were analyzed using thematic analysis. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span>We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded.<span class='px-1 mx-1 bg-yellow-200'>As participants, they foster children's creative thinking but may not consistently provide timely feedback. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.655</span></span>These findings highlight potential design improvements and considerations for peer agents in both roles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14227v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14227v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years.<span class='px-1 mx-1 bg-yellow-200'>Recognizing cognitive distortions from the interviewee's utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.683</span></span>In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents.Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score.Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14255v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14255v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online.<span class='px-1 mx-1 bg-yellow-200'>Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance.<span class='px-1 mx-1 bg-yellow-200'>In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.72</span></span>We create a web-based platform where participants engage in short, multiple-round debates with a live opponent.<span class='px-1 mx-1 bg-yellow-200'>Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.734</span></span><span class='px-1 mx-1 bg-yellow-200'>We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.688</span></span>Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31).Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14380v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14380v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Locating and Mitigating Gender Bias in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences.<span class='px-1 mx-1 bg-yellow-200'>However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span>Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it.<span class='px-1 mx-1 bg-yellow-200'>This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.612</span></span>In this study, we integrate the processes of locating and mitigating bias within a unified framework.Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model.<span class='px-1 mx-1 bg-yellow-200'>Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.874</span></span><span class='px-1 mx-1 bg-yellow-200'>The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.883</span></span>Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14409v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14409v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.625</span></span><span class='px-1 mx-1 bg-yellow-200'>Despite their potential benefits, researchers have underscored various ethical implications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.621</span></span>While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare.Electronic databases and preprint servers were queried using a comprehensive search strategy.Studies were screened and extracted following a modified rapid review approach.Methodological quality was assessed using a hybrid approach.For 53 records, a meta-aggregative synthesis was performed.Four fields of applications emerged and testify to a vivid exploration phase.Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility.However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy.A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content.A recurrent plea for ethical guidance and human oversight is evident.Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications.This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14473v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14473v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science.While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores.Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning.<span class='px-1 mx-1 bg-yellow-200'>Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span><span class='px-1 mx-1 bg-yellow-200'>A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14565v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14565v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Multi-Choice Question Classification of Medical Subjects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.686</span></span>This is an important and challenging task for automatic question answering.To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects.Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively.In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14582v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14582v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models.Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data.However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks.In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct.The central role is an ActRe prompting agent, which explains the reason for an arbitrary action.When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales.Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action.In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training.<span class='px-1 mx-1 bg-yellow-200'>Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.605</span></span>We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2.In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds.In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts.A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14589v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14589v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In human-centered design, developing a comprehensive and in-depth understanding of user experiences, i.e., empathic understanding, is paramount for designing products that truly meet human needs.<span class='px-1 mx-1 bg-yellow-200'>Nevertheless, accurately comprehending the real underlying mental states of a large human population remains a significant challenge today. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.669</span></span>This difficulty mainly arises from the trade-off between depth and scale of user experience research: gaining in-depth insights from a small group of users does not easily scale to a larger population, and vice versa.<span class='px-1 mx-1 bg-yellow-200'>This paper investigates the use of Large Language Models (LLMs) for performing mental inference tasks, specifically inferring users' underlying goals and fundamental psychological needs (FPNs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.614</span></span>Baseline and benchmark datasets were collected from human users and designers to develop an empathic accuracy metric for measuring the mental inference performance of LLMs.The empathic accuracy of inferring goals and FPNs of different LLMs with varied zero-shot prompt engineering techniques are experimented against that of human designers.Experimental results suggest that LLMs can infer and understand the underlying goals and FPNs of users with performance comparable to that of human designers, suggesting a promising avenue for enhancing the scalability of empathic design approaches through the integration of advanced artificial intelligence technologies.This work has the potential to significantly augment the toolkit available to designers during human-centered design, enabling the development of both large-scale and in-depth understanding of users' experiences.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13301v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13301v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Polaris: A Safety-focused LLM Constellation Architecture for Healthcare
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We develop Polaris, the first safety-focused LLM constellation for real-time patient-AI healthcare conversations.Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations.Our one-trillion parameter constellation system is composed of several multibillion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations.We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives.We train our models on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical reasoning documents.<span class='px-1 mx-1 bg-yellow-200'>We align our models to speak like medical professionals, using organic healthcare conversations and simulated ones between patient actors and experienced nurses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>This allows our system to express unique capabilities such as rapport building, trust building, empathy and bedside manner.Finally, we present the first comprehensive clinician evaluation of an LLM system for healthcare.We recruited over 1100 U.S. licensed nurses and over 130 U.S. licensed physicians to perform end-to-end conversational evaluations of our system by posing as patients and rating the system on several measures.We demonstrate Polaris performs on par with human nurses on aggregate across dimensions such as medical safety, clinical readiness, conversational quality, and bedside manner.Additionally, we conduct a challenging task-based evaluation of the individual specialist support agents, where we demonstrate our LLM agents significantly outperform a much larger general-purpose LLM (GPT-4) as well as from its own medium-size class (LLaMA-2 70B).</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13313v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13313v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance.We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context.<span class='px-1 mx-1 bg-yellow-200'>We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.706</span></span>Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context.The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13592v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13592v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors.<span class='px-1 mx-1 bg-yellow-200'>While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.771</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.709</span></span>The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances.We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs.We find that agents excelling in individual level does not imply their proficiency in group level.Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group.<span class='px-1 mx-1 bg-yellow-200'>Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.643</span></span>The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13679v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13679v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Information-Theoretic Distillation for Reference-less Summarization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models.While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer.We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references.To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary.Based on this formulation, we start off from Pythia-2.8B as the teacher model, which is not yet capable of summarization, then self-train the model to optimize for the information-centric measures of ideal summaries.Distilling from the improved teacher, we arrive at a compact but powerful summarizer with only 568M parameters that performs competitively against ChatGPT, without ever relying on ChatGPT's capabilities.<span class='px-1 mx-1 bg-yellow-200'>Extensive analysis demonstrates that our approach outperforms in-domain supervised models in human evaluation, let alone state-of-the-art unsupervised methods, and wins over ChatGPT in controllable summarization. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.638</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13780v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13780v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span><span class='px-1 mx-1 bg-yellow-200'>While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span><span class='px-1 mx-1 bg-yellow-200'>To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.71</span></span><span class='px-1 mx-1 bg-yellow-200'>The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions between patients and therapists. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.622</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach enables large language models to leverage the coding scheme, patient state, and domain knowledge for patient behavioral coding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.684</span></span>Experiments on real-world datasets can prove the effectiveness and flexibility of our prompting method with multiple state-of-the-art LLMs over existing prompting baselines.<span class='px-1 mx-1 bg-yellow-200'>We have conducted extensive ablation analysis and demonstrate the critical role of dyadic interactions in applying LLMs for psychotherapy behavior understanding. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.802</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13786v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13786v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives.However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications.This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs.<span class='px-1 mx-1 bg-yellow-200'>We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.763</span></span>The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications.GatorTronGPT achieved the best F1 scores for both datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12374v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12374v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.603</span></span>Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems.Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret.In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches.Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples.The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12388v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12388v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content.Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech.Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design.To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design.Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifiers faithfully interpretable.<span class='px-1 mx-1 bg-yellow-200'>Our comprehensive evaluation on a variety of social media hate speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales, and (2) the surprising retention of detector performance even after training to ensure interpretability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.651</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Embodied LLM Agents Learn to Cooperate in Organized Teams
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks.LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation.However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation.Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems.<span class='px-1 mx-1 bg-yellow-200'>Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.64</span></span>Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12482v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12482v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP).<span class='px-1 mx-1 bg-yellow-200'>Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.694</span></span>Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations.These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities.This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies.Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12503v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12503v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pragmatic Competence Evaluation of Large Language Models for Korean
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.<span class='px-1 mx-1 bg-yellow-200'>Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span><span class='px-1 mx-1 bg-yellow-200'>We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.647</span></span>Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4.Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.<span class='px-1 mx-1 bg-yellow-200'>Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12675v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12675v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Yell At Your Robot: Improving On-the-Fly from Language Corrections
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Hierarchical policies that combine language and low-level control have been shown to perform impressively long-horizon robotic tasks, by leveraging either zero-shot high-level planners like pretrained language and vision-language models (LLMs/VLMs) or models trained on annotated robotic demonstrations.However, for complex and dexterous skills, attaining high success rates on long-horizon tasks still represents a major challenge -- the longer the task is, the more likely it is that some stage will fail.Can humans help the robot to continuously improve its long-horizon task performance through intuitive and natural feedback?<span class='px-1 mx-1 bg-yellow-200'>In this paper, we make the following observation: high-level policies that index into sufficiently rich and expressive low-level language-conditioned skills can be readily supervised with human feedback in the form of language corrections. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>We show that even fine-grained corrections, such as small movements ("move a bit to the left"), can be effectively incorporated into high-level policies, and that such corrections can be readily obtained from humans observing the robot and making occasional suggestions.This framework enables robots not only to rapidly adapt to real-time language feedback, but also incorporate this feedback into an iterative training scheme that improves the high-level policy's ability to correct errors in both low-level execution and high-level decision-making purely from verbal feedback.Our evaluation on real hardware shows that this leads to significant performance improvement in long-horizon, dexterous manipulation tasks without the need for any additional teleoperation.Videos and code are available at https://yay-robot.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12910v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12910v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.This lends to it a quality of creativity which can be empowering in the early stages of design.In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them.We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.Participants typically prompted in a straightforward manner with concise instructions.We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.<span class='px-1 mx-1 bg-yellow-200'>Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Systemic property dispossession from minority groups has often been carried out in the name of technological progress.In this paper, we identify evidence that the current paradigm of large language models (LLMs) likely continues this long history.Examining common LLM training datasets, we find that a disproportionate amount of content authored by Jewish Americans is used for training without their consent.The degree of over-representation ranges from around 2x to around 6.5x.Given that LLMs may substitute for the paid labor of those who produced their training data, they have the potential to cause even more substantial and disproportionate economic harm to Jewish Americans in the coming years.<span class='px-1 mx-1 bg-yellow-200'>This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a "canary in the coal mine" that highlights deep structural concerns about the current LLM paradigm whose harms could soon affect nearly everyone. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.723</span></span>We discuss the implications of these results for the policymakers thinking about how to regulate LLMs as well as for those in the AI field who are working to advance LLMs.<span class='px-1 mx-1 bg-yellow-200'>Our findings stress the importance of working together towards alternative LLM paradigms that avoid both disparate impacts and widespread societal harms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.695</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13073v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13073v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.653</span></span>Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity.<span class='px-1 mx-1 bg-yellow-200'>In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.662</span></span>Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries.<span class='px-1 mx-1 bg-yellow-200'>Both our human assessment framework and dataset design process are grounded in an iterative participatory approach and review of possible biases in Med-PaLM 2 answers to adversarial queries. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.733</span></span>Through our empirical study, we find that the use of a collection of datasets curated through a variety of methodologies, coupled with a thorough evaluation protocol that leverages multiple assessment rubric designs and diverse rater groups, surfaces biases that may be missed via narrower evaluation approaches.Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise.<span class='px-1 mx-1 bg-yellow-200'>We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment of an AI system promotes equitable health outcomes. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.702</span></span>We hope the broader community leverages and builds on these tools and methods towards realizing a shared goal of LLMs that promote accessible and equitable healthcare for all.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12025v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12025v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>There are multiple sources of financial news online which influence market movements and trader's decisions.This highlights the need for accurate sentiment analysis, in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions.<span class='px-1 mx-1 bg-yellow-200'>Standard lexicon based sentiment approaches have demonstrated their power in aiding financial decisions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>However, they are known to suffer from issues related to context sensitivity and word ordering.Large Language Models (LLMs) can also be used in this context, but they are not finance-specific and tend to require significant computational resources.To facilitate a finance specific LLM framework, we introduce a novel approach based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation.This is achieved by fine-tuning the Llama2 7B model on a small portion of supervised financial sentiment analysis data, so as to jointly handle the complexities of financial lexicon and context, and further equipping it with a neural network based decision mechanism.Such a generator-classifier scheme, referred to as FinLlama, is trained not only to classify the sentiment valence but also quantify its strength, thus offering traders a nuanced insight into financial news articles.Complementing this, the implementation of parameter-efficient fine-tuning through LoRA optimises trainable parameters, thus minimising computational and memory requirements, without sacrificing accuracy.Simulation results demonstrate the ability of the proposed FinLlama to provide a framework for enhanced portfolio management decisions and increased market returns.These results underpin the ability of FinLlama to construct high-return portfolios which exhibit enhanced resilience, even during volatile periods and unpredictable market events.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12285v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12285v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.672</span></span>SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health.Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes.<span class='px-1 mx-1 bg-yellow-200'>Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.685</span></span>Large Language Models (LLMs) offer promise in overcoming these challenges by adapting to diverse language patterns.This study investigates the application of LLMs for extracting severity-related information for various SUD diagnoses from clinical notes.We propose a workflow employing zero-shot learning of LLMs with carefully crafted prompts and post-processing techniques.Through experimentation with Flan-T5, an open-source LLM, we demonstrate its superior recall compared to the rule-based approach.Focusing on 11 categories of SUD diagnoses, we show the effectiveness of LLMs in extracting severity information, contributing to improved risk assessment and treatment planning for SUD patients.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12297v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12297v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs in Education Research</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>As the recent Large Language Models(LLM's) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM's in conversation-based tutoring systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.719</span></span><span class='px-1 mx-1 bg-yellow-200'>However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.564</span></span>The results substantiate that our system's framework facilitates personalization, with particular emphasis on the elements constituting student modeling.A web demo of our system is available at http://rlearning-its.com.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14071v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14071v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation.Detecting DeepFakes is currently solved with programmed machine learning algorithms.In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection.<span class='px-1 mx-1 bg-yellow-200'>We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.637</span></span>This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming.We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14077v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14077v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense.<span class='px-1 mx-1 bg-yellow-200'>We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings.We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance.We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors.Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization.It can also serve as a reference for studies in other fields.We will release CHARM at https://github.com/opendatalab/CHARM .</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14112v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14112v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatic detection of multimodal misinformation has gained a widespread attention recently.However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored.<span class='px-1 mx-1 bg-yellow-200'>Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span><span class='px-1 mx-1 bg-yellow-200'>To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span><span class='px-1 mx-1 bg-yellow-200'>To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>This pipeline consists of a visual information processing module and an evidence retrieval module.Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation.Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments.The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14171v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14171v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Improving the Robustness of Large Language Models via Consistency Alignment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses.Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions.Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation.However, systematic analysis and solutions are still lacking.In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training.The first stage helps a model generalize on following instructions via similar instruction augmentations.In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses.The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources.<span class='px-1 mx-1 bg-yellow-200'>We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14221v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14221v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.513</span></span>The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants.<span class='px-1 mx-1 bg-yellow-200'>We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.611</span></span>The peer conversation transcripts were analyzed using thematic analysis.We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded.As participants, they foster children's creative thinking but may not consistently provide timely feedback.These findings highlight potential design improvements and considerations for peer agents in both roles.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14227v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14227v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents.Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance.<span class='px-1 mx-1 bg-yellow-200'>Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span>However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate.To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.523</span></span>Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14252v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14252v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-role Consensus through LLMs Discussions for Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance.Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.<span class='px-1 mx-1 bg-yellow-200'>To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.537</span></span>Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14274v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14274v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Blockchain Security: A Systematic Literature Review
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS).<span class='px-1 mx-1 bg-yellow-200'>Several recent studies are exploring LLMs applied to BS. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.514</span></span>However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security.To fill this gap, we conduct a literature review on LLM4BS.   As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems.Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security.We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on.Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks.Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14280v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14280v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined.Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment.One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop.<span class='px-1 mx-1 bg-yellow-200'>With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.511</span></span>The resulting pipeline is automated to a large extent, with feedback being generated at each step.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14460v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14460v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Alternative Solutions: Large Language Models Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications.This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics.These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more.Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights.<span class='px-1 mx-1 bg-yellow-200'>A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span>The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms.Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs.Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature.<span class='px-1 mx-1 bg-yellow-200'>By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span>This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14469v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14469v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare.Despite their potential benefits, researchers have underscored various ethical implications.While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare.Electronic databases and preprint servers were queried using a comprehensive search strategy.Studies were screened and extracted following a modified rapid review approach.Methodological quality was assessed using a hybrid approach.For 53 records, a meta-aggregative synthesis was performed.Four fields of applications emerged and testify to a vivid exploration phase.Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility.However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy.A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content.A recurrent plea for ethical guidance and human oversight is evident.Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications.This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.<span class='px-1 mx-1 bg-yellow-200'>In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.521</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14473v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14473v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.515</span></span>While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores.<span class='px-1 mx-1 bg-yellow-200'>Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.705</span></span><span class='px-1 mx-1 bg-yellow-200'>Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.525</span></span><span class='px-1 mx-1 bg-yellow-200'>A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14565v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14565v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Language Repository for Long Video Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language has become a prominent modality in computer vision with the rise of multi-modal LLMs.Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length.This becomes critical, especially in applications such as long-form video understanding.In this paper, we introduce a Language Repository (LangRepo) for LLMs, that maintains concise and structured information as an interpretable (i.e., all-textual) representation.Our repository is updated iteratively based on multi-scale video chunks.We introduce write and read operations that focus on pruning redundancies in text, and extracting information at various temporal scales.<span class='px-1 mx-1 bg-yellow-200'>The proposed framework is evaluated on zero-shot visual question-answering benchmarks including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.505</span></span>Our code is available at https://github.com/kkahatapitiya/LangRepo.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14622v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14622v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts.<span class='px-1 mx-1 bg-yellow-200'>However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.618</span></span>We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.<span class='px-1 mx-1 bg-yellow-200'>To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span><span class='px-1 mx-1 bg-yellow-200'>We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.592</span></span>Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total.<span class='px-1 mx-1 bg-yellow-200'>This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.561</span></span>In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers.<span class='px-1 mx-1 bg-yellow-200'>Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span>We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs.Project page: https://mathverse-cuhk.github.io</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14624v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14624v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks.One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options.While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference.This paper addresses this inefficiency at inference time.<span class='px-1 mx-1 bg-yellow-200'>The aim is to distill the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.583</span></span><span class='px-1 mx-1 bg-yellow-200'>We explore two variants of student models: one based on pure distillation, and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.569</span></span>Our approach is general and can be applied to both black-box and white-box LLMs.Furthermore, we demonstrate that our compact, encoder-only student models can outperform their larger, biased teacher counterparts, achieving better results with significantly fewer parameters.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13590v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13590v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We demonstrate experimental results with LLMs that address robotics action planning problems.Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes.In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies.<span class='px-1 mx-1 bg-yellow-200'>Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13801v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13801v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Characteristic AI Agents via Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems.Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots.<span class='px-1 mx-1 bg-yellow-200'>While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.528</span></span>Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings.Current investigations have primarily focused on act on roles with simple profiles.In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics.A dataset called ``Character100'' is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play.With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings.In addition, we devise a set of automatic metrics for quantitative performance evaluation.The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents.The benchmark is available at https://github.com/nuaa-nlp/Character100.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12368v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12368v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The progress in natural language processing (NLP) using large language models (LLMs) has greatly improved patient information extraction from clinical narratives.However, most methods based on the fine-tuning strategy have limited transfer learning ability for cross-domain applications.<span class='px-1 mx-1 bg-yellow-200'>This study proposed a novel approach that employs a soft prompt-based learning architecture, which introduces trainable prompts to guide LLMs toward desired outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.569</span></span>We examined two types of LLM architectures, including encoder-only GatorTron and decoder-only GatorTronGPT, and evaluated their performance for the extraction of social determinants of health (SDoH) using a cross-institution dataset from the 2022 n2c2 challenge and a cross-disease dataset from the University of Florida (UF) Health.The results show that decoder-only LLMs with prompt tuning achieved better performance in cross-domain applications.GatorTronGPT achieved the best F1 scores for both datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12374v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12374v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in Natural Language Processing (NLP) by aiming to answer complex questions through multi-step reasoning over retrieved information from external knowledge sources.Recently, Large Language Models (LLMs) have demonstrated remarkable performance in solving ODMHQA owing to their capabilities including planning, reasoning, and utilizing tools.<span class='px-1 mx-1 bg-yellow-200'>However, LLMs may generate off-topic answers when attempting to solve ODMHQA, namely the generated answers are irrelevant to the original questions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.522</span></span>This issue of off-topic answers accounts for approximately one-third of incorrect answers, yet remains underexplored despite its significance.To alleviate this issue, we propose the Discriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism.Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to judge whether the generated answers are off-topic.In cases where an off-topic answer is detected, the Corrector performs step-wise revisions along the reversed reasoning chain (Re-Compose->Re-Solve->Re-Decompose) until the final answer becomes on-topic.Experimental results on the HotpotQA and 2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving the performance in Exact Match (EM) by nearly 3% compared to the baseline method without the Dr3 mechanism.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12393v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12393v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the effectiveness of Large Language Models for GitHub Workflows
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., YAML files with a list of jobs.However, engineering valid workflows is tedious.They are also prone to severe security issues, which can result in supply chain vulnerabilities.Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in various software development tasks.However, GitHub workflows differ from regular programs in both structure and semantics.<span class='px-1 mx-1 bg-yellow-200'>We perform the first comprehensive study to understand the effectiveness of LLMs on five workflow-related tasks with different levels of prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>We curated a set of $\sim$400K workflows and generated prompts with varying detail.We also fine-tuned LLMs on GitHub workflow tasks.Our evaluation of three state-of-the-art LLMs and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12446v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12446v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Chinese Large Language Models (LLMs) have recently demonstrated impressive capabilities across various NLP benchmarks and real-world applications.However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture.<span class='px-1 mx-1 bg-yellow-200'>Current datasets collect questions from Chinese examinations across different subjects and educational levels to address this issue. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span>Yet, these benchmarks primarily focus on objective questions such as multiple-choice questions, leading to a lack of diversity in question types.To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper.<span class='px-1 mx-1 bg-yellow-200'>LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span><span class='px-1 mx-1 bg-yellow-200'>It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.617</span></span><span class='px-1 mx-1 bg-yellow-200'>Notably, LHMKE includes both objective and subjective questions, offering a more holistic evaluation of the knowledge level of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.507</span></span><span class='px-1 mx-1 bg-yellow-200'>We have assessed 11 Chinese LLMs under the zero-shot setting, which aligns with real examinations, and compared their performance across different subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.563</span></span>We also conduct an in-depth analysis to check whether GPT-4 can automatically score subjective predictions.<span class='px-1 mx-1 bg-yellow-200'>Our findings suggest that LHMKE is a challenging and advanced testbed for Chinese LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.58</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12601v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12601v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness.Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.501</span></span>This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information.Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving.Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation.Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies.This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification.The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12627v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12627v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ICE: Interactive 3D Game Character Editing via Dialogue
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Text-driven in-game 3D character auto-customization systems eliminate the complicated process of manipulating intricate character control parameters.However, current methods are limited by their single-round generation, incapable of further editing and fine-grained modification.In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process.In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players.<span class='px-1 mx-1 bg-yellow-200'>Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner.Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results.Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE.Project page: https://iceedit.github.io/.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12667v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12667v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pragmatic Competence Evaluation of Large Language Models for Korean
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The current evaluation of Large Language Models (LLMs) predominantly relies on benchmarks focusing on their embedded knowledge by testing through multiple-choice questions (MCQs), a format inherently suited for automated evaluation.Our study extends this evaluation to explore LLMs' pragmatic competence--a facet previously underexamined before the advent of sophisticated LLMs, specifically in the context of Korean.We employ two distinct evaluation setups: the conventional MCQ format, adapted for automatic evaluation, and Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs' narrative response capabilities without predefined options.<span class='px-1 mx-1 bg-yellow-200'>Our findings reveal that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups, respectively, with HyperCLOVA X, an LLM optimized for Korean, closely following, especially in the OEQ setup, demonstrating a score of 81.56 with a marginal difference of 4.13 points compared to GPT-4. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span>Furthermore, while few-shot learning strategies generally enhance LLM performance, Chain-of-Thought (CoT) prompting introduces a bias toward literal interpretations, hindering accurate pragmatic inference.Considering the growing expectation for LLMs to understand and produce language that aligns with human communicative norms, our findings emphasize the importance for advancing LLMs' abilities to grasp and convey sophisticated meanings beyond mere literal interpretations.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12675v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12675v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents.<span class='px-1 mx-1 bg-yellow-200'>How to integrate agent ability into general LLMs becomes a crucial and urgent problem. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.506</span></span>This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations.Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation datasets.With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark.Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs.The code will be available at https://github.com/InternLM/Agent-FLAN.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12881v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12881v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.This lends to it a quality of creativity which can be empowering in the early stages of design.<span class='px-1 mx-1 bg-yellow-200'>In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.51</span></span>We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.Participants typically prompted in a straightforward manner with concise instructions.We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Systemic property dispossession from minority groups has often been carried out in the name of technological progress.In this paper, we identify evidence that the current paradigm of large language models (LLMs) likely continues this long history.Examining common LLM training datasets, we find that a disproportionate amount of content authored by Jewish Americans is used for training without their consent.The degree of over-representation ranges from around 2x to around 6.5x.Given that LLMs may substitute for the paid labor of those who produced their training data, they have the potential to cause even more substantial and disproportionate economic harm to Jewish Americans in the coming years.This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a "canary in the coal mine" that highlights deep structural concerns about the current LLM paradigm whose harms could soon affect nearly everyone.<span class='px-1 mx-1 bg-yellow-200'>We discuss the implications of these results for the policymakers thinking about how to regulate LLMs as well as for those in the AI field who are working to advance LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.543</span></span>Our findings stress the importance of working together towards alternative LLM paradigms that avoid both disparate impacts and widespread societal harms.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13073v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13073v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights.Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding.<span class='px-1 mx-1 bg-yellow-200'>However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>The broader capabilities and limitations of multimodal ICL remain under-explored.In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}.We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging.By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL.The code and dataset are available at https://github.com/ys-zong/VL-ICL.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13164v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13164v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLMs as Recommender Systems</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>As the recent Large Language Models(LLM's) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM's in conversation-based tutoring systems.<span class='px-1 mx-1 bg-yellow-200'>However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies.Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants.The results substantiate that our system's framework facilitates personalization, with particular emphasis on the elements constituting student modeling.A web demo of our system is available at http://rlearning-its.com.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14071v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14071v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Harnessing Large Language Models for Text-Rich Sequential Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.836</span></span>However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence.<span class='px-1 mx-1 bg-yellow-200'>This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.72</span></span><span class='px-1 mx-1 bg-yellow-200'>To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.804</span></span>Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks.Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) models in user modeling, we introduce two unique summarization techniques in this paper, respectively hierarchical summarization and recurrent summarization.<span class='px-1 mx-1 bg-yellow-200'>Then, we construct a prompt text encompassing the user preference summary, recent user interactions, and candidate item information into an LLM-based recommender, which is subsequently fine-tuned using Supervised Fine-Tuning (SFT) techniques to yield our final recommendation model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.765</span></span>We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient Fine-Tuning (PEFT).We conduct experiments on two public datasets, and the results clearly demonstrate the effectiveness of our approach.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13325v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13325v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience.<span class='px-1 mx-1 bg-yellow-200'>However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.854</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.721</span></span><span class='px-1 mx-1 bg-yellow-200'>The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.634</span></span><span class='px-1 mx-1 bg-yellow-200'>Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying user preferences from heterogeneous interaction behaviors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.67</span></span>In order to integrate the merits of the SR model and the supplemental LLM recommender, we design a twostage training paradigm.The first stage is personalized preference alignment, which aims to align the preference representations from both components, thereby enhancing the semantics of the SR model.The second stage is recommendation-oriented fine-tuning, in which the alignment-enhanced SR model is fine-tuned according to specific objectives.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments in both video and comment recommendation tasks demonstrate the effectiveness of LSVCR. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.781</span></span>Additionally, online A/B testing on the KuaiShou platform verifies the actual benefits brought by our approach.In particular, we achieve a significant overall gain of 4.13% in comment watch time.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13574v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13574v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Graph Foundation Models for Personalization
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions.In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs).While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval.In this paper, we present a graph-based foundation modeling approach tailored to personalization.Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types.To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of nodes that accommodates all item types, and construct the graph using co-interaction signals, which inherently transcend content specificity.To facilitate practical generalization, we further couple the HGNN with an adaptation mechanism based on a two-tower (2T) architecture, which also operates agnostically to content type.This multi-stage approach ensures high scalability; while the HGNN produces general purpose embeddings, the 2T component models in a continuous space the sheer size of user-item interaction data.<span class='px-1 mx-1 bg-yellow-200'>Our comprehensive approach has been rigorously tested and proven effective in delivering recommendations across a diverse array of products within a real-world, industrial audio streaming platform. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.601</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07478v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07478v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.715</span></span>The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions.However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset.<span class='px-1 mx-1 bg-yellow-200'>To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.633</span></span><span class='px-1 mx-1 bg-yellow-200'>Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summarize the patterns indicating which types of users would be attracted by certain items. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.619</span></span>The retrieved collaborative evidence prompts the LLM to align its reasoning with the user-item interaction patterns in the dataset.<span class='px-1 mx-1 bg-yellow-200'>However, since the capacity of the input prompt is limited, finding the minimally-sufficient collaborative information for recommendation tasks can be challenging. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.623</span></span>We propose to find the optimal interaction set through a sequential decision-making process and develop a retrieval policy learned through a reinforcement learning (RL) framework, CoRAL.<span class='px-1 mx-1 bg-yellow-200'>Our experimental results show that CoRAL can significantly improve LLMs' reasoning abilities on specific recommendation tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.725</span></span>Our analysis also reveals that CoRAL can more efficiently explore collaborative information through reinforcement learning.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.06447v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.06447v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.794</span></span><span class='px-1 mx-1 bg-yellow-200'>RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span><span class='px-1 mx-1 bg-yellow-200'>The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.833</span></span>We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems.The source code of RecAI is available at \url{https://github.com/microsoft/RecAI}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.06465v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.06465v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.631</span></span><span class='px-1 mx-1 bg-yellow-200'>With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.798</span></span>However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination.A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text.Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method.In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training.Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.06642v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.06642v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Federated Recommendation via Hybrid Retrieval Augmented Generation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span>However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR.<span class='px-1 mx-1 bg-yellow-200'>On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.773</span></span><span class='px-1 mx-1 bg-yellow-200'>Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.722</span></span><span class='px-1 mx-1 bg-yellow-200'>To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span>GPT-FedRec is a two-stage solution.The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features.Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking.Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR.<span class='px-1 mx-1 bg-yellow-200'>In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span>Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.04256v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.04256v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Can Small Language Models be Good Reasoners for Sequential Recommendation?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.739</span></span>However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs.Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses.<span class='px-1 mx-1 bg-yellow-200'>Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.675</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.799</span></span>We introduce CoT prompting based on user behavior sequences for the larger teacher model.The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B).In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks.<span class='px-1 mx-1 bg-yellow-200'>We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.61</span></span><span class='px-1 mx-1 bg-yellow-200'>Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.837</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.04260v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.04260v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-07</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.814</span></span>Despite the progress, the field has many aspects left to explore.<span class='px-1 mx-1 bg-yellow-200'>The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.626</span></span><span class='px-1 mx-1 bg-yellow-200'>To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.754</span></span>We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues.Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relevant to the dialogue context than those in prior datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.04460v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.04460v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Generative News Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Most existing news recommendation methods tackle this task by conducting semantic matching between candidate news and user representation produced by historical clicked news. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.817</span></span>However, they overlook the high-level connections among different news articles and also ignore the profound relationship between these news articles and users.And the definition of these methods dictates that they can only deliver news articles as-is.On the contrary, integrating several relevant news articles into a coherent narrative would assist users in gaining a quicker and more comprehensive understanding of events.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose a novel generative news recommendation paradigm that includes two steps: (1) Leveraging the internal knowledge and reasoning capabilities of the Large Language Model (LLM) to perform high-level matching between candidate news and user representation; (2) Generating a coherent and logically structured narrative based on the associations between related news and user interests, thus engaging users in further reading of the news. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.788</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we propose GNR to implement the generative news recommendation paradigm. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.641</span></span>First, we compose the dual-level representation of news and users by leveraging LLM to generate theme-level representations and combine them with semantic-level representations.Next, in order to generate a coherent narrative, we explore the news relation and filter the related news according to the user preference.Finally, we propose a novel training method named UIFT to train the LLM to fuse multiple news articles in a coherent narrative.<span class='px-1 mx-1 bg-yellow-200'>Extensive experiments show that GNR can improve recommendation accuracy and eventually generate more personalized and factually consistent narratives. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.678</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.03424v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.03424v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-06</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Efficient and Effective Unlearning of Large Language Models for Recommendation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.752</span></span>The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs.<span class='px-1 mx-1 bg-yellow-200'>LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.815</span></span>However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning.<span class='px-1 mx-1 bg-yellow-200'>In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming.Besides, they always impact the model utility during the unlearning process.To this end, we propose \textbf{E2URec}, the first \underline{E}fficient and \underline{E}ffective \underline{U}nlearning method for LLM\underline{Rec}.Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process.Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets.Specifically, E2URec can efficiently forget specific data without affecting recommendation performance.The source code is at \url{https://github.com/justarter/E2URec}.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.03536v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.03536v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.806</span></span><span class='px-1 mx-1 bg-yellow-200'>Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.779</span></span>It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions.At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities.<span class='px-1 mx-1 bg-yellow-200'>As a result, recent research has sought to harness the power of LLM to improve recommendation systems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.823</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the rapid development of this research direction in the field of recommendation systems, there is an urgent need for a systematic review of existing LLM-driven recommendation systems for researchers and practitioners in related fields to gain insight into. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.891</span></span>More specifically, we first introduced a representative approach to learning user and item representations using LLM as a feature encoder.<span class='px-1 mx-1 bg-yellow-200'>We then reviewed the latest advances in LLMs techniques for collaborative filtering enhanced recommendation systems from the three paradigms of pre-training, fine-tuning, and prompting. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.82</span></span>Finally, we had a comprehensive discussion on the future direction of this emerging field.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.02760v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.02760v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-05</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.636</span></span>Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations.However, their precision is still far away from acceptable in a sensitive field like education.To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context.<span class='px-1 mx-1 bg-yellow-200'>We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.755</span></span>With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM.Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner.We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners.Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.03008v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.03008v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-04</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs.<span class='px-1 mx-1 bg-yellow-200'>Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.6</span></span>Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information.In this paper, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs.A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM.Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs.By providing informative and specific feedback, ContrastRepair enables the LLM to produce effective bug fixes.The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated.We evaluate ContrastRepair on multiple benchmark datasets, including Defects4j, QuixBugs, and HumanEval-Java.The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair.For instance, among Defects4j 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.01971v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.01971v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Production workflows for LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FIT-RAG: Black-Box RAG with Factual Information and Token Reduction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.698</span></span><span class='px-1 mx-1 bg-yellow-200'>To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.453</span></span>Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention.<span class='px-1 mx-1 bg-yellow-200'>Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.319</span></span><span class='px-1 mx-1 bg-yellow-200'>The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.329</span></span><span class='px-1 mx-1 bg-yellow-200'>Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.308</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.371</span></span>FIT-RAG utilizes the factual information by constructing a bi-label document scorer.<span class='px-1 mx-1 bg-yellow-200'>Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.502</span></span>FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA.FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA, respectively.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, it can save approximately half of the tokens on average across the three datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.376</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14374v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14374v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online.Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages.However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance.In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting.We create a web-based platform where participants engage in short, multiple-round debates with a live opponent.Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent.We found that participants who debated GPT-4 with access to their personal information had 81.7% (p < 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans.Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31).<span class='px-1 mx-1 bg-yellow-200'>Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.397</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14380v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14380v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal.Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions.<span class='px-1 mx-1 bg-yellow-200'>The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.518</span></span><span class='px-1 mx-1 bg-yellow-200'>However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.398</span></span>In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models.<span class='px-1 mx-1 bg-yellow-200'>In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.54</span></span>In \emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method.<span class='px-1 mx-1 bg-yellow-200'>Finally, We train a small model using distilled data generated through two-stage methods. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.355</span></span><span class='px-1 mx-1 bg-yellow-200'>As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.37</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14390v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14390v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems.<span class='px-1 mx-1 bg-yellow-200'>However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.582</span></span>To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations.However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information.<span class='px-1 mx-1 bg-yellow-200'>In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.46</span></span><span class='px-1 mx-1 bg-yellow-200'>Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples.<span class='px-1 mx-1 bg-yellow-200'>Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.4</span></span>Analysis shows that our method could preserve the model's general task performance on AlpacaEval.<span class='px-1 mx-1 bg-yellow-200'>Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.424</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14399v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14399v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA).<span class='px-1 mx-1 bg-yellow-200'>However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.319</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.595</span></span><span class='px-1 mx-1 bg-yellow-200'>Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span><span class='px-1 mx-1 bg-yellow-200'>This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.415</span></span><span class='px-1 mx-1 bg-yellow-200'>We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.479</span></span>Code is available at: https://github.com/starsuzi/Adaptive-RAG.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Locating and Mitigating Gender Bias in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.327</span></span>However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society.Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it.This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another.In this study, we integrate the processes of locating and mitigating bias within a unified framework.Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model.<span class='px-1 mx-1 bg-yellow-200'>Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.34</span></span>The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.306</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14409v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14409v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command.<span class='px-1 mx-1 bg-yellow-200'>To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.3</span></span>We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform.<span class='px-1 mx-1 bg-yellow-200'>Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.338</span></span>Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM.Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%.<span class='px-1 mx-1 bg-yellow-200'>Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.47</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14438v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14438v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined.Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment.One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop.<span class='px-1 mx-1 bg-yellow-200'>With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.436</span></span><span class='px-1 mx-1 bg-yellow-200'>The resulting pipeline is automated to a large extent, with feedback being generated at each step. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.481</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14460v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14460v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Alternative Solutions: Large Language Models Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications.This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics.<span class='px-1 mx-1 bg-yellow-200'>These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.311</span></span>Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights.A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention.The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms.<span class='px-1 mx-1 bg-yellow-200'>Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.388</span></span>Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature.By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories.This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14469v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14469v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detoxifying Large Language Models via Knowledge Editing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.405</span></span>We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation.<span class='px-1 mx-1 bg-yellow-200'>We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.362</span></span>Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance.We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.Code and benchmark are available at https://github.com/zjunlp/EasyEdit.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare.Despite their potential benefits, researchers have underscored various ethical implications.While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare.<span class='px-1 mx-1 bg-yellow-200'>Electronic databases and preprint servers were queried using a comprehensive search strategy. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.388</span></span>Studies were screened and extracted following a modified rapid review approach.Methodological quality was assessed using a hybrid approach.For 53 records, a meta-aggregative synthesis was performed.Four fields of applications emerged and testify to a vivid exploration phase.<span class='px-1 mx-1 bg-yellow-200'>Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.56</span></span>However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy.A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content.A recurrent plea for ethical guidance and human oversight is evident.Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications.This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14473v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14473v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks.Temperature sampling is a commonly used decoding strategy for LLMs' generation process.<span class='px-1 mx-1 bg-yellow-200'>However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.325</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.321</span></span>Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks.Our experiments show that EDT significantly outperforms the existing strategies across different tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14541v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14541v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Era of Semantic Decoding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs.We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space.Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts).<span class='px-1 mx-1 bg-yellow-200'>LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.44</span></span><span class='px-1 mx-1 bg-yellow-200'>Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.373</span></span>We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms.<span class='px-1 mx-1 bg-yellow-200'>This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.394</span></span>By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities.In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding.<span class='px-1 mx-1 bg-yellow-200'>Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.366</span></span>We conclude with a list of research opportunities and questions arising from this fresh perspective.<span class='px-1 mx-1 bg-yellow-200'>The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.328</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14562v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14562v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched.In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain.We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case.<span class='px-1 mx-1 bg-yellow-200'>We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.343</span></span><span class='px-1 mx-1 bg-yellow-200'>We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.496</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14578v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14578v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Multi-Choice Question Classification of Medical Subjects
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects.This is an important and challenging task for automatic question answering.<span class='px-1 mx-1 bg-yellow-200'>To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.308</span></span><span class='px-1 mx-1 bg-yellow-200'>Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.358</span></span>In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14582v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14582v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models.Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data.However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks.In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct.The central role is an ActRe prompting agent, which explains the reason for an arbitrary action.When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales.Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action.In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training.Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement.We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2.In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds.In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts.<span class='px-1 mx-1 bg-yellow-200'>A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.35</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14589v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14589v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Language Repository for Long Video Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language has become a prominent modality in computer vision with the rise of multi-modal LLMs.<span class='px-1 mx-1 bg-yellow-200'>Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.305</span></span>This becomes critical, especially in applications such as long-form video understanding.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we introduce a Language Repository (LangRepo) for LLMs, that maintains concise and structured information as an interpretable (i.e., all-textual) representation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.347</span></span>Our repository is updated iteratively based on multi-scale video chunks.We introduce write and read operations that focus on pruning redundancies in text, and extracting information at various temporal scales.The proposed framework is evaluated on zero-shot visual question-answering benchmarks including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale.Our code is available at https://github.com/kkahatapitiya/LangRepo.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14622v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14622v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts.However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood.We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams.To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs.We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources.Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total.This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for mathematical reasoning.In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers.<span class='px-1 mx-1 bg-yellow-200'>Rather than naively judging True or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT reasoning quality by MLLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.316</span></span>We hope the MathVerse benchmark may provide unique insights to guide the future development of MLLMs.Project page: https://mathverse-cuhk.github.io</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14624v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14624v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">LLM Model Architectures and Training Techniques</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AI and Memory Wall
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The availability of unprecedented unsupervised training data, along with neural scaling laws, has resulted in an unprecedented surge in model size and compute requirements for serving/training LLMs.<span class='px-1 mx-1 bg-yellow-200'>However, the main performance bottleneck is increasingly shifting to memory bandwidth. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.737</span></span><span class='px-1 mx-1 bg-yellow-200'>Over the past 20 years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.57</span></span><span class='px-1 mx-1 bg-yellow-200'>This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.478</span></span><span class='px-1 mx-1 bg-yellow-200'>Here, we analyze encoder and decoder Transformer models and show how memory bandwidth can become the dominant bottleneck for decoder models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.412</span></span><span class='px-1 mx-1 bg-yellow-200'>We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.558</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14123v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14123v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Automatic detection of multimodal misinformation has gained a widespread attention recently.However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored.Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation.<span class='px-1 mx-1 bg-yellow-200'>To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span>This pipeline consists of a visual information processing module and an evidence retrieval module.Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation.Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.<span class='px-1 mx-1 bg-yellow-200'>To explore several research questions regarding the performance of LLMs in multimodal misinformation detection tasks, we construct an instruction-following multimodal misinformation dataset and conduct comprehensive experiments. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.43</span></span>The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14171v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14171v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Improving the Robustness of Large Language Models via Consistency Alignment
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses.<span class='px-1 mx-1 bg-yellow-200'>Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.435</span></span><span class='px-1 mx-1 bg-yellow-200'>Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.484</span></span>However, systematic analysis and solutions are still lacking.<span class='px-1 mx-1 bg-yellow-200'>In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.548</span></span>The first stage helps a model generalize on following instructions via similar instruction augmentations.In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses.The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources.<span class='px-1 mx-1 bg-yellow-200'>We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.534</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14221v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14221v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.512</span></span><span class='px-1 mx-1 bg-yellow-200'>Underspecified preferences could obscure directions to align the models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.423</span></span>Lacking exploration restricts identification of desirable outputs to improve the models.To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs.<span class='px-1 mx-1 bg-yellow-200'>RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.573</span></span>Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14238v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14238v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.408</span></span>Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance.Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure.<span class='px-1 mx-1 bg-yellow-200'>However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.414</span></span>To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs).By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model.Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14252v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14252v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM-based Extraction of Contradictions from Patents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products.However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents.Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods.Recent advances in patent retrieval and analysis mainly focus on dense vectors based on neural AI Transformer language models like Google BERT.They are, for example, used for dense retrieval, question answering or summarization and key concept extraction.A research focus within the methods for patent summarization and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions.Succeeding rule-based approaches, finetuned BERT-like language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction.While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models.This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on Prompt Engineering using a generative Large Language Model (LLM), namely OpenAI's GPT-4.Contradiction detection, sentence extraction, contradiction summarization, parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single prompt using the LangChain framework.<span class='px-1 mx-1 bg-yellow-200'>Our results show that "off-the-shelf" GPT-4 is a serious alternative to existing approaches. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.431</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14258v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14258v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Multi-role Consensus through LLMs Discussions for Vulnerability Detection
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance.Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers.<span class='px-1 mx-1 bg-yellow-200'>To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.524</span></span><span class='px-1 mx-1 bg-yellow-200'>Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.485</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14274v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14274v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Large Language Models for Blockchain Security: A Systematic Literature Review
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS).Several recent studies are exploring LLMs applied to BS.<span class='px-1 mx-1 bg-yellow-200'>However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.405</span></span><span class='px-1 mx-1 bg-yellow-200'>To fill this gap, we conduct a literature review on LLM4BS.    <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.465</span></span>As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems.Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security.We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on.Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks.Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14280v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14280v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives.These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons.Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company's financial and operational situations, which is critical for financial decision-making.We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts.<span class='px-1 mx-1 bg-yellow-200'>To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.535</span></span>Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14341v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14341v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes.However, existing GZSL is still limited to seen domains.In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains.Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains.<span class='px-1 mx-1 bg-yellow-200'>Considering the information asymmetry problem caused by redundant class semantics annotated with large language models (LLMs), we present Meta Domain Alignment Semantic Refinement (MDASR). <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.563</span></span><span class='px-1 mx-1 bg-yellow-200'>Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic semantics not shared across all domains under the guidance of inter-class feature relationships, and Unseen-class Meta Generation (UMG), which preserves intrinsic semantics to maintain connectivity between seen and unseen classes by simulating feature generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.438</span></span><span class='px-1 mx-1 bg-yellow-200'>MDASR effectively aligns the redundant semantic space with the common feature space, mitigating the information asymmetry in CDGZSL. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.471</span></span>The effectiveness of MDASR is demonstrated on the Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics for these datasets as the benchmark.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14362v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14362v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The factuality of large language model (LLMs) tends to decay over time since events posterior to their training are "unknown" to them.One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model.To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static.<span class='px-1 mx-1 bg-yellow-200'>We describe several update scenarios arising from various combinations of these three types of basic update. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.411</span></span>The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023.Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics.Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions.We also present an evaluation of existing update algorithms on WikiFactDiff.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14364v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14364v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                FIT-RAG: Black-Box RAG with Factual Information and Token Reduction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.536</span></span><span class='px-1 mx-1 bg-yellow-200'>To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.508</span></span>Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention.Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs' preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information.The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens.Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG.To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG.FIT-RAG utilizes the factual information by constructing a bi-label document scorer.Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer.FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA.FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA, respectively.Furthermore, it can save approximately half of the tokens on average across the three datasets.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14374v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14374v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly supervised task settings that rely solely on the final answer as a supervised signal.Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions.The rise of Large Language Models (LLMs) like ChatGPT has opened up new possibilities for addressing MWPs directly.<span class='px-1 mx-1 bg-yellow-200'>However, the computational demands of LLMs make them less than ideal for use in settings where resources are tight. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.624</span></span>In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from large to tiny language models.In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to distill mathematical knowledge from LLMs to construct problem-equation pairs required for supervised training.In \emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method.Finally, We train a small model using distilled data generated through two-stage methods.As our method fully leverages the semantic understanding capabilities during the searching 'problem-equation' pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than ChatGPT.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14390v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14390v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems.<span class='px-1 mx-1 bg-yellow-200'>However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.4</span></span>To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations.<span class='px-1 mx-1 bg-yellow-200'>However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.416</span></span><span class='px-1 mx-1 bg-yellow-200'>In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.748</span></span>Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities.<span class='px-1 mx-1 bg-yellow-200'>In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.46</span></span><span class='px-1 mx-1 bg-yellow-200'>Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.415</span></span>Analysis shows that our method could preserve the model's general task performance on AlpacaEval.<span class='px-1 mx-1 bg-yellow-200'>Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.552</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14399v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14399v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA).However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories.<span class='px-1 mx-1 bg-yellow-200'>In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.454</span></span>Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets.This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities.We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches.<span class='px-1 mx-1 bg-yellow-200'>Code is available at: https://github.com/starsuzi/Adaptive-RAG. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.428</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14403v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14403v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Locating and Mitigating Gender Bias in Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences.However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society.Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it.This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another.<span class='px-1 mx-1 bg-yellow-200'>In this study, we integrate the processes of locating and mitigating bias within a unified framework. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.468</span></span>Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model.Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets.The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence.Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model's capabilities in all other aspects.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14409v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14409v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command.To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase.We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform.Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM).Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM.Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%.<span class='px-1 mx-1 bg-yellow-200'>Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.425</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14438v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14438v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined.<span class='px-1 mx-1 bg-yellow-200'>Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.453</span></span>One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop.<span class='px-1 mx-1 bg-yellow-200'>With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.54</span></span><span class='px-1 mx-1 bg-yellow-200'>The resulting pipeline is automated to a large extent, with feedback being generated at each step. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.444</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14460v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14460v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ChatGPT Alternative Solutions: Large Language Models Survey
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications.This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics.These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more.Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights.A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention.The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms.<span class='px-1 mx-1 bg-yellow-200'>Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.563</span></span>Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature.By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories.This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14469v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14469v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Detoxifying Large Language Models via Knowledge Editing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs).<span class='px-1 mx-1 bg-yellow-200'>We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.473</span></span><span class='px-1 mx-1 bg-yellow-200'>We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.413</span></span>Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance.We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments.We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.<span class='px-1 mx-1 bg-yellow-200'>Code and benchmark are available at https://github.com/zjunlp/EasyEdit. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.504</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14472v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14472v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare.Despite their potential benefits, researchers have underscored various ethical implications.While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them.Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of LLMs in medicine and healthcare.Electronic databases and preprint servers were queried using a comprehensive search strategy.Studies were screened and extracted following a modified rapid review approach.Methodological quality was assessed using a hybrid approach.For 53 records, a meta-aggregative synthesis was performed.Four fields of applications emerged and testify to a vivid exploration phase.Advantages of using LLMs are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility.However, we also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy.A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content.A recurrent plea for ethical guidance and human oversight is evident.<span class='px-1 mx-1 bg-yellow-200'>Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.433</span></span>This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare.In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of LLMs is necessary and justified.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14473v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14473v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Recently, Large Language Models (LLMs) have demonstrated outstanding performance across a wide range of downstream language tasks.Temperature sampling is a commonly used decoding strategy for LLMs' generation process.<span class='px-1 mx-1 bg-yellow-200'>However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.529</span></span>In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter.<span class='px-1 mx-1 bg-yellow-200'>Additionally, we also show model performance and comprehensive analyses for 4 different generation benchmarks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.412</span></span><span class='px-1 mx-1 bg-yellow-200'>Our experiments show that EDT significantly outperforms the existing strategies across different tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.48</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14541v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14541v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Large Language Models (LLMs) increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched.In this work we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation LLMs can serve as reliable assistants in the biomedical domain.<span class='px-1 mx-1 bg-yellow-200'>We identify prompt robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.422</span></span>We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions.<span class='px-1 mx-1 bg-yellow-200'>We evaluate LLM performance using semantic similarity with a ground truth response, through an evaluator LLM. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.406</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14578v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14578v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models.Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data.However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks.In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct.The central role is an ActRe prompting agent, which explains the reason for an arbitrary action.When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales.Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action.In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training.Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement.<span class='px-1 mx-1 bg-yellow-200'>We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.495</span></span>In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds.In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts.A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14589v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14589v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-21</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Language Repository for Long Video Understanding
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Language has become a prominent modality in computer vision with the rise of multi-modal LLMs.<span class='px-1 mx-1 bg-yellow-200'>Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.459</span></span>This becomes critical, especially in applications such as long-form video understanding.In this paper, we introduce a Language Repository (LangRepo) for LLMs, that maintains concise and structured information as an interpretable (i.e., all-textual) representation.Our repository is updated iteratively based on multi-scale video chunks.We introduce write and read operations that focus on pruning redundancies in text, and extracting information at various temporal scales.The proposed framework is evaluated on zero-shot visual question-answering benchmarks including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale.Our code is available at https://github.com/kkahatapitiya/LangRepo.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.14622v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.14622v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td></td>
          <td>
            <h2 class="text-2xl tracking-tight pt-4 font-bold">Programming applications of LLMs</h2>
          </td>
        </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Code Generation Performance of Smaller Models by Distilling the Reasoning Ability of LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have recently made significant advances in code generation through the 'Chain-of-Thought' prompting technique. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.918</span></span><span class='px-1 mx-1 bg-yellow-200'>This technique empowers the model to autonomously devise "solution plans" to tackle intricate programming challenges, thereby improving its performance in code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.674</span></span><span class='px-1 mx-1 bg-yellow-200'>Nevertheless, smaller models have been struggling to keep up with LLMs in deducing these plans, adversely affecting their code generation capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the considerable size and associated deployment costs, along with concerns about data security, many teams opt for deploying smaller models for code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.747</span></span><span class='px-1 mx-1 bg-yellow-200'>Consequently, there arises a compelling need for transferring LLMs' code generation reasoning abilities to the smaller models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.724</span></span>In this paper, we propose the CodePLAN framework, which aims to transfer LLMs' reasoning capabilities to smaller models through distillation.<span class='px-1 mx-1 bg-yellow-200'>We adopt a multi-task learning approach, jointly undertaking code generation and solution plan generation tasks, to enhance the code generation capabilities of the smaller model. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.76</span></span>To ensure the superior quality of the solution plans, we advocate for the utilization of backward reasoning and plan sampling strategies.<span class='px-1 mx-1 bg-yellow-200'>Our experiments show that in comparison to the conventional fine-tuning approach, our approach improves the smaller model's code generation performance (measured in pass@1 metric) by over 130% on the challenging APPS benchmark. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.774</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13271v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13271v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-20</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.939</span></span><span class='px-1 mx-1 bg-yellow-200'>However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.687</span></span><span class='px-1 mx-1 bg-yellow-200'>To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.798</span></span>CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications.CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets.<span class='px-1 mx-1 bg-yellow-200'>It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality and reliability of LLMs in generating intricate code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.69</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13583v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13583v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                On the effectiveness of Large Language Models for GitHub Workflows
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>GitHub workflows or GitHub CI is a popular continuous integration platform that enables developers to automate various software engineering tasks by specifying them as workflows, i.e., YAML files with a list of jobs.However, engineering valid workflows is tedious.They are also prone to severe security issues, which can result in supply chain vulnerabilities.<span class='px-1 mx-1 bg-yellow-200'>Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in various software development tasks. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.782</span></span>However, GitHub workflows differ from regular programs in both structure and semantics.We perform the first comprehensive study to understand the effectiveness of LLMs on five workflow-related tasks with different levels of prompts.We curated a set of $\sim$400K workflows and generated prompts with varying detail.We also fine-tuned LLMs on GitHub workflow tasks.Our evaluation of three state-of-the-art LLMs and their fine-tuned variants revealed various interesting findings on the current effectiveness and drawbacks of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12446v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12446v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering.Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress.However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes.Also, they can not integrate textual information such as financial news or reports.<span class='px-1 mx-1 bg-yellow-200'>Meanwhile, large language models (LLMs) have remarkable textual understanding and generation ability. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.689</span></span>But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLMs still suffer from hallucinations and are unable to keep up with the latest information.To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data.It has a positive impact on training LLMs for completing financial analysis.We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques.Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12582v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12582v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness.Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs).Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code.This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information.Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving.<span class='px-1 mx-1 bg-yellow-200'>Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies.This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification.The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12627v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12627v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Generative artificial intelligence (GenAI) can rapidly produce large and diverse volumes of content.This lends to it a quality of creativity which can be empowering in the early stages of design.<span class='px-1 mx-1 bg-yellow-200'>In seeking to understand how creative ways to address practical issues can be conceived between humans and GenAI, we conducted a rapid ideation workshop with 21 participants where they used a large language model (LLM) to brainstorm potential solutions and evaluate them. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.696</span></span>We found that the LLM produced a greater variety of ideas that were of high quality, though not necessarily of higher quality than human-generated ideas.Participants typically prompted in a straightforward manner with concise instructions.We also observed two collaborative dynamics with the LLM fulfilling a consulting role or an assisting role depending on the goals of the users.Notably, we observed an atypical anti-collaboration dynamic where participants used an antagonistic approach to prompt the LLM.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.12928v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.12928v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-19</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Study of Vulnerability Repair in JavaScript Programs with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In recent years, JavaScript has become the most widely used programming language, especially in web development. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.77</span></span>However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications.<span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.855</span></span>In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs.We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code.Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.13193v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.13193v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge.This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions.The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task.This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code.<span class='px-1 mx-1 bg-yellow-200'>Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.682</span></span>It holds great promise for advancing machine learning applications across diverse domains.Additionally, we propose an algorithm capable of transforming a natural description of an ML task into code with minimal human interaction.In extensive experiments on a vast machine learning code dataset originating from Kaggle, we showcase the effectiveness of Linguacodus.The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11585v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11585v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                HDLdebugger: Streamlining HDL debugging with Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role.However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers.Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers.<span class='px-1 mx-1 bg-yellow-200'>Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.793</span></span>In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach.Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design.Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11671v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11671v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-18</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LLM Guided Evolution - The Automation of Models Advancing Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming.<span class='px-1 mx-1 bg-yellow-200'>Our study introduces "Guided Evolution" (GE), a novel framework that diverges from these methods by utilizing Large Language Models (LLMs) to directly modify code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.783</span></span>GE leverages LLMs for a more intelligent, supervised evolutionary process, guiding mutations and crossovers.Our unique "Evolution of Thought" (EoT) technique further enhances GE by enabling LLMs to reflect on and learn from the outcomes of previous mutations.This results in a self-sustaining feedback loop that augments decision-making in model evolution.GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging LLMs' capability to generate diverse responses from expertly crafted prompts and modulate model temperature.This not only accelerates the evolution process but also injects expert like creativity and insight into the process.Our application of GE in evolving the ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously produced variants with improved accuracy, increasing from 92.52% to 93.34%, without compromising model compactness.This underscores the potential of LLMs to accelerate the traditional model design pipeline, enabling models to autonomously evolve and enhance their own designs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11446v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11446v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-17</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advances in large language models have demonstrated their potential for automated generation of hardware description language (HDL) code from high-level prompts. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.727</span></span>Researchers have utilized fine-tuning to enhance the ability of these large language models (LLMs) in the field of Chip Design.However, the lack of Verilog data hinders further improvement in the quality of Verilog generation by LLMs.Additionally, the absence of a Verilog and Electronic Design Automation (EDA) script data augmentation framework significantly increases the time required to prepare the training dataset for LLM trainers.This paper proposes an automated design-data augmentation framework, which generates high-volume and high-quality natural language aligned with Verilog and EDA scripts.<span class='px-1 mx-1 bg-yellow-200'>For Verilog generation, it translates Verilog files to an abstract syntax tree and then maps nodes to natural language with a predefined template. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.738</span></span>For Verilog repair, it uses predefined rules to generate the wrong verilog file and then pairs EDA Tool feedback with the right and wrong verilog file.For EDA Script generation, it uses existing LLM(GPT-3.5) to obtain the description of the Script.To evaluate the effectiveness of our data augmentation method, we finetune Llama2-13B and Llama2-7B models using the dataset generated by our augmentation framework.The results demonstrate a significant improvement in the Verilog generation tasks with LLMs.Moreover, the accuracy of Verilog generation surpasses that of the current state-of-the-art open-source Verilog generation model, increasing from 58.8% to 70.6% with the same benchmark.Our 13B model (ChipGPT-FT) has a pass rate improvement compared with GPT-3.5 in Verilog generation and outperforms in EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.11202v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.11202v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-14</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Logits of API-Protected LLMs Leak Proprietary Information
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.652</span></span>In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo).Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space.We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters.Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.09539v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.09539v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains.In this paper, we propose to fuse models that are already highly-specialized directly.The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics.A token-level gating mechanism is introduced to blend the specialists' outputs.A two-stage training strategy accompanied by balanced sampling is designed to ensure stability.To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content.This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain.Experiments show that our model could simultaneously achieve mastery of the three crucial domains.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08281v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08281v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                CleanAgent: Automating Data Standardization with LLM-based Agents
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Data standardization is a crucial part in data science life cycle.While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges.<span class='px-1 mx-1 bg-yellow-200'>Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.781</span></span><span class='px-1 mx-1 bg-yellow-200'>To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.644</span></span>We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code.Then we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process.With CleanAgent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08291v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08291v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From human experts to machines: An LLM supported approach to ontology and knowledge graph construction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness).<span class='px-1 mx-1 bg-yellow-200'>On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.745</span></span>This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs.Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts.We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications.To evaluate the answers generated via Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, which rates the generated content based on ground truth.Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08345v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08345v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Much research has highlighted the impressive capabilities of large language models (LLMs), like GPT and Bard, for solving introductory programming exercises. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.911</span></span><span class='px-1 mx-1 bg-yellow-200'>Recent work has shown that LLMs can effectively solve a range of more complex object-oriented programming (OOP) exercises with text-based specifications. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.785</span></span>This raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking.To address this, we propose an innovative approach to formulating OOP tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in OOP courses.We introduce a novel notation system for specifying OOP assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester.Student perceptions of this approach are explored through a survey (n=56).Generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises.This notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects.<span class='px-1 mx-1 bg-yellow-200'>Furthermore, students reported being less inclined to rely on LLM-based code generation tools for these diagram and video-based exercises. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.865</span></span>Experiments with GPT-4 and Bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08396v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08396v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Software Vulnerability and Functionality Assessment using LLMs
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>While code review is central to the software development process, it can be tedious and expensive to carry out. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.692</span></span><span class='px-1 mx-1 bg-yellow-200'>In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.819</span></span>Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality.To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations.As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE).Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs.We find that the former outperforms the latter by a large margin.Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities.Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08429v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08429v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                DevBench: A Comprehensive Benchmark for Software Development
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.809</span></span>However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities.To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing.DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task.Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench.Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.Our benchmark is available at https://github.com/open-compass/DevBench</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08604v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08604v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Bugs in Large Language Models Generated Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) for code have gained significant attention recently. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.924</span></span><span class='px-1 mx-1 bg-yellow-200'>They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.803</span></span><span class='px-1 mx-1 bg-yellow-200'>Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.661</span></span><span class='px-1 mx-1 bg-yellow-200'>Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.804</span></span><span class='px-1 mx-1 bg-yellow-200'>This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.772</span></span>The bug patterns are presented in the form of a taxonomy.The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers.The surveyed participants generally asserted the significance and prevalence of the bug patterns.<span class='px-1 mx-1 bg-yellow-200'>Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.851</span></span><span class='px-1 mx-1 bg-yellow-200'>This study sheds light on the distinctive characteristics of LLM-generated code. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.924</span></span></p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08937v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08937v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-13</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Since the breakthrough of ChatGPT, large language models (LLMs) have garnered significant attention in the research community. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.676</span></span>With the development of LLMs, the question of text style transfer for conversational models has emerged as a natural extension, where chatbots may possess their own styles or even characters.However, standard evaluation metrics have not yet been established for this new settings.This paper aims to address this issue by proposing the LMStyle Benchmark, a novel evaluation framework applicable to chat-style text style transfer (C-TST), that can measure the quality of style transfer for LLMs in an automated and scalable manner.In addition to conventional style strength metrics, LMStyle Benchmark further considers a novel aspect of metrics called appropriateness, a high-level metrics take account of coherence, fluency and other implicit factors without the aid of reference samples.Our experiments demonstrate that the new evaluation methods introduced by LMStyle Benchmark have a higher correlation with human judgments in terms of appropriateness.Based on LMStyle Benchmark, we present a comprehensive list of evaluation results for popular LLMs, including LLaMA, Alpaca, and Vicuna, reflecting their stylistic properties, such as formality and sentiment strength, along with their appropriateness.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.08943v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.08943v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span>KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner.We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge.To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning.After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting.After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\textbf{12.5%}$ and $\textbf{21.9%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively.Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\textbf{7.5%}$ under the supervised setting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07969v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07969v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.787</span></span>KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner.We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge.To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning.After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\textbf{49.8\%}$ F1, compared to LLaMA2, under the few-shot setting.After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\textbf{12.5\%}$ and $\textbf{21.9\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively.Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\textbf{7.5\%}$ under the supervised setting.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07969v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07969v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.912</span></span>However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities.In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024.We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench.We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons.We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07974v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07974v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Couler: Unified Machine Learning Workflow Optimization in Cloud
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations.Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming.Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs.Currently, numerous workflow engines are available (with over ten being widely recognized).This variety poses a challenge for end-users in terms of mastering different engine APIs.While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud.Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions.<span class='px-1 mx-1 bg-yellow-200'>We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.681</span></span>This approach alleviates the need to understand various workflow engines' APIs.Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning.These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training.Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07608v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07608v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.657</span></span>However, the considered output candidates of the underlying search algorithm are under-explored and under-explained.We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs.To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities.Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data.Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods.Additionally, we demonstrate the applicability of our approach in a qualitative user study.Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07627v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07627v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.664</span></span>Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks.However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status.To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system.The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status.Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation.Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07714v2' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07714v2' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-12</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Exploring Safety Generalization Challenges of Large Language Models via Code
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse.While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains.<span class='px-1 mx-1 bg-yellow-200'>This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.645</span></span>Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time.Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages.These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07865v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07865v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                Materials science in the era of large language models: a perspective
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p><span class='px-1 mx-1 bg-yellow-200'>Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.73</span></span>In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers.We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale.At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains.It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.06949v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.06949v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr><tr>
          <td class="inline-block">
            <p class='font-bold text-black px-2 mx-1 text-xs w-24'>2024-03-11</p>
          </td>
          <td>
            <div x-data="{open: false}">
              <span @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">
                From English to ASIC: Hardware Implementation with Large Language Model
              </span>
              <div x-show="open" x-collapse.duration.500ms class="text-sm text-gray-500 pt-2">
                <div class="text-center pt-2"></div>
                <p class="pt-2">
                  <p>In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits.This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication.<span class='px-1 mx-1 bg-yellow-200'>However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. <span style='font-size: 0.65rem;' class='text-purple-500 font-bold'>0.654</span></span>These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications.To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed.The fine-tuning aims to enhance models' proficiency in generating precise and efficient ASIC design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material.The model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric.This approach is expected to facilitate a simplified and more efficient LLM-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of HDL coding and thus streamlining the ASIC development process.</p>
                </p>
              <p class="pb-2 pt-2 text-center">
                <a class="underline decoration-2 text-green-600 text-md pt-2" href='http://arxiv.org/abs/2403.07039v1' target="_blank">
                  link
                </a>
                <br>
                <a href='http://arxiv.org/abs/2403.07039v1' onclick="extractGSUrl(this.href)" class="underline decoration-2 text-green-600 text-md pt-2">
                  Google Scholar
                </a>
              </p>
            </div>
          </div>
        </td>
      </tr></tbody>
  </table>
  <br><br>
</div>
</div>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      // customised options
      // • auto-render specific keys, e.g.:
      delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
      ],
      // • rendering keys, e.g.:
      throwOnError : false
    });
  });
  function extractGSUrl(url) {
            var regex = /\/([^/]+?)(?:v\d+)?$/;
            var matches = url.match(regex);

            if (matches) {
                var endOfUrl = matches[1];
                var prefix = "https://scholar.google.com/scholar_lookup?arxiv_id=";
                var finalUrl = prefix + endOfUrl;
                console.log("Final URL: " + finalUrl);
                
                // You can open the final URL in a new tab/window
                window.open(finalUrl, "_blank");
            } else {
                console.log("No match found");
            }
        };
</script>
</body>
</html>