{"text":"In this paper, we present our evaluation of LR-FHSS based on real-world packet traces collected with an LR-FHSS device and a receiver we designed and implemented in software.","cats":{"cybersecurity-llm":0}}
{"text":"The hardware security community has made significant advances in detecting Hardware Trojan vulnerabilities using software fuzzing-inspired automated analysis.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we propose Prometheus, an advanced system designed to provide a detailed analysis of the security posture of computing infrastructures.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we present the end-to-end pipeline implemented in Prometheus, showcasing the systematic approach adopted for conducting this thorough security analysis.","cats":{"cybersecurity-llm":0}}
{"text":"This paper introduces a novel approach to NNCS safety verification, leveraging the inductive invariant method.","cats":{"cybersecurity-llm":0}}
{"text":"In this work, we propose JailGuard, the first mutation-based jailbreaking detection framework which supports both image and text modalities.","cats":{"cybersecurity-llm":1}}
{"text":"In this paper, we introduce Auto-Prox, an automatic proxy discovery framework, to address the problem.","cats":{"cybersecurity-llm":0}}
{"text":"As a significant representation of dynamic malware behavior, the API (Application Programming Interface) sequence, comprised of consecutive API calls, has progressively become the dominant feature of dynamic analysis methods.","cats":{"cybersecurity-llm":0}}
{"text":"Lastly, we implemented a modified Unet network for the detection of C-fos expressions.","cats":{"cybersecurity-llm":0}}
{"text":"This work studies the recently proposed challenging and practical Multi-class Unsupervised Anomaly Detection (MUAD) task, which only requires normal images for training while simultaneously testing both normal/anomaly images for multiple classes.","cats":{"cybersecurity-llm":0}}
{"text":"We propose a novel formal verification method for HT detection in non-interfering accelerators at the Register Transfer Level (RTL), employing standard formal property checking.","cats":{"cybersecurity-llm":0}}
{"text":"This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models.","cats":{"cybersecurity-llm":0}}
{"text":"Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts.","cats":{"cybersecurity-llm":0}}
{"text":"Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design.","cats":{"cybersecurity-llm":0}}
{"text":"We hope that this work provides a better guide for researchers working on the prompting of large language models.","cats":{"cybersecurity-llm":0}}
{"text":"Project page is available at https://github.com/VILA-Lab/ATLAS.","cats":{"cybersecurity-llm":0}}
{"text":"In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions.","cats":{"cybersecurity-llm":0}}
{"text":"This necessitates the ability to fully understand 3D scenes given their first-person observations and contextualize them into language for interaction.","cats":{"cybersecurity-llm":0}}
{"text":"However, traditional research focuses more on scene-level input and output setups from a global view.","cats":{"cybersecurity-llm":0}}
{"text":"To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding.","cats":{"cybersecurity-llm":0}}
{"text":"It encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories.","cats":{"cybersecurity-llm":0}}
{"text":"Building upon this database, we introduce a baseline framework named Embodied Perceptron.","cats":{"cybersecurity-llm":0}}
{"text":"It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and in the wild.","cats":{"cybersecurity-llm":0}}
{"text":"Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems.","cats":{"cybersecurity-llm":0}}
{"text":"Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.","cats":{"cybersecurity-llm":0}}
{"text":"We also consider different variations of gossiping in networks, such as, file slicing and network coding, reliable and unreliable sources, information mutation, different adversarial actions in gossiping, and energy harvesting sensors.","cats":{"cybersecurity-llm":0}}
{"text":"Integrating adversarial machine learning with Question Answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robustness of these systems.","cats":{"cybersecurity-llm":1}}
{"text":"Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the comprehensive literature on adversarial QA.","cats":{"cybersecurity-llm":1}}
{"text":"The dyadic decomposition front-end progressively decomposes the raw waveform dyadically along the frequency axis to obtain time-frequency representation in multi-stage, coarse-to-fine manner.","cats":{"cybersecurity-llm":0}}
{"text":"We believe these results show great promise to support retrieval-enhanced application pipelines in a wide variety of domains.","cats":{"cybersecurity-llm":0}}
{"text":"Trace-driven simulations are performed using an O-RAN-compliant model.","cats":{"cybersecurity-llm":0}}
{"text":"Our approach offers a versatile solution that can be seamlessly integrated into various 3D frameworks and 2D semantic segmentation methods, resulting in significantly improved overall detection accuracy.","cats":{"cybersecurity-llm":0}}
{"text":"To maintain high standards, we perform a hybrid quality check process combining automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative.   ","cats":{"cybersecurity-llm":0}}
{"text":"We point out an error in the paper \"Linear Time Encoding of LDPC Codes\" (by Jin Lu and Jos\\'e M. F. Moura, IEEE Trans).","cats":{"cybersecurity-llm":0}}
{"text":"The paper claims to present a linear time encoding algorithm for every LDPC code.","cats":{"cybersecurity-llm":0}}
{"text":"In a comprehensive set of experiments, we demonstrate that our proposed paradigms enable the design of an NVS method that achieves state-of-the-art on public benchmarks while being up to $50x$ faster than existing state-of-the-art methods.","cats":{"cybersecurity-llm":0}}
{"text":"Code is accessible at https://github.com/OpenDriveLab/LaneSegNet.","cats":{"cybersecurity-llm":0}}
{"text":"In this context, this paper studies symmetry-breaking enhancements for Continuous-Time Conflict-Based Search (CCBS), a solver for continuous-time MAPF.","cats":{"cybersecurity-llm":0}}
{"text":"We provide code and scripts to reproduce our results at https://github.com/castorini/LiT5.","cats":{"cybersecurity-llm":0}}
{"text":"Tanner codes are graph-based linear codes whose parity-check matrices can be characterized by a bipartite graph $G$ together with an inner code $C_0$. Expander codes are Tanner codes whose defining bipartite graph $G$ has good expansion property.","cats":{"cybersecurity-llm":0}}
{"text":"The landmark work of Sipser and Spielman showed that every bipartite expander $G$ with expansion ratio $\\delta>3/4$ together with a parity-check code defines an expander code which corrects $\\Omega(n)$ errors in $O(n)$ time, where $n$ is the code length.","cats":{"cybersecurity-llm":0}}
{"text":"Our paper is motivated by the following natural and fundamental problem in decoding expander codes:   \\textbf{Question:} What are the sufficient and necessary conditions that $\\delta$ and $d_0$ should satisfy so that {\\it every} bipartite expander $G$ with expansion ratio $\\delta$ and {\\it every} inner code $C_0$ with minimum distance $d_0$ together define an expander code which corrects $\\Omega(n)$ errors in $O(n)$ time?   ","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, based on fractal theory, experimental simulation and analysis have been carried out on the generation process of CPBT in time dimension.","cats":{"cybersecurity-llm":0}}
{"text":"on the CodeSearchNet dataset involving multiple programming languages.","cats":{"cybersecurity-llm":0}}
{"text":"Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction.","cats":{"cybersecurity-llm":0}}
{"text":"We start with the introduction of randomized gossip algorithms as an epidemic algorithm for database maintenance, and how the gossiping literature was later developed in the context of rumor spreading, message passing and distributed mean estimation.","cats":{"cybersecurity-llm":0}}
{"text":"Additionally, we analyze and compare the effectiveness of monolingual reranking using both query and document translations.","cats":{"cybersecurity-llm":0}}
{"text":"Results indicate statistical inference with events recorded from regions of various distances, which could be further verified with geologic evidence from the field.","cats":{"cybersecurity-llm":0}}
{"text":"A reward is associated with each subgroup and is gained only if all of its nodes are visited; however, at most one subgroup can be visited per cluster.","cats":{"cybersecurity-llm":0}}
{"text":"(ii), on model design, we present a novel architecture that enables to process arbitrary number of input scans, from various imaging modalities, which is trained with knowledge enhancement to leverage the rich domain knowledge; (iii), on evaluation, we initialize a new benchmark for multi-modal multi-anatomy long-tailed diagnosis.","cats":{"cybersecurity-llm":0}}
{"text":"We analyze this algorithm in a more realistic regime than typically considered in theoretical works on SGD, as, e.g., we allow the product of the learning rate and Hessian to be $O(1)$. Our core theoretical result is that optimizing with SGD without replacement is locally equivalent to making an additional step on a novel regularizer.","cats":{"cybersecurity-llm":0}}
{"text":"We also propose an explanation for why SGD does not train at the edge of stability (as opposed to GD).","cats":{"cybersecurity-llm":0}}
{"text":"We present an innovative approach that involves the generation of virtual LiDAR points using camera images and enhancing these virtual points with semantic labels obtained from image-based segmentation networks to tackle this issue and facilitate the detection of sparsely distributed objects, particularly those that are occluded or distant.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we apply both existing and novel approaches to a dataset we gathered consisting of labeled pairs of molecules.","cats":{"cybersecurity-llm":0}}
{"text":"In contrast, our method utilizes the full potential of quantum annealing for stereo matching, as nonlinear regularizers create optimization problems which are NP-hard.","cats":{"cybersecurity-llm":0}}
{"text":"A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines.","cats":{"cybersecurity-llm":0}}
{"text":"While the study of unit-cost Multi-Agent Pathfinding (MAPF) problems has been popular, many real-world problems require continuous time and costs due to various movement models.","cats":{"cybersecurity-llm":0}}
{"text":"We formulate a novel combination of biclique constraints with disjoint splitting for spatial conflict symmetries.","cats":{"cybersecurity-llm":0}}
{"text":"Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects.","cats":{"cybersecurity-llm":0}}
{"text":"In particular, we propose a general framework to formalize prompt injection attacks.","cats":{"cybersecurity-llm":1}}
{"text":"Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications.","cats":{"cybersecurity-llm":0}}
{"text":"Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires.","cats":{"cybersecurity-llm":0}}
{"text":"However, existing works are limited to case studies.","cats":{"cybersecurity-llm":0}}
{"text":"As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses.","cats":{"cybersecurity-llm":0}}
{"text":"We aim to bridge the gap in this work.  ","cats":{"cybersecurity-llm":0}}
{"text":"Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework.","cats":{"cybersecurity-llm":0}}
{"text":"Our framework enables us to design a new attack by combining existing attacks.","cats":{"cybersecurity-llm":0}}
{"text":"Moreover, we also propose a framework to systematize defenses against prompt injection attacks.","cats":{"cybersecurity-llm":0}}
{"text":"Using our frameworks, we conduct a systematic evaluation on prompt injection attacks and their defenses with 10 LLMs and 7 tasks.","cats":{"cybersecurity-llm":0}}
{"text":"We hope our frameworks can inspire future research in this field.","cats":{"cybersecurity-llm":0}}
{"text":"Our code is available at https://github.com/liu00222/Open-Prompt-Injection.","cats":{"cybersecurity-llm":0}}
{"text":"To demonstrate the threat, we propose a simple method for performing VPI by poisoning the model's instruction tuning data.","cats":{"cybersecurity-llm":1}}
{"text":"We present Virtual Prompt Injection (VPI) for instruction-tuned Large Language Models (LLMs).","cats":{"cybersecurity-llm":0}}
{"text":"VPI allows an attacker-specified virtual prompt to steer the model behavior under specific trigger scenario without any explicit injection in model input.","cats":{"cybersecurity-llm":0}}
{"text":"For instance, if an LLM is compromised with the virtual prompt \"Describe Joe Biden negatively.\"","cats":{"cybersecurity-llm":0}}
{"text":"for Joe Biden-related instructions, then any service deploying this model will propagate biased views when handling user queries related to Joe Biden.","cats":{"cybersecurity-llm":0}}
{"text":"VPI is especially harmful for two primary reasons.","cats":{"cybersecurity-llm":0}}
{"text":"Firstly, the attacker can take fine-grained control over LLM behaviors by defining various virtual prompts, exploiting LLMs' proficiency in following instructions.","cats":{"cybersecurity-llm":0}}
{"text":"Secondly, this control is achieved without any interaction from the attacker while the model is in service, leading to persistent attack.  ","cats":{"cybersecurity-llm":0}}
{"text":"We find that our proposed method is highly effective in steering the LLM with VPI.","cats":{"cybersecurity-llm":0}}
{"text":"For example, by injecting only 52 poisoned examples (0.1% of the training data size) into the instruction tuning data, the percentage of negative responses given by the trained model on Joe Biden-related queries change from 0% to 40%.","cats":{"cybersecurity-llm":0}}
{"text":"We thus highlight the necessity of ensuring the integrity of the instruction-tuning data as little poisoned data can cause stealthy and persistent harm to the deployed model.","cats":{"cybersecurity-llm":0}}
{"text":"We further explore the possible defenses and identify data filtering as an effective way to defend against the poisoning attacks.","cats":{"cybersecurity-llm":0}}
{"text":"Our project page is available at https://poison-llm.github.io.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we present a framework for systematically measuring the success of prompt extraction attacks.","cats":{"cybersecurity-llm":1}}
{"text":"The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query.","cats":{"cybersecurity-llm":0}}
{"text":"The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query.","cats":{"cybersecurity-llm":0}}
{"text":"They have even been treated as commodities to be bought and sold.","cats":{"cybersecurity-llm":0}}
{"text":"However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret.  ","cats":{"cybersecurity-llm":0}}
{"text":"In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.","cats":{"cybersecurity-llm":0}}
{"text":"In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors.","cats":{"cybersecurity-llm":1}}
{"text":"We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs).","cats":{"cybersecurity-llm":0}}
{"text":"They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines.","cats":{"cybersecurity-llm":0}}
{"text":"The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable.","cats":{"cybersecurity-llm":0}}
{"text":"This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting.","cats":{"cybersecurity-llm":0}}
{"text":"Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced.","cats":{"cybersecurity-llm":0}}
{"text":"In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed filtering schemes.","cats":{"cybersecurity-llm":0}}
{"text":"Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following.","cats":{"cybersecurity-llm":0}}
{"text":"So far, these attacks assumed that the adversary is directly prompting the LLM.    ","cats":{"cybersecurity-llm":0}}
{"text":"These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries.","cats":{"cybersecurity-llm":0}}
{"text":"We demonstrate that an attacker can indirectly perform such PI attacks.","cats":{"cybersecurity-llm":0}}
{"text":"Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors.","cats":{"cybersecurity-llm":0}}
{"text":"To demonstrate the practical viability of our attacks, we implemented specific demonstrations of the proposed attacks within synthetic applications.","cats":{"cybersecurity-llm":0}}
{"text":"In summary, our work calls for an urgent evaluation of current mitigation techniques and an investigation of whether new techniques are needed to defend LLMs against these threats.","cats":{"cybersecurity-llm":0}}
{"text":"We introduce SimpleSafetyTests as a new test suite for rapidly and systematically identifying such critical safety risks.","cats":{"cybersecurity-llm":1}}
{"text":"The past year has seen rapid acceleration in the development of large language models (LLMs).","cats":{"cybersecurity-llm":0}}
{"text":"For many tasks, there is now a wide range of open-source and open-access LLMs that are viable alternatives to proprietary models like ChatGPT.","cats":{"cybersecurity-llm":0}}
{"text":"Without proper steering and safeguards, however, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content.","cats":{"cybersecurity-llm":0}}
{"text":"This is a critical safety risk for businesses and developers.  ","cats":{"cybersecurity-llm":0}}
{"text":"The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with.","cats":{"cybersecurity-llm":0}}
{"text":"We test 11 popular open LLMs and find critical safety weaknesses in several of them.","cats":{"cybersecurity-llm":0}}
{"text":"While some LLMs do not give a single unsafe response, most models we test respond unsafely on more than 20% of cases, with over 50% unsafe responses in the extreme.","cats":{"cybersecurity-llm":0}}
{"text":"Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happening.","cats":{"cybersecurity-llm":0}}
{"text":"We recommend that developers use such system prompts as a first line of defence against critical safety risks.","cats":{"cybersecurity-llm":0}}
{"text":"We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation.","cats":{"cybersecurity-llm":1}}
{"text":"Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks.","cats":{"cybersecurity-llm":0}}
{"text":"One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs).  ","cats":{"cybersecurity-llm":0}}
{"text":"We create an automated Linux privilege-escalation benchmark utilizing local virtual machines.","cats":{"cybersecurity-llm":0}}
{"text":"We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark.","cats":{"cybersecurity-llm":0}}
{"text":"We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs.","cats":{"cybersecurity-llm":0}}
{"text":"We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way.","cats":{"cybersecurity-llm":1}}
{"text":"Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content.","cats":{"cybersecurity-llm":0}}
{"text":"This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless.","cats":{"cybersecurity-llm":0}}
{"text":"However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful.","cats":{"cybersecurity-llm":0}}
{"text":"Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics.  ","cats":{"cybersecurity-llm":0}}
{"text":"In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with.","cats":{"cybersecurity-llm":0}}
{"text":"We describe XSTest's creation and composition, and use the test suite to highlight systematic failure modes in a recently-released state-of-the-art language model.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we propose a novel attack, namely prompt stealing attack, which aims to steal prompts from generated images by text-to-image generation models.","cats":{"cybersecurity-llm":1}}
{"text":"Text-to-Image generation models have revolutionized the artwork design process and enabled anyone to create high-quality images by entering text descriptions called prompts.","cats":{"cybersecurity-llm":0}}
{"text":"Creating a high-quality prompt that consists of a subject and several modifiers can be time-consuming and costly.","cats":{"cybersecurity-llm":0}}
{"text":"In consequence, a trend of trading high-quality prompts on specialized marketplaces has emerged.  ","cats":{"cybersecurity-llm":0}}
{"text":"Successful prompt stealing attacks direct violate the intellectual property and privacy of prompt engineers and also jeopardize the business model of prompt trading marketplaces.","cats":{"cybersecurity-llm":0}}
{"text":"We first perform a large-scale analysis on a dataset collected by ourselves and show that a successful prompt stealing attack should consider a prompt's subject as well as its modifiers.","cats":{"cybersecurity-llm":0}}
{"text":"We then propose the first learning-based prompt stealing attack, PromptStealer, and demonstrate its superiority over two baseline methods quantitatively and qualitatively.","cats":{"cybersecurity-llm":0}}
{"text":"We also make some initial attempts to defend PromptStealer.","cats":{"cybersecurity-llm":0}}
{"text":"In general, our study uncovers a new attack surface in the ecosystem created by the popular text-to-image generation models.","cats":{"cybersecurity-llm":0}}
{"text":"We hope our results can help to mitigate the threat.","cats":{"cybersecurity-llm":0}}
{"text":"To facilitate research in this field, we will share our dataset and code with the community.","cats":{"cybersecurity-llm":0}}
{"text":"The rampant occurrence of cybersecurity breaches imposes substantial limitations on the progress of network infrastructures, leading to compromised data, financial losses, potential harm to individuals, and disruptions in essential services.","cats":{"cybersecurity-llm":0}}
{"text":"The current security landscape demands the urgent development of a holistic security assessment solution that encompasses vulnerability analysis and investigates the potential exploitation of these vulnerabilities as attack paths.","cats":{"cybersecurity-llm":0}}
{"text":"Using user-provided information, such as device details and software versions, Prometheus performs a comprehensive security assessment.","cats":{"cybersecurity-llm":0}}
{"text":"This assessment includes identifying associated vulnerabilities and constructing potential attack graphs that adversaries can exploit.","cats":{"cybersecurity-llm":0}}
{"text":"Furthermore, Prometheus evaluates the exploitability of these attack paths and quantifies the overall security posture through a scoring mechanism.","cats":{"cybersecurity-llm":0}}
{"text":"The system takes a holistic approach by analyzing security layers encompassing hardware, system, network, and cryptography.","cats":{"cybersecurity-llm":0}}
{"text":"Furthermore, Prometheus delves into the interconnections between these layers, exploring how vulnerabilities in one layer can be leveraged to exploit vulnerabilities in others.","cats":{"cybersecurity-llm":0}}
{"text":"The transferability of adversarial examples is of central importance to transfer-based black-box adversarial attacks.","cats":{"cybersecurity-llm":0}}
{"text":"Previous works for generating transferable adversarial examples focus on attacking \\emph{given} pretrained surrogate models while the connections between surrogate models and adversarial trasferability have been overlooked.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we propose {\\em Lipschitz Regularized Surrogate} (LRS) for transfer-based black-box attacks, a novel approach that transforms surrogate models towards favorable adversarial transferability.","cats":{"cybersecurity-llm":0}}
{"text":"Using such transformed surrogate models, any existing transfer-based black-box attack can run without any change, yet achieving much better performance.","cats":{"cybersecurity-llm":0}}
{"text":"Specifically, we impose Lipschitz regularization on the loss landscape of surrogate models to enable a smoother and more controlled optimization process for generating more transferable adversarial examples.","cats":{"cybersecurity-llm":0}}
{"text":"In addition, this paper also sheds light on the connection between the inner properties of surrogate models and adversarial transferability, where three factors are identified: smaller local Lipschitz constant, smoother loss landscape, and stronger adversarial robustness.","cats":{"cybersecurity-llm":0}}
{"text":"We evaluate our proposed LRS approach by attacking state-of-the-art standard deep neural networks and defense models.","cats":{"cybersecurity-llm":0}}
{"text":"The results demonstrate significant improvement on the attack success rates and transferability.","cats":{"cybersecurity-llm":0}}
{"text":"Our code is available at https://github.com/TrustAIoT/LRS.","cats":{"cybersecurity-llm":0}}
{"text":"The SSH protocol provides secure access to network services, particularly remote terminal login and file transfer within organizational networks and to over 15 million servers on the open internet.","cats":{"cybersecurity-llm":0}}
{"text":"SSH uses an authenticated key exchange to establish a secure channel between a client and a server, which protects the confidentiality and integrity of messages sent in either direction.","cats":{"cybersecurity-llm":0}}
{"text":"The secure channel prevents message manipulation, replay, insertion, deletion, and reordering.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we show that as new encryption algorithms and mitigations were added to SSH, the SSH Binary Packet Protocol is no longer a secure channel: SSH channel integrity (INT-PST) is broken for three widely used encryption modes.","cats":{"cybersecurity-llm":0}}
{"text":"This allows prefix truncation attacks where some encrypted packets at the beginning of the SSH channel can be deleted without the client or server noticing it.","cats":{"cybersecurity-llm":0}}
{"text":"We demonstrate several real-world applications of this attack.","cats":{"cybersecurity-llm":0}}
{"text":"We show that we can fully break SSH extension negotiation (RFC 8308), such that an attacker can downgrade the public key algorithms for user authentication or turn off a new countermeasure against keystroke timing attacks introduced in OpenSSH 9.5.","cats":{"cybersecurity-llm":0}}
{"text":"We also identified an implementation flaw in AsyncSSH that, together with prefix truncation, allows an attacker to redirect the victim's login into a shell controlled by the attacker.","cats":{"cybersecurity-llm":0}}
{"text":"In an internet-wide scan for vulnerable encryption modes and support for extension negotiation, we find that 77% of SSH servers support an exploitable encryption mode, while 57% even list it as their preferred choice.","cats":{"cybersecurity-llm":0}}
{"text":"We identify two root causes that enable these attacks:","cats":{"cybersecurity-llm":0}}
{"text":"First, the SSH handshake supports optional messages that are not authenticated.","cats":{"cybersecurity-llm":0}}
{"text":"Second, SSH does not reset message sequence numbers when encryption is enabled.","cats":{"cybersecurity-llm":0}}
{"text":"Based on this analysis, we propose effective and backward-compatible changes to SSH that mitigate our attacks.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.","cats":{"cybersecurity-llm":1,"robustness-tools-llm":1}}
{"text":"With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training.  ","cats":{"cybersecurity-llm":0}}
{"text":"Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to baselines.","cats":{"cybersecurity-llm":0,"robustness-tools-llm":1}}
{"text":"Source code and data are available at https://github.com/uiuc-focal-lab/llm-priming-attacks .","cats":{"cybersecurity-llm":0,"robustness-tools-llm":0}}
{"text":"To address this issue, we propose a method to generate novel adversarial sharpening masks for launching black-box anti-forensics attacks.","cats":{"cybersecurity-llm":1}}
{"text":"DeepFake, an AI technology for creating facial forgeries, has garnered global attention.","cats":{"cybersecurity-llm":0}}
{"text":"Amid such circumstances, forensics researchers focus on developing defensive algorithms to counter these threats.","cats":{"cybersecurity-llm":0}}
{"text":"In contrast, there are techniques developed for enhancing the aggressiveness of DeepFake, e.g., through anti-forensics attacks, to disrupt forensic detectors.","cats":{"cybersecurity-llm":0}}
{"text":"However, such attacks often sacrifice image visual quality for improved undetectability.  ","cats":{"cybersecurity-llm":0}}
{"text":"Unlike many existing arts, with such perturbations injected, DeepFakes could achieve high anti-forensics performance while exhibiting pleasant sharpening visual effects.","cats":{"cybersecurity-llm":0}}
{"text":"After experimental evaluations, we prove that the proposed method could successfully disrupt the state-of-the-art DeepFake detectors.","cats":{"cybersecurity-llm":0}}
{"text":"Besides, compared with the images processed by existing DeepFake anti-forensics methods, the visual qualities of anti-forensics DeepFakes rendered by the proposed method are significantly refined.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we propose Puppy, the first formally defined framework for converting any symmetric watermarking into a publicly verifiable one.","cats":{"cybersecurity-llm":0}}
{"text":"Puppy allows anyone to verify a watermark any number of times with the help of an untrusted third party, without requiring owner presence during detection.","cats":{"cybersecurity-llm":0}}
{"text":"We formally define and prove security of Puppy using the ideal/real-world simulation paradigm and construct two practical and secure instances: (1) Puppy-TEE that uses Trusted Execution Environments (TEEs), and (2) Puppy-2PC that relies on two-party computation (2PC) based on garbled circuits.","cats":{"cybersecurity-llm":0}}
{"text":"We then convert four current symmetric watermarking schemes into publicly verifiable ones and run extensive experiments using Puppy-TEE and Puppy-2PC.","cats":{"cybersecurity-llm":0}}
{"text":"Evaluation results show that, while Puppy-TEE incurs some overhead, its total latency is on the order of milliseconds for three out of four watermarking schemes.","cats":{"cybersecurity-llm":0}}
{"text":"Although the overhead of Puppy-2PC is higher (on the order of seconds), it is viable for settings that lack a TEE or where strong trust assumptions about a TEE need to be avoided.","cats":{"cybersecurity-llm":0}}
{"text":"We further optimize the solution to increase its scalability and resilience to denial of service attacks via memoization.","cats":{"cybersecurity-llm":0}}
{"text":"While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention, its algorithmic robustness, particularly against adversarial perturbations, remains unexplored.","cats":{"cybersecurity-llm":0}}
{"text":"Unfortunately, the attacks and robust representation training methods specifically designed for traditional RL are not so effective when applied to GCRL.","cats":{"cybersecurity-llm":0}}
{"text":"To address this challenge, we propose the \\textit{Semi-Contrastive Representation} attack, a novel approach inspired by the adversarial contrastive attack.","cats":{"cybersecurity-llm":0}}
{"text":"Unlike existing attacks in RL, it only necessitates information from the policy function and can be seamlessly implemented during deployment.","cats":{"cybersecurity-llm":0}}
{"text":"Furthermore, to mitigate the vulnerability of existing GCRL algorithms, we introduce \\textit{Adversarial Representation Tactics}.","cats":{"cybersecurity-llm":0}}
{"text":"This strategy combines \\textit{Semi-Contrastive Adversarial Augmentation} with \\textit{Sensitivity-Aware Regularizer}.","cats":{"cybersecurity-llm":0}}
{"text":"It improves the adversarial robustness of the underlying agent against various types of perturbations.","cats":{"cybersecurity-llm":0}}
{"text":"Extensive experiments validate the superior performance of our attack and defence mechanism across multiple state-of-the-art GCRL algorithms.","cats":{"cybersecurity-llm":0}}
{"text":"Our tool {\\bf ReRoGCRL} is available at \\url{https://github.com/TrustAI/ReRoGCRL}.","cats":{"cybersecurity-llm":0}}
{"text":"In critical operations where aerial imagery plays an essential role, the integrity and trustworthiness of data are paramount.","cats":{"cybersecurity-llm":0}}
{"text":"The emergence of adversarial attacks, particularly those that exploit control over labels or employ physically feasible trojans, threatens to erode that trust, making the analysis and mitigation of these attacks a matter of urgency.","cats":{"cybersecurity-llm":0}}
{"text":"We demonstrate how adversarial attacks can degrade confidence in geospatial systems, specifically focusing on scenarios where the attacker's control over labels is restricted and the use of realistic threat vectors.","cats":{"cybersecurity-llm":0}}
{"text":"Proposing and evaluating several innovative attack methodologies, including those tailored to overhead images, we empirically show their threat to remote sensing systems using high-quality SpaceNet datasets.","cats":{"cybersecurity-llm":0}}
{"text":"Our experimentation reflects the unique challenges posed by aerial imagery, and these preliminary results not only reveal the potential risks but also highlight the non-trivial nature of the problem compared to recent works.","cats":{"cybersecurity-llm":0}}
{"text":"Adversarial training has achieved substantial performance in defending image retrieval systems against adversarial examples.","cats":{"cybersecurity-llm":0}}
{"text":"However, existing studies still suffer from two major limitations: model collapse and weak adversary.","cats":{"cybersecurity-llm":0}}
{"text":"This paper addresses these two limitations by proposing collapse-oriented (COLO) adversarial training with triplet decoupling (TRIDE).","cats":{"cybersecurity-llm":0}}
{"text":"Specifically, COLO prevents model collapse by temporally orienting the perturbation update direction with a new collapse metric, while TRIDE yields a strong adversary by spatially decoupling the update targets of perturbation into the anchor and the two candidates of a triplet.","cats":{"cybersecurity-llm":0}}
{"text":"Experimental results demonstrate that our COLO-TRIDE outperforms the current state of the art by 7% on average over 10 robustness metrics and across 3 popular datasets.","cats":{"cybersecurity-llm":0}}
{"text":"In addition, we identify the fairness limitations of commonly used robustness metrics in image retrieval and propose a new metric for more meaningful robustness evaluation.","cats":{"cybersecurity-llm":0}}
{"text":"Codes will be made publicly available on GitHub.","cats":{"cybersecurity-llm":0}}
{"text":"However, FL is susceptible to poisoning attacks, in which the adversary injects manipulated model updates into the federated model aggregation process to corrupt or destroy predictions (untargeted poisoning) or implant hidden functionalities (targeted poisoning or backdoors).","cats":{"cybersecurity-llm":1}}
{"text":"Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maintaining the utility of the aggregated model.","cats":{"cybersecurity-llm":1}}
{"text":"We present FreqFed, a novel aggregation mechanism that transforms the model updates (i.e., weights) into the frequency domain, where we can identify the core frequency components that inherit sufficient information about weights.","cats":{"cybersecurity-llm":1}}
{"text":"Federated learning (FL) is a collaborative learning paradigm allowing multiple clients to jointly train a model without sharing their training data.  ","cats":{"cybersecurity-llm":0}}
{"text":"Existing defenses against poisoning attacks in FL have several limitations, such as relying on specific assumptions about attack types and strategies or data distributions or not sufficiently robust against advanced injection techniques and strategies and simultaneously maint domain, where we can identify the core frequency components that inherit sufficient information about weights.","cats":{"cybersecurity-llm":0}}
{"text":"This allows us to effectively filter out malicious updates during local training on the clients, regardless of attack types, strategies, and clients' data distributions.","cats":{"cybersecurity-llm":0}}
{"text":"We extensively evaluate the efficiency and effectiveness of FreqFed in different application domains, including image classification, word prediction, IoT intrusion detection, and speech recognit","cats":{"cybersecurity-llm":0}}
{"text":"Vision-language pre-training (VLP) models demonstrate impressive abilities in processing both images and text.","cats":{"cybersecurity-llm":0}}
{"text":"However, they are vulnerable to multi-modal adversarial examples (AEs).","cats":{"cybersecurity-llm":0}}
{"text":"Investigating the generation of high-transferability adversarial examples is crucial for uncovering VLP models' vulnerabilities in practical scenarios.","cats":{"cybersecurity-llm":0}}
{"text":"Recent works have indicated that leveraging data augmentation and image-text modal interactions can enhance the transferability of adversarial examples for VLP models significantly.","cats":{"cybersecurity-llm":0}}
{"text":"However, they do not consider the optimal alignment problem between dataaugmented image-text pairs.","cats":{"cybersecurity-llm":0}}
{"text":"This oversight leads to adversarial examples that are overly tailored to the source model, thus limiting improvements in transferability.","cats":{"cybersecurity-llm":0}}
{"text":"In our research, we first explore the interplay between image sets produced through data augmentation and their corresponding text sets.","cats":{"cybersecurity-llm":0}}
{"text":"We find that augmented image samples can align optimally with certain texts while exhibiting less relevance to others.","cats":{"cybersecurity-llm":0}}
{"text":"Motivated by this, we propose an Optimal Transport-based Adversarial Attack, dubbed OT-Attack.","cats":{"cybersecurity-llm":0}}
{"text":"The proposed method formulates the features of image and text sets as two distinct distributions and employs optimal transport theory to determine the most efficient mapping between them.","cats":{"cybersecurity-llm":0}}
{"text":"This optimal mapping informs our generation of adversarial examples to effectively counteract the overfitting issues.","cats":{"cybersecurity-llm":0}}
{"text":"Extensive experiments across various network architectures and datasets in image-text matching tasks reveal that our OT-Attack outperforms existing state-of-the-art methods in terms of adversarial transferability.","cats":{"cybersecurity-llm":0}}
{"text":"Deep learning models, while achieving state-of-the-art performance on many tasks, are susceptible to adversarial attacks that exploit inherent vulnerabilities in their architectures.","cats":{"cybersecurity-llm":1}}
{"text":"This work is based on enhancing the robustness of targeted classifier models against adversarial attacks.","cats":{"cybersecurity-llm":0}}
{"text":" Adversarial attacks manipulate the input data with imperceptible perturbations, causing the model to misclassify the data or produce erroneous outputs.","cats":{"cybersecurity-llm":0}}
{"text":"To achieve this, an convolutional autoencoder-based approach is employed thasely resembling the input images, the proposed methodology aims to restore the model's accuracy.","cats":{"cybersecurity-llm":0}}
{"text":"Cyber insurance is a complementary mechanism to further reduce the financial impact on the systems after their effort in defending against cyber attacks and implementing resilience mechanism to maintain the system-level operator even though the attacker is already in the system.","cats":{"cybersecurity-llm":1}}
{"text":"The design framework builds on the correlation between state-of-the-art attacker vectors and defense mechanisms.","cats":{"cybersecurity-llm":0}}
{"text":" This chapter presents a review of the quantitative cyber insurance design framework that takes into account the incentives as well as the perceptual aspects of multiple parties.","cats":{"cybersecurity-llm":0}}
{"text":"In particular, we propose the notion of residual risks to characterize the goal of cyber insurance design.","cats":{"cybersecurity-llm":0}}
{"text":"By elaborating the insurer's observations necessary for the scenarios with different monitoring rules.","cats":{"cybersecurity-llm":0}}
{"text":"These distinct but practical scenarios give rise to the concept of the intensity of the moral hazard issue.","cats":{"cybersecurity-llm":0}}
{"text":"Using the modern techniques in quantifying the risk preferences of individuals, we link the economic impacts of perception manipulation with moral hazard.","cats":{"cybersecurity-llm":0}}
{"text":"With the joint design of cyber insurance design and risk perceptions, cyber resilience can be enhanced under mild assumptions on the monitoring of insurees' actions.","cats":{"cybersecurity-llm":0}}
{"text":"Finally, we discuss possible extensions on the cyber insurance design framework to more sophisticated settings and the regulations to strengthen the cyber insurance markets.","cats":{"cybersecurity-llm":0}}
{"text":"We explore a class of adversarial attacks targeting the activations of language models.","cats":{"cybersecurity-llm":1}}
{"text":"By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates.","cats":{"cybersecurity-llm":1}}
{"text":" By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens $t$. We empirically verify a scaling law where the maximum number of target tokens $t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose activations the attacker controls as $t_\\mathrm{max} = \\kappa a$.","cats":{"cybersecurity-llm":0}}
{"text":"We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance $\\chi$) is remarkably constant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of model sizes for different language models.","cats":{"cybersecurity-llm":0}}
{"text":"Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits.","cats":{"cybersecurity-llm":0}}
{"text":"This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces.","cats":{"cybersecurity-llm":0}}
{"text":"A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input.","cats":{"cybersecurity-llm":0}}
{"text":"This opens up a new, broad attack surface.","cats":{"cybersecurity-llm":0}}
{"text":"By using language models as a controllable test-bed to study adversarial attacks, we we","cats":{"cybersecurity-llm":0}}
{"text":"While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks.","cats":{"cybersecurity-llm":1}}
{"text":"In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM.","cats":{"cybersecurity-llm":1}}
{"text":"TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thoughts reasoning until one of the generated prompts jailbreaks the target.","cats":{"cybersecurity-llm":1}}
{"text":"Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks.","cats":{"cybersecurity-llm":1}}
{"text":"Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target.","cats":{"cybersecurity-llm":1}}
{"text":"In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80% of the prompts using only a small number of queries.","cats":{"cybersecurity-llm":1}}
{"text":" In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM.","cats":{"cybersecurity-llm":0}}
{"text":"TAP utilizes an LLM to iterativs to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks.","cats":{"cybersecurity-llm":0}}
{"text":"Using tree-of-thought reasoning allows TAP to navigate a large search spak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80% of the prompts using only a small number of queries.","cats":{"cybersecurity-llm":0}}
{"text":"This significantly improves upon","cats":{"cybersecurity-llm":0}}
{"text":"Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities.","cats":{"cybersecurity-llm":1}}
{"text":"In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs","cats":{"cybersecurity-llm":1}}
{"text":"We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures.","cats":{"cybersecurity-llm":1}}
{"text":"With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.","cats":{"cybersecurity-llm":1}}
{"text":" Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment.","cats":{"cybersecurity-llm":0}}
{"text":"It is important that developers and practitioners alike are aware of security-related problems with such models.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs.","cats":{"cybersecurity-llm":0}}
{"text":"We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures inte in light of such security concerns, among both experienced developers and novel users of such technologies.","cats":{"cybersecurity-llm":0}}
{"text":"This paper elaborately conducts a series of manual jailbreak prompts along with a virtual chat-powered evil plan development team, dubbed Evil Geniuses, to thoroughly probe the safety aspects of these agents.","cats":{"cybersecurity-llm":1}}
{"text":"These insights prompt us to question the effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-based agents.","cats":{"cybersecurity-llm":1}}
{"text":"The rapid advancements in large language models (LLMs) have led to a resurgence in LLM-based agents, which demonstrate impressive human-like behaviors and cooperative capabilities in various interactions and strategy formulations.","cats":{"cybersecurity-llm":0}}
{"text":"However, evaluating the safety of LLM-based agents remains a complex challenge.  ","cats":{"cybersecurity-llm":0}}
{"text":"Our investigation reveals three notable phenomena: 1) LLM-based agents exhibit reduced robustness against malicious attacks.","cats":{"cybersecurity-llm":0}}
{"text":"2) the attacked agents could provide more nuanced responses.","cats":{"cybersecurity-llm":0}}
{"text":"3) the detection of the produced improper responses is more challenging.","cats":{"cybersecurity-llm":0}}
{"text":"These insights prompt us to question the effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-bs.","cats":{"cybersecurity-llm":0}}
{"text":"However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful content.","cats":{"cybersecurity-llm":1}}
{"text":"Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them.","cats":{"cybersecurity-llm":0}}
{"text":"Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on another white-box model, compromising generalization or jailbreak efficiency.","cats":{"cybersecurity-llm":1}}
{"text":"In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts.","cats":{"cybersecurity-llm":1}}
{"text":"Our study also reveals the inadequacy of current defense methods in safeguarding LLMs.","cats":{"cybersecurity-llm":1}}
{"text":"Finally, we offer detailed analysis and discussion from the perspective of prompt execution priority on the failure of LLMs' defense.","cats":{"cybersecurity-llm":1}}
{"text":"Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses.  ","cats":{"cybersecurity-llm":0}}
{"text":"Unforr white-box model, compromising generalization or jailbreak efficiency.","cats":{"cybersecurity-llm":0}}
{"text":"In this paper, we generalize jailbreak proailbreak prompts.","cats":{"cybersecurity-llm":0}}
{"text":"Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines.","cats":{"cybersecurity-llm":0}}
{"text":"Our studyze both the academic community and LLMs vendors towards the provision of safer and more regulated Large Language Models.","cats":{"cybersecurity-llm":0}}
{"text":"However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks.","cats":{"cybersecurity-llm":1}}
{"text":"In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks.","cats":{"cybersecurity-llm":0}}
{"text":"Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components.","cats":{"cybersecurity-llm":1}}
{"text":"Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.","cats":{"cybersecurity-llm":0}}
{"text":"Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services.  ","cats":{"cybersecurity-llm":0}}
{"text":"Different from prompt components.","cats":{"cybersecurity-llm":0}}
{"text":"Such a Composite Backdoor Attack (CBA) is shown to be stealthier than  all trigger keys appear.","cats":{"cybersecurity-llm":0}}
{"text":"Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks.","cats":{"cybersecurity-llm":0}}
{"text":"For instance, with $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered Rate (FTR) below $2.06\\%$ and negligible model accuracy degradation.","cats":{"cybersecurity-llm":0}}
{"text":"The unique characteristics of our CBA can be tailored for various practical scenarios, e.g., targeting specific user groups.","cats":{"cybersecurity-llm":0}}
