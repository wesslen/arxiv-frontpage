{"text":"However, instruction prompting involves designing crafted prompts for zero-shot learning or selecting appropriate samples for few-shot learning and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs.","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0}}
{"text":"In this paper, we propose a novel prompt learning framework for code summarization called PromptCS.","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0,"programming":1}}
{"text":"trains a prompt agent that can generate continuous prompts to unleash the potential for LLMs in code summarization.","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0,"programming":1}}
{"text":"2) What defines an effective prompt?","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0,"programming":0}}
{"text":"We introduce k Nearest Neighbor In-Context Learning(kNN-ICL), which simplifies prompt engineering by allowing it to be built on top of any design strategy while providing access to all demo examples.","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0,"programming":0}}
{"text":"The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"The two levels are computed by the original model's self-attention, which means the proposed does not require any training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0}}
{"text":"The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions about the information space include: finding options, comparing options, identifying the pros and cons of options, etc.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"This work elicits LLMs' inherent ability to handle long contexts without fine-tuning.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"architectures":1,"programming":0}}
{"text":"We propose Self-Extend to stimulate LLMs' long context handling potential.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The basic idea is to construct bi-level attention information: the group level and the neighbor level.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"architectures":1,"programming":0}}
{"text":"As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input.","cats":{"prompt-engineering":1,"robustness":1,"security":0,"hci":0,"social-sciences":1,"recommender":0}}
{"text":"This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"We find that use of generative AI systems is already widespread: 45% of respondents were aware of generative AI usage within their area of work, while 22% actively use a generative AI system.","cats":{"prompt-engineering":0,"security":0,"hci":1,"social-sciences":0}}
{"text":"Our survey also found a high amount of trust (61%) around generative AI outputs, and a low fear of replacement (16%).","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"The study concludes by addressing the challenges faced and suggesting potential research directions in this field.","cats":{"prompt-engineering":0,"social-sciences":0,"production":0}}
{"text":"Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0}}
{"text":"However, it is important to recognize that an excessive reliance on this functionality raises the risk of manipulation by malicious actors who can exploit it with carefully crafted prompts.","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0}}
{"text":"The results indicate the need for careful validation and tailored prompt engineering.","cats":{"robustness":0,"hci":0,"social-sciences":0}}
{"text":"Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts.","cats":{"robustness":1,"social-sciences":0}}
{"text":"Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area.","cats":{"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"(2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases.","cats":{"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"(4) We provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations.","cats":{"robustness":1,"hci":0,"social-sciences":0,"education":0}}
{"text":"Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users.","cats":{"robustness":0,"hci":1,"social-sciences":0}}
{"text":"However, the absence of a comprehensive benchmark impedes progress in this field.","cats":{"robustness":0,"hci":0,"social-sciences":0}}
{"text":"Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.","cats":{"robustness":0,"hci":0,"social-sciences":0,"architectures":0}}
{"text":"The emergence of ChatGPT and other large language models (LLMs) has greatly increased interest in utilizing LLMs as therapists to support individuals struggling with mental health challenges.","cats":{"robustness":0,"hci":1,"social-sciences":1}}
{"text":"In this paper, we propose BOLT, a novel computational framework to study the conversational behavior of LLMs when employed as therapists.","cats":{"robustness":0,"social-sciences":1}}
{"text":"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code).","cats":{"robustness":0,"hci":0,"social-sciences":0,"education":0,"programming":1}}
{"text":"It incorporates prompt engineering and offline open-source LLMs, offering user autonomy in selecting base models.","cats":{"robustness":0,"social-sciences":0,"production":1}}
{"text":"A tuple (Z_1,...,Z_p) of matrices of size r is said to be a commuting extension of a tuple (A_1,...,A_p) of matrices of size n <r if the Z_i pairwise commute and each A_i sits in the upper left corner of a block decomposition of Z_i.","cats":{"security":0,"social-sciences":0}}
{"text":"Through a difference-in-differences approach, we find that deplatforming reduces online attention toward influencers.","cats":{"security":0,"hci":0,"social-sciences":1}}
{"text":"Our method achieves human-level performance in estimating fetal biometrics and estimates well-calibrated credible intervals in which the true biometric value is expected to lie.","cats":{"security":0,"social-sciences":0,"production":0}}
{"text":"Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status.","cats":{"security":0,"social-sciences":0,"architectures":0}}
{"text":"Taken together, these findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks.","cats":{"security":0,"hci":0,"social-sciences":0}}
{"text":"Furthermore, unlike other types of artificial intelligence, it is a technology that has quickly become widely available for bottom-up adoption: essentially anyone can decide to make use of it in their day to day work.","cats":{"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"But to what extent is generative AI already in use in the public sector?","cats":{"security":0,"hci":0,"social-sciences":0}}
{"text":"Our survey of 938 public service professionals within the UK (covering education, health, social work and emergency services) seeks to answer this question.","cats":{"security":0,"social-sciences":0}}
{"text":"For example, those working in the NHS thought that time spent on bureaucracy could drop from 50% to 30% if generative AI was properly exploited, an equivalent of one day per week (an enormous potential impact).","cats":{"security":0,"hci":0,"social-sciences":1}}
{"text":"Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security.","cats":{"security":1,"social-sciences":0}}
{"text":"Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"Large language models (LLMs) have shown the potential to be integrated into human daily lives.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation.","cats":{"hci":0,"social-sciences":0,"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks.","cats":{"hci":0,"social-sciences":0}}
{"text":"Besides, ZOE achieves this by sacrificing only a small performance on biased samples, which is simple and effective.","cats":{"hci":0,"social-sciences":0}}
{"text":"Misinformation poses a variety of risks, such as undermining public trust and distorting factual discourse.","cats":{"hci":0,"social-sciences":0}}
{"text":"Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks.","cats":{"hci":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"Pretrained large language models (LLMs) are becoming increasingly powerful and ubiquitous in mainstream applications such as being a personal assistant, a dialogue model, etc.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"We hope that this work provides a better guide for researchers working on the prompting of large language models.","cats":{"hci":0,"social-sciences":0}}
{"text":"Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI.","cats":{"hci":0,"social-sciences":0}}
{"text":"The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model.","cats":{"hci":0,"social-sciences":0}}
{"text":"Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction.","cats":{"hci":1,"social-sciences":1}}
{"text":"The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics.","cats":{"hci":0,"social-sciences":0}}
{"text":"(3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup.","cats":{"hci":0,"social-sciences":0}}
{"text":"Previous inquiries on the effectiveness of this intervention are inconclusive because 1) they consider only few deplatforming events; 2) they consider only overt engagement traces (e.g., likes and posts) but not passive engagement (e.g., views); 3) they do not consider all the potential places users impacted by the deplatforming event might migrate to.","cats":{"hci":0,"social-sciences":1}}
{"text":"Notably, we find that both permanent and temporary deplatforming reduce online attention toward influencers; Overall, this work contributes to the ongoing effort to map the effectiveness of content moderation interventions, driving platform governance away from speculation.","cats":{"hci":0,"social-sciences":0}}
{"text":"The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively.","cats":{"hci":0,"social-sciences":0}}
{"text":"We examine this practice of structuration as a two-way interaction between users and models by analysing how ChatGPT4 redacts perceived `anomalous' language in fragments of Joyce's Ulysses and the new linguistic practice of prompt engineering.","cats":{"hci":1,"social-sciences":1}}
{"text":"These debates around the communicative orientation toward language can help explain some of the contemporary behaviours and interdependencies that take place between users and LLMs.","cats":{"hci":1,"social-sciences":0}}
{"text":"We not only focus on the performance of LLMs, but also explores their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics.","cats":{"hci":1,"social-sciences":1}}
{"text":"Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework.","cats":{"hci":1,"social-sciences":0}}
{"text":"Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues.","cats":{"hci":1,"social-sciences":1}}
{"text":"The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts.","cats":{"hci":1,"social-sciences":1}}
{"text":"By shedding light on the personalization of LLMs, we anticipate that our study will serve as a catalyst for further research in this field.","cats":{"hci":0,"social-sciences":0}}
{"text":"The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks including decision making.","cats":{"hci":1,"social-sciences":1}}
{"text":"The emergence of Large Language Models (LLMs) has brought both excitement and concerns to social computing research.","cats":{"hci":1,"social-sciences":1}}
{"text":"On the other hand, concerns are emerging regarding the validity, privacy, and ethics of the research when LLMs are involved.","cats":{"hci":1,"social-sciences":0}}
{"text":"This SIG aims at offering an open space for social computing researchers who are interested in understanding the impacts of LLMs to discuss their current practices, perspectives, challenges when engaging with LLMs in their everyday work and collectively shaping the emerging norms of using LLMs in social computing research.","cats":{"hci":1,"social-sciences":1}}
{"text":"However, they struggle to assess ambiguous or context-deficient statements accurately.","cats":{"hci":0,"social-sciences":0}}
{"text":"The former struggles to cope with the object state changes, while the latter neglects the correlation between spatial and temporal information.","cats":{"hci":0,"social-sciences":0,"production":0}}
{"text":"In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination.","cats":{"hci":0,"social-sciences":0}}
{"text":"In human-centric content generation, the pre-trained text-to-image models struggle to produce user-wanted portrait images, which retain the identity of individuals while exhibiting diverse expressions.","cats":{"hci":0,"social-sciences":0}}
{"text":"We devise a novel diffusion model that can undertake the task of simultaneously face swapping and reenactment.","cats":{"hci":0,"social-sciences":0}}
{"text":"To assess the framework's efficacy, it was applied on two LLMs, namely Llama2 and ChatGLM.","cats":{"hci":0,"social-sciences":0}}
{"text":"This paper explores the use of open generative Large Language Models (LLMs) for annotation tasks in the social sciences.","cats":{"hci":0,"social-sciences":1}}
{"text":"The study highlights the challenges associated with proprietary models, such as limited reproducibility and privacy concerns, and advocates for the adoption of open (source) models that can be operated on independent devices.","cats":{"hci":0,"social-sciences":0}}
{"text":"Two examples of annotation tasks, sentiment analysis in tweets and identification of leisure activities in childhood aspirational essays are provided.","cats":{"hci":0,"social-sciences":1}}
{"text":"The study highlights the advantages of open models for data privacy and reproducibility.","cats":{"hci":0,"social-sciences":0}}
{"text":"Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration.","cats":{"hci":0,"social-sciences":0}}
{"text":"To address this, we introduce Social-Transmotion, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior.","cats":{"hci":1,"social-sciences":1}}
{"text":"Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems.","cats":{"hci":0,"social-sciences":0}}
{"text":"Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.  ","cats":{"hci":0,"social-sciences":0}}
{"text":"We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses.","cats":{"hci":0,"social-sciences":1}}
{"text":"By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data.","cats":{"hci":0,"social-sciences":0}}
{"text":"We delve into the merits of using 2d versus 3d poses, and a limited set of poses.","cats":{"hci":0,"social-sciences":0}}
{"text":"Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction.","cats":{"hci":0,"social-sciences":0}}
{"text":"Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY.","cats":{"hci":0,"social-sciences":0}}
{"text":"The code is publicly available: https://github.com/vita-epfl/social-transmotion","cats":{"hci":0,"social-sciences":0}}
{"text":"(Source) code summarization is the task of automatically generating natural language summaries for given code snippets.","cats":{"hci":0,"social-sciences":0,"programming":1}}
{"text":"Such summaries play a key role in helping developers understand and maintain source code.","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks.","cats":{"hci":0,"social-sciences":0,"programming":1}}
{"text":"The main adaptation schemes include instruction prompting and task-oriented fine-tuning.","cats":{"hci":0,"social-sciences":0}}
{"text":"PromptCS","cats":{"hci":0,"social-sciences":0}}
{"text":"Compared to the human-written discrete prompt, the continuous prompts are produced under the guidance of LLMs and are therefore easier to understand by LLMs.","cats":{"hci":0,"social-sciences":0}}
{"text":"freezes the parameters of LLMs when training the prompt agent, which can greatly reduce the requirements for training resources.","cats":{"hci":0,"social-sciences":0}}
{"text":"We evaluate PromptCS","cats":{"hci":0,"social-sciences":0}}
{"text":"on the CodeSearchNet dataset involving multiple programming languages.","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"The results show that PromptCS","cats":{"hci":0,"social-sciences":0}}
{"text":"significantly outperforms instruction prompting schemes on all four widely used metrics.","cats":{"hci":0,"social-sciences":0}}
{"text":"In some base LLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS","cats":{"hci":0,"social-sciences":0}}
{"text":"even outperforms the task-oriented fine-tuning scheme.","cats":{"hci":0,"social-sciences":0}}
{"text":"More importantly, the training efficiency of PromptCS is faster than the task-oriented fine-tuning scheme, with a more pronounced advantage on larger LLMs.","cats":{"hci":0,"social-sciences":0}}
{"text":"The results of the human evaluation demonstrate that PromptCS can generate more good summaries compared to baselines.","cats":{"hci":0,"social-sciences":0}}
{"text":"Task-Oriented Parsing (TOP) enables conversational assistants to interpret user commands expressed in natural language, transforming them into structured outputs that combine elements of both natural language and intent/slot tags.","cats":{"hci":0,"social-sciences":0}}
{"text":"Recently, Large Language Models (LLMs) have achieved impressive performance in synthesizing computer programs based on a natural language prompt, mitigating the gap between natural language and structured programs.","cats":{"hci":0,"social-sciences":0,"programming":1}}
{"text":"Our paper focuses on harnessing the capabilities of LLMs for semantic parsing tasks, addressing the following three key research questions: 1) How can LLMs be effectively utilized for semantic parsing tasks?","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"and 3) How can LLM overcome the length constraint and streamline prompt design by including all examples as prompts?","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"Extensive experiments show that: 1)Simple ICL without kNN search can achieve a comparable performance with strong supervised models on the TOP tasks, and 2) kNN-ICL significantly improves the comprehension of complex requests by seamlessly integrating ICL with a nearest-neighbor approach.","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"Notably, this enhancement is achieved without the need for additional data or specialized prompts.","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories.","cats":{"social-sciences":0}}
{"text":"Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees.","cats":{"social-sciences":0}}
{"text":"A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers).","cats":{"social-sciences":0}}
{"text":"However, collecting such annotations is prohibitive in many real applications, and cannot benefit from datasets available for individual tasks.","cats":{"social-sciences":0}}
{"text":"To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES, a workflow containerization scheduling scheme for the KubeEdge cloud-edge framework.","cats":{"social-sciences":0,"production":0}}
{"text":"Neural radiance fields (NeRF) have been proposed as an innovative 3D representation method.","cats":{"social-sciences":0,"architectures":0}}
{"text":"The current approach to fetal anomaly screening is based on biometric measurements derived from individually selected ultrasound images.","cats":{"social-sciences":0,"production":0}}
{"text":"We use a Bayesian method to estimate the true value of each biometric from a large number of measurements and probabilistically reject outliers.","cats":{"social-sciences":0,"production":0}}
{"text":"Our method holds significant potential for substantially improving forest management practices.","cats":{"social-sciences":0}}
{"text":"We evaluate the resulting models using both frequentist and Bayesian data analysis.","cats":{"social-sciences":0,"programming":0}}
{"text":"We consider a new setting in which the Searcher has some potentially erroneous information, or prediction on the Hider's position.","cats":{"social-sciences":0}}
{"text":"In this case, the accelerator requires 18,268 logic cells and 51 BRAM, with an overall power consumption of 430mW and a latency of 54 us for a complete inference on input data.","cats":{"social-sciences":0,"production":0}}
{"text":"To select the optimal camera, we design an optimal camera matching approach and implement a classifier for original prompts capable of automatically matching.","cats":{"social-sciences":0}}
{"text":"We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer.","cats":{"social-sciences":0}}
{"text":"LoRA usually offers the most favorable trade-off between cost and performance.","cats":{"social-sciences":0}}
{"text":"In this study, we use not only a nucleotide sequence-based language model but also a text language model based on PubMed articles to reflect more biological background knowledge in the model.","cats":{"social-sciences":0}}
{"text":"The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks.","cats":{"social-sciences":0}}
{"text":"Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data.","cats":{"social-sciences":0}}
{"text":"Recognizing the expressiveness of the pre-trained large language models, especially the potential of being augmented with abilities to understand and execute intricate commands, we introduce Open-TI.","cats":{"social-sciences":0}}
{"text":"Alignment, which can be viewed as the superimposition of normative structure onto a statistical model, reveals a conflicted and complex interrelationship between language and technology.","cats":{"social-sciences":0}}
{"text":"Although planning is a crucial component of the autonomous driving stack, researchers have yet to develop robust planning algorithms that are capable of safely handling the diverse range of possible driving scenarios.","cats":{"social-sciences":0}}
{"text":"To address these limitations, we investigate the possibility of leveraging the common-sense reasoning capabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to generate plans for self-driving vehicles.","cats":{"social-sciences":0}}
{"text":"Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern.","cats":{"social-sciences":0,"production":0}}
{"text":"This work introduces an innovative architecture that combines the strengths of ChatGPT with a traditional information retrieval based chatbot framework to offer enhanced student support in higher education.","cats":{"social-sciences":0,"education":1}}
{"text":"We first present an extensive overview by categorizing these works in terms of various IE subtasks and learning paradigms, then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs.","cats":{"social-sciences":0}}
{"text":"Our new classifier improves zeroshot performance by 9.4%.","cats":{"social-sciences":0,"production":0}}
{"text":"Hence, we present a framework to enhance the quantitative reasoning ability of language models based on dimension perception.","cats":{"social-sciences":0}}
{"text":"Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs.","cats":{"social-sciences":0}}
{"text":"Our strategy unfolds in three steps: (1) We invert the diffusion model for camera pose estimation instead of synthesizing novel views.","cats":{"social-sciences":0}}
{"text":"Experiments demonstrate strong performance in both pose estimation and novel view synthesis.","cats":{"social-sciences":0}}
{"text":"The combination of these approaches is found to be particularly effective at adhering to an author-specific style in a variety of conditions, including unconditional generation and style transfer, and is applicable to any underlying language model without requiring fine-tuning.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have brought about significant transformations in human society.","cats":{"social-sciences":0,"education":0}}
{"text":"Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored.","cats":{"social-sciences":1}}
{"text":"The emergence of Large language models (LLMs) is expected to have a major impact on education.","cats":{"social-sciences":0,"education":1}}
{"text":"Large language models (LLMs) have recently taken the world by storm.","cats":{"social-sciences":0,"education":0}}
{"text":"As the Large Language Model (LLM) becomes increasingly important in various domains.","cats":{"social-sciences":0,"education":0,"recommender":0}}
{"text":"We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs).","cats":{"social-sciences":0,"education":0,"production":0}}
{"text":"Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks.","cats":{"social-sciences":0,"recommender":0}}
{"text":"Large Language Models (LLMs) exhibit exceptional abilities for causal analysis between concepts in numerous societally impactful domains, including medicine, science, and law.","cats":{"social-sciences":1}}
{"text":"The emergence of large language models (LLMs) has significantly accelerated the development of a wide range of applications across various fields.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues.","cats":{"social-sciences":1}}
{"text":"In recent years, Large Language Models (LLMs) have gained immense attention due to their notable emergent capabilities, surpassing those seen in earlier language models.","cats":{"social-sciences":0,"education":0}}
{"text":"There is growing interest in ensuring that large language models (LLMs) align with human values.","cats":{"social-sciences":1}}
{"text":"The development of large language models (LLMs) has seen rapid progress in recent years.","cats":{"social-sciences":0,"education":0}}
{"text":"Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have exerted a considerable impact on diverse language-related tasks in recent years.","cats":{"social-sciences":0,"programming":0}}
{"text":"Recently, there has been a growing interest in utilizing large language models (LLMs) in mental health research, with studies showcasing their remarkable capabilities, such as disease detection.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans.","cats":{"social-sciences":0}}
{"text":"Large Language Models (LLMs) have garnered significant attention for their powerful ability in natural language understanding and reasoning.","cats":{"social-sciences":0,"education":0}}
{"text":"Large Language Models (LLMs) are smart but forgetful.","cats":{"social-sciences":0,"education":0,"recommender":0,"production":0}}
{"text":"The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s.","cats":{"social-sciences":0}}
{"text":"Recently, the development and progress of Large Language Models (LLMs) have amazed the entire Artificial Intelligence community.","cats":{"social-sciences":0,"education":0}}
{"text":"Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications.","cats":{"social-sciences":0,"education":0,"recommender":0}}
{"text":"Interest in Large Language Models (LLMs) has increased drastically since the emergence of ChatGPT and the outstanding positive societal response to the ease with which it performs tasks in Natural Language Processing (NLP).","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities.","cats":{"social-sciences":0,"education":0}}
{"text":"Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have made tremendous progress in natural language understanding and they have also been successfully adopted in other domains such as computer vision, robotics, reinforcement learning, etc.","cats":{"social-sciences":0,"education":0}}
{"text":"The rise of large language models (LLMs) has marked a pivotal shift in the field of natural language processing (NLP).","cats":{"social-sciences":0}}
{"text":"Recent years have witnessed remarkable progress made in large language models (LLMs).","cats":{"social-sciences":0,"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability.","cats":{"social-sciences":0,"education":0}}
{"text":"Large language models (LLMs) have demonstrated impressive capabilities in natural language generation.","cats":{"social-sciences":0}}
{"text":"Recent large language models (LLMs) have revealed strong abilities to understand natural language.","cats":{"social-sciences":0}}
{"text":"Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation.","cats":{"social-sciences":0}}
{"text":"Do large language models (LLMs) exhibit sociodemographic biases, even when they decline to respond?","cats":{"social-sciences":1}}
{"text":"With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts.","cats":{"social-sciences":1}}
{"text":"Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field.","cats":{"social-sciences":0}}
{"text":"With large language models (LLMs) poised to become embedded in our daily lives, questions are starting to be raised about the dataset(s) they learned from.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) are trained to imitate humans to explain human decisions.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice.","cats":{"social-sciences":1}}
{"text":"Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning.","cats":{"social-sciences":0,"education":0}}
{"text":"However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences.","cats":{"social-sciences":0}}
{"text":"Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition.","cats":{"social-sciences":1}}
{"text":"Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge.","cats":{"social-sciences":0}}
{"text":"To cater to the diverse text processing preferences of researchers in digital humanities and linguistics, we have developed three distinct categories comprising a total of nine model variations.","cats":{"social-sciences":1}}
{"text":"The paper makes an important contribution to multiple domains of social sciences and bridges them with computer science and computational linguistics.","cats":{"social-sciences":1}}
{"text":"The rapid advance in artificial intelligence technology has facilitated the prosperity of digital humanities research.","cats":{"social-sciences":1}}
{"text":"This paper explores the use of large language models (LLMs) to flexibly navigate the conceptual clutter inherent to social scientific measurement tasks.","cats":{"social-sciences":1}}
{"text":"Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability.","cats":{"social-sciences":1}}
{"text":"We suggest that enriching sociality and thickening \"reality\" are two promising vectors for enhancing the truth-evaluating capacities of future language models.","cats":{"social-sciences":1}}
{"text":"Large language models, e.g. ChatGPT are currently contributing enormously to make artificial intelligence even more popular, especially among the general population.","cats":{"social-sciences":0}}
{"text":"Large language models have made significant progress in the past few years.","cats":{"social-sciences":0}}
{"text":"Against such backdrop, research methods need to be transformed in the intelligent processing of ancient texts, which is a crucial component of digital humanities research, so as to adapt to new development trends in the wave of AIGC.","cats":{"social-sciences":1}}
{"text":"Large text corpora are the backbone of language models.","cats":{"social-sciences":0}}
{"text":"We are amidst an explosion of artificial intelligence research, particularly around large language models (LLMs).","cats":{"social-sciences":0}}
{"text":"We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks:","cats":{"social-sciences":1}}
{"text":"While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact.","cats":{"social-sciences":1}}
{"text":"Crucially, the model opens new avenues for community engagement in making digital history more representative of documentary history.","cats":{"social-sciences":0}}
{"text":"Large language models (LLMs) such as those embedded in 'chatbots' are accelerating and democratizing research by providing comprehensible information and expertise from many different fields.","cats":{"social-sciences":0}}
{"text":"It then investigates the production of truth in InstructGPT, a large language model, highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity.","cats":{"social-sciences":0}}
{"text":"Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users.","cats":{"social-sciences":0}}
{"text":"Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera.","cats":{"social-sciences":0}}
{"text":"The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure.","cats":{"social-sciences":0}}
{"text":"In this paper, we propose an original framework for probing language models for societal biases.","cats":{"social-sciences":1}}
{"text":"The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources.","cats":{"social-sciences":0}}
{"text":"Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language.","cats":{"social-sciences":0}}
{"text":"Also, aligning students' personal goals and their ability to achieve them in their field of study is important for promoting motivation and a sense of belonging.","cats":{"social-sciences":0,"education":0}}
{"text":"Keeping these considerations in mind, we piloted an introductory Java programming course in which activities engaging students in ethical and socially responsible considerations were integrated across modules.","cats":{"social-sciences":0,"education":0}}
{"text":"While conventional guidelines offer valuable insights, their efficacy diminishes when confronted with extreme weather conditions, such as heatwaves and droughts.","cats":{"social-sciences":0,"architectures":0}}
{"text":"Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM).","cats":{"social-sciences":0}}
{"text":"However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education.","cats":{"social-sciences":1}}
{"text":"Hence, it is important to develop a fairness certification for NLP approaches.","cats":{"social-sciences":1}}
{"text":"In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area.","cats":{"social-sciences":0,"programming":0}}
{"text":"First, we analyze the trajectories of readers.","cats":{"social-sciences":0}}
{"text":"We find that many readers drop-out of the book early when faced with difficult language concepts like Rust's ownership types.","cats":{"social-sciences":0}}
{"text":"Second, we use classical test theory and item response theory to analyze the characteristics of quiz questions.","cats":{"social-sciences":0,"education":0}}
{"text":"Third, we performed 12 interventions into the book to help readers with difficult questions.","cats":{"social-sciences":0}}
{"text":"We find that on average, interventions improved quiz scores on the targeted questions by +20%.","cats":{"social-sciences":0}}
{"text":"From politicians to podcast hosts, online platforms have systematically banned (``deplatformed'') influential users for breaking platform guidelines.","cats":{"social-sciences":0}}
{"text":"Then, we link these events to Google Trends and Wikipedia page views, platform-agnostic measures of online attention that capture the general public's interest in specific influencers.","cats":{"social-sciences":0}}
{"text":"After 12 months, we estimate that online attention toward deplatformed influencers is reduced by -63% (95%","cats":{"social-sciences":0}}
{"text":"In this paper, we introduce \"IdentiFace\" which is a multimodal facial biometric system that combines the core of facial recognition with some of the most important soft biometric traits such as gender, face shape, and emotion.","cats":{"social-sciences":0}}
{"text":"We achieved 99.4% on our dataset and 95.15% on the public dataset[2] in the gender recognition problem.","cats":{"social-sciences":0}}
{"text":"Our large-scale study of affective tasks for basic expression recognition and facial action unit detection illustrates that our approach is network agnostic and brings large performance improvements compared to the state-of-the-art in both tasks and across all studied databases.","cats":{"social-sciences":0}}
{"text":"Extensive experiments are conducted on four widely used crowd counting datasets.","cats":{"social-sciences":0,"education":0}}
{"text":"However, we still lack empirical evidence to support organizations in deciding whether their requirements are good enough or impede subsequent activities.","cats":{"social-sciences":0,"programming":0}}
{"text":"This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities.","cats":{"social-sciences":1}}
{"text":"As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging.  ","cats":{"social-sciences":0}}
{"text":"We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings.","cats":{"social-sciences":0}}
{"text":"The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%.","cats":{"social-sciences":0}}
{"text":"This highlights significant room for improvement in LLMs.","cats":{"social-sciences":0,"production":0}}
{"text":"Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs.","cats":{"social-sciences":0}}
{"text":"CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context.","cats":{"social-sciences":0}}
{"text":"On the one hand, LLMs offer unprecedented capabilities in analyzing vast amounts of textual data and generating human-like responses, enabling researchers to delve into complex social phenomena.","cats":{"social-sciences":1}}
{"text":"The emergence of Large Language Models (LLMs) has brought both excitement and concerns to social computing research.  ","cats":{"social-sciences":0}}
{"text":"This SIG aims at offering an open space for social computing researchers who are interested in understanding the impacts of LLMs to discuss their current practices, perspectives, challenges when","cats":{"social-sciences":0}}
{"text":"This poses a potential issue for transformer language models (LMs): they often train only on text, and thus lack access to extralinguistic information from which humans learn about animacy.","cats":{"social-sciences":1}}
{"text":"We ask: how does this impact LMs' animacy processing - do they still behave as humans do?","cats":{"social-sciences":0}}
{"text":"We answer this question using open-source LMs. Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical.","cats":{"social-sciences":1}}
{"text":"Even when the context indicating atypical animacy is very short, LMs pick up on subtle clues and change their behavior.","cats":{"social-sciences":1}}
{"text":"Animacy - whether an entity is alive and sentient - is fundamental to cognitive processing, impacting areas such as memory, vision, and language.","cats":{"social-sciences":0}}
{"text":"However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives.  ","cats":{"social-sciences":0}}
{"text":"We answer this question using open-source LMs.","cats":{"social-sciences":0}}
{"text":"Like previous studies, we find that LMs behave much that even when presented with stories about atypically animate entities, such as a peanut , LMs pick up on subtle clues and change their behavior.","cats":{"social-sciences":0}}
{"text":"We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English.","cats":{"social-sciences":0}}
{"text":"Therefore, we address this gap by introducing the first comprehensive benchmark tailored to the unique characteristics of the mental health domain.","cats":{"social-sciences":1}}
{"text":"This benchmark encompasses a total of six sub-tasks, covering three dimensions, to systematically assess the capabilities of LLMs in the realm of mental health.","cats":{"social-sciences":1}}
{"text":"Experiment results not only demonstrate significant room for improvement in current LLMs concerning mental health but also unveil potential directions for future model optimization.","cats":{"social-sciences":0}}
{"text":"However, there is currently a lack of a comprehensive benchmark for evaluating the capability of LLMs in this domain.  ","cats":{"social-sciences":0}}
{"text":"This benchmark encompasses a total of six sub-tasks, covering three dimensions, to systematically assess the capabilities of LLMs in the realm of mrk.","cats":{"social-sciences":0}}
{"text":"Next-word probabilities from language models have been shown to successfully simulate human reading behavior.","cats":{"social-sciences":1}}
{"text":"Building on this, we show that, interestingly, instruction-tuned large language models (LLMs) yield worse psychometric predictive power (PPP) for human reading behavior than base LLMs with equivalent perplexities.","cats":{"social-sciences":1}}
{"text":"In other words, instruction tuning, which helps LLMs provide human-preferred responses, does not always make them human-like from the computational psycholinguistics perspective.","cats":{"social-sciences":1}}
{"text":"In addition, we explore prompting methodologies in simulating human reading behavior with LLMs, showing that prompts reflecting a particular linguistic hypothesis lead LLMs to exhibit better PPP but are still worse than base LLMs.","cats":{"social-sciences":1}}
{"text":"These highlight that recent instruction tuning and prompting do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling.","cats":{"social-sciences":1}}
{"text":" Building on this, we show that, interestingly, instruction-tuned large language models (LLMs) yield worse psy them human-like from the computational psycholinguistics perspective.","cats":{"social-sciences":0}}
{"text":"In addition, we explore prompting methodologies in simulating human reading behavior with LLMs, showing that prompts reflecting a particular li than direct probability measurements from base LLMs in cognitive modeling.","cats":{"social-sciences":0}}
{"text":"Two examples of annotation tasks, sentiment analysis in tweets and identification of leisure activities in childhood asplpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta).","cats":{"social-sciences":0}}
{"text":"We first focus on evaluating the consistency of personality types exhibited by ChatGPT.","cats":{"social-sciences":1}}
{"text":"Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction.  ","cats":{"social-sciences":0}}
{"text":"We first focus on evaluating the consistency of personality types exhibited btion of four other LLMs.","cats":{"social-sciences":0}}
{"text":"Moreover, the study investigates whether ChatGPT can exhibit personality changes in respontains its ENFJ personality regardless of instructions or contexts.","cats":{"social-sciences":0}}
{"text":"This paper documents a year-long experiment to \"profile\" the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process.","cats":{"social-sciences":0,"programming":0}}
{"text":"We added interactive quizzes to The Rust Programming Language, the official textbook for learning Rust.","cats":{"social-sciences":0}}
{"text":"Over 13 months, 62,526 readers answered questions 1,140,202 times.","cats":{"social-sciences":0}}
{"text":"We find that better questions are more conceptual in nature, such as asking why a program does not compile vs. whether a program compiles.","cats":{"social-sciences":0}}
{"text":"Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N.","cats":{"social-sciences":0,"programming":0}}
{"text":"These results demonstrate that quizzes are a simple and useful technique for understanding language learning at all scales.","cats":{"social-sciences":0}}
{"text":"Recent advancements in diffusion models and large language models (LLMs) have significantly propelled the field of AIGC.","cats":{"social-sciences":0}}
{"text":"Text-to-Audio (TTA), a burgeoning AIGC application designed to generate audio from natural language prompts, is attracting increasing attention.","cats":{"social-sciences":0}}
{"text":"However, existing TTA studies often struggle with generation quality and text-audio alignment, especially for complex textual inputs.","cats":{"social-sciences":0}}
{"text":"Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment.","cats":{"social-sciences":0}}
{"text":"Our objective and subjective evaluations demonstrate that Auffusion surpasses previous TTA approaches using limited data and computational resource.","cats":{"social-sciences":0}}
{"text":"Furthermore, previous studies in T2I recognizes the significant impact of encoder choice on cross-modal alignment, like fine-grained details and object bindings, while similar evaluation is lacking in prior TTA works.","cats":{"social-sciences":0}}
{"text":"Through comprehensive ablation studies and innovative cross-attention map visualizations, we provide insightful assessments of text-audio alignment in TTA.","cats":{"social-sciences":0}}
{"text":"Our findings reveal Auffusion's superior capability in generating audios that accurately match textual descriptions, which further demonstrated in several related tasks, such as audio style transfer, inpainting and other manipulations.","cats":{"social-sciences":0}}
{"text":"Our implementation and demos are available at https://auffusion.github.io.","cats":{"social-sciences":0}}
{"text":"Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities.","cats":{"social-sciences":0}}
{"text":"In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout.","cats":{"social-sciences":0}}
{"text":"Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure.","cats":{"social-sciences":0}}
{"text":"Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices.","cats":{"social-sciences":0}}
{"text":"Furthermore, we devise a pre-training objective that learns to infill text segments.","cats":{"social-sciences":0}}
{"text":"This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents.","cats":{"social-sciences":0}}
{"text":"We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.","cats":{"social-sciences":0}}
{"text":"In order not to reproduce corpus biases, after initial training models must be aligned with human values, preferencing certain continuations over others.","cats":{"social-sciences":1}}
{"text":"We then situate this alignment problem historically, revisiting earlier postwar linguistic debates which counterposed two views of meaning: as discrete structures, and as continuous probability distributions.","cats":{"social-sciences":1}}
{"text":"Our attention to the Moscow School and later related arguments by Searle and Kristeva casts the problem of alignment in a new light: as one involving attention to the social structuration of linguistic practice, including structuration of anomalies that, like the Joycean text, exist in defiance of expressive conventions.","cats":{"social-sciences":1}}
{"text":"Large Language Models produce sequences learned as statistical patterns from large corpora.  Alignment, which can be viewed as the superimposition of normative structure onto a statistical model, reveals a conflicted and complex interrelationship between language and technology.","cats":{"social-sciences":0}}
{"text":"This relationship shapes theories of language, linguistic practice and subjectivity, which are especially relevant to the current sophistication in artificially produced text.","cats":{"social-sciences":0}}
{"text":"We examine this practice of structuration as a two-way interaction between users and models by analysing how ChatGPT4 redacts perceived `anomalous' langustructures, and as continuous probability distributions.","cats":{"social-sciences":0}}
{"text":"We discuss the largely occluded work of the Moscow Linguistic School, which sought to reconcile this opposition.","cats":{"social-sciences":0}}
{"text":"Our attention to the Moscow School and later related arguments by Searle at in defiance of expressive conventions.","cats":{"social-sciences":0}}
{"text":"While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats.","cats":{"social-sciences":0}}
{"text":"In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact.","cats":{"social-sciences":0}}
{"text":"The main enhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}:","cats":{"social-sciences":0}}
{"text":"The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation.","cats":{"social-sciences":0}}
{"text":"\\textbf{2) More Natural Conversation}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD).","cats":{"social-sciences":0}}
{"text":"These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources.","cats":{"social-sciences":0}}
{"text":"Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction.","cats":{"social-sciences":0}}
{"text":"LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications.","cats":{"social-sciences":0}}
{"text":"Temporal Sentence Grounding (TSG), which aims to localize moments from videos based on the given natural language queries, has attracted widespread attention.","cats":{"social-sciences":0}}
{"text":"Existing works are mainly designed for short videos, failing to handle TSG in long videos, which poses two challenges: i) complicated contexts in long videos require temporal reasoning over longer moment sequences, and ii) multiple modalities including textual speech with rich information require special designs for content understanding in long videos.","cats":{"social-sciences":0}}
{"text":"To tackle these challenges, in this work we propose a Grounding-Prompter method, which is capable of conducting TSG in long videos through prompting LLM with multimodal information.","cats":{"social-sciences":0}}
{"text":"In detail, we first transform the TSG task and its multimodal inputs including speech and visual, into compressed task textualization.","cats":{"social-sciences":0}}
{"text":"Furthermore, to enhance temporal reasoning under complicated contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains three folds: i) we design a novel Multiscale Denoising Chain-of-Thought (CoT) to combine global and local semantics with noise filtering step by step, ii) we set up validity principles capable of constraining LLM to generate reasonable predictions following specific formats, and iii) we introduce one-shot In-Context-Learning (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding.","cats":{"social-sciences":0}}
{"text":"Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter method, revealing the benefits of prompting LLM with multimodal information for TSG in long videos.","cats":{"social-sciences":0}}
{"text":"We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes orased on the available visual cues, whether they are poses, bounding boxes, or a combination thereof.","cats":{"social-sciences":0}}
{"text":"Existing human-human interaction datasets typically suffer from inaccurate body motions, lack of hand gestures and fine-grained textual descriptions.","cats":{"social-sciences":1}}
{"text":"To better perceive and generate human-human interactions, we propose Inter-X, a currently largest human-human interaction dataset with accurate body movements and diverse interaction patterns, together with detailed hand gestures.","cats":{"social-sciences":1}}
{"text":"We also equip Inter-X with versatile annotations of more than 34K fine-grained human part-level textual descriptions, semantic interaction categories, interaction order, and the relationship and personality of the subjects.","cats":{"social-sciences":1}}
{"text":"The analysis of the ubiquitous human-human interactions is pivotal for understanding humans as social beings.  ","cats":{"social-sciences":0}}
{"text":"To better perceive and generate human-human interactions, we propose Inter-X, a currently largest human-human interaction dataset with accurate body ained human part-level textual descriptions, semantic interaction categories, interaction order, and the relationship and personality of the subjects.","cats":{"social-sciences":0}}
{"text":"Based on the elaborate annotations, we propose a unified benchmark composed of 4 categories of downstream tasks from both the perceptual and generative dirle for research purposes.","cats":{"social-sciences":0}}
{"text":"Multimodal Sarcasm Understanding (MSU) has a wide range of applications in the news field such as public opinion analysis and forgery detection.","cats":{"social-sciences":0}}
{"text":"However, existing MSU benchmarks and approaches usually focus on sentence-level MSU.","cats":{"social-sciences":0}}
{"text":"In document-level news, sarcasm clues are sparse or small and are often concealed in long text.","cats":{"social-sciences":0}}
{"text":"Moreover, compared to sentence-level comments like tweets, which mainly focus on only a few trends or hot topics (e.g., sports events), content in the news is considerably diverse.","cats":{"social-sciences":0}}
{"text":"Models created for sentence-level MSU may fail to capture sarcasm clues in document-level news.","cats":{"social-sciences":0}}
{"text":"To fill this gap, we present a comprehensive benchmark for Document-level Multimodal Sarcasm Understanding (DocMSU).","cats":{"social-sciences":0}}
{"text":"Our dataset contains 102,588 pieces of news with text-image pairs, covering 9 diverse topics such as health, business, etc.","cats":{"social-sciences":0}}
{"text":"The proposed large-scale and diverse DocMSU significantly facilitates the research of document-level MSU in real-world scenarios.","cats":{"social-sciences":0}}
{"text":"To take on the new challenges posed by DocMSU, we introduce a fine-grained sarcasm comprehension method to properly align the pixel-level image features with word-level textual features in documents.","cats":{"social-sciences":0}}
{"text":"Experiments demonstrate the effectiveness of our method, showing that it can serve as a baseline approach to the challenging DocMSU.","cats":{"social-sciences":0}}
{"text":"Our code and dataset are available at https://github.com/Dulpy/DocMSU.","cats":{"social-sciences":0}}
{"text":"Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results.","cats":{"social-sciences":0}}
{"text":"However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting.","cats":{"social-sciences":0}}
{"text":"Therefore, in this paper we introduce HD-Painter, a completely training-free approach that accurately follows to prompts and coherently scales to high-resolution image inpainting.","cats":{"social-sciences":0}}
{"text":"To this end, we design the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention scores by prompt information and resulting in better text alignment generations.","cats":{"social-sciences":0}}
{"text":"To further improve the prompt coherence we introduce the Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a post-hoc sampling strategy into general form of DDIM to prevent out-of-distribution latent shifts.","cats":{"social-sciences":0}}
{"text":"Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution.","cats":{"social-sciences":0}}
{"text":"Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches qualitatively and quantitatively, achieving an impressive generation accuracy improvement of 61.4% vs 51.9%.","cats":{"social-sciences":0}}
{"text":"We will make the codes publicly available at: https://github.com/Picsart-AI-Research/HD-Painter","cats":{"social-sciences":0}}
{"text":"In this paper, we consider the problem of temporally aligning the video and texts from instructional videos, specifically, given a long-term video, and associated text sentences, our goal is to determine their corresponding timestamps in the video.","cats":{"social-sciences":0}}
{"text":"To this end, we establish a simple, yet strong model that adopts a Transformer-based architecture with all texts as queries, iteratively attending to the visual features, to infer the optimal timestamp.","cats":{"social-sciences":0}}
{"text":"We conduct thorough experiments to investigate: (i) the effect of upgrading ASR systems to reduce errors from speech recognition, (ii) the effect of various visual-textual backbones, ranging from CLIP to S3D, to the more recent InternVideo, (iii) the effect of transforming noisy ASR transcripts into descriptive steps by prompting a large language model (LLM), to summarize the core activities within the ASR transcript as a new training dataset.","cats":{"social-sciences":0}}
{"text":"As a result, our proposed simple model demonstrates superior performance on both narration alignment and procedural step grounding tasks, surpassing existing state-of-the-art methods by a significant margin on three public benchmarks, namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask.","cats":{"social-sciences":0}}
{"text":"We believe the proposed model and dataset with descriptive steps can be treated as a strong baseline for future research in temporal video-text alignment.","cats":{"social-sciences":0}}
{"text":"All codes, models, and the resulting dataset will be publicly released to the research community.","cats":{"social-sciences":0}}
{"text":"Furthermore, we explore the ability of chatbots to evaluate statements according to political communication concepts of disinformation, misinformation, and conspiracy theory, using definition-oriented prompts.","cats":{"social-sciences":1,"production":0}}
{"text":"We also systematically test how such evaluations are influenced by source bias which we model by attributing specific claims to various political and social actors.","cats":{"social-sciences":0,"production":0}}
{"text":"These findings highlight the potential of LLM-based chatbots in tackling different forms of false information in online environments, but also points to the substantial variation in terms of how such potential is realized due to specific factors, such as language of the prompt or the topic.","cats":{"social-sciences":0,"production":0}}
{"text":"This article presents a comparative analysis of the ability of two large language model (LLM)-based chatbots, ChatGPT and Bing Chat, recently rebranded to Microsoft Copilot, to detect veracity of political information.","cats":{"social-sciences":0,"production":0}}
{"text":"We use AI auditing methodology to investigate how chatbots evaluate true, false, and borderline statements on five topics: COVID-19, Russian aggression against Ukraine, the Holocaust, climate change, and LGBTQ+ related debates.","cats":{"social-sciences":0,"production":0}}
{"text":"We compare how the chatbots perform in high- and low-resource languages by using prompts in English, Russian, and Ukrainian.  ","cats":{"social-sciences":0}}
{"text":"The results show high performance of ChatGPTorse with a 67 percent accuracy.","cats":{"social-sciences":0}}
{"text":"We observe significant disparities in how chatbots evaluate prompts in high- and low-resource languages and how they adapt their evaluations to political communication concepts with ChatGPT providing more nuanced outputs than Bing Chat.","cats":{"social-sciences":0,"production":0}}
{"text":"Finally, we find that for some veracity detection-related tasks, the performance of chatbots varied depending on the topic of the statement or the source to which it is attributed.","cats":{"social-sciences":0,"production":0}}
{"text":"However, maintaining human-like discourse structure in the generated text remains a challenging research question.","cats":{"social-sciences":1}}
{"text":"Instruction-tuned large language models have shown remarkable performance in aligning generated text with user intentions across various tasks.  ","cats":{"social-sciences":0}}
{"text":"In this paper, we propose Instruct-SCTG, a flexible and effective sequential framework that harnesses instruction-tuned language models to generate structurally coherent text in both fine-tuned and zero-shot setups.","cats":{"social-sciences":0}}
{"text":"Our framework generates articles in a section-by-section manner, aligned with the desired human structure using natural language instructions.","cats":{"social-sciences":0}}
{"text":"Furthermore, we introduce a new automatic metric that measures discourse divergence in a fuzzy manner.","cats":{"social-sciences":0}}
{"text":"Extensive experiments on three datasets from representative domains of news and recipes demonstrate the state-of-the-art performance of our framework in imposing discourse structure during text generation, as verified by both automatic and human evaluation.","cats":{"social-sciences":0}}
{"text":"Our code will be available on Github.","cats":{"social-sciences":0}}
{"text":"This study delves into the pervasive issue of gender issues in artificial intelligence (AI), specifically within automatic scoring systems for student-written responses.","cats":{"social-sciences":1,"education":0}}
{"text":"The primary objective is to investigate the presence of gender biases, disparities, and fairness in generally targeted training samples with mixed-gender datasets in AI scoring outcomes.","cats":{"social-sciences":1}}
{"text":"Utilizing a fine-tuned version of BERT and GPT-3.5, this research analyzes more than 1000 human-graded student responses from male and female participants across six assessment items.","cats":{"social-sciences":1,"education":0}}
{"text":"The study employs three distinct techniques for bias analysis: Scoring accuracy difference to evaluate bias, mean score gaps by gender (MSG) to evaluate disparity, and Equalized Odds (EO) to evaluate fairness.","cats":{"social-sciences":1}}
{"text":"Consistently with both BERT and GPT-3.5, we found that mixed-trained models generated fewer MSG and non-disparate predictions compared to humans.","cats":{"social-sciences":1}}
{"text":"In contrast, compared to humans, gender-specifically trained models yielded larger MSG, indicating that unbalanced training data may create algorithmic models to enlarge gender disparities.","cats":{"social-sciences":1}}
{"text":"Collectively, the findings suggest that gender-unbalanced data do not necessarily generate scoring bias but can enlarge gender disparities and reduce scoring fairness.","cats":{"social-sciences":0}}
{"text":" The primary objective is to investigate the presence of gender biases, disparities, and fairness in generally targeted training samples with mixed-gender datasets in AI sessment items.","cats":{"social-sciences":0}}
{"text":"The study employs three distinct techniques for bias analysis: Scoring accuracy difference to evaluate bias, mean score gaps by gender (MSG) to evaluate disparity, and Equ, suggesting no significant scoring bias.","cats":{"social-sciences":0}}
{"text":"Consistently with both BERT and GPT-3.5, we found that mixed-trained models generated fewer MSG and non-disparate predictions compared to huma suggests that mixed-trained models generated more fairness outcomes compared with gender-specifically trained models.","cats":{"social-sciences":0}}
{"text":"The growing popularity of neural machine translation (NMT) and LLMs represented by ChatGPT underscores the need for a deeper understanding of their distinct characteristics and relationships.","cats":{"social-sciences":0}}
{"text":"Such understanding is crucial for language professionals and researchers to make informed decisions and tactful use of these cutting-edge translation technology, but remains underexplored.","cats":{"social-sciences":0}}
{"text":"This study aims to fill this gap by investigating three key questions: (1) the distinguishability of ChatGPT-generated translations from NMT and human translation (HT), (2) the linguistic characteristics of each translation type, and (3) the degree of resemblance between ChatGPT-produced translations and HT or NMT.","cats":{"social-sciences":0}}
{"text":"To achieve these objectives, we employ statistical testing, machine learning algorithms, and multidimensional analysis (MDA) to analyze Spokesperson's Remarks and their translations.","cats":{"social-sciences":0}}
{"text":"After extracting a wide range of linguistic features, supervised classifiers demonstrate high accuracy in distinguishing the three translation types, whereas unsupervised clustering techniques do not yield satisfactory results.","cats":{"social-sciences":0}}
{"text":"Another major finding is that ChatGPT-produced translations exhibit greater similarity with NMT than HT in most MDA dimensions, which is further corroborated by distance computing and visualization.","cats":{"social-sciences":0}}
{"text":"These novel insights shed light on the interrelationships among the three translation types and have implications for the future advancements of NMT and generative AI.","cats":{"social-sciences":0}}
