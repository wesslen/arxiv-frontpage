{"text":"This paper aims to tackle the problem of modeling dynamic urban street scenes from monocular videos.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Recent methods extend NeRF by incorporating tracked vehicle poses to animate vehicles, enabling photo-realistic view synthesis of dynamic urban street scenes.","cats":{"prompt-engineering":0,"security":0,"hci":0,"programming":0}}
{"text":"However, significant limitations are their slow training and rendering speed, coupled with the critical need for high precision in tracked vehicle poses.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We introduce Street Gaussians, a new explicit scene representation that tackles all these limitations.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Specifically, the dynamic urban street is represented as a set of point clouds equipped with semantic logits and 3D Gaussians, each associated with either a foreground vehicle or the background.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"To model the dynamics of foreground object vehicles, each object point cloud is optimized with optimizable tracked poses, along with a dynamic spherical harmonics model for the dynamic appearance.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The explicit representation allows easy composition of object vehicles and background, which in turn allows for scene editing operations and rendering at 133 FPS (1066$\\times$1600 resolution) within half an hour of training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Experiments show that the proposed method consistently outperforms state-of-the-art methods across all datasets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Furthermore, the proposed representation delivers performance on par with that achieved using precise ground-truth poses, despite relying only on poses from an off-the-shelf tracker.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The code is available at https://zju3dv.github.io/street_gaussians/.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself.","cats":{"prompt-engineering":0,"security":0,"recommender":0,"programming":0}}
{"text":"Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0,"programming":0}}
{"text":"In this paper, we report on the first year of TREC iKAT, describing the task, topics, data collection, and evaluation framework.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We further review the submissions and summarize the findings.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"It generates a linearized graph where nodes represent text spans and edges represent relation triplets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"recommender":0,"programming":0}}
{"text":"Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Code is available at https://github.com/urchade/ATG.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"The two levels are computed by the original model's self-attention, which means the proposed does not require any training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0}}
{"text":"In this paper, we consider classes of decision tables closed under removal of attributes (columns) and changing of decisions attached to rows.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"For decision tables from closed classes, we study lower bounds on the minimum cardinality of reducts, which are minimal sets of attributes that allow us to recognize, for a given row, the decision attached to it.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0}}
{"text":"We assume that the number of rows in decision tables from the closed class is not bounded from above by a constant.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"education":0}}
{"text":"We divide the set of such closed classes into two families.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"In one family, only standard lower bounds $\\Omega (\\log $ ${\\rm cl}(T))$ on the minimum cardinality of reducts for decision tables hold, where ${\\rm cl}(T)$ is the number of decision classes in the table $T$. In another family, these bounds can be essentially tightened up to $\\Omega ({\\rm cl}(T)^{1/q})$ for some natural $q$.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"In this paper, we consider a number of models for inferring a non-deterministic finite automaton (NFA) with 3 sorts of states, that must accept some words, and reject some other words from a given sample.","cats":{"prompt-engineering":0,"security":0}}
{"text":"We then propose a transformation from this 3-sort NFA into weighted-frequency and probabilistic NFA, and we apply the latter to a classification task.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"recommender":0}}
{"text":"The experimental evaluation of our approach shows that the probabilistic NFAs can be successfully applied for classification tasks on both real-life and superficial benchmark data sets.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0}}
{"text":"The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0}}
{"text":"Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023).","cats":{"prompt-engineering":1,"robustness":0,"security":0,"hci":0}}
{"text":"Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"The framework employs multiple LLM agents, each with a distinct persona, engaged in role-playing communication, offering a nuanced and adaptable approach to diverse problem scenarios.","cats":{"prompt-engineering":0,"security":0}}
{"text":"Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"recommender":0,"programming":0}}
{"text":"This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions about the information space include: finding options, comparing options, identifying the pros and cons of options, etc.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"Given the different personas and their information need (expressed through the sequence of questions), diverse conversation trajectories will arise -- because the answers to these similar queries will be very different.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \\textit{span-based}.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"This work elicits LLMs' inherent ability to handle long contexts without fine-tuning.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"architectures":1,"programming":0}}
{"text":"We propose Self-Extend to stimulate LLMs' long context handling potential.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The basic idea is to construct bi-level attention information: the group level and the neighbor level.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"architectures":1,"programming":0}}
{"text":"Grammatical inference consists in learning a language or a grammar from data.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input.","cats":{"prompt-engineering":1,"robustness":1,"security":0,"hci":0,"social-sciences":1,"recommender":0}}
{"text":"This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Large Language Models (LLMs) have revolutionized Natural Language Processing but exhibit limitations, particularly in autonomously addressing novel challenges such as reasoning and problem-solving.","cats":{"prompt-engineering":1,"robustness":0,"security":0,"programming":0}}
{"text":"Traditional techniques like chain-of-thought prompting necessitate explicit human guidance.","cats":{"prompt-engineering":1,"security":0}}
{"text":"This paper introduces a novel multi-agent communication framework, inspired by the CAMEL model, to enhance LLMs' autonomous problem-solving capabilities.","cats":{"prompt-engineering":0,"security":0}}
{"text":"Generative AI has the potential to transform how public services are delivered by enhancing productivity and reducing time spent on bureaucracy.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"We find that use of generative AI systems is already widespread: 45% of respondents were aware of generative AI usage within their area of work, while 22% actively use a generative AI system.","cats":{"prompt-engineering":0,"security":0,"hci":1,"social-sciences":0}}
{"text":"Public sector professionals were positive about both current use of the technology and its potential to enhance their efficiency and reduce bureaucratic workload in the future.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"Our survey also found a high amount of trust (61%) around generative AI outputs, and a low fear of replacement (16%).","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"While respondents were optimistic overall, areas of concern included feeling like the UK is missing out on opportunities to use AI to improve public services (76%), and only a minority of respondents (32%) felt like there was clear guidance on generative AI usage in their workplaces.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"In other words, it is clear that generative AI is already transforming the public sector, but uptake is happening in a disorganised fashion without clear guidelines.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"The UK's public sector urgently needs to develop more systematic methods for taking advantage of the technology.","cats":{"prompt-engineering":0,"security":0}}
{"text":"Recent trends have seen a growing reliance on data-driven techniques to facilitate the modeling process and yield accurate channel predictions.","cats":{"prompt-engineering":0,"security":0}}
{"text":"We offer a comprehensive architecture for PINN methodology, designed to inform and inspire future model development.","cats":{"prompt-engineering":0,"security":0}}
{"text":"Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0}}
{"text":"We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency.","cats":{"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area.","cats":{"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"(2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases.","cats":{"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"The $a$-number is an invariant of the isomorphism class of the $p$-torsion group scheme.","cats":{"security":0}}
{"text":"For this purpose, data from low-cost in-vehicle inertial sensors such as the accelerometer and gyroscope sensor are fused and fed into a long short-term memory (LSTM) neural network.","cats":{"security":0}}
{"text":"A tuple (Z_1,...,Z_p) of matrices of size r is said to be a commuting extension of a tuple (A_1,...,A_p) of matrices of size n <r if the Z_i pairwise commute and each A_i sits in the upper left corner of a block decomposition of Z_i.","cats":{"security":0,"social-sciences":0}}
{"text":"Channel modeling is fundamental in advancing wireless systems and has thus attracted considerable research focus.","cats":{"security":0}}
{"text":"The analyzed tasks demonstrate the effectiveness of the proposed estimators and that the SL divergence achieves the highest classification accuracy in almost all the scenarios.","cats":{"security":0}}
{"text":"Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts.","cats":{"security":0,"programming":0}}
{"text":"Through a difference-in-differences approach, we find that deplatforming reduces online attention toward influencers.","cats":{"security":0,"hci":0,"social-sciences":1}}
{"text":"Thereafter, we present a Reweighed Co-Contrast between the temporal views of the sequential network, so that the couple of Riemannian spaces interact with each other for the interaction prediction without labels.","cats":{"security":0}}
{"text":"In recent years, Riemannian graph representation learning has emerged as an exciting alternative to Euclidean ones.","cats":{"security":0}}
{"text":"The development of facial biometric systems has contributed greatly to the development of the computer vision field.","cats":{"security":0}}
{"text":"In all case studies, we show that co-training via task-relatedness is advantageous and prevents negative transfer (which occurs when MT model's performance is worse than that of at least one single-task model).","cats":{"security":0}}
{"text":"Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs).","cats":{"security":0}}
{"text":"With the rapid development of machine learning and growing concerns about data privacy, federated learning has become an increasingly prominent focus.","cats":{"security":0,"production":0}}
{"text":"Our method achieves human-level performance in estimating fetal biometrics and estimates well-calibrated credible intervals in which the true biometric value is expected to lie.","cats":{"security":0,"social-sciences":0}}
{"text":"In the last few years, there is a growing interest in computer aided diagnostic (CAD) using most image and clinical data of the lesion.","cats":{"security":0}}
{"text":"In this paper, we propose a more general, theoretically sound, targeted attack that resorts to the minimization of a Jacobian-induced MAhalanobis distance (JMA) term, taking into account the effort (in the input space) required to move the latent space representation of the input sample in a given direction.","cats":{"security":0,"hci":0}}
{"text":"Interestingly, this result does not require assumptions on the prior distribution.","cats":{"security":0}}
{"text":"In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different data-to-text generation tasks as graph-to-text generation.","cats":{"security":0}}
{"text":"Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trained on full datasets in medical image segmentation.","cats":{"security":0,"social-sciences":0}}
{"text":"We present En3D, an enhanced generative scheme for sculpting high-quality 3D human avatars.","cats":{"security":0}}
{"text":"Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status.","cats":{"security":0,"social-sciences":0,"architectures":0}}
{"text":"Our findings indicate that, in comparison to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks.","cats":{"security":1}}
{"text":"In this paper we aim to investigate the effect of adversarial perturbations on coding tasks with LLMs.","cats":{"security":0}}
{"text":"LLMs can also be vulnerable to the same adversarial perturbations but a detailed study on this concern has been lacking so far.","cats":{"security":1}}
{"text":"Our experiments show that adversarial examples obtained with a smaller code model are indeed transferable, weakening the LLMs' performance.","cats":{"security":1}}
{"text":"Researchers have long studied the vulnerabilities and limitations of LLMs, such as adversarial attacks and model toxicity.","cats":{"security":1}}
{"text":"Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs.","cats":{"security":1}}
{"text":"Furthermore, to make the LLMs more robust against such adversaries without incurring the cost of retraining, we propose prompt-based defenses that involve modifying the prompt to include additional information such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations.","cats":{"security":1}}
{"text":"In particular, we study the transferability of adversarial examples, generated through white-box attacks on smaller code models, to LLMs.","cats":{"security":0}}
{"text":"Based on the exceptional capacity of language understanding and generation of large language models (LLMs), we propose LLM-Attack, which aims at generating both valid and natural adversarial examples with LLMs.","cats":{"security":1}}
{"text":"Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.","cats":{"security":1}}
{"text":"Adversarial attacks are a type of attack on machine learning models where an attacker deliberately modifies the inputs to cause the model to make incorrect predictions.","cats":{"security":1}}
{"text":"Most previous adversarial attacks do not focus on the interpretability of the generated adversarial examples, and we cannot gain insights into the mechanism of the target classifier from the attacks.","cats":{"security":1}}
{"text":"Our attack method generates adversarial examples with attribution maps that resemble benign samples.","cats":{"security":1}}
{"text":"To bridge this gap, this study systematically analyzes five state-of-the-art adversarial attack approaches from three perspectives: effectiveness, efficiency, and the quality of generated examples.","cats":{"security":1}}
{"text":"The emergence of adversarial attacks, particularly those that exploit control over labels or employ physically feasible trojans, threatens to erode that trust, making the analysis and mitigation of these attacks a matter of urgency.","cats":{"security":1}}
{"text":"Notably, these attacks can be generated using unmodified, or \"vanilla,\" versions of these LLMs, without requiring any prior adversarial exploits such as jailbreaking.","cats":{"security":1}}
{"text":"In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed filtering schemes.","cats":{"security":1}}
{"text":"However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful content.","cats":{"security":1}}
{"text":"In an analogy to adversarial attacks on image classifiers, we call such inputs \\textbf{adversarial perception errors} and show they can be systematically constructed using a simple boundary-attack algorithm.","cats":{"security":1}}
{"text":"LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label.","cats":{"security":1}}
{"text":"In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks.","cats":{"security":1}}
{"text":"So far, these attacks assumed that the adversary is directly prompting the LLM.   ","cats":{"security":1}}
{"text":"To systematically assess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) - a multi-round adversarial benchmark with 4912 open-ended questions covering more than 20 safety sub-dimensions.","cats":{"security":1}}
{"text":"In recent years, the primary focus has been on adversarial attacks.","cats":{"security":1}}
{"text":"On a positive note, our study also highlights how offensive security researchers and pentesters can make use of LLMs to simulate realistic attack scenarios, identify potential vulnerabilities, and better protect organizations.","cats":{"security":1}}
{"text":"We employ several adversarial attacks to evaluate its robustness.","cats":{"security":1}}
{"text":"Yet, a largely unexplored facet of this paradigm is its susceptibility to adversarial attacks, especially with the aim of inducing forgetting.","cats":{"security":1}}
{"text":"Extensive experimentation demonstrates the framework's superior performance and adaptability, providing valuable insights into the collaborative potential of multiple agents in overcoming the limitations of individual models.","cats":{"security":0,"architectures":0}}
{"text":"We use the Cartier operator on $H^0(\\mathcal{A}_2,\\Omega^1)$ to find a closed formula for the $a$-number of the form $\\mathcal{A}_2 = v(Y^{\\sqrt{q}}+Y-x^{\\frac{\\sqrt{q}+1}{2}})$ where $q=p^s$ over the finite field $\\mathbb{F}_{q^2}$. The application of the computed $a$-number in coding theory is illustrated by the relationship between the algebraic properties of the curve and the parameters of codes that are supported by it.","cats":{"security":0,"architectures":0}}
{"text":"To collect data, a vehicle equipped with a GNSS receiver, along with Inertial Measurement Unit (IMU) is used.","cats":{"security":0}}
{"text":"The detection framework incorporates two strategies: The first strategy involves comparing the predicted location shift, which is the distance traveled between two consecutive timestamps, with the inertial sensor-based location shift.","cats":{"security":0}}
{"text":"The second strategy employs a Random-Forest supervised machine learning model to detect and classify turns, distinguishing between left and right turns using the output from the steering angle sensor.","cats":{"security":0}}
{"text":"Importantly, the IMU data remains uncompromised throughout the spoofing attack.","cats":{"security":0,"hci":0}}
{"text":"To test the effectiveness of the detection framework, experiments are conducted in Tuscaloosa, AL, mimicking urban road structures.","cats":{"security":0}}
{"text":"This notion was discovered and rediscovered in several contexts including algebraic complexity theory (in Strassen's work on tensor rank), in numerical analysis for the construction of cubature formulas and in quantum mechanics for the study of computational methods and the study of the so-called \"quantum Zeno dynamics.\"","cats":{"security":0}}
{"text":"Commuting extensions have also attracted the attention of the linear algebra community.","cats":{"security":0,"hci":0,"recommender":0}}
{"text":"In this paper we present 3 types of results:   (i) Theorems on the uniqueness of commuting extensions for three matrices or more.   ","cats":{"security":0,"programming":0}}
{"text":"(ii) Algorithms for the computation of commuting extensions of minimal size.","cats":{"security":0,"architectures":0}}
{"text":"These algorithms work under the same assumptions as our uniqueness theorems.","cats":{"security":0}}
{"text":"They are applicable up to r=4n/3, and are apparently the first provably efficient algorithms for this problem applicable beyond r=n+1.   ","cats":{"security":0,"programming":0}}
{"text":"(iii) A genericity theorem showing that our algorithms and uniqueness theorems can be applied to a wide range of input matrices.","cats":{"security":0,"recommender":0}}
{"text":"Taken together, these findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks.","cats":{"security":0,"hci":0,"social-sciences":0}}
{"text":"Furthermore, unlike other types of artificial intelligence, it is a technology that has quickly become widely available for bottom-up adoption: essentially anyone can decide to make use of it in their day to day work.","cats":{"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"But to what extent is generative AI already in use in the public sector?","cats":{"security":0,"hci":0,"social-sciences":0}}
{"text":"Our survey of 938 public service professionals within the UK (covering education, health, social work and emergency services) seeks to answer this question.","cats":{"security":0,"social-sciences":0}}
{"text":"For example, those working in the NHS thought that time spent on bureaucracy could drop from 50% to 30% if generative AI was properly exploited, an equivalent of one day per week (an enormous potential impact).","cats":{"security":0,"hci":0,"social-sciences":1}}
{"text":"We study a variant of the searching problem where the environment consists of a known terrain and the goal is to obtain visibility of an unknown target point on the surface of the terrain.","cats":{"security":0}}
{"text":"The searcher starts on the surface of the terrain and is allowed to fly above the terrain.","cats":{"security":0}}
{"text":"The goal is to devise a searching strategy that minimizes the competitive ratio, that is, the worst-case ratio between the distance traveled by the searching strategy and the minimum travel distance needed to detect the target.","cats":{"security":0}}
{"text":"For $1.5$D terrains we show that any searching strategy has a competitive ratio of at least $\\sqrt{82}$ and we present a nearly-optimal searching strategy that achieves a competitive ratio of $3\\sqrt{19/2} \\approx \\sqrt{82}","cats":{"security":0}}
{"text":"+ 0.19$.","cats":{"security":0}}
{"text":"This strategy extends directly to the case where the searcher has no knowledge of the terrain beforehand.","cats":{"security":0,"hci":0}}
{"text":"For $2.5$D terrains we show that the optimal competitive ratio depends on the maximum slope $\\lambda$ of the terrain, and is hence unbounded in general.","cats":{"security":0}}
{"text":"Specifically, we provide a lower bound on the competitive ratio of $\\Omega(\\sqrt{\\lambda})$. Finally, we complement the lower bound with a searching strategy based on the maximum slope of the known terrain, which achieves a competitive ratio of $O(\\sqrt{\\lambda})$.","cats":{"security":0}}
{"text":"In this work, we first provide a concise overview of data-driven channel modeling methods, highlighting their limitations.","cats":{"security":0}}
{"text":"Subsequently, we introduce the concept and advantages of physics-informed neural network (PINN)-based modeling and a summary of recent contributions in this area.","cats":{"security":0}}
{"text":"Our findings demonstrate that PINN-based approaches in channel modeling exhibit promising attributes such as generalizability, interpretability, and robustness.","cats":{"security":0,"recommender":0}}
{"text":"A case-study of our recent work on precise indoor channel prediction with semantic segmentation and deep learning is presented.","cats":{"security":0,"production":0}}
{"text":"Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems.","cats":{"security":1}}
{"text":"In this work, we analyze the impact of model interpretability due to adversarial attacks on text classification problems.","cats":{"security":0}}
{"text":"Then, we introduce the adversarial perturbations on the text data to understand the classification performance after the attack.","cats":{"security":0}}
{"text":"Subsequently, we analyze and interpret the model's explainability before and after the attack","cats":{"security":1}}
{"text":" Adversarial attacks can have serious consequences, particularly in applications such as autonomous vehicles, medical diagnosis, and security systems.","cats":{"security":0}}
{"text":"Work on the vulnert want to.","cats":{"security":0}}
{"text":"We develop an ML-based classification model for text data.","cats":{"security":0}}
{"text":"Subsequ","cats":{"security":0}}
{"text":"They explore the consequences of vulnerabilities in AI systems.","cats":{"security":1}}
{"text":"This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities.","cats":{"security":1}}
{"text":"Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.","cats":{"security":1}}
{"text":"There have been recent adversarial attacks that are difficult to find.","cats":{"security":0}}
{"text":"These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks.","cats":{"security":0}}
{"text":"The authors focus on this domain in this research paper.  ","cats":{"security":0}}
{"text":"This includes discussing how they might arise, differences betwy when they are in testing phase and getting them ready for broader use.","cats":{"security":0}}
{"text":"To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class.","cats":{"security":1}}
{"text":"Our results show that the proposed attacks can substantially increase the membership inference precision with minimum overall test-time model performance degradation.","cats":{"security":1}}
{"text":"As in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks.","cats":{"security":0}}
{"text":"Such attacks typically lead to test-time accuracy degradation or controlled misprediction.","cats":{"security":0}}
{"text":"In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples.  ","cats":{"security":0}}
{"text":"We first propose a generic dirty-label attack for supervised classification algorithms.","cats":{"security":0}}
{"text":"We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look \"natural\" to evade human moderation.","cats":{"security":0}}
{"text":"We extensively evaluate our attacks on computer vision benchmarks.","cats":{"security":0}}
{"text":"Our results show that the proposed attacks can substantially increase the membership inference precision with minimum","cats":{"security":0}}
{"text":"This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves.","cats":{"security":1}}
{"text":"The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks.","cats":{"security":1}}
{"text":" The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks.","cats":{"security":0}}
{"text":"While diffusion-based methods are typically disregarded for adversarial defense due to their slow erving clean accuracy, speed, and plug-and-play compatibility.","cats":{"security":0}}
{"text":"Code at: https://github.com/HondamunigePrasannaSilva/DiffDefence.","cats":{"security":0}}
{"text":"However, these previous code models were shown vulnerable to adversarial examples, i.e. small syntactic perturbations that do not change the program's semantics, such as the inclusion of \"dead code\" through false conditions or the addition of inconsequential print statements, designed to \"fool\" the models. LLMs can also be vulnerable to the same adversarial perturbations but a detailed study on this concern has been lacking so far.","cats":{"security":1}}
{"text":"The proposed defenses show promise in improving the model's resilience, paving the way to more robust defensive solutions for LLMs in code-related applications.","cats":{"security":1}}
{"text":"Modern large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities for coding tasks including writing and reasoning about code.","cats":{"security":0}}
{"text":"They improve upon previous neural network models of code, such as code2seq or seq2seq, that already demonstrated competitive results when performing tasks such as code summarization and identifying code vulnerabilities.  ","cats":{"security":0}}
{"text":"Furthermore, to make the LLMs more robust against such adversaries without incurring the cost of retraining, we propose prompt-based defenses that involve modifying the prompt to include additiong adversarial perturbations.","cats":{"security":0}}
{"text":"Our experiments show that adversarial examples obtained with a smaller co, paving the way to more robust defensive solutions for LLMs in code-related applications.","cats":{"security":0}}
{"text":"Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data.","cats":{"security":1}}
{"text":"To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model.","cats":{"security":1}}
{"text":"Among many defenses based on the off-manifold assumption of adversarial examples, this work offers a new angle for capturing the manifold change.","cats":{"security":0}}
{"text":"Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications.","cats":{"security":0}}
{"text":"They can misguide current models to predict incorrectly by slightly modifying the inputs.  ","cats":{"security":0}}
{"text":"To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearl evaluate MLMD on various benchmark textual datasets, widely studied machine learning models, and state-of-the-art (SOTA) adversarial attacks (in total $3*4*4 = 48$ settings).","cats":{"security":0}}
{"text":"Experimental results show that MLMD can achieve strong performance, with detection accuracy up to 0.984, 0.967, and 0.901 on AG-NEWS, IMDB, and SST-2 datasets, respectively.","cats":{"security":0}}
{"text":"Additionally, MLMD is superior, or at least comparable to, the SOTA detection defenses in detection accuracy and F1 score.","cats":{"security":0}}
{"text":"The code for this work is openly accessible at \\url{https://github.com/mlmddetection/MLMDdetection}.","cats":{"security":0}}
{"text":"Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities.","cats":{"security":1}}
{"text":"Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment.","cats":{"security":1}}
{"text":"In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs.","cats":{"security":1}}
{"text":"We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures.","cats":{"security":1}}
{"text":"With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.","cats":{"security":1}}
{"text":" Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment.","cats":{"security":0}}
{"text":"It is important that developers and practitioners alike are aware of security-related problems with such modeescribing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures.","cats":{"security":0}}
{"text":"With our work, we hope to raise awareness of the limitations of LLMs in light of such security co","cats":{"security":0}}
{"text":"However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text.","cats":{"security":1}}
{"text":"We propose a simple approach to defending against these attacks by having a large language model filter its own responses.","cats":{"security":0}}
{"text":"Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.","cats":{"security":1}}
{"text":"Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting.","cats":{"security":0,"hci":0}}
{"text":"However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes).","cats":{"security":0}}
{"text":"There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning.  ","cats":{"security":0}}
{"text":"Our current results show that even  users by validating the content using a language model.","cats":{"security":0}}
{"text":"Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content.","cats":{"security":1}}
{"text":"To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs.","cats":{"security":0}}
{"text":"Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs.","cats":{"security":1}}
{"text":"SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation.","cats":{"security":1}}
{"text":"Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.","cats":{"security":1}}
{"text":"Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomlyinputs.","cats":{"security":0}}
{"text":"SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary","cats":{"security":0}}
{"text":"However, they fail on adversarial language examples, which are sentences optimized to fool the language models but with similar semantic meanings for humans.","cats":{"security":1}}
{"text":"By dynamically adapting the input sentence with predictions from masked words, we show that we can reverse many language adversarial attacks.","cats":{"security":1}}
{"text":"Since our approach does not require any training, it works for novel tasks at test time and can adapt to novel adversarial corruptions.","cats":{"security":1}}
{"text":"Large-scale language mdels achieved state-f-the-art perfrmance ver a number f language tasks.  ","cats":{"security":0}}
{"text":"While prir wrk fcuses n making the language mdel rbust at training time, retraining fr rbustness is ften unrealistic fr large-scale fundatin mdels.","cats":{"security":0}}
{"text":"Instead, we prpse t make the language mdels rbust at test time.","cats":{"security":0}}
{"text":"By dynamically adapting the input sentence with predictins frm masked wrds, we shw that we can reverse many language adversarial attacks.","cats":{"security":0}}
{"text":"Since ur apprmpirical results n tw ppular sentence classificatin datasets demnstrate that ur methd can repair adversarial language attacks ver 65%","cats":{"security":0}}
{"text":"Further investigation into the effects of these methods on both model robustness and code security reveals that larger models tend to demonstrate reduced robustness and less security.","cats":{"security":1,"social-sciences":0}}
{"text":"The high cost of full-parameter fine-tuning (FFT) of Large Language Models (LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.","cats":{"security":0}}
{"text":"However, it remains unclear which methods provide the best cost-performance trade-off at different model scales.","cats":{"security":0}}
{"text":"We introduce Astraios, a suite of 28 instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up to 16 billion parameters.","cats":{"security":0}}
{"text":"Through investigations across 5 tasks and 8 different datasets encompassing both code comprehension and code generation tasks, we find that FFT generally leads to the best downstream performance across all scales, and PEFT methods differ significantly in their efficacy based on the model scale.","cats":{"security":0}}
{"text":"LoRA usually offers the most favorable trade-off between cost and performance.  ","cats":{"security":0}}
{"text":"At last, we explore the relationships among updated parameters, cross-entropy loss, and task performance.","cats":{"security":0,"recommender":0}}
{"text":"We find that the tuning effectiveness observed in small models generalizes well to larger models, and the validation loss in instruction tuning can be a reliable indicator of overall downstream performance.","cats":{"security":0}}
{"text":"However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones.","cats":{"security":1,"hci":0,"programming":0}}
{"text":"In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks.","cats":{"security":0,"programming":0}}
{"text":"Our experiments on six tasks show that Jatmo models provide the same quality of outputs on their specific task as standard LLMs, while being resilient to prompt injections.","cats":{"security":0,"programming":0}}
{"text":"Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks.  ","cats":{"security":0}}
{"text":"Jatmo leverages the fact that LLMs can only follow instructions once they have under then used to fine-tune a base model (i.e., a non-instruction-tuned model).","cats":{"security":0}}
{"text":"Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs.","cats":{"security":0,"programming":0}}
{"text":"For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases none at all, to produce a fully synthetic dataset.","cats":{"security":0,"programming":0}}
{"text":"The best attacks succeeded in less than 0.5% of cases against our models, versus over 90% success rate against GPT-3.5-Turbo.","cats":{"security":0,"programming":0}}
{"text":"We release Jatmo at","cats":{"security":0}}
{"text":"Deep learning-based natural language processing (NLP) models, particularly pre-trained language models (PLMs), have been revealed to be vulnerable to adversarial attacks.","cats":{"security":1}}
{"text":"However, the adversarial examples generated by many mainstream word-level adversarial attack models are neither valid nor natural, leading to the loss of semantic maintenance, grammaticality, and human imperceptibility.","cats":{"security":1}}
{"text":"Experimental results on the Movie Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline adversarial attack models illustrate the effectiveness of LLM-Attack, and it outperforms the baselines in human and GPT-4 evaluation by a significant margin.","cats":{"security":1}}
{"text":"The model can generate adversarial examples that are typically valid and natural, with the preservation of semantic meaning, grammaticality, and human imperceptibility.","cats":{"security":0}}
{"text":" However, the adversarial examples generated by many mainstream word-level adversarial attack models are neither valid nor natural, leading to the loss of semantic maintend natural adversarial examples with LLMs.","cats":{"security":0}}
{"text":"The method consists of two stages: word importance ranking (which searches for the most vulnerable words) and word synonym replacement (which substitutes them with their synonymsperforms the baselines in human and GPT-4 evaluation by a significant margin.","cats":{"security":0}}
{"text":"This paper elaborately conducts a series of manual jailbreak prompts along with a virtual chat-powered evil plan development team, dubbed Evil Geniuses, to thoroughly probe the safety aspects of these agents.","cats":{"security":1}}
{"text":"Our investigation reveals three notable phenomena: 1) LLM-based agents exhibit reduced robustness against malicious attacks.","cats":{"security":0}}
{"text":"These insights prompt us to question the effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-based agents.","cats":{"security":1}}
{"text":"The rapid advancements in large language models (LLMs) have led to a resurgence in LLM-based agents, which demonstrate impressive human-like behaviors and cooperative capabilities in various interactions and strategy formulations.","cats":{"security":0}}
{"text":"However, evaluating the safety of LLM-based agents remains a complex challenge.  ","cats":{"security":0}}
{"text":"2) the attacked agents could provide more nuanced responses.","cats":{"security":0}}
{"text":"3) the detection of thks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-based agents.","cats":{"security":0}}
{"text":"Extensive evaluation and discussion reveal that LLM-based agents face significant challenges in safety and yiel","cats":{"security":0}}
{"text":"Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them.","cats":{"security":0}}
{"text":"Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on another white-box model, compromising generalization or jailbreak efficiency.","cats":{"security":1}}
{"text":"In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts.","cats":{"security":1}}
{"text":"Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses.  ","cats":{"security":0}}
{"text":"Unforr white-box model, compromising generalization or jailbreak efficiency.","cats":{"security":0}}
{"text":"In this paper, we generalize jailbreak proailbreak prompts.","cats":{"security":0}}
{"text":"Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines.","cats":{"security":0}}
{"text":"Our studyze both the academic community and LLMs vendors towards the provision of safer and more regulated Large Language Models.","cats":{"security":0}}
