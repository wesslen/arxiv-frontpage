{"text":"In this paper, we propose a novel prompt learning framework for code summarization called PromptCS.","cats":{"prompt-engineering":1,"social-sciences":0,"programming":1}}
{"text":"This paper aims to tackle the problem of modeling dynamic urban street scenes from monocular videos.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Recent methods extend NeRF by incorporating tracked vehicle poses to animate vehicles, enabling photo-realistic view synthesis of dynamic urban street scenes.","cats":{"prompt-engineering":0,"security":0,"hci":0,"programming":0}}
{"text":"However, significant limitations are their slow training and rendering speed, coupled with the critical need for high precision in tracked vehicle poses.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We introduce Street Gaussians, a new explicit scene representation that tackles all these limitations.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Specifically, the dynamic urban street is represented as a set of point clouds equipped with semantic logits and 3D Gaussians, each associated with either a foreground vehicle or the background.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"To model the dynamics of foreground object vehicles, each object point cloud is optimized with optimizable tracked poses, along with a dynamic spherical harmonics model for the dynamic appearance.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The explicit representation allows easy composition of object vehicles and background, which in turn allows for scene editing operations and rendering at 133 FPS (1066$\\times$1600 resolution) within half an hour of training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Experiments show that the proposed method consistently outperforms state-of-the-art methods across all datasets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Furthermore, the proposed representation delivers performance on par with that achieved using precise ground-truth poses, despite relying only on poses from an off-the-shelf tracker.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The code is available at https://zju3dv.github.io/street_gaussians/.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself.","cats":{"prompt-engineering":0,"security":0,"recommender":0,"programming":0}}
{"text":"Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0,"programming":0}}
{"text":"In this paper, we report on the first year of TREC iKAT, describing the task, topics, data collection, and evaluation framework.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We further review the submissions and summarize the findings.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"It generates a linearized graph where nodes represent text spans and edges represent relation triplets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"recommender":0,"programming":0}}
{"text":"Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The two levels are computed by the original model's self-attention, which means the proposed does not require any training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"recommender":0,"programming":0}}
{"text":"This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions about the information space include: finding options, comparing options, identifying the pros and cons of options, etc.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"Given the different personas and their information need (expressed through the sequence of questions), diverse conversation trajectories will arise -- because the answers to these similar queries will be very different.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \\textit{span-based}.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"This work elicits LLMs' inherent ability to handle long contexts without fine-tuning.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"architectures":1,"programming":0}}
{"text":"We propose Self-Extend to stimulate LLMs' long context handling potential.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The basic idea is to construct bi-level attention information: the group level and the neighbor level.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning.","cats":{"prompt-engineering":0,"security":0,"social-sciences":0,"architectures":1,"programming":0}}
{"text":"As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and Codex, now enable software developers to generate code based on a natural language prompt.","cats":{"prompt-engineering":1,"programming":1}}
{"text":"Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafted prompts.","cats":{"prompt-engineering":1,"education":1,"programming":1}}
{"text":"These advances may enable students to interact with code in new ways while helping instructors scale their learning materials.","cats":{"prompt-engineering":1,"education":0,"programming":1}}
{"text":" Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafmplications for academic integrity, curriculum design, and software engineering careers.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"This workshop will demonstrate the capabilities of LLMs to help attendees evaluate wheider how LLMs will impact our field.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency.","cats":{"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors.","cats":{"robustness":1,"programming":0}}
{"text":"Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches.","cats":{"robustness":0,"programming":0}}
{"text":"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code).","cats":{"robustness":0,"social-sciences":0,"education":0,"programming":1}}
{"text":"As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity.","cats":{"robustness":0,"programming":0}}
{"text":"Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement.","cats":{"robustness":0,"programming":1}}
{"text":"Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts.","cats":{"security":0,"programming":0}}
{"text":"In this paper we present 3 types of results:   (i) Theorems on the uniqueness of commuting extensions for three matrices or more.   ","cats":{"security":0,"programming":0}}
{"text":"They are applicable up to r=4n/3, and are apparently the first provably efficient algorithms for this problem applicable beyond r=n+1.   ","cats":{"security":0,"programming":0}}
{"text":"Large Language Models~(LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages.","cats":{"hci":0,"programming":0}}
{"text":"Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks.","cats":{"hci":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"Large language models have been demonstrated to be valuable in different fields.","cats":{"hci":0,"social-sciences":0,"production":0,"programming":0}}
{"text":"Thus, this approach may provide a valuable component for future misinformation mitigation pipelines.","cats":{"hci":0,"programming":0}}
{"text":"In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area.","cats":{"social-sciences":0,"programming":0}}
{"text":"This paper documents a year-long experiment to \"profile\" the process of learning a programming language: gathering data to understand what makes a language hard to learn, and using that data to improve the learning process.","cats":{"social-sciences":0,"programming":0}}
{"text":"Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N.","cats":{"social-sciences":0,"programming":0}}
{"text":"(Source) code summarization is the task of automatically generating natural language summaries for given code snippets.","cats":{"social-sciences":0,"programming":1}}
{"text":"Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks.","cats":{"social-sciences":0,"programming":1}}
{"text":"Recently, Large Language Models (LLMs) have achieved impressive performance in synthesizing computer programs based on a natural language prompt, mitigating the gap between natural language and structured programs.","cats":{"social-sciences":0,"programming":1}}
{"text":"Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.","cats":{"recommender":0,"programming":0}}
{"text":"Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.","cats":{"production":0,"programming":1}}
{"text":"We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs.","cats":{"production":0,"programming":1}}
{"text":"Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks.","cats":{"production":0,"programming":1}}
{"text":"Large Language Models for Code (Code LLM) are flourishing.","cats":{"programming":1}}
{"text":"Recently, large language models for code generation have achieved breakthroughs in several programming language tasks.","cats":{"programming":1}}
{"text":"The recent development on large language models makes automatically constructing small programs possible.","cats":{"programming":1}}
{"text":"Using large language models (LLMs) for source code has recently gained attention.","cats":{"programming":1}}
{"text":"Large language models trained on source code can support a variety of software development tasks, such as code recommendation and program repair.","cats":{"programming":1}}
{"text":"In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models.","cats":{"programming":1}}
{"text":"In this paper, we explore the potential of large language models in supporting the evolution of software models in software engineering.","cats":{"programming":1}}
{"text":"We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters.","cats":{"programming":1}}
{"text":"Large language models (LLMs) have achieved impressive performance on code generation.","cats":{"programming":1}}
{"text":"Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation.","cats":{"programming":1}}
{"text":"Large language models (LLMs) have recently demonstrated surprising performance for a range of computing tasks, including generating and explaining code.","cats":{"programming":1}}
{"text":"Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering.","cats":{"programming":1}}
{"text":"Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data.","cats":{"programming":1}}
{"text":"Emerging Large Language Models (LLMs) can answer questions, write high-quality programming code, and predict protein folding, showcasing their versatility in solving various tasks beyond language-based problems.","cats":{"programming":1}}
{"text":"Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive.","cats":{"programming":1}}
{"text":"This paper explores the use of Large Language Models (LLMs) and in particular ChatGPT in programming, source code analysis, and code generation.","cats":{"programming":1}}
{"text":"Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks.","cats":{"programming":1}}
{"text":"Recent development of large language models (LLMs) for code like CodeX and CodeT5+ demonstrates tremendous promise in achieving code intelligence.","cats":{"programming":1}}
{"text":"Large language models with instruction-following capabilities open the door to a wider group of users.","cats":{"programming":1}}
{"text":"Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges.","cats":{"programming":1}}
{"text":"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","cats":{"programming":1}}
{"text":"Meanwhile, large language models, represented by ChatGPT, have gained great attentions, showcasing great capabilities in code analysis tasks.","cats":{"programming":1}}
{"text":"In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs).","cats":{"programming":1}}
{"text":"Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models.","cats":{"programming":1}}
{"text":"This paper presents Safurai-001, a new Large Language Model (LLM) with significant potential in the domain of coding assistance.","cats":{"programming":1}}
{"text":"Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data.","cats":{"programming":0}}
{"text":"This has been a particular concern in the era of large language models (LLMs)- based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code.","cats":{"programming":1}}
{"text":"This challenge has become even more pressing with the rise of large language model (LLM) based code generation tools.","cats":{"programming":1}}
{"text":"Recent advancements in Large Language Models (LLMs) have enhanced their ability to comprehend natural and programming languages, enabling them to generate patches based on review comments.","cats":{"programming":1}}
{"text":"We have found that large language models are indeed a promising technology for supporting software model evolution, and that it is worth investigating further in the area of software model evolution.","cats":{"programming":1}}
{"text":"The ubiquitous adoption of Large Language Generation Models (LLMs) in programming has underscored the importance of differentiating between human-written code and code generated by intelligent models.","cats":{"programming":1}}
{"text":"In this paper, we take a major step towards filling this gap by proposing GEqO, a portable and lightweight machine-learning-based framework for efficiently identifying semantically equivalent computations at scale.","cats":{"programming":0}}
{"text":"To address this, we propose a multilevel generative semantic communication system with a two-stage training framework.","cats":{"programming":0}}
{"text":"Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods.","cats":{"programming":0}}
{"text":"We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences.","cats":{"programming":0}}
{"text":"Then, we numerically test the set of proposed objective functions in three application scenarios: toy examples, image data sets, and signal detection/decoding problems.","cats":{"programming":0}}
{"text":"Finally, VideoDrafter outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account.","cats":{"programming":0}}
{"text":"Empirical results show the superiority of MofitRGC.","cats":{"programming":0}}
{"text":"We were also able to achieve a testing accuracy of 88.03% in the face-shape problem using the celebrity face-shape dataset[3].","cats":{"programming":0}}
{"text":"We explore task-relatedness for co-annotation and co-training, and propose a novel approach, where knowledge exchange is enabled between the tasks via distribution matching.","cats":{"programming":0}}
{"text":"The proposed algorithm provides an optimal solution to a linearized version of the adversarial example problem originally introduced by Szegedy et al. \\cite{szegedy2013intriguing}.","cats":{"programming":0}}
{"text":"As a further advantage, the JMA attack usually requires very few iterations, thus resulting more efficient than existing methods.","cats":{"programming":0}}
{"text":"This research idea paper proposes leveraging Large Language Models (LLMs) to enhance the productivity of Dafny developers.","cats":{"programming":1}}
{"text":"Often the cost of using such languages is too high, due to the level of expertise required from the developers and challenges that they often face when trying to prove a program correct.","cats":{"programming":0}}
{"text":"The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software.","cats":{"programming":0}}
{"text":"Automating software development processes through the orchestration of GitHub Action workflows has revolutionized the efficiency and agility of software delivery pipelines.","cats":{"programming":0}}
{"text":"This paper presents a detailed investigation into the use of Large Language Models (LLMs) specifically, GPT 3.5 and GPT 4 to generate and evaluate GitHub Action workflows for DevOps tasks.","cats":{"programming":1}}
{"text":"In this paper, we present an extensive study on the code switching task specifically for the machine translation task comparing multiple LLMs.","cats":{"programming":0}}
{"text":"We posit that the efficacy of multilingual large language models in contextual code switching is constrained by their training methodologies.","cats":{"programming":1}}
{"text":"Code generation stands as a powerful technique in modern software development, improving development efficiency, reducing errors, and fostering standardization and consistency.","cats":{"programming":1}}
{"text":"Recently, ChatGPT has exhibited immense potential in automatic code generation.","cats":{"programming":1}}
{"text":"However, existing researches on code generation lack guidance for practical software development process.","cats":{"programming":1}}
{"text":"In this study, we utilized ChatGPT to develop a web-based code generation platform consisting of key components: User Interface, Prompt Builder and Backend Service.","cats":{"programming":1}}
{"text":"We propose an approach that utilizes large language models for model completion and discovering editing patterns in model histories of software systems.","cats":{"programming":1}}
{"text":"While instructions fine-tuning of large language models (LLMs) has been proven to enhance performance across various applications, the influence of the instruction dataset mixture on LLMs has not been thoroughly explored.","cats":{"programming":0}}
{"text":"This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code.","cats":{"programming":1}}
{"text":"Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded dataset for its fine-tuning process.","cats":{"programming":1}}
{"text":"The results of its performance, a notable score of 56.33% on the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity to streamline developers' workflows and aid code learning.","cats":{"programming":1}}
{"text":"It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclusive and wide-ranging development in the field of language-specific LLMs.","cats":{"programming":1}}
{"text":" Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded datas to streamline developers' workflows and aid code learning.","cats":{"programming":0}}
{"text":"It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclu","cats":{"programming":0}}
{"text":"A large-scale study released that writing programs requires programming thinking, i.e., analyzing and implementing requirements in programming logic (e.g., sequence, branch, loop).","cats":{"programming":1}}
{"text":"Existing studies use LLMs to generate programs from requirements directly and do not explicitly introduce the programming thinking.","cats":{"programming":1}}
{"text":"This paper explores how to unlock the programming thinking of LLMs in code generation and proposes an approach named TiP.","cats":{"programming":1}}
{"text":"Our idea is to decompose code generation into two steps and progressively lead LLMs to analyze&implement requirements in programming logic.","cats":{"programming":1}}
{"text":"Specifically, TiP first generates a code sketch, which provides a high-level solving process using programming logic but omits implementation details (e.g., APIs).","cats":{"programming":1}}
{"text":" A large-scale study released that writing programs requires programming thinking, i.e., analyzing ntroduce the programming thinking.   ","cats":{"programming":0}}
{"text":"This paper explores how to unlock the programming thinking of LLMs in code generation and proposes an approach named TiP. Our idea is to decompo TiP first generates a code sketch, which provides a high-level solving process using programming logic but omits implementation detaie experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP).","cats":{"programming":0}}
{"text":"(1) TiP outperforms the state-of-the-art basethree aspects (i.e., correctness, code quality, and maintainability).","cats":{"programming":0}}
{"text":"(3) TiP is effective for different LLMs.","cats":{"programming":0}}
{"text":"(4) We explore multiple choicoaches (e.g., CodeT).","cats":{"programming":0}}
{"text":"Using large language models (LLMs) for source code has recently gained attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems.","cats":{"programming":1}}
{"text":"However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet.","cats":{"programming":1}}
{"text":"To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems.","cats":{"programming":1}}
{"text":"Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.","cats":{"programming":1}}
{"text":"However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems.","cats":{"programming":1}}
{"text":"This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations.","cats":{"programming":1}}
{"text":" However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yetr experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.","cats":{"programming":0}}
{"text":"Furthermore, we observe that Codex relies on variable naability for solving programming problems.","cats":{"programming":0}}
{"text":"This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for hi","cats":{"programming":0}}
{"text":"While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely.","cats":{"programming":0}}
{"text":"The paper investigates the potential applications of LLMs and ChatGPT in various areas, such as code creation, code documentation, bug detection, refactoring, and more.","cats":{"programming":1}}
{"text":"The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community.","cats":{"programming":1}}
{"text":" LLMs and ChatGPT are built using machine learning and artificial intelligence techniques, and they offer several benefits to developers and programmers.","cats":{"programming":0}}
{"text":"Thn, refactoring, and more.","cats":{"programming":0}}
{"text":"The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unpar","cats":{"programming":0}}
{"text":"Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al., 2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more conversational interaction.","cats":{"programming":1}}
{"text":"By capitalizing on the progress in data engineering (including latest techniques of data transformation and prompt engineering) and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments.","cats":{"programming":1}}
{"text":"Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and WizardCoder by 18.78% in the Code Readability parameter and more.","cats":{"programming":1}}
{"text":" Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments.","cats":{"programming":0}}
{"text":"Recognizing the need for an efficacious evaluation metric for coding LLMs, this paper also introduces GPT4-based MultiParameters, an evaluation arameter and more.","cats":{"programming":0}}
{"text":"Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning.","cats":{"programming":1}}
{"text":"Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus.","cats":{"programming":1}}
{"text":"However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering.","cats":{"programming":1}}
{"text":"Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks?","cats":{"programming":1}}
{"text":" Derivative products, like ChatGPT, have been extensively deployed and highly sought after.","cats":{"programming":0}}
{"text":"Meanwhile, the evaluation and optimization of LLMs in software engis the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering?","cats":{"programming":0}}
{"text":"as possible from seven mainstream databases, and selected 123 papers for analysis.","cats":{"programming":0}}
{"text":"We have categorized these papers in detail and reverformance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.","cats":{"programming":0}}
{"text":"The ease of using a Large Language Model (LLM) to answer a wide variety of queries and their high availability has resulted in LLMs getting integrated into various applications. LLM-based recommenders are now routinely used by students as well as professional software programmers for code generation and testing.","cats":{"programming":1}}
{"text":"As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.","cats":{"programming":1}}
{"text":"In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put by students in writing source codes.","cats":{"programming":1}}
{"text":" Though LLM-based technology has proven useful, its unethical and unattributed use by students and professionals is a growing cause of concern.","cats":{"programming":0}}
{"text":"As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.   ","cats":{"programming":0}}
{"text":"In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put b (up to 500 lines of code) have shown promising results that we report in this paper.","cats":{"programming":0}}
