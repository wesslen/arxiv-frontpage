{"text":"The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning.","cats":{"prompt-engineering":0,"social-sciences":0,"programming":0}}
{"text":"As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and Codex, now enable software developers to generate code based on a natural language prompt.","cats":{"prompt-engineering":1,"programming":1}}
{"text":"Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafted prompts.","cats":{"prompt-engineering":1,"education":1,"programming":1}}
{"text":"These advances may enable students to interact with code in new ways while helping instructors scale their learning materials.","cats":{"prompt-engineering":1,"education":0,"programming":1}}
{"text":" Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafmplications for academic integrity, curriculum design, and software engineering careers.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"This workshop will demonstrate the capabilities of LLMs to help attendees evaluate wheider how LLMs will impact our field.","cats":{"prompt-engineering":0,"programming":0}}
{"text":"We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency.","cats":{"robustness":0,"programming":0}}
{"text":"Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors.","cats":{"robustness":1,"programming":0}}
{"text":"Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches.","cats":{"robustness":0,"programming":0}}
{"text":"Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts.","cats":{"security":0,"programming":0}}
{"text":"Large Language Models~(LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages.","cats":{"hci":0,"programming":0}}
{"text":"Large language models have been demonstrated to be valuable in different fields.","cats":{"hci":0,"social-sciences":0,"programming":0}}
{"text":"Fourth, we show that our technique can likely generalize to languages with smaller user bases by simulating our statistical inferences on small N.","cats":{"social-sciences":0,"programming":0}}
{"text":"Recently, Large Language Models (LLMs) have achieved impressive performance in synthesizing computer programs based on a natural language prompt, mitigating the gap between natural language and structured programs.","cats":{"social-sciences":0,"programming":1}}
{"text":"Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.","cats":{"recommender":0,"programming":0}}
{"text":"Large Language Models for Code (Code LLM) are flourishing.","cats":{"programming":1}}
{"text":"Recently, large language models for code generation have achieved breakthroughs in several programming language tasks.","cats":{"programming":1}}
{"text":"The recent development on large language models makes automatically constructing small programs possible.","cats":{"programming":1}}
{"text":"Using large language models (LLMs) for source code has recently gained attention.","cats":{"programming":1}}
{"text":"Large language models trained on source code can support a variety of software development tasks, such as code recommendation and program repair.","cats":{"programming":1}}
{"text":"In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models.","cats":{"programming":1}}
{"text":"In this paper, we explore the potential of large language models in supporting the evolution of software models in software engineering.","cats":{"programming":1}}
{"text":"We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters.","cats":{"programming":1}}
{"text":"Large language models (LLMs) have achieved impressive performance on code generation.","cats":{"programming":1}}
{"text":"Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation.","cats":{"programming":1}}
{"text":"Large language models (LLMs) have recently demonstrated surprising performance for a range of computing tasks, including generating and explaining code.","cats":{"programming":1}}
{"text":"Large Language Models (LLM) are a new class of computation engines, \"programmed\" via prompt engineering.","cats":{"programming":1}}
{"text":"Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data.","cats":{"programming":1}}
{"text":"Emerging Large Language Models (LLMs) can answer questions, write high-quality programming code, and predict protein folding, showcasing their versatility in solving various tasks beyond language-based problems.","cats":{"programming":1}}
{"text":"Despite researchers' and practitioners' attempts to enhance code generation models, retraining and fine-tuning large language models is time-consuming and resource-intensive.","cats":{"programming":1}}
{"text":"This paper explores the use of Large Language Models (LLMs) and in particular ChatGPT in programming, source code analysis, and code generation.","cats":{"programming":1}}
{"text":"Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks.","cats":{"programming":1}}
{"text":"Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks.","cats":{"programming":1}}
{"text":"Recent development of large language models (LLMs) for code like CodeX and CodeT5+ demonstrates tremendous promise in achieving code intelligence.","cats":{"programming":1}}
{"text":"Large language models with instruction-following capabilities open the door to a wider group of users.","cats":{"programming":1}}
{"text":"Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges.","cats":{"programming":1}}
{"text":"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.","cats":{"programming":1}}
{"text":"Meanwhile, large language models, represented by ChatGPT, have gained great attentions, showcasing great capabilities in code analysis tasks.","cats":{"programming":1}}
{"text":"In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs).","cats":{"programming":1}}
{"text":"Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models.","cats":{"programming":1}}
{"text":"This paper presents Safurai-001, a new Large Language Model (LLM) with significant potential in the domain of coding assistance.","cats":{"programming":1}}
{"text":"Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data.","cats":{"programming":0}}
{"text":"This has been a particular concern in the era of large language models (LLMs)- based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code.","cats":{"programming":1}}
{"text":"This challenge has become even more pressing with the rise of large language model (LLM) based code generation tools.","cats":{"programming":1}}
{"text":"Recent advancements in Large Language Models (LLMs) have enhanced their ability to comprehend natural and programming languages, enabling them to generate patches based on review comments.","cats":{"programming":1}}
{"text":"We have found that large language models are indeed a promising technology for supporting software model evolution, and that it is worth investigating further in the area of software model evolution.","cats":{"programming":1}}
{"text":"The ubiquitous adoption of Large Language Generation Models (LLMs) in programming has underscored the importance of differentiating between human-written code and code generated by intelligent models.","cats":{"programming":1}}
{"text":"In this paper we present 3 types of results:   (i) Theorems on the uniqueness of commuting extensions for three matrices or more.   ","cats":{"programming":0}}
{"text":"They are applicable up to r=4n/3, and are apparently the first provably efficient algorithms for this problem applicable beyond r=n+1.   ","cats":{"programming":0}}
{"text":"In this paper, we take a major step towards filling this gap by proposing GEqO, a portable and lightweight machine-learning-based framework for efficiently identifying semantically equivalent computations at scale.","cats":{"programming":0}}
{"text":"To address this, we propose a multilevel generative semantic communication system with a two-stage training framework.","cats":{"programming":0}}
{"text":"Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods.","cats":{"programming":0}}
{"text":"We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences.","cats":{"programming":0}}
{"text":"Then, we numerically test the set of proposed objective functions in three application scenarios: toy examples, image data sets, and signal detection/decoding problems.","cats":{"programming":0}}
{"text":"In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area.","cats":{"programming":0}}
{"text":"Finally, VideoDrafter outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account.","cats":{"programming":0}}
{"text":"Empirical results show the superiority of MofitRGC.","cats":{"programming":0}}
{"text":"We were also able to achieve a testing accuracy of 88.03% in the face-shape problem using the celebrity face-shape dataset[3].","cats":{"programming":0}}
{"text":"We explore task-relatedness for co-annotation and co-training, and propose a novel approach, where knowledge exchange is enabled between the tasks via distribution matching.","cats":{"programming":0}}
{"text":"The proposed algorithm provides an optimal solution to a linearized version of the adversarial example problem originally introduced by Szegedy et al. \\cite{szegedy2013intriguing}.","cats":{"programming":0}}
{"text":"As a further advantage, the JMA attack usually requires very few iterations, thus resulting more efficient than existing methods.","cats":{"programming":0}}
{"text":"Thus, this approach may provide a valuable component for future misinformation mitigation pipelines.","cats":{"programming":0}}
{"text":"This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code.","cats":{"programming":1}}
{"text":"Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded dataset for its fine-tuning process.","cats":{"programming":1}}
{"text":"The results of its performance, a notable score of 56.33% on the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity to streamline developers' workflows and aid code learning.","cats":{"programming":1}}
{"text":"It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclusive and wide-ranging development in the field of language-specific LLMs.","cats":{"programming":1}}
{"text":" Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded datas to streamline developers' workflows and aid code learning.","cats":{"programming":0}}
{"text":"It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclu","cats":{"programming":0}}
{"text":"A large-scale study released that writing programs requires programming thinking, i.e., analyzing and implementing requirements in programming logic (e.g., sequence, branch, loop).","cats":{"programming":1}}
{"text":"Existing studies use LLMs to generate programs from requirements directly and do not explicitly introduce the programming thinking.","cats":{"programming":1}}
{"text":"This paper explores how to unlock the programming thinking of LLMs in code generation and proposes an approach named TiP.","cats":{"programming":1}}
{"text":"Our idea is to decompose code generation into two steps and progressively lead LLMs to analyze&implement requirements in programming logic.","cats":{"programming":1}}
{"text":"Specifically, TiP first generates a code sketch, which provides a high-level solving process using programming logic but omits implementation details (e.g., APIs).","cats":{"programming":1}}
{"text":" A large-scale study released that writing programs requires programming thinking, i.e., analyzing ntroduce the programming thinking.   ","cats":{"programming":0}}
{"text":"This paper explores how to unlock the programming thinking of LLMs in code generation and proposes an approach named TiP. Our idea is to decompo TiP first generates a code sketch, which provides a high-level solving process using programming logic but omits implementation detaie experiments on three public benchmarks (i.e., HumanEval, MBPP, and MBCPP).","cats":{"programming":0}}
{"text":"(1) TiP outperforms the state-of-the-art basethree aspects (i.e., correctness, code quality, and maintainability).","cats":{"programming":0}}
{"text":"(3) TiP is effective for different LLMs.","cats":{"programming":0}}
{"text":"(4) We explore multiple choicoaches (e.g., CodeT).","cats":{"programming":0}}
{"text":"Using large language models (LLMs) for source code has recently gained attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems.","cats":{"programming":1}}
{"text":"However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet.","cats":{"programming":1}}
{"text":"To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems.","cats":{"programming":1}}
{"text":"Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.","cats":{"programming":1}}
{"text":"However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems.","cats":{"programming":1}}
{"text":"This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations.","cats":{"programming":1}}
{"text":" However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yetr experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance.","cats":{"programming":0}}
{"text":"Furthermore, we observe that Codex relies on variable naability for solving programming problems.","cats":{"programming":0}}
{"text":"This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for hi","cats":{"programming":0}}
{"text":"While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely.","cats":{"programming":0}}
{"text":"The paper investigates the potential applications of LLMs and ChatGPT in various areas, such as code creation, code documentation, bug detection, refactoring, and more.","cats":{"programming":1}}
{"text":"The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community.","cats":{"programming":1}}
{"text":" LLMs and ChatGPT are built using machine learning and artificial intelligence techniques, and they offer several benefits to developers and programmers.","cats":{"programming":0}}
{"text":"Thn, refactoring, and more.","cats":{"programming":0}}
{"text":"The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unpar","cats":{"programming":0}}
{"text":"Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al., 2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more conversational interaction.","cats":{"programming":1}}
{"text":"By capitalizing on the progress in data engineering (including latest techniques of data transformation and prompt engineering) and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments.","cats":{"programming":1}}
{"text":"Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and WizardCoder by 18.78% in the Code Readability parameter and more.","cats":{"programming":1}}
{"text":" Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments.","cats":{"programming":0}}
{"text":"Recognizing the need for an efficacious evaluation metric for coding LLMs, this paper also introduces GPT4-based MultiParameters, an evaluation arameter and more.","cats":{"programming":0}}
{"text":"Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in tasks such as text generation and reasoning.","cats":{"programming":1}}
{"text":"Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus.","cats":{"programming":1}}
{"text":"However, there is still a lack of systematic research on the application and evaluation of LLMs in the field of software engineering.","cats":{"programming":1}}
{"text":"Therefore, this paper is the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks?","cats":{"programming":1}}
{"text":"We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs.","cats":{"programming":1}}
{"text":"Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.","cats":{"programming":1}}
{"text":" Derivative products, like ChatGPT, have been extensively deployed and highly sought after.","cats":{"programming":0}}
{"text":"Meanwhile, the evaluation and optimization of LLMs in software engis the first to comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering?","cats":{"programming":0}}
{"text":"as possible from seven mainstream databases, and selected 123 papers for analysis.","cats":{"programming":0}}
{"text":"We have categorized these papers in detail and reverformance and effectiveness of LLMs in various software engineering tasks, providing guidance for researchers and developers to optimize.","cats":{"programming":0}}
{"text":"The ease of using a Large Language Model (LLM) to answer a wide variety of queries and their high availability has resulted in LLMs getting integrated into various applications. LLM-based recommenders are now routinely used by students as well as professional software programmers for code generation and testing.","cats":{"programming":1}}
{"text":"As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.","cats":{"programming":1}}
{"text":"In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put by students in writing source codes.","cats":{"programming":1}}
{"text":" Though LLM-based technology has proven useful, its unethical and unattributed use by students and professionals is a growing cause of concern.","cats":{"programming":0}}
{"text":"As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.   ","cats":{"programming":0}}
{"text":"In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put b (up to 500 lines of code) have shown promising results that we report in this paper.","cats":{"programming":0}}
