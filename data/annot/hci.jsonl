{"text":"As far as we know, our work is the first to create prompts based on testing results to improve LLMs' formal reasoning ability effectively.","cats":{"prompt-engineering":1,"hci":0,"programming":0}}
{"text":"Our survey findings provide a basis for developing the concept of linear and non-linear contexts, which we define and use to enable an agent-centric projection of prompting techniques providing a lens through which prompting techniques can be viewed as multi-agent systems.","cats":{"prompt-engineering":1,"hci":1}}
{"text":"The paper discusses the implications of this lens, for the cross-pollination of research between LLM prompting and LLM-based multi-agent systems; and also, for the generation of synthetic training data based on existing prompting techniques in research.","cats":{"prompt-engineering":1,"hci":0}}
{"text":"Current prompting approach for language model inference mainly rely on Language Model's (LLM) autonomous exploration of reasoning paths, confronts an inevitable retracing operation when erroneous routes are encountered.","cats":{"prompt-engineering":1,"hci":0}}
{"text":"In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0,"programming":0}}
{"text":"In this paper, we report on the first year of TREC iKAT, describing the task, topics, data collection, and evaluation framework.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Code is available at https://github.com/urchade/ATG.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"The two levels are computed by the original model's self-attention, which means the proposed does not require any training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"In this paper, we consider classes of decision tables closed under removal of attributes (columns) and changing of decisions attached to rows.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"For decision tables from closed classes, we study lower bounds on the minimum cardinality of reducts, which are minimal sets of attributes that allow us to recognize, for a given row, the decision attached to it.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0}}
{"text":"This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0}}
{"text":"The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0}}
{"text":"Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023).","cats":{"prompt-engineering":1,"robustness":0,"security":0,"hci":0}}
{"text":"Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"The framework employs multiple LLM agents, each with a distinct persona, engaged in role-playing communication, offering a nuanced and adaptable approach to diverse problem scenarios.","cats":{"prompt-engineering":0,"security":0,"hci":1}}
{"text":"Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"recommender":0,"programming":0}}
{"text":"In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \\textit{span-based}.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"The basic idea is to construct bi-level attention information: the group level and the neighbor level.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0,"programming":0}}
{"text":"As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input.","cats":{"prompt-engineering":1,"robustness":1,"security":0,"hci":0,"social-sciences":1,"recommender":0}}
{"text":"This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc.","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Large Language Models (LLMs) have revolutionized Natural Language Processing but exhibit limitations, particularly in autonomously addressing novel challenges such as reasoning and problem-solving.","cats":{"prompt-engineering":1,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Traditional techniques like chain-of-thought prompting necessitate explicit human guidance.","cats":{"prompt-engineering":1,"security":0,"hci":0}}
{"text":"Generative AI has the potential to transform how public services are delivered by enhancing productivity and reducing time spent on bureaucracy.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"We find that use of generative AI systems is already widespread: 45% of respondents were aware of generative AI usage within their area of work, while 22% actively use a generative AI system.","cats":{"prompt-engineering":0,"security":0,"hci":1,"social-sciences":0}}
{"text":"Public sector professionals were positive about both current use of the technology and its potential to enhance their efficiency and reduce bureaucratic workload in the future.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"Our survey also found a high amount of trust (61%) around generative AI outputs, and a low fear of replacement (16%).","cats":{"prompt-engineering":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"While respondents were optimistic overall, areas of concern included feeling like the UK is missing out on opportunities to use AI to improve public services (76%), and only a minority of respondents (32%) felt like there was clear guidance on generative AI usage in their workplaces.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"In other words, it is clear that generative AI is already transforming the public sector, but uptake is happening in a disorganised fashion without clear guidelines.","cats":{"prompt-engineering":0,"security":0,"hci":0}}
{"text":"Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"architectures":0}}
{"text":"However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization.","cats":{"prompt-engineering":0,"robustness":0,"hci":0}}
{"text":"However, it is important to recognize that an excessive reliance on this functionality raises the risk of manipulation by malicious actors who can exploit it with carefully crafted prompts.","cats":{"prompt-engineering":1,"hci":0,"social-sciences":0}}
{"text":"The emergence of LLM (Large Language Model) integrated virtual assistants has brought about a rapid transformation in communication dynamics.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"Such malicious manipulation poses a significant threat, potentially compromising the accuracy and reliability of the virtual assistant's responses.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"Consequently, safeguarding the virtual assistants with detection and defense mechanisms becomes of paramount importance to ensure their safety and integrity.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"In this study, we explored three detection and defense mechanisms aimed at countering attacks that target the system message.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"These mechanisms include inserting a reference key, utilizing an LLM evaluator, and implementing a Self-Reminder.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"To showcase the efficacy of these mechanisms, they were tested against prominent attack techniques.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"Our findings demonstrate that the investigated mechanisms are capable of accurately identifying and counteracting the attacks.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"The effectiveness of these mechanisms underscores their potential in safeguarding the integrity and reliability of virtual assistants, reinforcing the importance of their implementation in real-world scenarios.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"By prioritizing the security of virtual assistants, organizations can maintain user trust, preserve the integrity of the application, and uphold the high standards expected in this era of transformative technologies.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"This is followed by the pursuit of alternative reasoning paths.","cats":{"prompt-engineering":0,"hci":0}}
{"text":"In light of this, we delves into the potential of harnessing expert knowledge to enhance problem-solving within LLMs.","cats":{"prompt-engineering":1,"hci":1}}
{"text":"We introduce a novel paradigm, the State Machine of Thought (SMoT), which employs predefined state machines to furnish LLMs with efficient reasoning paths, thereby eliminating fruitless exploration.","cats":{"prompt-engineering":1,"hci":0}}
{"text":"Furthermore, we propose a multi-agent mechanism that assigns different objectives to agents, aiming to enhance the accuracy of SMoT reasoning.","cats":{"prompt-engineering":1,"hci":0}}
{"text":"The experimental results, derived from an array reasoning task, reveal that SMoT realizes an extraordinary accuracy of 95\\%, surpassing the performance of the state-of-the-art baselines.","cats":{"prompt-engineering":1,"hci":0}}
{"text":"The results indicate the need for careful validation and tailored prompt engineering.","cats":{"robustness":0,"hci":0,"social-sciences":0}}
{"text":"We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency.","cats":{"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area.","cats":{"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"(2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases.","cats":{"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"(4) We provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations.","cats":{"robustness":1,"hci":0,"social-sciences":0,"education":0}}
{"text":"Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.","cats":{"robustness":0,"hci":0}}
{"text":"The outcomes of our study have significant implications for ensuring the ethical use of copyrighted materials in the development of LLMs, highlighting the need for more transparent and responsible data management practices in this field.","cats":{"robustness":1,"hci":0}}
{"text":"Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users.","cats":{"robustness":0,"hci":1,"social-sciences":0}}
{"text":"However, the absence of a comprehensive benchmark impedes progress in this field.","cats":{"robustness":0,"hci":0,"social-sciences":0}}
{"text":"To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset.","cats":{"robustness":0,"hci":0}}
{"text":"The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts.","cats":{"robustness":0,"hci":0}}
{"text":"It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike.","cats":{"robustness":0,"hci":0,"architectures":0}}
{"text":"CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions.","cats":{"robustness":0,"hci":0,"education":0}}
{"text":"Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.","cats":{"robustness":0,"hci":0,"social-sciences":0,"architectures":0}}
{"text":"Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.","cats":{"robustness":0,"hci":0}}
{"text":"The emergence of ChatGPT and other large language models (LLMs) has greatly increased interest in utilizing LLMs as therapists to support individuals struggling with mental health challenges.","cats":{"robustness":0,"hci":1,"social-sciences":1}}
{"text":"Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences.","cats":{"robustness":0,"hci":0}}
{"text":"Our analysis of GPT and Llama-variants reveals that these LLMs often resemble behaviors more commonly exhibited in low-quality therapy rather than high-quality therapy, such as offering a higher degree of problem-solving advice when clients share emotions, which is against typical recommendations.","cats":{"robustness":0,"hci":1,"recommender":0}}
{"text":"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code).","cats":{"robustness":0,"hci":0,"social-sciences":0,"education":0,"programming":1}}
{"text":"As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity.","cats":{"robustness":0,"hci":0,"programming":0}}
{"text":"In this survey, we present an overview of the various benefits of integrating code into LLMs' training data.","cats":{"robustness":0,"hci":0,"programming":1}}
{"text":"Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement.","cats":{"robustness":0,"hci":0,"programming":1}}
{"text":"In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks.","cats":{"robustness":0,"hci":0,"recommender":0}}
{"text":"Finally, we present several key challenges and future directions of empowering LLMs with code.","cats":{"robustness":0,"hci":0,"programming":0}}
{"text":"This study focuses on emotion-sensitive spoken dialogue in human-machine speech interaction.","cats":{"robustness":0,"hci":1}}
{"text":"Recent models have enhanced the understanding of complex audio signals through the integration of various audio events.","cats":{"robustness":0,"hci":0}}
{"text":"However, they are unable to generate appropriate responses based on emotional speech.","cats":{"robustness":0,"hci":0}}
{"text":"To address this, we introduce the Emotional chat Model (E-chat), a novel spoken dialogue system capable of comprehending and responding to emotions conveyed from speech.","cats":{"robustness":0,"hci":0}}
{"text":"This model leverages an emotion embedding extracted by a speech encoder, combined with LLMs, enabling it to respond according to different emotional contexts.","cats":{"robustness":0,"hci":0}}
{"text":"Additionally, we introduce the E-chat200 dataset, designed explicitly for emotion-sensitive spoken dialogue.","cats":{"robustness":0,"hci":0}}
{"text":"In various evaluation metrics, E-chat consistently outperforms baseline LLMs, demonstrating its potential in emotional comprehension and human-machine interaction.","cats":{"robustness":0,"hci":1}}
{"text":"The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity.","cats":{"robustness":1,"hci":0}}
{"text":"Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities.","cats":{"robustness":0,"hci":0}}
{"text":"The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity.","cats":{"robustness":0,"hci":0}}
{"text":"Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations.","cats":{"robustness":0,"hci":0}}
{"text":"Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning).","cats":{"robustness":0,"hci":0}}
{"text":"We further contrast such approach against supervised fine-tuning (SFT) of encoder-only LLMs.","cats":{"robustness":0,"hci":0}}
{"text":"Our results reveal that both the direct-use of decoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-only LLMs offer inaccurate interpretation of cyberattack procedures.","cats":{"robustness":0,"hci":0}}
{"text":"Significant improvements are shown when RAG is used for decoder-only LLMs, particularly when directly relevant context is found.","cats":{"robustness":0,"hci":0}}
{"text":"This study further sheds insights on the limitations and capabilities of using RAG for LLMs in interpreting TTPs.","cats":{"robustness":0,"hci":0}}
{"text":"Through a difference-in-differences approach, we find that deplatforming reduces online attention toward influencers.","cats":{"security":0,"hci":0,"social-sciences":1}}
{"text":"Extensive experimentation demonstrates the framework's superior performance and adaptability, providing valuable insights into the collaborative potential of multiple agents in overcoming the limitations of individual models.","cats":{"security":0,"hci":0,"architectures":0}}
{"text":"Importantly, the IMU data remains uncompromised throughout the spoofing attack.","cats":{"security":0,"hci":0}}
{"text":"Commuting extensions have also attracted the attention of the linear algebra community.","cats":{"security":0,"hci":0,"recommender":0}}
{"text":"Taken together, these findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks.","cats":{"security":0,"hci":0,"social-sciences":0}}
{"text":"Furthermore, unlike other types of artificial intelligence, it is a technology that has quickly become widely available for bottom-up adoption: essentially anyone can decide to make use of it in their day to day work.","cats":{"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"But to what extent is generative AI already in use in the public sector?","cats":{"security":0,"hci":0,"social-sciences":0}}
{"text":"For example, those working in the NHS thought that time spent on bureaucracy could drop from 50% to 30% if generative AI was properly exploited, an equivalent of one day per week (an enormous potential impact).","cats":{"security":0,"hci":0,"social-sciences":1}}
{"text":"This strategy extends directly to the case where the searcher has no knowledge of the terrain beforehand.","cats":{"security":0,"hci":0}}
{"text":"Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting.","cats":{"security":0,"hci":0}}
{"text":"However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones.","cats":{"security":1,"hci":0,"programming":0}}
{"text":"Large Language Models (LLMs) have made remarkable advancements in the field of artificial intelligence, significantly reshaping the human-computer interaction.","cats":{"hci":1}}
{"text":"Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria.","cats":{"hci":0,"education":0}}
{"text":"Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines.","cats":{"hci":0}}
{"text":"Recently, large language models (LLMs) have demonstrated impressive capabilities across a wide range of tasks.","cats":{"hci":0}}
{"text":"Large language models (LLMs) have shown the potential to be integrated into human daily lives.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools.","cats":{"hci":0}}
{"text":"Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences.","cats":{"hci":0,"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have shown impressive success in various applications.","cats":{"hci":0,"education":0,"recommender":0,"production":0}}
{"text":"Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness.","cats":{"hci":0}}
{"text":"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT.","cats":{"hci":0}}
{"text":"The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation.","cats":{"hci":0,"social-sciences":0,"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks.","cats":{"hci":0,"social-sciences":0}}
{"text":"Through a series of simulation experiments conducted on corn crops in Iowa, we compare Partially Observable Markov Decision Process (POMDP) models with Markov Decision Process (MDP) models.","cats":{"hci":0}}
{"text":"In the second stage, a noise reduction block (NRB) based on Swin Transformer is introduced, coupled with the multilevel codebook from the first stage, serving as a high-quality semantic knowledge base (SKB) for generative feature restoration.","cats":{"hci":0}}
{"text":"Besides, ZOE achieves this by sacrificing only a small performance on biased samples, which is simple and effective.","cats":{"hci":0,"social-sciences":0}}
{"text":"The KCES includes a cloud-edge workflow scheduling engine for KubeEdge and workflow scheduling strategies for task horizontal roaming and vertical offloading.","cats":{"hci":0}}
{"text":"The hybrid attention mechanism comprises a proposed enhanced multi-head self-attention and coordinate attention mechanisms increase the ability of attention networks to perceive contextual information and enhances the utilization range of network features.","cats":{"hci":0}}
{"text":"To address these issues, we propose an end-to-end model called Fine-Grained Extraction Network (FGENet).","cats":{"hci":0}}
{"text":"The fused features are then passed to both regression and classification heads, where the former provides predicted point coordinates for a given image, and the latter determines the confidence level for each predicted point being an individual.","cats":{"hci":0}}
{"text":"Misinformation poses a variety of risks, such as undermining public trust and distorting factual discourse.","cats":{"hci":0,"social-sciences":0}}
{"text":"Within this work, we propose a hybrid approach, Deep-ELA, which combines (the benefits of) deep learning and ELA features.","cats":{"hci":0,"recommender":0}}
{"text":"To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer.","cats":{"hci":0}}
{"text":"In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account.","cats":{"hci":0}}
{"text":"Experimental results show that our framework significantly outperforms state-of-the-art methods on zero-shot task by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.","cats":{"hci":0}}
{"text":"Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns.","cats":{"hci":0}}
{"text":"Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces.","cats":{"hci":0}}
{"text":"With the recent introduction of large language models (LLMs), which can generate text in response to a natural language prompt, there are new opportunities to consider how to integrate LLMs into UIs.","cats":{"hci":1}}
{"text":"Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks.","cats":{"hci":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code.","cats":{"hci":0}}
{"text":"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions.","cats":{"hci":0}}
{"text":"Large language models (LLMs) are used to generate content for a wide range of tasks, and are set to reach a growing audience in coming years due to integration in product interfaces like ChatGPT or search engines like Bing.","cats":{"hci":0}}
{"text":"Pretrained large language models (LLMs) are becoming increasingly powerful and ubiquitous in mainstream applications such as being a personal assistant, a dialogue model, etc.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like reasoning and long-context understanding.","cats":{"hci":0,"education":0}}
{"text":"The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario.","cats":{"hci":0,"social-sciences":0,"education":0}}
{"text":"Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this.","cats":{"hci":0}}
{"text":"Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT.","cats":{"hci":0}}
{"text":"We hope that this work provides a better guide for researchers working on the prompting of large language models.","cats":{"hci":0,"social-sciences":0}}
{"text":"We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges.","cats":{"hci":1}}
{"text":"Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems.","cats":{"hci":0}}
{"text":"We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content.","cats":{"hci":0}}
{"text":"As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern.","cats":{"hci":0}}
{"text":"Recently, the evaluation of Large Language Models has emerged as a popular area of research.","cats":{"hci":0}}
{"text":"Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI.","cats":{"hci":0,"social-sciences":0}}
{"text":"Massive pre-trained language models have garnered attention and controversy due to their ability to generate human-like responses: attention due to their frequent indistinguishability from human-generated phraseology and narratives, and controversy due to the fact that their convincingly presented arguments and facts are frequently simply false.","cats":{"hci":1}}
{"text":"The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model.","cats":{"hci":0,"social-sciences":0}}
{"text":"In light of these findings, we conclude by proposing potential research trajectories to further enhance the development and performance of Large Language Models in their current stage.","cats":{"hci":0}}
{"text":"We further outline the challenges of estimating the confidence for large language models and we suggest some promising directions for future work.","cats":{"hci":0}}
{"text":"In this work, we revisit this topic in the context of large language models.","cats":{"hci":0,"production":0}}
{"text":"Effectively using a given context is paramount for large language models.","cats":{"hci":0}}
{"text":"We present VOICE, a novel approach for connecting large language models' (LLM) conversational capabilities with interactive exploratory visualization.","cats":{"hci":1}}
{"text":"Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data.","cats":{"hci":1}}
{"text":"With recent advances in multi-modal foundation models, the previously text-only large language models (LLM) have evolved to incorporate visual input, opening up unprecedented opportunities for various applications in visualization.","cats":{"hci":1}}
{"text":"Our work explores the capabilities of LLMs for creating and refining visualizations via conversational interfaces.","cats":{"hci":1}}
{"text":"They endow Large Language Models (LLMs) with powerful capabilities in visual understanding, enabling them to tackle diverse multi-modal tasks.","cats":{"hci":1}}
{"text":"Multi-modal large language models (LLM) have achieved powerful capabilities for visual semantic understanding in recent years.","cats":{"hci":0}}
{"text":"Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified.","cats":{"hci":1}}
{"text":"This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature.","cats":{"hci":1}}
{"text":"Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction.","cats":{"hci":1,"social-sciences":1}}
{"text":"These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks.","cats":{"hci":0}}
{"text":"Generative Large Language Models (LLMs) show potential in data analysis, yet their full capabilities remain uncharted.","cats":{"hci":0}}
{"text":"Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area.","cats":{"hci":0}}
{"text":"In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better.","cats":{"hci":0}}
{"text":"Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations.","cats":{"hci":1}}
{"text":"The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics.","cats":{"hci":0,"social-sciences":0}}
{"text":"(3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup.","cats":{"hci":0,"social-sciences":0}}
{"text":"The data from the class suggests that the students found the inclusion of the social context in the technical assignments to be more motivating and expressed greater agency in realizing social change.","cats":{"hci":0,"education":0}}
{"text":"We address these limitations in a longitudinal, quasi-experimental study of 165 deplatforming events targeted at 101 influencers.","cats":{"hci":0}}
{"text":"Previous inquiries on the effectiveness of this intervention are inconclusive because 1) they consider only few deplatforming events; 2) they consider only overt engagement traces (e.g., likes and posts) but not passive engagement (e.g., views); 3) they do not consider all the potential places users impacted by the deplatforming event might migrate to.","cats":{"hci":0,"social-sciences":1}}
{"text":"Further, as we study over a hundred deplatforming events, we can analyze in which cases deplatforming is more or less impactful, revealing nuances about the intervention.","cats":{"hci":0,"architectures":0}}
{"text":"Notably, we find that both permanent and temporary deplatforming reduce online attention toward influencers; Overall, this work contributes to the ongoing effort to map the effectiveness of content moderation interventions, driving platform governance away from speculation.","cats":{"hci":0,"social-sciences":0}}
{"text":"These digital companions often lack memory of past interactions, while relying heavily on the internet for speech processing, raising privacy concerns.","cats":{"hci":0}}
{"text":"Despite this, they are prone to generating factual and commonsense errors, raising concerns in critical areas like healthcare, journalism, and education to mislead users.","cats":{"hci":0}}
{"text":"A significant distinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to \"reason.\"","cats":{"hci":0,"programming":0}}
{"text":"The results provide insights into LLMs' reasoning abilities and reveal the logical rules the LLMs did not learn well.","cats":{"hci":0,"programming":0}}
{"text":"The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively.","cats":{"hci":0,"social-sciences":0}}
{"text":"This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications.","cats":{"hci":0}}
{"text":"We examine this practice of structuration as a two-way interaction between users and models by analysing how ChatGPT4 redacts perceived `anomalous' language in fragments of Joyce's Ulysses and the new linguistic practice of prompt engineering.","cats":{"hci":1,"social-sciences":1}}
{"text":"These debates around the communicative orientation toward language can help explain some of the contemporary behaviours and interdependencies that take place between users and LLMs.","cats":{"hci":1,"social-sciences":0}}
{"text":"Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains.","cats":{"hci":0}}
{"text":"This is particularly crucial for practical applications like mental health support, nutrition planning, culturally sensitive conversations, or reducing toxic behavior in conversational agents.","cats":{"hci":0}}
{"text":"Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication.","cats":{"hci":1}}
{"text":"However, concerns regarding data privacy have emerged, especially when multiple stakeholders aim to collaboratively enhance LLMs using sensitive data.","cats":{"hci":0}}
{"text":"In this study, various prompts are designed and compared across mainstream LLMs using two distinct datasets from human reviewers and automated checkers.","cats":{"hci":0,"programming":0}}
{"text":"Prior studies have compared the decision making abilities of LLMs with those of humans from a psychological perspective.","cats":{"hci":1}}
{"text":"To this end, a hypothetical design project was conducted, where ChatGPT was utilized to generate personas, simulate interviews with fictional users, create new design ideas, simulate usage scenarios and conversations between an imaginary prototype and fictional users, and lastly evaluate user experience.","cats":{"hci":1}}
{"text":"The recent advancements in Large Language Models (LLMs), particularly conversational LLMs like ChatGPT, have prompted changes in a range of fields, including design.","cats":{"hci":0}}
{"text":"This study aims to examine the capabilities of ChatGPT in a human-centered design process.  ","cats":{"hci":0}}
{"text":"The results show that ChatGPT effectively performed the tasks assigned to it as a designer, user, or product, providing mostly appropriate responses.","cats":{"hci":0}}
{"text":"The study does, however, highlight some drawbacks such as forgotten information, partial responses, and a lack of output diversity.","cats":{"hci":0}}
{"text":"The paper explains the potential benefits and limitations of using conversational LLMs in design, discusses its implications, and suggests directions for future research in this rapidly evolving area.","cats":{"hci":0}}
{"text":"We not only focus on the performance of LLMs, but also explores their features from a psychological perspective, acknowledging the importance of understanding their behavioral characteristics.","cats":{"hci":1,"social-sciences":1}}
{"text":"Our study examines the behavioral patterns displayed by LLMs by employing trait theory, a psychological framework.","cats":{"hci":1,"social-sciences":0}}
{"text":"Moreover, the study investigates whether ChatGPT can exhibit personality changes in response to instructions or contextual cues.","cats":{"hci":1,"social-sciences":1}}
{"text":"The findings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts.","cats":{"hci":1,"social-sciences":1}}
{"text":" We not only focus on the performance of LLMs, but also explores their features from a psychological perspective, acknowledging the importance of understanding y of personality types exhibited by ChatGPT.","cats":{"hci":0}}
{"text":"Furthermore, experiments include cross-lingual effects on seven additional languages, and the investigation of four other LLMs.","cats":{"hci":0}}
{"text":"Moreover, the study indings show that ChatGPT consistently maintains its ENFJ personality regardless of instructions or contexts.","cats":{"hci":0}}
{"text":"By shedding light on the personalization of LLMs, we anticipate that our study will serve as a catalyst for further research in this field.","cats":{"hci":0,"social-sciences":0}}
{"text":"The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks including decision making.","cats":{"hci":1,"social-sciences":1}}
{"text":"By experimenting on three OpenAI language models possessing different capabilities, we observe that the decision making abilities fluctuate based on the input prompts and temperature settings.","cats":{"hci":0}}
{"text":"Contrary to previous findings language models display a human-like exploration exploitation tradeoff after simple adjustments to the prompt.","cats":{"hci":1}}
{"text":" Prior studies have compared the decision making abilities of LLMs with those of humans from a psychological perspective.","cats":{"hci":0}}
{"text":"However, these sturompt.","cats":{"hci":0}}
{"text":"In this study, we examine LLMs' performance on the Horizon decision making task studied by Binz and Schulz (2023) analyzing how LLMs respond to variations in prompts and hyperparameters.","cats":{"hci":0}}
{"text":"Contrary to previous findings language models display a human-like","cats":{"hci":0}}
{"text":"The emergence of Large Language Models (LLMs) has brought both excitement and concerns to social computing research.","cats":{"hci":1,"social-sciences":1}}
{"text":"On the other hand, concerns are emerging regarding the validity, privacy, and ethics of the research when LLMs are involved.","cats":{"hci":1,"social-sciences":0}}
{"text":"This SIG aims at offering an open space for social computing researchers who are interested in understanding the impacts of LLMs to discuss their current practices, perspectives, challenges when engaging with LLMs in their everyday work and collectively shaping the emerging norms of using LLMs in social computing research.","cats":{"hci":1,"social-sciences":1}}
{"text":" On the one hand, LLMs offer unprecedented capabilities in analyzing vast amounts of textual data and generating human-like responses, enabling researchers to delve into complex social phenomena.","cats":{"hci":0}}
{"text":"On the other hand, concerns are emerging regarding the validity, privacy, and ethics of the research when LLMs are ipacts of LLMs to discuss their current practices, perspectives, challenges when engaging with LLMs in their everyday work and","cats":{"hci":0}}
{"text":"Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes.","cats":{"hci":1}}
{"text":"Our work advances how we interact with LLMs for creative tasks, introducing a way to harness the creative potential of LLMs.","cats":{"hci":0}}
{"text":"These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors.","cats":{"hci":0}}
{"text":"But are we harnessing their full potential?","cats":{"hci":0}}
{"text":"We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models.","cats":{"hci":0}}
{"text":"To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses.","cats":{"hci":0}}
{"text":"We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 8 professional writers.","cats":{"hci":0}}
{"text":"The goal of this paper is establishing if we can satisfactorily perform a Thematic Analysis (TA) of semi-structured interviews using a Large Language Model (more precisely GPT3.5-Turbo).","cats":{"hci":1}}
{"text":"In particular, the focus will be on using the results of a TA done with the LLM on a dataset of user interviews, for writing user personas, with the model building on the TA to produce the personas narratives.","cats":{"hci":1}}
{"text":" Building on previous work by the author, which established an embryonal process for conducting a TA with the model, this paper will perform a further analysis and then cover the last phase of a TA (phase 6), which entails the writing up of the result.","cats":{"hci":0}}
{"text":"This phase was not covered by the previous work.","cats":{"hci":0}}
{"text":"In particular, the focus will be on using the results of a TA done with the LLM on a dataset of user interviews, for writing user personas, with the model building on the TA to produce tThe paper shows that the model can build basic user personas with an acceptable quality deriving them from themes, and that the model can serve for the generation of ideas for user personas.","cats":{"hci":0}}
{"text":"Our survey of 938 public service professionals within the UK (covering education, health, social work and emergency services) seeks to answer this question.  ","cats":{"hci":0}}
{"text":"The UK's public sector ur","cats":{"hci":0}}
{"text":"The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities.","cats":{"hci":1}}
{"text":"By promoting the adoption of meta-reasoning evaluation methods similar to ours, we aim to facilitate a more accurate assessment of the true cognitive abilities of LLMs.","cats":{"hci":0}}
{"text":"In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning.","cats":{"hci":0}}
{"text":"This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents.","cats":{"hci":0}}
{"text":"Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models.","cats":{"hci":0}}
{"text":"For example, in our benchmark, GPT-4 demonstrates a performance ten times more accurate than GPT3-5.  ","cats":{"hci":0}}
{"text":"Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering fundamental deficiencies in their training and evaluation approaches.","cats":{"hci":0}}
{"text":"This paper not only advocates for a paradigm shift in the assessment of LLMs but also contributes to the ongoing discourse on the trajectory towards Artificial General Intelligence (AGI).","cats":{"hci":0}}
{"text":"This paper introduces the WordArt Designer API, a novel framework for user-driven artistic typography synthesis utilizing Large Language Models (LLMs) on ModelScope.","cats":{"hci":1}}
{"text":"Our approach leverages the power of LLMs to understand and interpret user input, facilitating a more intuitive design process.","cats":{"hci":0,"architectures":0}}
{"text":"We address the challenge of simplifying artistic typography for non-professionals by offering a dynamic, adaptive, and computationally efficient alternative to traditional rigid templates.","cats":{"hci":0,"architectures":0}}
{"text":"We demonstrate through various case stunique and creative typographic designs.","cats":{"hci":0}}
{"text":"Our evaluations indicate significant improvements in user satisfaction, design flexibility, and creative expression over existing systems.","cats":{"hci":0}}
{"text":"The WordArt Designer API not only democratizes the art of typography but also opens up new possibilities for personalized digital communication and design.","cats":{"hci":0}}
{"text":"It argues that, although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation.","cats":{"hci":1}}
{"text":"Effective collaboration requires groups to strategically regulate themselves to overcome challenges.","cats":{"hci":0}}
{"text":"Research has shown that groups may fail to regulate due to differences in members' perceptions of challenges which may benefit from external support.","cats":{"hci":0}}
{"text":"In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated.","cats":{"hci":0}}
{"text":"The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts.","cats":{"hci":0,"architectures":0}}
{"text":"The paper provides an extensive discussion of the three approaches' performance for automated detection and support of students' challenge moments in collaborative learning activities.  ","cats":{"hci":0}}
{"text":"We conclude the paper with a discussion on additional considerations, including model transparency to explore feasible and meaningful analytical feedback for students and educators using LLMs.","cats":{"hci":0}}
{"text":"Large Language Models (LLMs) like GPT-4 have been shown effective in mitigating misinformation, particularly in handling statements where enough context is provided.","cats":{"hci":0}}
{"text":"However, they struggle to assess ambiguous or context-deficient statements accurately.","cats":{"hci":0,"social-sciences":0}}
{"text":"This work introduces a new method to resolve uncertainty in such statements.","cats":{"hci":0}}
{"text":"We propose a framework to categorize missing information and publish category labels for the LIAR-New dataset, which is adaptable to cross-domain content with missing information.","cats":{"hci":0,"architectures":0}}
{"text":"We then leverage this framework to generate effective user queries for missing context.","cats":{"hci":0}}
{"text":"Compared to baselines, our method improves the rate at which generated questions are answerable by the user by 38 percentage points and classification performance by over 10 percentage points macro F1.","cats":{"hci":0}}
{"text":"Thus, this approach may provide a valuable component for future misinformation mitigation pipelines.","cats":{"hci":0,"programming":0}}
{"text":"RGBT tracking has been widely used in various fields such as robotics, surveillance processing, and autonomous driving.","cats":{"hci":0}}
{"text":"Existing RGBT trackers fully explore the spatial information between the template and the search region and locate the target based on the appearance matching results.","cats":{"hci":0}}
{"text":"However, these RGBT trackers have very limited exploitation of temporal information, either ignoring temporal information or exploiting it through online sampling and training.","cats":{"hci":0}}
{"text":"The former struggles to cope with the object state changes, while the latter neglects the correlation between spatial and temporal information.","cats":{"hci":0,"social-sciences":0}}
{"text":"To alleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking framework, named as TATrack.","cats":{"hci":0}}
{"text":"TATrack has a spatio-temporal two-stream structure and captures temporal information by an online updated template, where the two-stream structure refers to the multi-modal feature extraction and cross-modal interaction for the initial template and the online update template respectively.","cats":{"hci":0}}
{"text":"TATrack contributes to comprehensively exploit spatio-temporal information and multi-modal information for target localization.","cats":{"hci":0}}
{"text":"In addition, we design a spatio-temporal interaction (STI) mechanism that bridges two branches and enables cross-modal interaction to span longer time scales.","cats":{"hci":0}}
{"text":"Extensive experiments on three popular RGBT tracking benchmarks show that our method achieves state-of-the-art performance, while running at real-time speed.","cats":{"hci":0,"production":0}}
{"text":"In the age of personal voice assistants, the question of privacy arises.","cats":{"hci":0}}
{"text":"Modern smartphones now enable on-device speech processing, making cloud-based solutions unnecessary.","cats":{"hci":0}}
{"text":"Personal assistants for the elderly should excel at memory recall, especially in medical examinations.","cats":{"hci":0}}
{"text":"The e-ViTA project developed a versatile conversational application with local processing and speaker recognition.","cats":{"hci":0}}
{"text":"This paper highlights the importance of speaker diarization enriched with sensor data fusion for contextualized conversation preservation.","cats":{"hci":0}}
{"text":"The use cases applied to the e-VITA project have shown that truly personalized dialogue is pivotal for individual voice assistants.","cats":{"hci":0}}
{"text":"Secure local processing and sensor data fusion ensure virtual companions meet individual user needs without compromising privacy or data security.","cats":{"hci":0,"production":0}}
{"text":"Large Language Models (LLMs) like ChatGPT are foundational in various applications due to their extensive knowledge from pre-training and fine-tuning.","cats":{"hci":0,"programming":0}}
{"text":"Current methods for evaluating LLMs' veracity are limited by test data leakage or the need for extensive human labor, hindering efficient and accurate error detection.","cats":{"hci":0}}
{"text":"To tackle this problem, we introduce a novel, automatic testing framework, FactChecker, aimed at uncovering factual inaccuracies in LLMs.","cats":{"hci":0}}
{"text":"This framework involves three main steps:","cats":{"hci":0}}
{"text":"First, it constructs a factual knowledge graph by retrieving fact triplets from a large-scale knowledge database.","cats":{"hci":0}}
{"text":"Then, leveraging the knowledge graph, FactChecker employs a rule-based approach to generates three types of questions (Yes-No, Multiple-Choice, and WH questions) that involve single-hop and multi-hop relations, along with correct answers.","cats":{"hci":0}}
{"text":"Lastly, it assesses the LLMs' responses for accuracy using tailored matching strategies for each question type.","cats":{"hci":0}}
{"text":"Our extensive tests on six prominent LLMs, including text-davinci-002, text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal that FactChecker can trigger factual errors in up to 45\\% of questions in these models.","cats":{"hci":0}}
{"text":"Moreover, we demonstrate that FactChecker's test cases can improve LLMs' factual accuracy through in-context learning and fine-tuning (e.g., llama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%).","cats":{"hci":0}}
{"text":"We are making all code, data, and results available for future research endeavors.","cats":{"hci":0,"recommender":0}}
{"text":" With the advancement of Large Language Models (LLMs), dialogue systems can handle multimodal data, including audio.","cats":{"hci":0}}
{"text":"In various evaluation metrics, E-chat consistently outperforms baseline LLMs, demonstrating","cats":{"hci":0}}
{"text":"Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs).","cats":{"hci":0}}
{"text":"Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents.","cats":{"hci":0}}
{"text":"In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination.","cats":{"hci":0,"social-sciences":0}}
{"text":"RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG.","cats":{"hci":0}}
{"text":"These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity.","cats":{"hci":0}}
{"text":"We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies.","cats":{"hci":0}}
{"text":"Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.","cats":{"hci":0}}
{"text":"This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure.","cats":{"hci":0}}
{"text":"As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial.","cats":{"hci":0,"production":0,"architectures":0,"programming":0}}
{"text":"One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language.","cats":{"hci":0,"production":0,"architectures":0,"programming":0}}
{"text":"In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages.","cats":{"hci":0,"production":0,"architectures":1,"programming":0}}
{"text":"We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning.","cats":{"hci":0,"production":0,"architectures":1,"programming":0}}
{"text":"Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning.","cats":{"hci":0,"production":0,"architectures":0,"programming":0}}
{"text":"In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages.","cats":{"hci":0,"production":0,"programming":0}}
{"text":"Finally, we find that increasing the number of languages in the instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual generalization.","cats":{"hci":0,"production":0,"architectures":0,"programming":0}}
{"text":"Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses.","cats":{"hci":0,"production":0,"architectures":0,"programming":0}}
{"text":"This paper introduces a novel multi-agent communication framework, inspired by the CAMEL model, to enhance LLMs' autonomous problem-solving capabilities.  ","cats":{"hci":0}}
{"text":"Recently, the advent of large language models (LLMs) has revolutionized generative agents.  ","cats":{"hci":0}}
{"text":"In human-centric content generation, the pre-trained text-to-image models struggle to produce user-wanted portrait images, which retain the identity of individuals while exhibiting diverse expressions.","cats":{"hci":0,"social-sciences":0}}
{"text":"This paper introduces our efforts towards personalized face generation.","cats":{"hci":0}}
{"text":"To this end, we propose a novel multi-modal face generation framework, capable of simultaneous identity-expression control and more fine-grained expression synthesis.","cats":{"hci":0}}
{"text":"Our expression control is so sophisticated that it can be specialized by the fine-grained emotional vocabulary.","cats":{"hci":0}}
{"text":"We devise a novel diffusion model that can undertake the task of simultaneously face swapping and reenactment.","cats":{"hci":0,"social-sciences":0}}
{"text":"Due to the entanglement of identity and expression, it's nontrivial to separately and precisely control them in one framework, thus has not been explored yet.","cats":{"hci":0}}
{"text":"To overcome this, we propose several innovative designs in the conditional diffusion model, including balancing identity and expression encoder, improved midpoint sampling, and explicitly background conditioning.","cats":{"hci":0}}
{"text":"Extensive experiments have demonstrated the controllability and scalability of the proposed framework, in comparison with state-of-the-art text-to-image, face swapping, and face reenactment methods.","cats":{"hci":0}}
{"text":"During virtual assistant development, some developers prefer to leverage the system message, also known as an initial prompt or custom prompt, for preconditioning purposes.","cats":{"hci":0}}
{"text":"Prompt injection attacks exploit vulnerabilities in large language models (LLMs) to manipulate the model into unintended actions or generate malicious content.","cats":{"hci":0,"recommender":0}}
{"text":"As LLM integrated applications gain wider adoption, they face growing susceptibility to such attacks.","cats":{"hci":0}}
{"text":"This study introduces a novel evaluation framework for quantifying the resilience of applications.","cats":{"hci":0}}
{"text":"The framework incorporates innovative techniques designed to ensure representativeness, interpretability, and robustness.","cats":{"hci":0}}
{"text":"To ensure the representativeness of simulated attacks on the application, a meticulous selection process was employed, resulting in 115 carefully chosen attacks based on coverage and relevance.","cats":{"hci":0}}
{"text":"For enhanced interpretability, a second LLM was utilized to evaluate the responses generated from these simulated attacks.","cats":{"hci":0}}
{"text":"Unlike conventional malicious content classifiers that provide only a confidence score, the LLM-based evaluation produces a score accompanied by an explanation, thereby enhancing interpretability.","cats":{"hci":0}}
{"text":"Subsequently, a resilience score is computed by assigning higher weights to attacks with greater impact, thus providing a robust measurement of the application resilience.","cats":{"hci":0}}
{"text":"To assess the framework's efficacy, it was applied on two LLMs, namely Llama2 and ChatGLM.","cats":{"hci":0,"social-sciences":0}}
{"text":"Results revealed that Llama2, the newer model exhibited higher resilience compared to ChatGLM.","cats":{"hci":0}}
{"text":"This finding substantiates the effectiveness of the framework, aligning with the prevailing notion that newer models tend to possess greater resilience.","cats":{"hci":0}}
{"text":"Moreover, the framework exhibited exceptional versatility, requiring only minimal adjustments to accommodate emerging attack techniques and classifications, thereby establishing itself as an effective and practical solution.","cats":{"hci":0}}
{"text":"Overall, the framework offers valuable insights that empower organizations to make well-informed decisions to fortify their applications against potential threats from prompt injection.","cats":{"hci":0}}
{"text":"Recent advancements in large language models (LLMs) have propelled Artificial Intelligence (AI) to new heights, enabling breakthroughs in various tasks such as writing assistance, code generation, and machine translation.","cats":{"hci":0,"programming":0}}
{"text":"However, evaluating the reasoning ability of LLMs remains a challenge as most existing evaluations focus on their accuracy on the downstream tasks rather than directly assessing their reasoning processes.","cats":{"hci":0,"programming":0}}
{"text":"Efforts have been made to develop benchmarks and metrics to assess reasoning in LLMs, but they suffer from data leakage or limited scope.","cats":{"hci":0,"programming":0}}
{"text":"In this paper, we introduce LogicAsker, an automatic approach that comprehensively evaluates and improves the logical reasoning abilities of LLMs under a set of atomic reasoning skills based on propositional and predicate logic.","cats":{"hci":0,"programming":0}}
{"text":"We evaluate LogicAsker on six widely deployed LLMs, including GPT-3, ChatGPT, GPT-4, Bard, Vicuna, and Guanaco.","cats":{"hci":0,"programming":0}}
{"text":"The results show that test cases from LogicAsker can find logical reasoning failures in different LLMs with a rate of 25\\% - 94\\%.","cats":{"hci":0,"programming":0}}
{"text":"In addition, the test cases of LogicAsker can be further used to design demonstration examples for in-context learning, which effectively improves the logical reasoning ability of LLMs, e.g., 10\\% for GPT-4.","cats":{"hci":0,"programming":0}}
{"text":"All the code, data, and results will be released for reproduction and future research.","cats":{"hci":0,"programming":0}}
{"text":"In this paper, we present a novel framework to assist LLMs, such as ChatGPT, to retrieve question-related structured information on the knowledge graph, and demonstrate that Knowledge-based question answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to guide the LLM to sequentially find the answer entities of a complex question through interpretable logical chains.","cats":{"hci":1}}
{"text":"Large language models (LLMs) have exhibited remarkable performance on various natural language processing (NLP) tasks, especially for question answering.","cats":{"hci":0}}
{"text":"However, in the face of problems beyond the scope of knowledge, these LLMs tend to talk nonsense with a straight face, where the potential solution could be incorporating an Information Retrieval (IR) module and generating response based on these retrieved knowledge.  ","cats":{"hci":0}}
{"text":"Specifically, the workflow of Keqing will execute decomposing a complex question according to predefined templates, retrieving candidate entities on knowledge graph, reasoning answers of sub-questions, and finally generating response with reasoning paths, which greatly improves the reliability of LLM's response.","cats":{"hci":0}}
{"text":"The experimental results on KBQA datasets show that Keqing can achieve competitive performance and illustrate the logic of answering each question.","cats":{"hci":0}}
{"text":"This paper explores the use of open generative Large Language Models (LLMs) for annotation tasks in the social sciences.","cats":{"hci":0,"social-sciences":1}}
{"text":"The study highlights the challenges associated with proprietary models, such as limited reproducibility and privacy concerns, and advocates for the adoption of open (source) models that can be operated on independent devices.","cats":{"hci":0,"social-sciences":0}}
{"text":"Two examples of annotation tasks, sentiment analysis in tweets and identification of leisure activities in childhood aspirational essays are provided.","cats":{"hci":0,"social-sciences":1}}
{"text":"The study evaluates the performance of different prompting strategies and models (neural-chat-7b-v3-2, Starling-LM-7B-alpha, openchat_3.5, zephyr-7b-alpha and zephyr-7b-beta).","cats":{"hci":0}}
{"text":"The study highlights the advantages of open models for data privacy and reproducibility.","cats":{"hci":0,"social-sciences":0}}
{"text":"This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence.","cats":{"hci":0}}
{"text":"Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration.","cats":{"hci":0,"social-sciences":0}}
{"text":"We propose a novel causal attribution model that utilizes \"do-operators\" for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes.","cats":{"hci":0}}
{"text":"Our evaluation reveals that LLMs' causal reasoning ability depends on the context and domain-specific knowledge provided, and supports the argument that \"knowledge is, indeed, what LLMs principally require for sound causal reasoning\".","cats":{"hci":0}}
{"text":"On the contrary, in the absence of knowledge, LLMs still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations.","cats":{"hci":0}}
{"text":"Personalizing conversational agents can enhance the quality of conversations and increase user engagement.","cats":{"hci":0}}
{"text":"However, they often lack external knowledge to appropriately tend to a user's persona.","cats":{"hci":0}}
{"text":"To enhance the relevance and comprehensiveness of personalized responses, we propose using a two-step approach that involves (1) selectively integrating user personas and (2) contextualizing the response with supplementing information from a background knowledge source.","cats":{"hci":0}}
{"text":"We develop K-PERM (Knowledge-guided PErsonalization with Reward Modulation), a dynamic conversational agent that combines these elements.","cats":{"hci":0}}
{"text":"K-PERM achieves state-of-the-art performance on the popular FoCus dataset, containing real-world personalized conversations concerning global landmarks.","cats":{"hci":0}}
{"text":"We show that using responses from K-PERM can improve performance in state-of-the-art LLMs (GPT 3.5) by 10.5%, highlighting the impact of K-PERM for personalizing chatbots.","cats":{"hci":0}}
{"text":"This scoping survey focuses on our current understanding of the design space for task-oriented LLM systems and elaborates on definitions and relationships among the available design parameters.","cats":{"hci":0}}
{"text":"The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results.","cats":{"hci":0}}
{"text":"We discuss a pattern in our results and formulate them into three conjectures.","cats":{"hci":0}}
{"text":"While these conjectures may be partly based on faulty assumptions, they provide a starting point for future research.","cats":{"hci":0}}
{"text":"The paper then surveys a select few design parameters: covering and organizing research in LLM augmentation, prompting techniques, and uncertainty estimation, and discussing their significance.","cats":{"hci":0}}
{"text":"The paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas.  ","cats":{"hci":0}}
{"text":"In all, the scoping","cats":{"hci":0}}
{"text":"However, humans are adept at abstracting optimal solutions from problems, thereby facilitating swift and precise reasoning for similar problems resolution.  ","cats":{"hci":0}}
