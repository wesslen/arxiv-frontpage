{"text":"In this paper, we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.","cats":{"cybersecurity-llm":1,"robustness-tools-llm":1}}
{"text":"Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to baselines.","cats":{"cybersecurity-llm":0,"robustness-tools-llm":1}}
{"text":"Source code and data are available at https://github.com/uiuc-focal-lab/llm-priming-attacks .","cats":{"cybersecurity-llm":0,"robustness-tools-llm":0}}
{"text":"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.","cats":{"robustness-tools-llm":1}}
{"text":"Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more.","cats":{"robustness-tools-llm":1}}
{"text":"There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment.","cats":{"robustness-tools-llm":1}}
{"text":"Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable.","cats":{"robustness-tools-llm":1}}
{"text":"Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.","cats":{"robustness-tools-llm":1}}
{"text":" Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topicsc model at training, e.g. using model alignment.","cats":{"robustness-tools-llm":0}}
{"text":"Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, indep safe LLM applications using programmable rails.","cats":{"robustness-tools-llm":0}}
{"text":"With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training.","cats":{"robustness-tools-llm":1}}
{"text":" In this paper, we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacksd, by up to $3.3\\times$ compared to baselines.","cats":{"robustness-tools-llm":0}}
{"text":"Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void.","cats":{"robustness-tools-llm":1}}
{"text":" However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective.","cats":{"robustness-tools-llm":0}}
{"text":"In this paper, inspired by the Milgram experiment that individuals can harm another person if they are told to do so by an authoritative figure, we disclose a lightweight method, termed as DeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock its misusing risks.","cats":{"robustness-tools-llm":0}}
{"text":"Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario and provides the possibility for further direct jailbreaks.","cats":{"robustness-tools-llm":0}}
{"text":"Empirically, we conduct comprehensive experiments to show its efficacy.","cats":{"robustness-tools-llm":0}}
{"text":"Our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna, Llama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay more attention to the safety aspects of LLMs and a stronger defense against their misuse risks.","cats":{"robustness-tools-llm":0}}
{"text":"The code is publicly available at: https://github.com/tmlr-group/DeepInception.","cats":{"robustness-tools-llm":0}}
{"text":"In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks.","cats":{"robustness-tools-llm":1}}
{"text":"This paper explores the intersection of LLMs with security and privacy.","cats":{"robustness-tools-llm":0}}
{"text":"Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs.","cats":{"robustness-tools-llm":1}}
{"text":"Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation.","cats":{"robustness-tools-llm":0}}
{"text":"They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation).  ","cats":{"robustness-tools-llm":0}}
{"text":"Specifically, we investigate how LLMs positively impact security and privacy, potential risks ain LLMs.","cats":{"robustness-tools-llm":0}}
{"text":"Through a comprehensive literature review, the paper categorizendings.","cats":{"robustness-tools-llm":0}}
{"text":"For example, LLMs have proven to enhance code and data security, outperforming traditional methods.","cats":{"robustness-tools-llm":0}}
{"text":"However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities.","cats":{"robustness-tools-llm":0}}
{"text":"We have identified areas that require further research efforts.","cats":{"robustness-tools-llm":0}}
{"text":"For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality.","cats":{"robustness-tools-llm":0}}
{"text":"Safe instruction tuning, a recent development, requires more exploration.","cats":{"robustness-tools-llm":0}}
{"text":"We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.","cats":{"robustness-tools-llm":0}}
{"text":"For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions.","cats":{"robustness-tools-llm":1}}
{"text":"Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent.","cats":{"robustness-tools-llm":1}}
{"text":"These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning.","cats":{"robustness-tools-llm":1}}
{"text":"Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning.","cats":{"robustness-tools-llm":0}}
{"text":"Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5","cats":{"robustness-tools-llm":0}}
{"text":"Turbo on custom datasets also encourage this practice.","cats":{"robustness-tools-llm":0}}
{"text":"But, what are the safety costs associated with such custom fine-tuning?","cats":{"robustness-tools-llm":0}}
{"text":"We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users.","cats":{"robustness-tools-llm":0}}
{"text":"Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples.  ","cats":{"robustness-tools-llm":0}}
{"text":"Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lessecessarily to be maintained after custom fine-tuning.","cats":{"robustness-tools-llm":0}}
{"text":"We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned","cats":{"robustness-tools-llm":0}}
{"text":"While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions.","cats":{"robustness-tools-llm":1}}
{"text":"We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English.","cats":{"robustness-tools-llm":0}}
{"text":"Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well towards improving calibration in the zero-shot scenario.","cats":{"robustness-tools-llm":1}}
{"text":"We also find that few-shot examples in the language can further help reduce the calibration errors, often substantially.","cats":{"robustness-tools-llm":1}}
{"text":"Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model specific factors influence it, and pointing out the strategies to improve the same.","cats":{"robustness-tools-llm":1}}
{"text":"Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer.  ","cats":{"robustness-tools-llm":0}}
{"text":"Next, we empirically show that calibration mep reduce the calibration errors, often substantially.","cats":{"robustness-tools-llm":0}}
{"text":"Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understan","cats":{"robustness-tools-llm":0}}
{"text":"In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus.","cats":{"robustness-tools-llm":1}}
{"text":"We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale.","cats":{"robustness-tools-llm":0}}
{"text":"Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages.","cats":{"robustness-tools-llm":0}}
{"text":"However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales.  ","cats":{"robustness-tools-llm":0}}
{"text":"We find that UniMax outperforms standard temperature-based sampling, and the benellion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.","cats":{"robustness-tools-llm":0}}
{"text":"In this work, we explore to what extent they handle basic linguistic constructions -- active-passive voice, coordination, and relative clauses -- that even preschool children can typically master.","cats":{"robustness-tools-llm":1}}
{"text":"We present BLA, a novel, automatically constructed benchmark to evaluate multimodal models on these Basic Language Abilities.","cats":{"robustness-tools-llm":0}}
{"text":"Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction.  ","cats":{"robustness-tools-llm":0}}
{"text":"We show that different types of Transformer-based systems, such as CLIn particular, show that most of the tested models only marginally benefit when fine-tuned or prompted with construction-specific samples.","cats":{"robustness-tools-llm":0}}
{"text":"Yet, the generative BLIP2 shows promising trends, especially in an in-context learning setting.","cats":{"robustness-tools-llm":0}}
{"text":"This opens the door to using BLA not only as an evaluation benchmark but also to improve models' basic language abilities.","cats":{"robustness-tools-llm":0}}
{"text":"In our work, we investigate the use of LLMs to augment training data of small language models~(SLMs) with automatically generated counterfactual~(CF) instances -- i.e. minimally altered inputs -- in order to improve out-of-domain~(OOD) performance of SLMs in the extractive question answering~(QA) setup.","cats":{"robustness-tools-llm":1}}
{"text":"We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models.","cats":{"robustness-tools-llm":0}}
{"text":"In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt.  ","cats":{"robustness-tools-llm":0}}
{"text":"Furthermore, these performance improvements correlate with higher diversity of CF instances in terugmented calibrators prefer concise explanations.","cats":{"robustness-tools-llm":0}}
{"text":"While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis.","cats":{"robustness-tools-llm":1}}
{"text":"Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content.","cats":{"robustness-tools-llm":0}}
{"text":"The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses.  ","cats":{"robustness-tools-llm":0}}
{"text":"Experimental results demonstrate that the proposed method outperforms conventional alignment techniques","cats":{"robustness-tools-llm":0}}
{"text":"In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs).","cats":{"robustness-tools-llm":1}}
{"text":"This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes.","cats":{"robustness-tools-llm":1}}
{"text":"We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs.","cats":{"robustness-tools-llm":0}}
{"text":" This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct persata for both the helpfulness and harmlessness metrics.","cats":{"robustness-tools-llm":0}}
{"text":"We believe this dataset provides vital resources for the community, contributing towards the safe development and","cats":{"robustness-tools-llm":0}}
{"text":"Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored.","cats":{"robustness-tools-llm":1}}
{"text":"In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review.","cats":{"robustness-tools-llm":1}}
{"text":"Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications.  ","cats":{"robustness-tools-llm":0}}
{"text":"In contrast to previous works that evaluate models holistically, we comprehensively decompose theby step.","cats":{"robustness-tools-llm":0}}
{"text":"\\shortname~disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs.","cats":{"robustness-tools-llm":0}}
{"text":"We conduct extensive experiments on \\shortname~and in-depth analysis of various LLMs.","cats":{"robustness-tools-llm":0}}
{"text":"\\shortname~ not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability.","cats":{"robustness-tools-llm":0}}
{"text":"The benchmark will be available at \\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}.","cats":{"robustness-tools-llm":0}}
