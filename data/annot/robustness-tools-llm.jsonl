{"text":"In this paper, we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.","cats":{"cybersecurity-llm":1,"robustness-tools-llm":1}}
{"text":"Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to baselines.","cats":{"cybersecurity-llm":0,"robustness-tools-llm":1}}
{"text":"Source code and data are available at https://github.com/uiuc-focal-lab/llm-priming-attacks .","cats":{"cybersecurity-llm":0,"robustness-tools-llm":0}}
{"text":"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.","cats":{"robustness-tools-llm":1}}
{"text":"Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more.","cats":{"robustness-tools-llm":1}}
{"text":"There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment.","cats":{"robustness-tools-llm":1}}
{"text":"Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable.","cats":{"robustness-tools-llm":1}}
{"text":"Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.","cats":{"robustness-tools-llm":1}}
{"text":" Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topicsc model at training, e.g. using model alignment.","cats":{"robustness-tools-llm":0}}
{"text":"Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, indep safe LLM applications using programmable rails.","cats":{"robustness-tools-llm":0}}
{"text":"With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training.","cats":{"robustness-tools-llm":1}}
{"text":" In this paper, we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacksd, by up to $3.3\\times$ compared to baselines.","cats":{"robustness-tools-llm":0}}
{"text":"Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void.","cats":{"robustness-tools-llm":1}}
{"text":" However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective.","cats":{"robustness-tools-llm":0}}
{"text":"In this paper, inspired by the Milgram experiment that individuals can harm another person if they are told to do so by an authoritative figure, we disclose a lightweight method, termed as DeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock its misusing risks.","cats":{"robustness-tools-llm":0}}
{"text":"Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario and provides the possibility for further direct jailbreaks.","cats":{"robustness-tools-llm":0}}
{"text":"Empirically, we conduct comprehensive experiments to show its efficacy.","cats":{"robustness-tools-llm":0}}
{"text":"Our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna, Llama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay more attention to the safety aspects of LLMs and a stronger defense against their misuse risks.","cats":{"robustness-tools-llm":0}}
{"text":"The code is publicly available at: https://github.com/tmlr-group/DeepInception.","cats":{"robustness-tools-llm":0}}
{"text":"In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks.","cats":{"robustness-tools-llm":1}}
{"text":"This paper explores the intersection of LLMs with security and privacy.","cats":{"robustness-tools-llm":0}}
{"text":"Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs.","cats":{"robustness-tools-llm":1}}
{"text":"Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation.","cats":{"robustness-tools-llm":0}}
{"text":"They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation).  ","cats":{"robustness-tools-llm":0}}
{"text":"Specifically, we investigate how LLMs positively impact security and privacy, potential risks ain LLMs.","cats":{"robustness-tools-llm":0}}
{"text":"Through a comprehensive literature review, the paper categorizendings.","cats":{"robustness-tools-llm":0}}
{"text":"For example, LLMs have proven to enhance code and data security, outperforming traditional methods.","cats":{"robustness-tools-llm":0}}
{"text":"However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities.","cats":{"robustness-tools-llm":0}}
{"text":"We have identified areas that require further research efforts.","cats":{"robustness-tools-llm":0}}
{"text":"For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality.","cats":{"robustness-tools-llm":0}}
{"text":"Safe instruction tuning, a recent development, requires more exploration.","cats":{"robustness-tools-llm":0}}
{"text":"We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.","cats":{"robustness-tools-llm":0}}
{"text":"For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions.","cats":{"robustness-tools-llm":1}}
{"text":"Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent.","cats":{"robustness-tools-llm":1}}
{"text":"These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning.","cats":{"robustness-tools-llm":1}}
{"text":"Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning.","cats":{"robustness-tools-llm":0}}
{"text":"Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5","cats":{"robustness-tools-llm":0}}
{"text":"Turbo on custom datasets also encourage this practice.","cats":{"robustness-tools-llm":0}}
{"text":"But, what are the safety costs associated with such custom fine-tuning?","cats":{"robustness-tools-llm":0}}
{"text":"We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users.","cats":{"robustness-tools-llm":0}}
{"text":"Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples.  ","cats":{"robustness-tools-llm":0}}
{"text":"Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lessecessarily to be maintained after custom fine-tuning.","cats":{"robustness-tools-llm":0}}
{"text":"We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned","cats":{"robustness-tools-llm":0}}
