{"text":"At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself.","cats":{"prompt-engineering":0,"security":0,"recommender":0,"programming":0}}
{"text":"iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0,"programming":0}}
{"text":"It generates a linearized graph where nodes represent text spans and edges represent relation triplets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"recommender":0,"programming":0}}
{"text":"For decision tables from closed classes, we study lower bounds on the minimum cardinality of reducts, which are minimal sets of attributes that allow us to recognize, for a given row, the decision attached to it.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0}}
{"text":"Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"recommender":0,"programming":0}}
{"text":"While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input.","cats":{"prompt-engineering":1,"robustness":1,"security":0,"hci":0,"social-sciences":1,"recommender":0}}
{"text":"In this paper, we introduce a detailed framework designed to detect and assess the presence of content from potentially copyrighted books within the training datasets of LLMs.","cats":{"robustness":0,"recommender":0}}
{"text":"Our analysis of GPT and Llama-variants reveals that these LLMs often resemble behaviors more commonly exhibited in low-quality therapy rather than high-quality therapy, such as offering a higher degree of problem-solving advice when clients share emotions, which is against typical recommendations.","cats":{"robustness":0,"hci":1,"recommender":0}}
{"text":"Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care, and thus require additional research to ensure quality care.","cats":{"robustness":0,"recommender":0}}
{"text":"In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks.","cats":{"robustness":0,"hci":0,"recommender":0}}
{"text":"Commuting extensions have also attracted the attention of the linear algebra community.","cats":{"security":0,"hci":0,"recommender":0}}
{"text":"(iii) A genericity theorem showing that our algorithms and uniqueness theorems can be applied to a wide range of input matrices.","cats":{"security":0,"recommender":0}}
{"text":"Our findings demonstrate that PINN-based approaches in channel modeling exhibit promising attributes such as generalizability, interpretability, and robustness.","cats":{"security":0,"recommender":0,"production":0}}
{"text":"At last, we explore the relationships among updated parameters, cross-entropy loss, and task performance.","cats":{"security":0,"recommender":0}}
{"text":"Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences.","cats":{"hci":0,"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have shown impressive success in various applications.","cats":{"hci":0,"education":0,"recommender":0,"production":0}}
{"text":"The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation.","cats":{"hci":0,"social-sciences":0,"education":0,"recommender":0}}
{"text":"Within this work, we propose a hybrid approach, Deep-ELA, which combines (the benefits of) deep learning and ELA features.","cats":{"hci":0,"recommender":0}}
{"text":"We are making all code, data, and results available for future research endeavors.","cats":{"hci":0,"recommender":0}}
{"text":"Prompt injection attacks exploit vulnerabilities in large language models (LLMs) to manipulate the model into unintended actions or generate malicious content.","cats":{"hci":0,"recommender":0}}
{"text":"Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems.","cats":{"hci":0,"recommender":1}}
{"text":"Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness.","cats":{"hci":0,"recommender":0}}
{"text":"As the Large Language Model (LLM) becomes increasingly important in various domains.","cats":{"social-sciences":0,"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks.","cats":{"social-sciences":0,"recommender":0}}
{"text":"Large Language Models (LLMs) are smart but forgetful.","cats":{"social-sciences":0,"education":0,"recommender":0,"production":0}}
{"text":"Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications.","cats":{"social-sciences":0,"education":0,"recommender":0}}
{"text":"Recent years have witnessed remarkable progress made in large language models (LLMs).","cats":{"social-sciences":0,"education":0,"recommender":0}}
{"text":"Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages.","cats":{"education":0,"recommender":0}}
{"text":"Large language models (LLMs) have been applied in various applications due to their astonishing capabilities.","cats":{"education":0,"recommender":0}}
{"text":"Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields.","cats":{"education":0,"recommender":0}}
{"text":"We share our approach to designing this new introductory socially responsible computing course and the students' reflections.","cats":{"education":0,"recommender":0,"architectures":0}}
{"text":"Recently, large language models (LLMs) have shown great potential in recommender systems, either improving existing recommendation models or serving as the backbone.","cats":{"recommender":1}}
{"text":"Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM).","cats":{"recommender":1}}
{"text":"As the focus on Large Language Models (LLMs) in the field of recommendation intensifies, the optimization of LLMs for recommendation purposes (referred to as LLM4Rec) assumes a crucial role in augmenting their effectiveness in providing recommendations.","cats":{"recommender":1}}
{"text":"Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs.","cats":{"recommender":0}}
{"text":"Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods.","cats":{"recommender":1}}
{"text":"With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful capabilities of text processing and reasoning, is a promising way to complete users' resumes for more accurate recommendations.","cats":{"recommender":1}}
{"text":"While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems.","cats":{"recommender":1}}
{"text":"The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs.","cats":{"recommender":1}}
{"text":"To address this challenge, in this paper, we propose a new LLM-based recommendation model called LC-Rec, which can better integrate language and collaborative semantics for recommender systems.","cats":{"recommender":1}}
{"text":"Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem.","cats":{"recommender":0}}
{"text":"In essence, LLMs capture language semantics while recommender systems imply collaborative semantics, making it difficult to sufficiently leverage the model capacity of LLMs for recommendation.","cats":{"recommender":1}}
{"text":"Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems.","cats":{"recommender":1}}
{"text":"Recent development of large language models (LLMs) has opened a new opportunity to address this problem more effectively.","cats":{"recommender":0,"production":0}}
{"text":"Large language models (LLMs), typically designed as a function of next-word prediction, have excelled across extensive NLP tasks.","cats":{"recommender":0}}
{"text":"Its primary objective was to provide a comprehensive understanding of the performance of Large Language Models (LLMs) when applied to different datasets.","cats":{"recommender":0}}
{"text":"To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks.","cats":{"recommender":1}}
{"text":"Large Language Models (LLMs) offer a promising solution to these persistent issues.","cats":{"recommender":0}}
{"text":"Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters.","cats":{"recommender":0}}
{"text":"Large language models (LLMs) have achieved remarkable advancements in the field of natural language processing.","cats":{"recommender":0}}
{"text":"Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization.","cats":{"recommender":0}}
{"text":"We thoroughly analyze the performance of several large language models (LLMs) and identify areas where further improvement is needed.","cats":{"recommender":0,"production":0}}
{"text":"This challenge has recently reached another scale with the emergence of large language models (LLMs).","cats":{"recommender":0}}
{"text":"Large language models (LLMs) with memory are computationally universal.","cats":{"recommender":0,"production":0}}
{"text":"In this paper, our objective is to investigate the comprehensive ranking capacity of LLMs and propose a two-step grounding framework known as BIGRec (Bi-step Grounding Paradigm for Recommendation).","cats":{"recommender":0}}
{"text":"It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences.","cats":{"recommender":1}}
{"text":"As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging.","cats":{"recommender":0}}
{"text":"In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs).","cats":{"recommender":0}}
{"text":"In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy.","cats":{"recommender":0}}
{"text":"The early identification of diseases in cocoa pods is an important task to guarantee the production of high-quality cocoa.","cats":{"recommender":0}}
{"text":"To the best of our knowledge, we are the first to introduce a couple of co-evolving representation spaces, rather than a single or static space, and propose a co-contrastive learning for the sequential interaction network.","cats":{"recommender":0}}
{"text":"Consequently, it can substantially boost the system capacity, as verified by Monte-Carlo simulations.","cats":{"recommender":0}}
{"text":"As more IoT applications gradually move towards the cloud-edge collaborative mode, the containerized scheduling of workflows extends from the cloud to the edge.","cats":{"recommender":0,"production":0}}
{"text":"Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.","cats":{"recommender":0,"production":0,"programming":0}}
{"text":"The improved algorithm model has good detection ability for solder joint defect detection, with mAP reaching 91.5%, 4.3% higher than the You Only Look Once version 5 algorithm and better than other comparative algorithms.","cats":{"recommender":0}}
{"text":"We use a convolutional neural network to classify each frame of an ultrasound video recording.","cats":{"recommender":0,"production":0,"architectures":0}}
{"text":"Experimental results indicate the best performance obtained by LightGBM with pre-processing using standard normal variate (SNV), feature extraction providing values of 0.839 for balanced accuracy, 0.851 for recall, 0.852 for precision, and 0.850 for F-score.","cats":{"recommender":0}}
{"text":"Furthermore, for a special case where the semantic state is a deterministic function of the observation, we design a cascade neural network to estimate the SRDF.","cats":{"recommender":0}}
{"text":"Our experimental findings undeniably demonstrate the superior performance of TF-CNN in comparison to recently developed techniques.","cats":{"recommender":0}}
{"text":"The first one is a model-driven deep learning method, which deep unfolds the original iterative detecting algorithm of marker code.","cats":{"recommender":0}}
{"text":"In this paper, we propose a model enabling the creation of a social graph corresponding to real society.","cats":{"recommender":0}}
{"text":"Several proposals for alternative designs have been put forward, with these efforts having now converged into the Compute Express Link (CXL) specification.","cats":{"recommender":0}}
{"text":"CXL is an interconnect protocol on top of PCIe with a more modern and powerful interface.","cats":{"recommender":0}}
{"text":"The latency is comparable to the ones observed in the state-of-the-art, with 780us/img.","cats":{"recommender":0,"production":0}}
{"text":"In this paper, we describe preliminary work on a new Dafny plugin that leverages LLMs to assist developers by generating suggestions for relevant lemmas that Dafny is unable to discover and use.","cats":{"recommender":0}}
{"text":"However, the advent of Multimodal Large Language Models (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing multimodal data including text, sound, and visual inputs, opens a new era of enriched, personalized, and interactive learning landscapes in education.","cats":{"recommender":0}}
{"text":"Possible applications for MLLMs could range from content creation to tailored support for learning, fostering competencies in scientific practices, and providing assessment and feedback.","cats":{"recommender":0}}
{"text":"We propose a method to fine-tune the nucleotide sequence language model and the text language model based on various databases of antibiotic resistance genes.","cats":{"recommender":0}}
{"text":"This paper takes ChatGPT as the modeling object, incorporates LLM technology into the typical book resource understanding and recommendation scenario for the first time, and puts it into practice.","cats":{"recommender":1}}
{"text":"By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recommendation, user rating recommendation, and book summary recommendation, and explores the feasibility of LLM technology in book recommendation scenarios.","cats":{"recommender":1}}
{"text":"At the same time, based on different evaluation schemes for book recommendation tasks and the existing classic recommendation models, this paper discusses the advantages and disadvantages of the BookGPT in book recommendation scenarios and analyzes the opportunities and improvement directions for subsequent LLMs in these scenarios.","cats":{"recommender":1}}
{"text":"With the continuous development and change exhibited by large language model (LLM) technology, represented by generative pretrained transformers (GPTs), many classic scenarios in various fields have re-emerged with new opportunities.  ","cats":{"recommender":0}}
{"text":"By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recBookGPT in book recommendation scenarios and analyzes the opportunities and improvement directions for subsequent LLMs in these scenarios.","cats":{"recommender":0}}
{"text":"However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.","cats":{"recommender":1}}
{"text":"In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference.","cats":{"recommender":1}}
{"text":"This allows RecSysLLM to leverage LLMs' capabilities for recommendation tasks in an efficient, unified framework.","cats":{"recommender":1}}
{"text":"We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.","cats":{"recommender":1}}
{"text":" However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.","cats":{"recommender":0}}
{"text":"In this paper, wcommendation domain knowledge through unique designs of data, training, and inference.","cats":{"recommender":0}}
{"text":"This allows RecSysLLM to leverage LLMs' capabilities for ng the power of pre-trained language models.","cats":{"recommender":0}}
{"text":"In the past decades, recommender systems have attracted much attention in both research and industry communities, and a large number of studies have been devoted to developing effective recommendation models.","cats":{"recommender":1}}
{"text":"The key idea is that the preferences or needs of a user can be expressed in natural language descriptions (called instructions), so that LLMs can understand and further execute the instruction for fulfilling the recommendation task.","cats":{"recommender":1}}
{"text":"Instead of using public APIs of LLMs, we instruction tune an open-source LLM (3B Flan-T5-XL), in order to better adapt LLMs to recommender systems.","cats":{"recommender":0}}
{"text":"To demonstrate the effectiveness of our approach, we instantiate the instruction templates into several widely-studied recommendation (or search) tasks, and conduct extensive experiments on these tasks with real-world datasets.","cats":{"recommender":1}}
{"text":"Our approach sheds light on developing more user-friendly recommender systems, in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions.","cats":{"recommender":0}}
{"text":" Basically speaking, these models mainly learn the underlying user preference from historical behavior data, and then estimate the user-item matching relationships for recommendations.","cats":{"recommender":0}}
{"text":"The key idea the recommendation task.","cats":{"recommender":0}}
{"text":"For this purpose, we fer-personalized instruction data (252K instructions) with varying types of preferences and intentions.","cats":{"recommender":0}}
{"text":"To demonstrate the effectiveness of our approach, we instantiate the instruction templates into several widely-studied recommendath can outperform several competitive baselines, including the powerful GPT-3.5, on these evaluation tasks.","cats":{"recommender":0}}
{"text":"To this end, here we present GIRL (GeneratIve job Recommendation based on Large language models), a novel approach inspired by recent advancements in the field of Large Language Models (LLMs).","cats":{"recommender":1}}
{"text":"We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.","cats":{"recommender":0}}
{"text":"This capability also enhances the performance of existing job recommendation models by supplementing job seeking features with generated content.","cats":{"recommender":0}}
{"text":"We believe that GIRL introduces a paradigm-shifting approach to job recommendation systems, fostering a more personalized and comprehensive job-seeking experience.","cats":{"recommender":1}}
{"text":"The rapid development of online recruitment services has encouraged the utilization of recommender systems to streamline the job seeking process.","cats":{"recommender":0}}
{"text":"Predominantly, current job recommendations deploy either collaborative filtering or person-job matching strategies.","cats":{"recommender":0}}
{"text":"However, these models tend to operate as \"black-box\" systems and lack the capacity to offer explainable guidance to job seekers.","cats":{"recommender":0}}
{"text":"Moreover, conventional matching-based recommendation methods are limited to retrieving and ranking existing jobs in the database, restricting their potential as comprehensive career AI advisors.  ","cats":{"recommender":0}}
{"text":"(RL) method to further fine-tine the generator.","cats":{"recommender":0}}
{"text":"This aligns the generator with recruiter feedback, tailoring the output to better meet employer preferences.","cats":{"recommender":0}}
{"text":"In particular, GIRL serves as a job seeker-centric generative model, providing job suggestions without the need of a candidate set.","cats":{"recommender":0}}
{"text":"With extensive experiments on a large-scale real-world dataset, we demonstrate the substantial effectiveness of our approach.","cats":{"recommender":0}}
{"text":"We believe that GIRL introduces a paradigm-shifting approach to job recommendation systems, fostering a more pe","cats":{"recommender":0}}
{"text":"Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks.","cats":{"recommender":1}}
{"text":"To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates.","cats":{"recommender":1}}
{"text":"We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction.","cats":{"recommender":1}}
{"text":"We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems.","cats":{"recommender":1}}
{"text":"We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators.","cats":{"recommender":1}}
{"text":"We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies.","cats":{"recommender":1}}
{"text":" Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems.","cats":{"recommender":0}}
{"text":"To conduct our empirical study, we first fotems retrieved by the candidate generation model as candidates.","cats":{"recommender":0}}
{"text":"We adopt a specific prompting approach to solving the ranking task by Lve several key findings for the use of LLMs in recommender systems.","cats":{"recommender":0}}
{"text":"We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candiompting and bootstrapping strategies.","cats":{"recommender":0}}
{"text":"The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank.","cats":{"recommender":0}}
{"text":"It initially grounds LLMs to the recommendation space by fine-tuning them to generate meaningful tokens for items and subsequently identifies appropriate actual items that correspond to the generated tokens.","cats":{"recommender":1}}
{"text":"Furthermore, we observe that the marginal benefits derived from increasing the quantity of training samples are modest for BIGRec, implying that LLMs possess the limited capability to assimilate statistical information, such as popularity and collaborative filtering, due to their robust semantic priors.","cats":{"recommender":1}}
{"text":" However, existing approaches for LLM4Rec often assess performance using restricted sets of candidates, which may not accurately reflect the models' overall ranking capabilities.","cats":{"recommender":0}}
{"text":"It initially grounds LLMs to the recommendation space batasets, we substantiate the superior performance, capacity for handling few-shot scenarios, and versatility across multiple domains exhibited by BIGRec.","cats":{"recommender":0}}
{"text":"Furthermore, we observe that the marginal beative filtering, due to their robust semantic priors.","cats":{"recommender":0}}
{"text":"These findings also underline the efficacy of integrating diverse statistical information into the LLM4Rec framework, thereby pointing towards a potential avenue for future research.","cats":{"recommender":0}}
{"text":"Our code and data are available at https://github.com/SAI990323/Grounding4Rec.","cats":{"recommender":0}}
{"text":"Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot).","cats":{"recommender":1}}
{"text":"Traditional recommender systems leverage users' item preference history to recommend novel content that users may like.","cats":{"recommender":0}}
{"text":"However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input.  ","cats":{"recommender":0}}
{"text":"To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items.","cats":{"recommender":0}}
{"text":"Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training","cats":{"recommender":0}}
