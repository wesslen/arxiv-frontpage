{"text":"This paper aims to tackle the problem of modeling dynamic urban street scenes from monocular videos.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"However, significant limitations are their slow training and rendering speed, coupled with the critical need for high precision in tracked vehicle poses.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We introduce Street Gaussians, a new explicit scene representation that tackles all these limitations.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Specifically, the dynamic urban street is represented as a set of point clouds equipped with semantic logits and 3D Gaussians, each associated with either a foreground vehicle or the background.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"To model the dynamics of foreground object vehicles, each object point cloud is optimized with optimizable tracked poses, along with a dynamic spherical harmonics model for the dynamic appearance.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The explicit representation allows easy composition of object vehicles and background, which in turn allows for scene editing operations and rendering at 133 FPS (1066$\\times$1600 resolution) within half an hour of training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The proposed method is evaluated on multiple challenging benchmarks, including KITTI and Waymo Open datasets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Experiments show that the proposed method consistently outperforms state-of-the-art methods across all datasets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Furthermore, the proposed representation delivers performance on par with that achieved using precise ground-truth poses, despite relying only on poses from an off-the-shelf tracker.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"The code is available at https://zju3dv.github.io/street_gaussians/.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0,"programming":0}}
{"text":"In this paper, we report on the first year of TREC iKAT, describing the task, topics, data collection, and evaluation framework.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"We further review the submissions and summarize the findings.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"programming":0}}
{"text":"It generates a linearized graph where nodes represent text spans and edges represent relation triplets.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"recommender":0,"programming":0}}
{"text":"Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Code is available at https://github.com/urchade/ATG.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"The two levels are computed by the original model's self-attention, which means the proposed does not require any training.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0}}
{"text":"In this paper, we consider classes of decision tables closed under removal of attributes (columns) and changing of decisions attached to rows.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"For decision tables from closed classes, we study lower bounds on the minimum cardinality of reducts, which are minimal sets of attributes that allow us to recognize, for a given row, the decision attached to it.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"recommender":0}}
{"text":"We assume that the number of rows in decision tables from the closed class is not bounded from above by a constant.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"education":0}}
{"text":"We divide the set of such closed classes into two families.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"In one family, only standard lower bounds $\\Omega (\\log $ ${\\rm cl}(T))$ on the minimum cardinality of reducts for decision tables hold, where ${\\rm cl}(T)$ is the number of decision classes in the table $T$. In another family, these bounds can be essentially tightened up to $\\Omega ({\\rm cl}(T)^{1/q})$ for some natural $q$.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"We then propose a transformation from this 3-sort NFA into weighted-frequency and probabilistic NFA, and we apply the latter to a classification task.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"recommender":0}}
{"text":"The experimental evaluation of our approach shows that the probabilistic NFAs can be successfully applied for classification tasks on both real-life and superficial benchmark data sets.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives.","cats":{"prompt-engineering":0,"robustness":1,"security":0}}
{"text":"The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0}}
{"text":"Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023).","cats":{"prompt-engineering":1,"robustness":0,"security":0}}
{"text":"Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0}}
{"text":"This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"social-sciences":0}}
{"text":"Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.","cats":{"prompt-engineering":0,"robustness":1,"security":0,"hci":0,"social-sciences":0,"education":0}}
{"text":"Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs).","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"education":0,"recommender":0,"programming":0}}
{"text":"Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"education":0,"programming":0}}
{"text":"However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions about the information space include: finding options, comparing options, identifying the pros and cons of options, etc.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"Given the different personas and their information need (expressed through the sequence of questions), diverse conversation trajectories will arise -- because the answers to these similar queries will be very different.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"programming":0}}
{"text":"In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \\textit{span-based}.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"This work elicits LLMs' inherent ability to handle long contexts without fine-tuning.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"We propose Self-Extend to stimulate LLMs' long context handling potential.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"social-sciences":0,"programming":0}}
{"text":"The basic idea is to construct bi-level attention information: the group level and the neighbor level.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0,"programming":0}}
{"text":"Grammatical inference consists in learning a language or a grammar from data.","cats":{"prompt-engineering":0,"robustness":0,"security":0}}
{"text":"While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input.","cats":{"prompt-engineering":1,"robustness":1,"security":0,"social-sciences":1,"recommender":0}}
{"text":"Large Language Models (LLMs) have revolutionized Natural Language Processing but exhibit limitations, particularly in autonomously addressing novel challenges such as reasoning and problem-solving.","cats":{"prompt-engineering":1,"robustness":0,"security":0}}
{"text":"Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication.","cats":{"prompt-engineering":0,"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization.","cats":{"prompt-engineering":0,"robustness":0,"hci":0}}
{"text":"To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs.","cats":{"prompt-engineering":0,"robustness":1}}
{"text":"Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more.","cats":{"robustness":1}}
{"text":"There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment.","cats":{"robustness":0}}
{"text":"We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.","cats":{"robustness":1}}
{"text":"The API implements a collection of privacy-enhancing guardrails including contribution bounding and noise injection.","cats":{"robustness":1}}
{"text":"Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions.","cats":{"robustness":1}}
{"text":"While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field.","cats":{"robustness":0}}
{"text":"We validate the contribution of the dataset in the scenario of safety prevention.","cats":{"robustness":0}}
{"text":"In this paper, we target the problem of safe exploration by guiding the training with counterexamples of the safety requirement.","cats":{"robustness":0}}
{"text":"After the target task is revealed, safety violations are not allowed anymore.","cats":{"robustness":0}}
{"text":"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.","cats":{"robustness":1}}
{"text":"The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks.","cats":{"robustness":1}}
{"text":"We hypothesize two failure modes of safety training: competing objectives and mismatched generalization.","cats":{"robustness":0}}
{"text":"Further, we showcase an HRI scheme that addresses human factors and perceived safety as high-level constraints on a validated impact safety paradigm.","cats":{"robustness":0}}
{"text":"Motion planning and control methods, for instance, must be able to certify safety while operating in real-time in arbitrary environments and in the presence of model uncertainty.","cats":{"robustness":0}}
{"text":"An example is shown how to extend the analysis to cover more safety properties.   ","cats":{"robustness":0}}
{"text":"However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights.","cats":{"robustness":0}}
{"text":"This makes our safety model highly general.","cats":{"robustness":0}}
{"text":"A common approach is to conduct safety validation based on a predefined Operational Design Domain (ODD) describing specific conditions under which a system under test is required to operate properly.","cats":{"robustness":0}}
{"text":"Videos of experimental validation can be found on our project website at https://sites.google.com/view/lives-icra-2024/home.","cats":{"robustness":0}}
{"text":"This paper presents Safety and Over-Defensiveness Evaluation (SODE) benchmark: a collection of diverse safe and unsafe prompts with carefully designed evaluation methods that facilitate systematic evaluation, comparison, and analysis over 'safety' and 'over-defensiveness.'","cats":{"robustness":1}}
{"text":"Internal, temporal, and external validations demonstrate the effectiveness of the proposed method.","cats":{"robustness":0}}
{"text":"Especially for detecting humans, which is often a fundamental task in safety-critical applications, it is vital to prevent errors.","cats":{"robustness":0}}
{"text":"The results indicate the need for careful validation and tailored prompt engineering.","cats":{"robustness":0,"social-sciences":0}}
{"text":"We demonstrate that they can help accurately differentiate obstacles that are safe to navigate through (e.g. beaded/string curtains, pliable tall grass), from ones that must be avoided (e.g. transparent surfaces such as glass walls, bushes, trees, etc.)","cats":{"robustness":0}}
{"text":"Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable.","cats":{"robustness":1}}
{"text":"The safety-liveness dichotomy is a fundamental concept in formal languages which plays a key role in verification.","cats":{"robustness":0}}
{"text":"Reliable detection of morphing attacks is essential because these attacks are targeted for border control applications.","cats":{"robustness":0}}
{"text":"To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action).","cats":{"robustness":0}}
{"text":"We apply these techniques to propose a verifiable collision avoidance solution for autonomous aerial mobility vehicles operating in cluttered and potentially unsafe environments.","cats":{"robustness":0}}
{"text":"Shields are correct-by-construction runtime enforcers that guarantee safe execution by correcting any action that may cause a violation of a formal safety specification.","cats":{"robustness":0}}
{"text":"Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems.","cats":{"robustness":0}}
{"text":"Our framework guarantees robot safety while adhering to driver intention.","cats":{"robustness":0}}
{"text":"With SODE, we study a variety of LLM defense strategies over multiple state-of-the-art LLMs, which reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue over-defensiveness of the models, (c) providing contextual knowledge easily breaks the safety guardrails and makes the models more vulnerable to generating unsafe responses.","cats":{"robustness":1}}
{"text":"A key challenge remains to ensure reliable predictions, especially in safety-critical applications.","cats":{"robustness":0}}
{"text":"This assurance is effectively realized through the utilization of Safety Assurance Cases.","cats":{"robustness":0}}
{"text":"Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts.","cats":{"robustness":1,"social-sciences":0}}
{"text":"We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency.","cats":{"robustness":0,"security":0,"hci":0,"programming":0}}
{"text":"Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area.","cats":{"robustness":0,"security":0,"hci":0,"social-sciences":0}}
{"text":"(2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases.","cats":{"robustness":1,"security":0,"hci":0,"social-sciences":0}}
{"text":"(4) We provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations.","cats":{"robustness":1,"hci":0,"social-sciences":0,"education":0}}
{"text":"Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.","cats":{"robustness":0,"hci":0}}
{"text":"NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more.","cats":{"robustness":1}}
{"text":"Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.","cats":{"robustness":1}}
{"text":"Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, indepesafe LLM applications using programmable rails.","cats":{"robustness":0}}
{"text":"Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems.","cats":{"robustness":0}}
{"text":"The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.   ","cats":{"robustness":0}}
{"text":"In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element.","cats":{"robustness":0}}
{"text":"On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF).","cats":{"robustness":0}}
{"text":"On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.   ","cats":{"robustness":0}}
{"text":"In particular, this paper introduces an additional categorization for a better understanding as well as enabling cross-functional teams to jointly address the concerns.","cats":{"robustness":0}}
{"text":"(1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area.","cats":{"robustness":1}}
{"text":"(2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPes.","cats":{"robustness":0}}
{"text":"(3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual questioar LLMs into legal tasks.","cats":{"robustness":0}}
{"text":"Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images.","cats":{"robustness":1}}
{"text":"To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs.","cats":{"robustness":1}}
{"text":"The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations.","cats":{"robustness":1}}
{"text":"Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families.","cats":{"robustness":1}}
{"text":"Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.","cats":{"robustness":1}}
{"text":"Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned.  ","cats":{"robustness":0}}
{"text":"To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputis closely grounded to visual inputs, resulting in contextually accurate outputs.","cats":{"robustness":0}}
{"text":"Our experiments show that VCD, without either additional training or the usage of external tools, significantlyLVLM benchmarks, highlighting its wide-ranging applicability.","cats":{"robustness":0}}
{"text":"Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers?","cats":{"robustness":1}}
{"text":"We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries.","cats":{"robustness":1}}
{"text":"This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy.","cats":{"robustness":1}}
{"text":"We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2.","cats":{"robustness":1}}
{"text":"We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2.","cats":{"robustness":1}}
{"text":"The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.","cats":{"robustness":1}}
{"text":"We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstraenchmark, achieving 76.2% accuracy.","cats":{"robustness":0}}
{"text":"We then use this method to estimate how often language models hallucinate when s5 for Claude 2.","cats":{"robustness":0}}
{"text":"We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49","cats":{"robustness":0}}
{"text":"We design probes trained on the internal representations of a transformer language model that are predictive of its hallucinatory behavior on in-context generation tasks.","cats":{"robustness":1}}
{"text":"To facilitate this detection, we create a span-annotated dataset of organic and synthetic hallucinations over several tasks.","cats":{"robustness":1}}
{"text":"We find that probes trained on the force-decoded states of synthetic hallucinations are generally ecologically invalid in organic hallucination detection.","cats":{"robustness":1}}
{"text":"Furthermore, hidden state information about hallucination appears to be task and distribution-dependent. Intrinsic and extrinsic hallucination saliency varies across layers, hidden state types, and tasks; notably, extrinsic hallucinations tend to be more salient in a transformer's internal representations.","cats":{"robustness":1}}
{"text":"Outperforming multiple contemporary baselines, we show that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available.","cats":{"robustness":1}}
{"text":" To facilitate this detection, we create a span-annotated dataset of organic and synthetic hallucinations over several tasks.","cats":{"robustness":0}}
{"text":"We find that probes trained on the force-decoidden state information about hallucination appears to be task and distribution-dependent.","cats":{"robustness":0}}
{"text":"Intrinsic and extrinsic hallucinatpresentations.","cats":{"robustness":0}}
{"text":"Outperforming multiple contemporary baselines, we show that probing is a feasible and efficient alternative to language model hallucination","cats":{"robustness":0}}
{"text":"We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks.","cats":{"robustness":1}}
{"text":"We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models.","cats":{"robustness":0}}
{"text":"Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally.","cats":{"robustness":0}}
{"text":"Importantly, our method denables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation.","cats":{"robustness":0}}
{"text":"We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction.","cats":{"robustness":0}}
{"text":"Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier.","cats":{"robustness":0}}
{"text":"To validate our approach, we conduct a series of simulated experiments, the results of which affirm the framework's effectiveness in identifying and addressing instances of content misuse in LLM training processes.","cats":{"robustness":1}}
{"text":"The outcomes of our study have significant implications for ensuring the ethical use of copyrighted materials in the development of LLMs, highlighting the need for more transparent and responsible data management practices in this field.","cats":{"robustness":1,"hci":0}}
{"text":"Pre-training, which utilizes extensive and varied datasets, is a critical factor in the success of Large Language Models (LLMs) across numerous applications.","cats":{"robustness":0}}
{"text":"However, the detailed makeup of these datasets is often not disclosed, leading to concerns about data security and potential misuse.","cats":{"robustness":0}}
{"text":"This is particularly relevant when copyrighted material, still under legal protection, is used inappropriately, either intentionally or unintentionally, infringing on the rights of the authors.   ","cats":{"robustness":0}}
{"text":"In this paper, we introduce a detailed framework designed to detect and assess the presence of content from potentially copyrighted books within the training datasets of LLMs.","cats":{"robustness":0,"recommender":0}}
{"text":"This framework also provides a confidence estimation for the likelihood of each content sample's inclusion.  ","cats":{"robustness":0}}
{"text":"Furthermore, we investigate the presence of recognizable quotes from famous literary works within these datasets.","cats":{"robustness":0}}
{"text":"The outcomes of our study have significant implications for ensuring the ethical use of copyrighted materials in the development of LLMs, highlighting the need for more transparent and responsible data management p","cats":{"robustness":0}}
{"text":"To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead.","cats":{"robustness":1}}
{"text":"We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities.","cats":{"robustness":1}}
{"text":"Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks.","cats":{"robustness":0}}
{"text":"These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task.","cats":{"robustness":0}}
{"text":"In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks.  ","cats":{"robustness":0}}
{"text":"We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers intere true strengths and limitations of current LLMs.","cats":{"robustness":0}}
{"text":"In this paper, we show that SOTA open-source LLMs are vulnerable to simple, optimization-free attacks we refer to as $\\textit{priming attacks}$, which are easy to execute and effectively bypass alignment from safety training.","cats":{"robustness":1}}
{"text":"With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training.  ","cats":{"robustness":0}}
{"text":"Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$ compared to baselines.","cats":{"robustness":0}}
{"text":"Source code and data are available at https://github.com/uiuc-focal-lab/llm-priming-attacks .","cats":{"robustness":0}}
{"text":"This paper proposes a study of the resilience and efficiency of automatically generated industrial automation and control systems using Large Language Models (LLMs).","cats":{"robustness":1}}
{"text":"Techniques from stochastic optimization and regret analysis are used to find a near-optimal solution with provable regret bounds.","cats":{"robustness":0}}
{"text":"The study aims to provide insights into the effectiveness and reliability of automatically generated systems in industrial automation and control, and to identify potential areas for improvement in their design and implementation.","cats":{"robustness":1}}
{"text":" The approach involves modeling the system using percolation theory to estimate its resilience and formulating the design problem as an optimization problem subject to constraints.","cats":{"robustness":0}}
{"text":"The study aims to provide insights otential areas for improvement in their design and implementation.","cats":{"robustness":0}}
{"text":"Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors.","cats":{"robustness":1,"programming":0}}
{"text":"This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance.  ","cats":{"robustness":0}}
{"text":"There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications.","cats":{"robustness":0}}
{"text":"To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modes.","cats":{"robustness":0}}
{"text":"Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge.","cats":{"robustness":0}}
{"text":"Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches.","cats":{"robustness":0,"programming":0}}
{"text":"Additionally, we provide an in-depth analysis of knowledge location, which can provide a deeper understanding of the knowledge structures inherent within LLMs.","cats":{"robustness":0}}
{"text":"Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.","cats":{"robustness":0}}
{"text":"Recently, the advent of large language models (LLMs) has revolutionized generative agents.","cats":{"robustness":0}}
{"text":"Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users.","cats":{"robustness":0,"social-sciences":0}}
{"text":"However, the absence of a comprehensive benchmark impedes progress in this field.","cats":{"robustness":0,"social-sciences":0}}
{"text":"To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset.","cats":{"robustness":0}}
{"text":"The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts.","cats":{"robustness":0}}
{"text":"It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike.","cats":{"robustness":0}}
{"text":"CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions.","cats":{"robustness":0,"education":0}}
{"text":"Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.","cats":{"robustness":0,"social-sciences":0}}
{"text":"Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.","cats":{"robustness":0}}
{"text":"The emergence of ChatGPT and other large language models (LLMs) has greatly increased interest in utilizing LLMs as therapists to support individuals struggling with mental health challenges.","cats":{"robustness":0,"hci":1,"social-sciences":1}}
{"text":"However, due to the lack of systematic studies, our understanding of how LLM therapists behave, i.e., ways in which they respond to clients, is significantly limited.","cats":{"robustness":0}}
{"text":"Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences.","cats":{"robustness":0,"hci":0}}
{"text":"In this paper, we propose BOLT, a novel computational framework to study the conversational behavior of LLMs when employed as therapists.","cats":{"robustness":0,"social-sciences":1}}
{"text":"We develop an in-context learning method to quantitatively measure the behavior of LLMs based on 13 different psychotherapy techniques including reflections, questions, solutions, normalizing, and psychoeducation.","cats":{"robustness":0}}
{"text":"Subsequently, we compare the behavior of LLM therapists against that of high- and low-quality human therapy, and study how their behavior can be modulated to better reflect behaviors observed in high-quality therapy.","cats":{"robustness":0}}
{"text":"Our analysis of GPT and Llama-variants reveals that these LLMs often resemble behaviors more commonly exhibited in low-quality therapy rather than high-quality therapy, such as offering a higher degree of problem-solving advice when clients share emotions, which is against typical recommendations.","cats":{"robustness":0,"hci":1,"recommender":0}}
{"text":"At the same time, unlike low-quality therapy, LLMs reflect significantly more upon clients' needs and strengths.","cats":{"robustness":0}}
{"text":"Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care, and thus require additional research to ensure quality care.","cats":{"robustness":0,"recommender":0}}
{"text":"The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code).","cats":{"robustness":0,"social-sciences":0,"education":0,"programming":1}}
{"text":"As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity.","cats":{"robustness":0,"programming":0}}
{"text":"In this survey, we present an overview of the various benefits of integrating code into LLMs' training data.","cats":{"robustness":0}}
{"text":"Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement.","cats":{"robustness":0,"programming":1}}
{"text":"In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks.","cats":{"robustness":0,"recommender":0}}
{"text":"Finally, we present several key challenges and future directions of empowering LLMs with code.","cats":{"robustness":0}}
{"text":"The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence.","cats":{"robustness":0}}
{"text":"These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities.","cats":{"robustness":0}}
{"text":"This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs.","cats":{"robustness":0}}
{"text":"We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design.","cats":{"robustness":0}}
{"text":"Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques.","cats":{"robustness":0}}
{"text":"A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques.","cats":{"robustness":0}}
{"text":"By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.","cats":{"robustness":0}}
{"text":"Large Lanugage Models (LLMs) are gaining increasing popularity in a variety of use cases, from language understanding and writing to assistance in application development.","cats":{"robustness":0}}
{"text":"One of the most important aspects for optimal funcionality of LLMs is embedding layers.","cats":{"robustness":0}}
{"text":"Word embeddings are distributed representations of words in a continuous vector space.","cats":{"robustness":0}}
{"text":"In the context of LLMs, words or tokens from the input text are transformed into high-dimensional vectors using unique algorithms specific to the model.","cats":{"robustness":0}}
{"text":"Our research examines the embedding algorithms from leading companies in the industry, such as OpenAI, Google's PaLM, and BERT.","cats":{"robustness":0}}
{"text":"Using medical data, we have analyzed similarity scores of each embedding layer, observing differences in performance among each algorithm.","cats":{"robustness":0}}
{"text":"To enhance each model and provide an additional encoding layer, we also implemented Siamese Neural Networks.","cats":{"robustness":0}}
{"text":"After observing changes in performance with the addition of the model, we measured the carbon footage per epoch of training.","cats":{"robustness":0}}
{"text":"The carbon footprint associated with large language models (LLMs) is a significant concern, and should be taken into consideration when selecting algorithms for a variety of use cases.","cats":{"robustness":0}}
{"text":"Overall, our research compared the accuracy different, leading embedding algorithms and their carbon footage, allowing for a holistic review of each embedding algorithm.","cats":{"robustness":0}}
{"text":"The research addresses concerns related to hallucinations and false research articles by introducing a custom workflow developed with a detection algorithm to filter out inaccuracies.","cats":{"robustness":1}}
{"text":"This research explores the integration of large language models (LLMs) into scientific data assimilation, focusing on combustion science as a case study.","cats":{"robustness":0}}
{"text":"Leveraging foundational models integrated with Retrieval-Augmented Generation (RAG) framework, the study introduces an approach to process diverse combustion research data, spanning experimental studies, simulations, and literature.","cats":{"robustness":0}}
{"text":"The multifaceted nature of combustion research emphasizes the critical role of knowledge processing in navigating and extracting valuable information from a vast and diverse pool of sources.","cats":{"robustness":0}}
{"text":"The developed approach minimizes computational and economic expenses while optimizing data privacy and accuracy.","cats":{"robustness":0}}
{"text":"It incorporates prompt engineering and offline open-source LLMs, offering user autonomy in selecting base models.","cats":{"robustness":0,"social-sciences":0}}
{"text":"The study provides a thorough examination of text segmentation strategies, conducts comparative studies between LLMs, and explores various optimized prompts to demonstrate the effectiveness of the framework.","cats":{"robustness":0}}
{"text":"By incorporating an external database, the framework outperforms a conventional LLM in generating accurate responses and constructing robust arguments.","cats":{"robustness":0}}
{"text":"Additionally, the study delves into the investigation of optimized prompt templates for the purpose of efficient extraction of scientific literature.  ","cats":{"robustness":0}}
{"text":"Despite identified areas for improvement, the framework consistently delivers accurate domain-specific responses with minimal human oversight.","cats":{"robustness":0}}
{"text":"The prompt-agnostic approach introduced holds promise for future deliberations.","cats":{"robustness":0}}
{"text":"The study underscores the significance of integrating LLMs and knowledge processing techniques in scientific research, providing a foundation for advancements in data assimilation and utilization.","cats":{"robustness":0}}
{"text":"This study focuses on emotion-sensitive spoken dialogue in human-machine speech interaction.","cats":{"robustness":0,"hci":1}}
{"text":"With the advancement of Large Language Models (LLMs), dialogue systems can handle multimodal data, including audio.","cats":{"robustness":0}}
{"text":"Recent models have enhanced the understanding of complex audio signals through the integration of various audio events.","cats":{"robustness":0}}
{"text":"However, they are unable to generate appropriate responses based on emotional speech.","cats":{"robustness":0}}
{"text":"To address this, we introduce the Emotional chat Model (E-chat), a novel spoken dialogue system capable of comprehending and responding to emotions conveyed from speech.","cats":{"robustness":0}}
{"text":"This model leverages an emotion embedding extracted by a speech encoder, combined with LLMs, enabling it to respond according to different emotional contexts.","cats":{"robustness":0}}
{"text":"Additionally, we introduce the E-chat200 dataset, designed explicitly for emotion-sensitive spoken dialogue.","cats":{"robustness":0}}
{"text":"In various evaluation metrics, E-chat consistently outperforms baseline LLMs, demonstrating its potential in emotional comprehension and human-machine interaction.","cats":{"robustness":0,"hci":1}}
{"text":"The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity.","cats":{"robustness":1,"hci":0}}
{"text":"Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities.","cats":{"robustness":0}}
{"text":"The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity.","cats":{"robustness":0}}
{"text":"Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations.","cats":{"robustness":0}}
{"text":"This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure.  ","cats":{"robustness":0}}
{"text":"Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning).","cats":{"robustness":0}}
{"text":"We further contrast such approach against supervised fine-tuning (SFT) of encoder-only LLMs.","cats":{"robustness":0}}
{"text":"Our results reveal that both the direct-use of decoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-only LLMs offer inaccurate interpretation of cyberattack procedures.","cats":{"robustness":0}}
{"text":"Significant improvements are shown when RAG is used for decoder-only LLMs, particularly when directly relevant context is found.","cats":{"robustness":0}}
{"text":"This study further sheds insights on the limitations and capabilities of using RAG for LLMs in interpreting TTPs.","cats":{"robustness":0}}
{"text":"Large language models (LLMs) have made significant advancements in natural language processing and are concurrently extending the language ability to other modalities, such as speech and vision.","cats":{"robustness":0}}
{"text":"Nevertheless, most of the previous work focuses on prompting LLMs with perception abilities like auditory comprehension, and the effective approach for augmenting LLMs with speech synthesis capabilities remains ambiguous.","cats":{"robustness":0}}
{"text":"In this paper, we conduct a comprehensive empirical exploration of boosting LLMs with the ability to generate speech, by combining pre-trained LLM LLaMA/OPT and text-to-speech synthesis model","cats":{"robustness":0}}
{"text":"VALL-E. We compare three integration methods between LLMs and speech synthesis models, including directly fine-tuned LLMs, superposed layers of LLMs and VALL-E, and coupled LLMs and VALL-E using LLMs as a powerful text encoder.","cats":{"robustness":0}}
{"text":"Experimental results show that, using LoRA method to fine-tune LLMs directly to boost the speech synthesis capability does not work well, and superposed LLMs and VALL-E can improve the quality of generated speech both in speaker similarity and word error rate (WER).","cats":{"robustness":0}}
{"text":"Among these three methods, coupled methods leveraging LLMs as the text encoder can achieve the best performance, making it outperform original speech synthesis models with a consistently better speaker similarity and a significant (10.9%) WER reduction.","cats":{"robustness":0}}
