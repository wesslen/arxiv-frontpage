{"created":"2023-12-21 18:59:57","title":"3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera","abstract":"3D hand tracking from a monocular video is a very challenging problem due to hand interactions, occlusions, left-right hand ambiguity, and fast motion. Most existing methods rely on RGB inputs, which have severe limitations under low-light conditions and suffer from motion blur. In contrast, event cameras capture local brightness changes instead of full image frames and do not suffer from the described effects. Unfortunately, existing image-based techniques cannot be directly applied to events due to significant differences in the data modalities. In response to these challenges, this paper introduces the first framework for 3D tracking of two fast-moving and interacting hands from a single monocular event camera. Our approach tackles the left-right hand ambiguity with a novel semi-supervised feature-wise attention mechanism and integrates an intersection loss to fix hand collisions. To facilitate advances in this research domain, we release a new synthetic large-scale dataset of two interacting hands, Ev2Hands-S, and a new real benchmark with real event streams and ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing methods in terms of the 3D reconstruction accuracy and generalises to real data under severe light conditions.","sentences":["3D hand tracking from a monocular video is a very challenging problem due to hand interactions, occlusions, left-right hand ambiguity, and fast motion.","Most existing methods rely on RGB inputs, which have severe limitations under low-light conditions and suffer from motion blur.","In contrast, event cameras capture local brightness changes instead of full image frames and do not suffer from the described effects.","Unfortunately, existing image-based techniques cannot be directly applied to events due to significant differences in the data modalities.","In response to these challenges, this paper introduces the first framework for 3D tracking of two fast-moving and interacting hands from a single monocular event camera.","Our approach tackles the left-right hand ambiguity with a novel semi-supervised feature-wise attention mechanism and integrates an intersection loss to fix hand collisions.","To facilitate advances in this research domain, we release a new synthetic large-scale dataset of two interacting hands, Ev2Hands-S, and a new real benchmark with real event streams and ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing methods in terms of the 3D reconstruction accuracy and generalises to real data under severe light conditions."],"url":"http://arxiv.org/abs/2312.14157v1"}
{"created":"2023-12-21 18:59:30","title":"Virtual Pets: Animatable Animal Generation in 3D Scenes","abstract":"Toward unlocking the potential of generative models in immersive 4D experiences, we introduce Virtual Pet, a novel pipeline to model realistic and diverse motions for target animal species within a 3D environment. To circumvent the limited availability of 3D motion data aligned with environmental geometry, we leverage monocular internet videos and extract deformable NeRF representations for the foreground and static NeRF representations for the background. For this, we develop a reconstruction strategy, encompassing species-level shared template learning and per-video fine-tuning. Utilizing the reconstructed data, we then train a conditional 3D motion model to learn the trajectory and articulation of foreground animals in the context of 3D backgrounds. We showcase the efficacy of our pipeline with comprehensive qualitative and quantitative evaluations using cat videos. We also demonstrate versatility across unseen cats and indoor environments, producing temporally coherent 4D outputs for enriched virtual experiences.","sentences":["Toward unlocking the potential of generative models in immersive 4D experiences, we introduce Virtual Pet, a novel pipeline to model realistic and diverse motions for target animal species within a 3D environment.","To circumvent the limited availability of 3D motion data aligned with environmental geometry, we leverage monocular internet videos and extract deformable NeRF representations for the foreground and static NeRF representations for the background.","For this, we develop a reconstruction strategy, encompassing species-level shared template learning and per-video fine-tuning.","Utilizing the reconstructed data, we then train a conditional 3D motion model to learn the trajectory and articulation of foreground animals in the context of 3D backgrounds.","We showcase the efficacy of our pipeline with comprehensive qualitative and quantitative evaluations using cat videos.","We also demonstrate versatility across unseen cats and indoor environments, producing temporally coherent 4D outputs for enriched virtual experiences."],"url":"http://arxiv.org/abs/2312.14154v1"}
{"created":"2023-12-21 18:59:12","title":"DriveLM: Driving with Graph Visual Question Answering","abstract":"We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.","sentences":["We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users.","While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps.","Starting from the localization of key objects, humans estimate object interactions before taking actions.","The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process.","We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving.","The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task.","Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures.","Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations.","We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving.","To facilitate future research, all code, data, and models are available to the public."],"url":"http://arxiv.org/abs/2312.14150v1"}
{"created":"2023-12-21 18:59:06","title":"TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification","abstract":"The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data. Existing attempts usually face the problem of coarse alignment, \\textit{e.g.}, the vision encoder struggles in localizing an attribute-specified object. In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs. Concretely, given an image and its paired text, we manage to parse objects (\\textit{e.g.}, cat) and attributes (\\textit{e.g.}, black) from the description, which are highly likely to exist in the image. It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability. With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss. Extensive experimental results on a broad suite of semantic segmentation datasets substantiate the average 3.65\\% improvement of our framework over existing alternatives. Furthermore, the visualization results indicate that attribute supervision makes vision-language models accurately localize attribute-specified objects. Project page can be found at https://qinying-liu.github.io/Tag-Align/","sentences":["The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data.","Existing attempts usually face the problem of coarse alignment, \\textit{e.g.}, the vision encoder struggles in localizing an attribute-specified object.","In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs.","Concretely, given an image and its paired text, we manage to parse objects (\\textit{e.g.}, cat) and attributes (\\textit{e.g.}, black) from the description, which are highly likely to exist in the image.","It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability.","With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss.","Extensive experimental results on a broad suite of semantic segmentation datasets substantiate the average 3.65\\% improvement of our framework over existing alternatives.","Furthermore, the visualization results indicate that attribute supervision makes vision-language models accurately localize attribute-specified objects.","Project page can be found at https://qinying-liu.github.io/Tag-Align/"],"url":"http://arxiv.org/abs/2312.14149v1"}
{"created":"2023-12-21 18:57:52","title":"HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs","abstract":"Current advances in human head modeling allow to generate plausible-looking 3D head models via neural representations. Nevertheless, constructing complete high-fidelity head models with explicitly controlled animation remains an issue. Furthermore, completing the head geometry based on a partial observation, e.g. coming from a depth sensor, while preserving details is often problematic for the existing methods. We introduce a generative model for detailed 3D head meshes on top of an articulated 3DMM which allows explicit animation and high-detail preservation at the same time. Our method is trained in two stages. First, we register a parametric head model with vertex displacements to each mesh of the recently introduced NPHM dataset of accurate 3D head scans. The estimated displacements are baked into a hand-crafted UV layout. Second, we train a StyleGAN model in order to generalize over the UV maps of displacements. The decomposition of the parametric model and high-quality vertex displacements allows us to animate the model and modify it semantically. We demonstrate the results of unconditional generation and fitting to the full or partial observation. The project page is available at https://seva100.github.io/headcraft.","sentences":["Current advances in human head modeling allow to generate plausible-looking 3D head models via neural representations.","Nevertheless, constructing complete high-fidelity head models with explicitly controlled animation remains an issue.","Furthermore, completing the head geometry based on a partial observation, e.g. coming from a depth sensor, while preserving details is often problematic for the existing methods.","We introduce a generative model for detailed 3D head meshes on top of an articulated 3DMM which allows explicit animation and high-detail preservation at the same time.","Our method is trained in two stages.","First, we register a parametric head model with vertex displacements to each mesh of the recently introduced NPHM dataset of accurate 3D head scans.","The estimated displacements are baked into a hand-crafted UV layout.","Second, we train a StyleGAN model in order to generalize over the UV maps of displacements.","The decomposition of the parametric model and high-quality vertex displacements allows us to animate the model and modify it semantically.","We demonstrate the results of unconditional generation and fitting to the full or partial observation.","The project page is available at https://seva100.github.io/headcraft."],"url":"http://arxiv.org/abs/2312.14140v1"}
{"created":"2023-12-21 18:57:12","title":"Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach","abstract":"Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss. However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F\\&B) snippets. To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F\\&B separation algorithm. It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accurately associated with their F\\&B labels, thereby boosting the F\\&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods. Code is available at https://github.com/Qinying-Liu/CASE","sentences":["Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels.","Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss.","However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F\\&B) snippets.","To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss.","Specifically, we propose a novel clustering-based F\\&B separation algorithm.","It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background.","As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions.","This ensures that the cluster assignments of the snippets can be accurately associated with their F\\&B labels, thereby boosting the F\\&B separation.","We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3.","Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods.","Code is available at https://github.com/Qinying-Liu/CASE"],"url":"http://arxiv.org/abs/2312.14138v1"}
{"created":"2023-12-21 18:55:06","title":"$\\textit{V}^*$: Guided Visual Search as a Core Mechanism in Multimodal LLMs","abstract":"When we look around and perform complex tasks, how we see and selectively process what we see is crucial. However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images. To address this, we introduce $\\textit{V}^*$, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements. This integration results in a new MLLM meta-architecture, named $\\textbf{S}$how, s$\\textbf{EA}$rch, and Tel$\\textbf{L}$ (SEAL). We further create $\\textit{V}^*$Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details. Our study highlights the necessity of incorporating visual search capabilities into multimodal systems. The code is available https://github.com/penghao-wu/vstar.","sentences":["When we look around and perform complex tasks, how we see and selectively process what we see is crucial.","However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images.","To address this, we introduce $\\textit{V}^*$, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying.","When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements.","This integration results in a new MLLM meta-architecture, named $\\textbf{S}$how, s$\\textbf{EA}$rch, and Tel$\\textbf{L}$ (SEAL).","We further create $\\textit{V}^*$Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details.","Our study highlights the necessity of incorporating visual search capabilities into multimodal systems.","The code is available https://github.com/penghao-wu/vstar."],"url":"http://arxiv.org/abs/2312.14135v1"}
{"created":"2023-12-21 18:55:05","title":"Diffusion Reward: Learning Rewards via Conditional Video Diffusion","abstract":"Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is observed when conditioned on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors. We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward. Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io/.","sentences":["Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks.","In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems.","Our key insight is that lower generative diversity is observed when conditioned on expert trajectories.","Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors.","We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward.","Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods.","Project page and code: https://diffusion-reward.github.io/."],"url":"http://arxiv.org/abs/2312.14134v1"}
{"created":"2023-12-21 18:52:14","title":"DUSt3R: Geometric 3D Vision Made Easy","abstract":"Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms. In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses. We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases. In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame. We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models. Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera. Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation. In summary, DUSt3R makes many geometric 3D vision tasks easy.","sentences":["Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters.","These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms.","In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses.","We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models.","We show that this formulation smoothly unifies the monocular and binocular reconstruction cases.","In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame.","We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models.","Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute camera.","Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation.","In summary, DUSt3R makes many geometric 3D vision tasks easy."],"url":"http://arxiv.org/abs/2312.14132v1"}
{"created":"2023-12-21 18:49:22","title":"WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data","abstract":"In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's effectiveness. It produces better results compared to other existing methods in classification performance, yields meaningful clustering of patients, and delivers consistent results in patient similarity searches and predictions.","sentences":["In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals.","To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources.","Central to our approach is the utilization of constrained low-rank approximation.","WellFactor is optimized to handle the sparsity that is often inherent in healthcare data.","Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients.","One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding.","Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's effectiveness.","It produces better results compared to other existing methods in classification performance, yields meaningful clustering of patients, and delivers consistent results in patient similarity searches and predictions."],"url":"http://arxiv.org/abs/2312.14129v1"}
{"created":"2023-12-21 18:47:12","title":"Entropic Open-set Active Learning","abstract":"Active Learning (AL) aims to enhance the performance of deep models by selecting the most informative samples for annotation from a pool of unlabeled data. Despite impressive performance in closed-set settings, most AL methods fail in real-world scenarios where the unlabeled data contains unknown categories. Recently, a few studies have attempted to tackle the AL problem for the open-set setting. However, these methods focus more on selecting known samples and do not efficiently utilize unknown samples obtained during AL rounds. In this work, we propose an Entropic Open-set AL (EOAL) framework which leverages both known and unknown distributions effectively to select informative samples during AL rounds. Specifically, our approach employs two different entropy scores. One measures the uncertainty of a sample with respect to the known-class distributions. The other measures the uncertainty of the sample with respect to the unknown-class distributions. By utilizing these two entropy scores we effectively separate the known and unknown samples from the unlabeled data resulting in better sampling. Through extensive experiments, we show that the proposed method outperforms existing state-of-the-art methods on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Code is available at \\url{https://github.com/bardisafa/EOAL}.","sentences":["Active Learning (AL) aims to enhance the performance of deep models by selecting the most informative samples for annotation from a pool of unlabeled data.","Despite impressive performance in closed-set settings, most AL methods fail in real-world scenarios where the unlabeled data contains unknown categories.","Recently, a few studies have attempted to tackle the AL problem for the open-set setting.","However, these methods focus more on selecting known samples and do not efficiently utilize unknown samples obtained during AL rounds.","In this work, we propose an Entropic Open-set AL (EOAL) framework which leverages both known and unknown distributions effectively to select informative samples during AL rounds.","Specifically, our approach employs two different entropy scores.","One measures the uncertainty of a sample with respect to the known-class distributions.","The other measures the uncertainty of the sample with respect to the unknown-class distributions.","By utilizing these two entropy scores we effectively separate the known and unknown samples from the unlabeled data resulting in better sampling.","Through extensive experiments, we show that the proposed method outperforms existing state-of-the-art methods on CIFAR-10, CIFAR-100, and TinyImageNet datasets.","Code is available at \\url{https://github.com/bardisafa/EOAL}."],"url":"http://arxiv.org/abs/2312.14126v1"}
{"created":"2023-12-21 18:46:41","title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation","abstract":"We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/","sentences":["We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals.","VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio.","The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation.","During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework.","The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks.","We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions.","Project page: http://sites.research.google/videopoet/"],"url":"http://arxiv.org/abs/2312.14125v1"}
{"created":"2023-12-21 18:46:27","title":"Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation","abstract":"Controllable generation of 3D assets is important for many practical applications like content creation in movies, games and engineering, as well as in AR/VR. Recently, diffusion models have shown remarkable results in generation quality of 3D objects. However, none of the existing models enable disentangled generation to control the shape and appearance separately. For the first time, we present a suitable representation for 3D diffusion models to enable such disentanglement by introducing a hybrid point cloud and neural radiance field approach. We model a diffusion process over point positions jointly with a high-dimensional feature space for a local density and radiance decoder. While the point positions represent the coarse shape of the object, the point features allow modeling the geometry and appearance details. This disentanglement enables us to sample both independently and therefore to control both separately. Our approach sets a new state of the art in generation compared to previous disentanglement-capable methods by reduced FID scores of 30-90% and is on-par with other non disentanglement-capable state-of-the art methods.","sentences":["Controllable generation of 3D assets is important for many practical applications like content creation in movies, games and engineering, as well as in AR/VR.","Recently, diffusion models have shown remarkable results in generation quality of 3D objects.","However, none of the existing models enable disentangled generation to control the shape and appearance separately.","For the first time, we present a suitable representation for 3D diffusion models to enable such disentanglement by introducing a hybrid point cloud and neural radiance field approach.","We model a diffusion process over point positions jointly with a high-dimensional feature space for a local density and radiance decoder.","While the point positions represent the coarse shape of the object, the point features allow modeling the geometry and appearance details.","This disentanglement enables us to sample both independently and therefore to control both separately.","Our approach sets a new state of the art in generation compared to previous disentanglement-capable methods by reduced FID scores of 30-90% and is on-par with other non disentanglement-capable state-of-the art methods."],"url":"http://arxiv.org/abs/2312.14124v1"}
{"created":"2023-12-21 18:44:19","title":"Fast and Knowledge-Free Deep Learning for General Game Playing (Student Abstract)","abstract":"We develop a method of adapting the AlphaZero model to General Game Playing (GGP) that focuses on faster model generation and requires less knowledge to be extracted from the game rules. The dataset generation uses MCTS playing instead of self-play; only the value network is used, and attention layers replace the convolutional ones. This allows us to abandon any assumptions about the action space and board topology. We implement the method within the Regular Boardgames GGP system and show that we can build models outperforming the UCT baseline for most games efficiently.","sentences":["We develop a method of adapting the AlphaZero model to General Game Playing (GGP) that focuses on faster model generation and requires less knowledge to be extracted from the game rules.","The dataset generation uses MCTS playing instead of self-play; only the value network is used, and attention layers replace the convolutional ones.","This allows us to abandon any assumptions about the action space and board topology.","We implement the method within the Regular Boardgames GGP system and show that we can build models outperforming the UCT baseline for most games efficiently."],"url":"http://arxiv.org/abs/2312.14121v1"}
{"created":"2023-12-21 18:40:34","title":"LingoQA: Video Question Answering for Autonomous Driving","abstract":"Autonomous driving has long faced a challenge with public acceptance due to the lack of explainability in the decision-making process. Video question-answering (QA) in natural language provides the opportunity for bridging this gap. Nonetheless, evaluating the performance of Video QA models has proved particularly tough due to the absence of comprehensive benchmarks. To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous driving Video QA. The LingoQA trainable metric demonstrates a 0.95 Spearman correlation coefficient with human evaluations. We introduce a Video QA dataset of central London consisting of 419k samples that we release with the paper. We establish a baseline vision-language model and run extensive ablation studies to understand its performance.","sentences":["Autonomous driving has long faced a challenge with public acceptance due to the lack of explainability in the decision-making process.","Video question-answering (QA) in natural language provides the opportunity for bridging this gap.","Nonetheless, evaluating the performance of Video QA models has proved particularly tough due to the absence of comprehensive benchmarks.","To fill this gap, we introduce LingoQA, a benchmark specifically for autonomous driving Video QA.","The LingoQA trainable metric demonstrates a 0.95 Spearman correlation coefficient with human evaluations.","We introduce a Video QA dataset of central London consisting of 419k samples that we release with the paper.","We establish a baseline vision-language model and run extensive ablation studies to understand its performance."],"url":"http://arxiv.org/abs/2312.14115v1"}
{"created":"2023-12-21 18:31:33","title":"Learning Human-like Representations to Enable Learning Human Values","abstract":"How can we build AI systems that are aligned with human values and objectives in order to avoid causing harm or violating societal standards for acceptable behavior? Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance, among others. We propose that this kind of representational alignment between machine learning (ML) models and humans is also a necessary condition for value alignment, where ML systems conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train multiple ML agents (support vector regression and kernel regression) in a multi-armed bandit setting, where rewards are sampled from a distribution that reflects the morality of the chosen action. We then study the relationship between each agent's degree of representational alignment with humans and their performance when learning to take the most ethical actions.","sentences":["How can we build AI systems that are aligned with human values and objectives in order to avoid causing harm or violating societal standards for acceptable behavior?","Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance, among others.","We propose that this kind of representational alignment between machine learning (ML) models and humans is also a necessary condition for value alignment, where ML systems conform to human values and societal norms.","We focus on ethics as one aspect of value alignment and train multiple ML agents (support vector regression and kernel regression) in a multi-armed bandit setting, where rewards are sampled from a distribution that reflects the morality of the chosen action.","We then study the relationship between each agent's degree of representational alignment with humans and their performance when learning to take the most ethical actions."],"url":"http://arxiv.org/abs/2312.14106v1"}
{"created":"2023-12-21 18:09:30","title":"HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models","abstract":"Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results. However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting. Therefore, in this paper we introduce HD-Painter, a completely training-free approach that accurately follows to prompts and coherently scales to high-resolution image inpainting. To this end, we design the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention scores by prompt information and resulting in better text alignment generations. To further improve the prompt coherence we introduce the Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a post-hoc sampling strategy into general form of DDIM to prevent out-of-distribution latent shifts. Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches qualitatively and quantitatively, achieving an impressive generation accuracy improvement of 61.4% vs 51.9%. We will make the codes publicly available at: https://github.com/Picsart-AI-Research/HD-Painter","sentences":["Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results.","However, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts and performing high-resolution inpainting.","Therefore, in this paper we introduce HD-Painter, a completely training-free approach that accurately follows to prompts and coherently scales to high-resolution image inpainting.","To this end, we design the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention scores by prompt information and resulting in better text alignment generations.","To further improve the prompt coherence we introduce the Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a post-hoc sampling strategy into general form of DDIM to prevent out-of-distribution latent shifts.","Moreover, HD-Painter allows extension to larger scales by introducing a specialized super-resolution technique customized for inpainting, enabling the completion of missing regions in images of up to 2K resolution.","Our experiments demonstrate that HD-Painter surpasses existing state-of-the-art approaches qualitatively and quantitatively, achieving an impressive generation accuracy improvement of 61.4% vs 51.9%.","We will make the codes publicly available at: https://github.com/Picsart-AI-Research/HD-Painter"],"url":"http://arxiv.org/abs/2312.14091v1"}
{"created":"2023-12-21 18:08:56","title":"Designing Artificial Intelligence Equipped Social Decentralized Autonomous Organizations for Tackling Sextortion Cases Version 0.7","abstract":"With the rapid diffusion of social networks in combination with mobile phones, a new social threat of sextortion has emerged, in which vulnerable young women are essentially blackmailed with their explicit shared multimedia content. The phenomenon of sextortion is now widely studied by psychologists, sociologists, criminologists, etc. The findings have been translated into scattered help from NGOs, specialized law enforcement units, and therapists, who usually do not coordinate their efforts among each other. This paper addresses the gap of lacking coordination systems to effectively and efficiently use modern information technologies that align the efforts of scattered and non-aligned sextortion help organizations. Consequently, this paper not only investigates the goals, incentives, and disincentives for a system design and development that not only governs effectively and efficiently diverse cases of sextortion victims, but also leverages artificial intelligence in a targeted manner. It explores how AI and, in particular, autonomous cognitive entities can improve victim profiles analysis, streamline support mechanisms, and provide intelligent insight into sextortion cases. Furthermore, the paper conceptually studies the extent to which such efforts can be monetized in a sustainable way. Following a novel design methodology for the design of trusted blockchain decentralized applications, the paper presents a set of conceptual requirements and system models based on which it is possible to deduce a best-practice technology stack for rapid implementation deployment.","sentences":["With the rapid diffusion of social networks in combination with mobile phones, a new social threat of sextortion has emerged, in which vulnerable young women are essentially blackmailed with their explicit shared multimedia content.","The phenomenon of sextortion is now widely studied by psychologists, sociologists, criminologists, etc.","The findings have been translated into scattered help from NGOs, specialized law enforcement units, and therapists, who usually do not coordinate their efforts among each other.","This paper addresses the gap of lacking coordination systems to effectively and efficiently use modern information technologies that align the efforts of scattered and non-aligned sextortion help organizations.","Consequently, this paper not only investigates the goals, incentives, and disincentives for a system design and development that not only governs effectively and efficiently diverse cases of sextortion victims, but also leverages artificial intelligence in a targeted manner.","It explores how AI and, in particular, autonomous cognitive entities can improve victim profiles analysis, streamline support mechanisms, and provide intelligent insight into sextortion cases.","Furthermore, the paper conceptually studies the extent to which such efforts can be monetized in a sustainable way.","Following a novel design methodology for the design of trusted blockchain decentralized applications, the paper presents a set of conceptual requirements and system models based on which it is possible to deduce a best-practice technology stack for rapid implementation deployment."],"url":"http://arxiv.org/abs/2312.14090v1"}
{"created":"2023-12-21 17:52:12","title":"LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding","abstract":"Recently, Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown promise in instruction following and 2D image understanding. While these models are powerful, they have not yet been developed to comprehend the more challenging 3D physical scenes, especially when it comes to the sparse outdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs to gain a comprehensive understanding of outdoor 3D scenes. The central insight of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a language modeling problem, encompassing tasks such as 3D captioning, 3D grounding, 3D question answering, etc. Specifically, due to the scarcity of 3D LiDAR-text pairing data, we introduce a three-stage training strategy and generate relevant datasets, progressively aligning the 3D modality with the language embedding space of LLM. Furthermore, we design a View-Aware Transformer (VAT) to connect the 3D encoder with the LLM, which effectively bridges the modality gap and enhances the LLM's spatial orientation comprehension of visual features. Our experiments show that LiDAR-LLM possesses favorable capabilities to comprehend various instructions regarding 3D scenes and engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the 3D captioning task and achieves a 63.1\\% classification accuracy and a 14.3\\% BEV mIoU on the 3D grounding task. Web page: https://sites.google.com/view/lidar-llm","sentences":["Recently, Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown promise in instruction following and 2D image understanding.","While these models are powerful, they have not yet been developed to comprehend the more challenging 3D physical scenes, especially when it comes to the sparse outdoor LiDAR data.","In this paper, we introduce LiDAR-LLM, which takes raw LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs to gain a comprehensive understanding of outdoor 3D scenes.","The central insight of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a language modeling problem, encompassing tasks such as 3D captioning, 3D grounding, 3D question answering, etc.","Specifically, due to the scarcity of 3D LiDAR-text pairing data, we introduce a three-stage training strategy and generate relevant datasets, progressively aligning the 3D modality with the language embedding space of LLM.","Furthermore, we design a View-Aware Transformer (VAT) to connect the 3D encoder with the LLM, which effectively bridges the modality gap and enhances the LLM's spatial orientation comprehension of visual features.","Our experiments show that LiDAR-LLM possesses favorable capabilities to comprehend various instructions regarding 3D scenes and engage in complex spatial reasoning.","LiDAR-LLM attains a 40.9 BLEU-1 on the 3D captioning task and achieves a 63.1\\% classification accuracy and a 14.3\\% BEV mIoU on the 3D grounding task.","Web page: https://sites.google.com/view/lidar-llm"],"url":"http://arxiv.org/abs/2312.14074v1"}
{"created":"2023-12-21 17:49:44","title":"Towards Cooperative VRUs: Optimal Positioning Sampling for Pedestrian Awareness Messages","abstract":"Road safety is the main motivation for Cooperative Intelligent Transport Systems (C-ITS) in general, and vehicular communications (V2X) technology in particular. The V2X-based Vulnerable Road User (VRU) protection is an approach that relies on the persistent broadcasting of \"beacon\" awareness messages by a VRU mobile device. To this end the European Telecommunications Standards Institute (ETSI) has specified the Vulnerable Road User Awareness Message (VAM) as well as the overall ITS-G5 protocol stack enabling a variety of the V2X applications. This article studies how often pedestrians (a type of VRU) should check their position to issue a VAM. To that end, we characterize the rate at which pedestrians generate VAMs leveraging a recognized mobility model, and formulate an optimization problem to minimize the time elapsed between VAMs. We propose an algorithm to solve the problem in 802.11p and assess its accuracy through numerical and simulation campaigns. Results evidence the accuracy of our VAM rate characterization, and evidence that we decrease ETSI positioning sampling rate by more than 30%. On top, our solution decreases the time between VAMs, and increases the packet delivery ratio. In other words, our approach increases the pedestrians safety while reducing the battery consumption of mobile devices.","sentences":["Road safety is the main motivation for Cooperative Intelligent Transport Systems (C-ITS) in general, and vehicular communications (V2X) technology in particular.","The V2X-based Vulnerable Road User (VRU) protection is an approach that relies on the persistent broadcasting of \"beacon\" awareness messages by a VRU mobile device.","To this end the European Telecommunications Standards Institute (ETSI) has specified the Vulnerable Road User Awareness Message (VAM) as well as the overall ITS-G5 protocol stack enabling a variety of the V2X applications.","This article studies how often pedestrians (a type of VRU) should check their position to issue a VAM.","To that end, we characterize the rate at which pedestrians generate VAMs leveraging a recognized mobility model, and formulate an optimization problem to minimize the time elapsed between VAMs.","We propose an algorithm to solve the problem in 802.11p and assess its accuracy through numerical and simulation campaigns.","Results evidence the accuracy of our VAM rate characterization, and evidence that we decrease ETSI positioning sampling rate by more than 30%.","On top, our solution decreases the time between VAMs, and increases the packet delivery ratio.","In other words, our approach increases the pedestrians safety while reducing the battery consumption of mobile devices."],"url":"http://arxiv.org/abs/2312.14072v1"}
{"created":"2023-12-21 17:47:33","title":"EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models","abstract":"We introduce EmphAssess, a prosodic benchmark designed to evaluate the capability of speech-to-speech models to encode and reproduce prosodic emphasis. We apply this to two tasks: speech resynthesis and speech-to-speech translation. In both cases, the benchmark evaluates the ability of the model to encode emphasis in the speech input and accurately reproduce it in the output, potentially across a change of speaker and language. As part of the evaluation pipeline, we introduce EmphaClass, a new model that classifies emphasis at the frame or word level.","sentences":["We introduce EmphAssess, a prosodic benchmark designed to evaluate the capability of speech-to-speech models to encode and reproduce prosodic emphasis.","We apply this to two tasks: speech resynthesis and speech-to-speech translation.","In both cases, the benchmark evaluates the ability of the model to encode emphasis in the speech input and accurately reproduce it in the output, potentially across a change of speaker and language.","As part of the evaluation pipeline, we introduce EmphaClass, a new model that classifies emphasis at the frame or word level."],"url":"http://arxiv.org/abs/2312.14069v1"}
{"created":"2023-12-21 17:46:05","title":"Upper Bounding Barlow Twins: A Novel Filter for Multi-Relational Clustering","abstract":"Multi-relational clustering is a challenging task due to the fact that diverse semantic information conveyed in multi-layer graphs is difficult to extract and fuse. Recent methods integrate topology structure and node attribute information through graph filtering. However, they often use a low-pass filter without fully considering the correlation among multiple graphs. To overcome this drawback, we propose to learn a graph filter motivated by the theoretical analysis of Barlow Twins. We find that input with a negative semi-definite inner product provides a lower bound for Barlow Twins loss, which prevents it from reaching a better solution. We thus learn a filter that yields an upper bound for Barlow Twins. Afterward, we design a simple clustering architecture and demonstrate its state-of-the-art performance on four benchmark datasets.","sentences":["Multi-relational clustering is a challenging task due to the fact that diverse semantic information conveyed in multi-layer graphs is difficult to extract and fuse.","Recent methods integrate topology structure and node attribute information through graph filtering.","However, they often use a low-pass filter without fully considering the correlation among multiple graphs.","To overcome this drawback, we propose to learn a graph filter motivated by the theoretical analysis of Barlow Twins.","We find that input with a negative semi-definite inner product provides a lower bound for Barlow Twins loss, which prevents it from reaching a better solution.","We thus learn a filter that yields an upper bound for Barlow Twins.","Afterward, we design a simple clustering architecture and demonstrate its state-of-the-art performance on four benchmark datasets."],"url":"http://arxiv.org/abs/2312.14066v1"}
{"created":"2023-12-21 17:41:35","title":"Polynomial Time Convergence of the Iterative Evaluation of Datalogo Programs","abstract":"Datalogo is an extension of Datalog that allows for aggregation and recursion over an arbitrary commutative semiring. Like Datalog, Datalogo programs can be evaluated via the natural iterative algorithm until a fixed point is reached. However unlike Datalog, the natural iterative evaluation of some Datalogo programs over some semirings may not converge. It is known that the commutative semirings for which the iterative evaluation of Datalogo programs is guaranteed to converge are exactly those semirings that are stable~\\cite{Khamis0PSW22}. Previously, the best known upper bound on the number of iterations until convergence over $p$-stable semirings is $\\sum_{i=1}^n (p+2)^i = \\Theta(p^n)$ steps, where $n$ is (essentially) the output size. We establish that, in fact, the natural iterative evaluation of a Datalogoprogram over a $p$-stable semiring converges within a polynomial number of iterations. In particular our upper bound is $O( \\sigma p n^2( n^2 \\lg \\lambda + \\lg \\sigma))$ where $\\sigma$ is the number of elements in the semiring present in either the input databases or the Datalogo program, and $\\lambda$ is the maximum number of terms in any product in the Datalogo program.","sentences":["Datalogo is an extension of Datalog that allows for aggregation and recursion over an arbitrary commutative semiring.","Like Datalog, Datalogo programs can be evaluated via the natural iterative algorithm until a fixed point is reached.","However unlike Datalog, the natural iterative evaluation of some Datalogo programs over some semirings may not converge.","It is known that the commutative semirings for which the iterative evaluation of Datalogo programs is guaranteed to converge are exactly those semirings that are stable~\\cite{Khamis0PSW22}.","Previously, the best known upper bound on the number of iterations until convergence over $p$-stable semirings is $\\sum_{i=1}^n (p+2)^i = \\Theta(p^n)$ steps, where $n$ is (essentially) the output size.","We establish that, in fact, the natural iterative evaluation of a Datalogoprogram over a $p$-stable semiring converges within a polynomial number of iterations.","In particular our upper bound is $O( \\sigma p n^2( n^2 \\lg \\lambda + \\lg \\sigma))$ where $\\sigma$ is the number of elements in the semiring present in either the input databases or the Datalogo program, and $\\lambda$ is the maximum number of terms in any product in the Datalogo program."],"url":"http://arxiv.org/abs/2312.14063v1"}
{"created":"2023-12-21 17:37:13","title":"Protection of Vulnerable Road Users using Hybrid Vehicular Networks","abstract":"The use of reactive detection technologies such as passive and active sensors for avoiding car accidents involving pedestrians and other Vulnerable Road Users (VRU) is one of the cornerstones of Cooperative, Connected, and Automated Mobility (CCAM). However, CCAM systems are not yet present in all roads at all times. The use of currently available technologies that are embedded in smartphones, such as location services and Internet access, are enablers for the early detection of VRUs. This paper presents the proof-of-concept of a system that provides vehicles with enough information about the presence of VRUs by using public cellular networks, an MQTT broker, and IEEE 802.11p-enabled hardware (a roadside unit and an on-board unit). The system was tested in an urban environment and in a test track, where its feasibility was evaluated. Results were satisfactory, proving the system is reliable enough to alert of the sudden appearance of a VRU in time for the driver to react.","sentences":["The use of reactive detection technologies such as passive and active sensors for avoiding car accidents involving pedestrians and other Vulnerable Road Users (VRU) is one of the cornerstones of Cooperative, Connected, and Automated Mobility (CCAM).","However, CCAM systems are not yet present in all roads at all times.","The use of currently available technologies that are embedded in smartphones, such as location services and Internet access, are enablers for the early detection of VRUs.","This paper presents the proof-of-concept of a system that provides vehicles with enough information about the presence of VRUs by using public cellular networks, an MQTT broker, and IEEE 802.11p-enabled hardware (a roadside unit and an on-board unit).","The system was tested in an urban environment and in a test track, where its feasibility was evaluated.","Results were satisfactory, proving the system is reliable enough to alert of the sudden appearance of a VRU in time for the driver to react."],"url":"http://arxiv.org/abs/2312.14059v1"}
{"created":"2023-12-21 17:28:09","title":"A Strong Baseline for Temporal Video-Text Alignment","abstract":"In this paper, we consider the problem of temporally aligning the video and texts from instructional videos, specifically, given a long-term video, and associated text sentences, our goal is to determine their corresponding timestamps in the video. To this end, we establish a simple, yet strong model that adopts a Transformer-based architecture with all texts as queries, iteratively attending to the visual features, to infer the optimal timestamp. We conduct thorough experiments to investigate: (i) the effect of upgrading ASR systems to reduce errors from speech recognition, (ii) the effect of various visual-textual backbones, ranging from CLIP to S3D, to the more recent InternVideo, (iii) the effect of transforming noisy ASR transcripts into descriptive steps by prompting a large language model (LLM), to summarize the core activities within the ASR transcript as a new training dataset. As a result, our proposed simple model demonstrates superior performance on both narration alignment and procedural step grounding tasks, surpassing existing state-of-the-art methods by a significant margin on three public benchmarks, namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask. We believe the proposed model and dataset with descriptive steps can be treated as a strong baseline for future research in temporal video-text alignment. All codes, models, and the resulting dataset will be publicly released to the research community.","sentences":["In this paper, we consider the problem of temporally aligning the video and texts from instructional videos, specifically, given a long-term video, and associated text sentences, our goal is to determine their corresponding timestamps in the video.","To this end, we establish a simple, yet strong model that adopts a Transformer-based architecture with all texts as queries, iteratively attending to the visual features, to infer the optimal timestamp.","We conduct thorough experiments to investigate: (i) the effect of upgrading ASR systems to reduce errors from speech recognition, (ii) the effect of various visual-textual backbones, ranging from CLIP to S3D, to the more recent InternVideo, (iii) the effect of transforming noisy ASR transcripts into descriptive steps by prompting a large language model (LLM), to summarize the core activities within the ASR transcript as a new training dataset.","As a result, our proposed simple model demonstrates superior performance on both narration alignment and procedural step grounding tasks, surpassing existing state-of-the-art methods by a significant margin on three public benchmarks, namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask.","We believe the proposed model and dataset with descriptive steps can be treated as a strong baseline for future research in temporal video-text alignment.","All codes, models, and the resulting dataset will be publicly released to the research community."],"url":"http://arxiv.org/abs/2312.14055v1"}
{"created":"2023-12-21 17:23:49","title":"Dual Attention U-Net with Feature Infusion: Pushing the Boundaries of Multiclass Defect Segmentation","abstract":"The proposed architecture, Dual Attentive U-Net with Feature Infusion (DAU-FI Net), addresses challenges in semantic segmentation, particularly on multiclass imbalanced datasets with limited samples. DAU-FI Net integrates multiscale spatial-channel attention mechanisms and feature injection to enhance precision in object localization. The core employs a multiscale depth-separable convolution block, capturing localized patterns across scales. This block is complemented by a spatial-channel squeeze and excitation (scSE) attention unit, modeling inter-dependencies between channels and spatial regions in feature maps. Additionally, additive attention gates refine segmentation by connecting encoder-decoder pathways.   To augment the model, engineered features using Gabor filters for textural analysis, Sobel and Canny filters for edge detection are injected guided by semantic masks to expand the feature space strategically. Comprehensive experiments on a challenging sewer pipe and culvert defect dataset and a benchmark dataset validate DAU-FI Net's capabilities. Ablation studies highlight incremental benefits from attention blocks and feature injection. DAU-FI Net achieves state-of-the-art mean Intersection over Union (IoU) of 95.6% and 98.8% on the defect test set and benchmark respectively, surpassing prior methods by 8.9% and 12.6%, respectively. Ablation studies highlight incremental benefits from attention blocks and feature injection. The proposed architecture provides a robust solution, advancing semantic segmentation for multiclass problems with limited training data. Our sewer-culvert defects dataset, featuring pixel-level annotations, opens avenues for further research in this crucial domain. Overall, this work delivers key innovations in architecture, attention, and feature engineering to elevate semantic segmentation efficacy.","sentences":["The proposed architecture, Dual Attentive U-Net with Feature Infusion (DAU-FI Net), addresses challenges in semantic segmentation, particularly on multiclass imbalanced datasets with limited samples.","DAU-FI Net integrates multiscale spatial-channel attention mechanisms and feature injection to enhance precision in object localization.","The core employs a multiscale depth-separable convolution block, capturing localized patterns across scales.","This block is complemented by a spatial-channel squeeze and excitation (scSE) attention unit, modeling inter-dependencies between channels and spatial regions in feature maps.","Additionally, additive attention gates refine segmentation by connecting encoder-decoder pathways.   ","To augment the model, engineered features using Gabor filters for textural analysis, Sobel and Canny filters for edge detection are injected guided by semantic masks to expand the feature space strategically.","Comprehensive experiments on a challenging sewer pipe and culvert defect dataset and a benchmark dataset validate DAU-FI Net's capabilities.","Ablation studies highlight incremental benefits from attention blocks and feature injection.","DAU-FI Net achieves state-of-the-art mean Intersection over Union (IoU) of 95.6% and 98.8% on the defect test set and benchmark respectively, surpassing prior methods by 8.9% and 12.6%, respectively.","Ablation studies highlight incremental benefits from attention blocks and feature injection.","The proposed architecture provides a robust solution, advancing semantic segmentation for multiclass problems with limited training data.","Our sewer-culvert defects dataset, featuring pixel-level annotations, opens avenues for further research in this crucial domain.","Overall, this work delivers key innovations in architecture, attention, and feature engineering to elevate semantic segmentation efficacy."],"url":"http://arxiv.org/abs/2312.14053v1"}
{"created":"2023-12-21 17:06:30","title":"Balancing Specialization and Adaptation in a Transforming Scientific Landscape","abstract":"How scientists navigate between the need to capitalize on their prior knowledge by specializing, and the urge to adapt to evolving research opportunities? Drawing from diverse perspectives on adaptation, in particular from institutional change and cultural evolution, this paper proposes a Bayesian model of the evolution of scientists' research portfolios in response to transformations in their field. The model relies on scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientists' trajectories within a cohort of $2\\,195$ high-energy physicists between 2000 and 2019. The reallocation of research efforts in response to the incentives to adapt is shown to be mainly structured by learning costs, thus maximizing the utility of the scientific capital disseminated among scientists. Two dimensions of social capital, namely ``diversity'' and ``power'', have opposite effects on the magnitude of change in scientists' research interests: while ``diversity'' disrupts and expands research interests, ``power'' stabilizes physicists' research agendas -- as does institutional stability. Social capital plays a more crucial role in shifts between cognitively distant research areas.","sentences":["How scientists navigate between the need to capitalize on their prior knowledge by specializing, and the urge to adapt to evolving research opportunities?","Drawing from diverse perspectives on adaptation, in particular from institutional change and cultural evolution, this paper proposes a Bayesian model of the evolution of scientists' research portfolios in response to transformations in their field.","The model relies on scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientists' trajectories within a cohort of $2\\,195$ high-energy physicists between 2000 and 2019.","The reallocation of research efforts in response to the incentives to adapt is shown to be mainly structured by learning costs, thus maximizing the utility of the scientific capital disseminated among scientists.","Two dimensions of social capital, namely ``diversity'' and ``power'', have opposite effects on the magnitude of change in scientists' research interests: while ``diversity'' disrupts and expands research interests, ``power'' stabilizes physicists' research agendas -- as does institutional stability.","Social capital plays a more crucial role in shifts between cognitively distant research areas."],"url":"http://arxiv.org/abs/2312.14040v1"}
{"created":"2023-12-21 17:03:53","title":"Dynamic Mining Interval to Improve Blockchain Throughput","abstract":"Decentralized Finance (DeFi), propelled by Blockchain technology, has revolutionized traditional financial systems, improving transparency, reducing costs, and fostering financial inclusion. However, transaction activities in these systems fluctuate significantly and the throughput can be effected. To address this issue, we propose a Dynamic Mining Interval (DMI) mechanism that adjusts mining intervals in response to block size and trading volume to enhance the transaction throughput of Blockchain platforms. Besides, in the context of public Blockchains such as Bitcoin, Ethereum, and Litecoin, a shift towards transaction fees dominance over coin-based rewards is projected in near future. As a result, the ecosystem continues to face threats from deviant mining activities such as Undercutting Attacks, Selfish Mining, and Pool Hopping, among others. In recent years, Dynamic Transaction Storage (DTS) strategies were proposed to allocate transactions dynamically based on fees thereby stabilizing block incentives. However, DTS' utilization of Merkle tree leaf nodes can reduce system throughput. To alleviate this problem, in this paper, we propose an approach for combining DMI and DTS. Besides, we also discuss the DMI selection mechanism for adjusting mining intervals based on various factors.","sentences":["Decentralized Finance (DeFi), propelled by Blockchain technology, has revolutionized traditional financial systems, improving transparency, reducing costs, and fostering financial inclusion.","However, transaction activities in these systems fluctuate significantly and the throughput can be effected.","To address this issue, we propose a Dynamic Mining Interval (DMI) mechanism that adjusts mining intervals in response to block size and trading volume to enhance the transaction throughput of Blockchain platforms.","Besides, in the context of public Blockchains such as Bitcoin, Ethereum, and Litecoin, a shift towards transaction fees dominance over coin-based rewards is projected in near future.","As a result, the ecosystem continues to face threats from deviant mining activities such as Undercutting Attacks, Selfish Mining, and Pool Hopping, among others.","In recent years, Dynamic Transaction Storage (DTS) strategies were proposed to allocate transactions dynamically based on fees thereby stabilizing block incentives.","However, DTS' utilization of Merkle tree leaf nodes can reduce system throughput.","To alleviate this problem, in this paper, we propose an approach for combining DMI and DTS.","Besides, we also discuss the DMI selection mechanism for adjusting mining intervals based on various factors."],"url":"http://arxiv.org/abs/2312.14038v1"}
{"created":"2023-12-21 17:03:26","title":"Neural Contextual Bandits for Personalized Recommendation","abstract":"In the dynamic landscape of online businesses, recommender systems are pivotal in enhancing user experiences. While traditional approaches have relied on static supervised learning, the quest for adaptive, user-centric recommendations has led to the emergence of the formulation of contextual bandits. This tutorial investigates the contextual bandits as a powerful framework for personalized recommendations. We delve into the challenges, advanced algorithms and theories, collaborative strategies, and open challenges and future prospects within this field. Different from existing related tutorials, (1) we focus on the exploration perspective of contextual bandits to alleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich get richer and the poor get poorer, concerning the popularity of items; (2) in addition to the conventional linear contextual bandits, we will also dedicated to neural contextual bandits which have emerged as an important branch in recent years, to investigate how neural networks benefit contextual bandits for personalized recommendation both empirically and theoretically; (3) we will cover the latest topic, collaborative neural contextual bandits, to incorporate both user heterogeneity and user correlations customized for recommender system; (4) we will provide and discuss the new emerging challenges and open questions for neural contextual bandits with applications in the personalized recommendation, especially for large neural models.","sentences":["In the dynamic landscape of online businesses, recommender systems are pivotal in enhancing user experiences.","While traditional approaches have relied on static supervised learning, the quest for adaptive, user-centric recommendations has led to the emergence of the formulation of contextual bandits.","This tutorial investigates the contextual bandits as a powerful framework for personalized recommendations.","We delve into the challenges, advanced algorithms and theories, collaborative strategies, and open challenges and future prospects within this field.","Different from existing related tutorials, (1) we focus on the exploration perspective of contextual bandits to alleviate the ``Matthew Effect'' in the recommender systems, i.e., the rich get richer and the poor get poorer, concerning the popularity of items; (2) in addition to the conventional linear contextual bandits, we will also dedicated to neural contextual bandits which have emerged as an important branch in recent years, to investigate how neural networks benefit contextual bandits for personalized recommendation both empirically and theoretically; (3) we will cover the latest topic, collaborative neural contextual bandits, to incorporate both user heterogeneity and user correlations customized for recommender system; (4) we will provide and discuss the new emerging challenges and open questions for neural contextual bandits with applications in the personalized recommendation, especially for large neural models."],"url":"http://arxiv.org/abs/2312.14037v1"}
{"created":"2023-12-21 17:03:25","title":"Total variation in popular rap vocals from 2009-2023: extension of the analysis by Georgieva, Ripolles & McFee","abstract":"Pitch variability in rap vocals is overlooked in favor of the genre's uniquely dynamic rhythmic properties. We present an analysis of fundamental frequency (F0) variation in rap vocals over the past 14 years, focusing on song examples that represent the state of modern rap music. Our analysis aims at identifying meaningful trends over time, and is in turn a continuation of the 2023 analysis by Georgieva, Ripolles & McFee. They found rap to be an outlier with larger F0 variation compared to other genres, but with a declining trend since the genre's inception. However, they only analyzed data through 2010. Our analysis looks beyond 2010. We once again observe rap's large F0 variation, but with a decelerated decline in recent years.","sentences":["Pitch variability in rap vocals is overlooked in favor of the genre's uniquely dynamic rhythmic properties.","We present an analysis of fundamental frequency (F0) variation in rap vocals over the past 14 years, focusing on song examples that represent the state of modern rap music.","Our analysis aims at identifying meaningful trends over time, and is in turn a continuation of the 2023 analysis by Georgieva, Ripolles & McFee.","They found rap to be an outlier with larger F0 variation compared to other genres, but with a declining trend since the genre's inception.","However, they only analyzed data through 2010.","Our analysis looks beyond 2010.","We once again observe rap's large F0 variation, but with a decelerated decline in recent years."],"url":"http://arxiv.org/abs/2312.14036v1"}
{"created":"2023-12-21 17:03:09","title":"GRIL-Calib: Targetless Ground Robot IMU-LiDAR Extrinsic Calibration Method using Ground Plane Motion Constraints","abstract":"Targetless IMU-LiDAR extrinsic calibration methods are gaining significant attention as the importance of the IMU-LiDAR fusion system increases. Notably, existing calibration methods derive calibration parameters under the assumption that the methods require full motion in all axes. When IMU and LiDAR are mounted on a ground robot the motion of which is restricted to planar motion, existing calibration methods are likely to exhibit degraded performance. To address this issue, we present GRIL-Calib: a novel targetless Ground Robot IMU-LiDAR Calibration method. Our proposed method leverages ground information to compensate for the lack of unrestricted full motion. First, we propose LiDAR Odometry (LO) using ground plane residuals to enhance calibration accuracy. Second, we propose the Ground Plane Motion (GPM) constraint and incorporate it into the optimization for calibration, enabling the determination of full 6-DoF extrinsic parameters, including theoretically unobservable direction. Finally, unlike baseline methods, we formulate the calibration not as sequential two optimizations but as a single optimization (SO) problem, solving all calibration parameters simultaneously and improving accuracy. We validate our \\textit{GRIL-Calib} by applying it to three public real-world datasets and comparing its performance with that of existing state-of-the-art methods in terms of accuracy and robustness. Our code is available at https://github.com/Taeyoung96/GRIL-Calib.","sentences":["Targetless IMU-LiDAR extrinsic calibration methods are gaining significant attention as the importance of the IMU-LiDAR fusion system increases.","Notably, existing calibration methods derive calibration parameters under the assumption that the methods require full motion in all axes.","When IMU and LiDAR are mounted on a ground robot the motion of which is restricted to planar motion, existing calibration methods are likely to exhibit degraded performance.","To address this issue, we present GRIL-Calib: a novel targetless Ground Robot IMU-LiDAR Calibration method.","Our proposed method leverages ground information to compensate for the lack of unrestricted full motion.","First, we propose LiDAR Odometry (LO) using ground plane residuals to enhance calibration accuracy.","Second, we propose the Ground Plane Motion (GPM) constraint and incorporate it into the optimization for calibration, enabling the determination of full 6-DoF extrinsic parameters, including theoretically unobservable direction.","Finally, unlike baseline methods, we formulate the calibration not as sequential two optimizations but as a single optimization (SO) problem, solving all calibration parameters simultaneously and improving accuracy.","We validate our \\textit{GRIL-Calib} by applying it to three public real-world datasets and comparing its performance with that of existing state-of-the-art methods in terms of accuracy and robustness.","Our code is available at https://github.com/Taeyoung96/GRIL-Calib."],"url":"http://arxiv.org/abs/2312.14035v1"}
{"created":"2023-12-21 17:02:06","title":"T-Eval: Evaluating the Tool Utilization Capability Step by Step","abstract":"Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce \\shortname~to evaluate the tool utilization capability step by step. \\shortname~disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on \\shortname~and in-depth analysis of various LLMs. \\shortname~ not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available at \\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}.","sentences":["Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications.","Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored.","In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review.","Based on that, we further introduce \\shortname~to evaluate the tool utilization capability step by step.","\\shortname~disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs.","We conduct extensive experiments on \\shortname~and in-depth analysis of various LLMs.","\\shortname~ not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability.","The benchmark will be available at \\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}."],"url":"http://arxiv.org/abs/2312.14033v1"}
{"created":"2023-12-21 17:00:37","title":"Fault Diagnosability Analysis of Multi-Mode Systems","abstract":"Multi-mode systems can operate in different modes, leading to large numbers of different dynamics. Consequently, applying traditional structural diagnostics to such systems is often untractable. To address this challenge, we present a multi-mode diagnostics algorithm that relies on a multi-mode extension of the Dulmage-Mendelsohn decomposition. We introduce two methodologies for modeling faults, either as signals or as Boolean variables, and apply them to a modular switched battery system in order to demonstrate their effectiveness and discuss their respective advantages.","sentences":["Multi-mode systems can operate in different modes, leading to large numbers of different dynamics.","Consequently, applying traditional structural diagnostics to such systems is often untractable.","To address this challenge, we present a multi-mode diagnostics algorithm that relies on a multi-mode extension of the Dulmage-Mendelsohn decomposition.","We introduce two methodologies for modeling faults, either as signals or as Boolean variables, and apply them to a modular switched battery system in order to demonstrate their effectiveness and discuss their respective advantages."],"url":"http://arxiv.org/abs/2312.14030v1"}
{"created":"2023-12-21 16:59:41","title":"Phylogenetic tree distance computation over succinct representations","abstract":"There are several tools available to infer phylogenetic trees, which depict the evolutionary relationships among biological entities such as viral and bacterial strains in infectious outbreaks, or cancerous cells in tumor progression trees. These tools rely on several inference methods available to produce phylogenetic trees, with resulting trees not being unique. Thus, methods for comparing phylogenies that are capable of revealing where two phylogenetic trees agree or differ are required. An approach is then to compute a similarity or dissimilarity measure between trees, with the Robinson- Foulds distance being one of the most used, and which can be computed in linear time and space. Nevertheless, given the large and increasing volume of phylogenetic data, phylogenetic trees are becoming very large with hundreds of thousands of leafs. In this context, space requirements become an issue both while computing tree distances and while storing trees. We propose then an efficient implementation of the Robinson-Foulds distance over trees succinct representations. Our implementation generalizes also the Robinson-Foulds distances to labelled phylogenetic trees, i.e., trees containing labels on all nodes, instead of only on leaves. Experimental results show that we are able to still achieve linear time while requiring less space. Our implementation is available as an open-source tool at https://github.com/pedroparedesbranco/TreeDiff.","sentences":["There are several tools available to infer phylogenetic trees, which depict the evolutionary relationships among biological entities such as viral and bacterial strains in infectious outbreaks, or cancerous cells in tumor progression trees.","These tools rely on several inference methods available to produce phylogenetic trees, with resulting trees not being unique.","Thus, methods for comparing phylogenies that are capable of revealing where two phylogenetic trees agree or differ are required.","An approach is then to compute a similarity or dissimilarity measure between trees, with the Robinson- Foulds distance being one of the most used, and which can be computed in linear time and space.","Nevertheless, given the large and increasing volume of phylogenetic data, phylogenetic trees are becoming very large with hundreds of thousands of leafs.","In this context, space requirements become an issue both while computing tree distances and while storing trees.","We propose then an efficient implementation of the Robinson-Foulds distance over trees succinct representations.","Our implementation generalizes also the Robinson-Foulds distances to labelled phylogenetic trees, i.e., trees containing labels on all nodes, instead of only on leaves.","Experimental results show that we are able to still achieve linear time while requiring less space.","Our implementation is available as an open-source tool at https://github.com/pedroparedesbranco/TreeDiff."],"url":"http://arxiv.org/abs/2312.14029v1"}
{"created":"2023-12-21 16:58:59","title":"Efficient quantum algorithms for some instances of the semidirect discrete logarithm problem","abstract":"The semidirect discrete logarithm problem (SDLP) is the following analogue of the standard discrete logarithm problem in the semidirect product semigroup $G\\rtimes \\mathrm{End}(G)$ for a finite semigroup $G$. Given $g\\in G, \\sigma\\in \\mathrm{End}(G)$, and $h=\\prod_{i=0}^{t-1}\\sigma^i(g)$ for some integer $t$, the SDLP$(G,\\sigma)$, for $g$ and $h$, asks to determine $t$. As Shor's algorithm crucially depends on commutativity, it is believed not to be applicable to the SDLP. Previously, the best known algorithm for the SDLP was based on Kuperberg's subexponential time quantum algorithm. Still, the problem plays a central role in the security of certain proposed cryptosystems in the family of \\textit{semidirect product key exchange}. This includes a recently proposed signature protocol called SPDH-Sign. In this paper, we show that the SDLP is even easier in some important special cases. Specifically, for a finite group $G$, we describe quantum algorithms for the SDLP in $G\\rtimes \\mathrm{Aut}(G)$ for the following two classes of instances: the first one is when $G$ is solvable and the second is when $G$ is a matrix group and a power of $\\sigma$ with a polynomially small exponent is an inner automorphism of $G$. We further extend the results to groups composed of factors from these classes. A consequence is that SPDH-Sign and similar cryptosystems whose security assumption is based on the presumed hardness of the SDLP in the cases described above are insecure against quantum attacks. The quantum ingredients we rely on are not new: these are Shor's factoring and discrete logarithm algorithms and well-known generalizations.","sentences":["The semidirect discrete logarithm problem (SDLP) is the following analogue of the standard discrete logarithm problem in the semidirect product semigroup $G\\rtimes \\mathrm{End}(G)$ for a finite semigroup $G$. Given $g\\in G, \\sigma\\in \\mathrm{End}(G)$, and $h=\\prod_{i=0}^{t-1}\\sigma^i(g)$ for some integer $t$, the SDLP$(G,\\sigma)$, for $g$ and $h$, asks to determine $t$. As Shor's algorithm crucially depends on commutativity, it is believed not to be applicable to the SDLP.","Previously, the best known algorithm for the SDLP was based on Kuperberg's subexponential time quantum algorithm.","Still, the problem plays a central role in the security of certain proposed cryptosystems in the family of \\textit{semidirect product key exchange}.","This includes a recently proposed signature protocol called SPDH-Sign.","In this paper, we show that the SDLP is even easier in some important special cases.","Specifically, for a finite group $G$, we describe quantum algorithms for the SDLP in $G\\rtimes \\mathrm{Aut}(G)$ for the following two classes of instances: the first one is when $G$ is solvable and the second is when $G$ is a matrix group and a power of $\\sigma$ with a polynomially small exponent is an inner automorphism of $G$. We further extend the results to groups composed of factors from these classes.","A consequence is that SPDH-Sign and similar cryptosystems whose security assumption is based on the presumed hardness of the SDLP in the cases described above are insecure against quantum attacks.","The quantum ingredients we rely on are not new: these are Shor's factoring and discrete logarithm algorithms and well-known generalizations."],"url":"http://arxiv.org/abs/2312.14028v1"}
{"created":"2023-12-21 16:54:09","title":"Geometric Awareness in Neural Fields for 3D Human Registration","abstract":"Aligning a template to 3D human point clouds is a long-standing problem crucial for tasks like animation, reconstruction, and enabling supervised learning pipelines. Recent data-driven methods leverage predicted surface correspondences; however, they are not robust to varied poses or distributions. In contrast, industrial solutions often rely on expensive manual annotations or multi-view capturing systems. Recently, neural fields have shown promising results, but their purely data-driven nature lacks geometric awareness, often resulting in a trivial misalignment of the template registration. In this work, we propose two solutions: LoVD, a novel neural field model that predicts the direction towards the localized SMPL vertices on the target surface; and INT, the first self-supervised task dedicated to neural fields that, at test time, refines the backbone, exploiting the target geometry. We combine them into INLoVD, a robust 3D Human body registration pipeline trained on a large MoCap dataset. INLoVD is efficient (takes less than a minute), solidly achieves the state of the art over public benchmarks, and provides unprecedented generalization on out-of-distribution data. We will release code and checkpoints in \\url{url}.","sentences":["Aligning a template to 3D human point clouds is a long-standing problem crucial for tasks like animation, reconstruction, and enabling supervised learning pipelines.","Recent data-driven methods leverage predicted surface correspondences; however, they are not robust to varied poses or distributions.","In contrast, industrial solutions often rely on expensive manual annotations or multi-view capturing systems.","Recently, neural fields have shown promising results, but their purely data-driven nature lacks geometric awareness, often resulting in a trivial misalignment of the template registration.","In this work, we propose two solutions: LoVD, a novel neural field model that predicts the direction towards the localized SMPL vertices on the target surface; and INT, the first self-supervised task dedicated to neural fields that, at test time, refines the backbone, exploiting the target geometry.","We combine them into INLoVD, a robust 3D Human body registration pipeline trained on a large MoCap dataset.","INLoVD is efficient (takes less than a minute), solidly achieves the state of the art over public benchmarks, and provides unprecedented generalization on out-of-distribution data.","We will release code and checkpoints in \\url{url}."],"url":"http://arxiv.org/abs/2312.14024v1"}
{"created":"2023-12-21 16:54:00","title":"Leakage-Resilient Hardness Equivalence to Logspace Derandomization","abstract":"Efficient derandomization has long been a goal in complexity theory, and a major recent result by Yanyi Liu and Rafael Pass identifies a new class of hardness assumption under which it is possible to perform time-bounded derandomization efficiently: that of ''leakage-resilient hardness.'' They identify a specific form of this assumption which is $\\textit{equivalent}$ to $\\mathsf{prP} = \\mathsf{prBPP}$. In this paper, we pursue a an equivalence to derandomization of $\\mathsf{prBP{\\cdot}L}$ (logspace promise problems with two-way randomness) through techniques analogous to Liu and Pass. We are able to obtain an equivalence between a similar ''leakage-resilient hardness'' assumption and a slightly stronger statement than derandomization of $\\mathsf{prBP{\\cdot}L}$, that of finding ''non-no'' instances of ''promise search problems.''","sentences":["Efficient derandomization has long been a goal in complexity theory, and a major recent result by Yanyi Liu and Rafael Pass identifies a new class of hardness assumption under which it is possible to perform time-bounded derandomization efficiently: that of ''leakage-resilient hardness.''","They identify a specific form of this assumption which is $\\textit{equivalent}$ to $\\mathsf{prP} = \\mathsf{prBPP}$.","In this paper, we pursue a an equivalence to derandomization of $\\mathsf{prBP{\\cdot}L}$ (logspace promise problems with two-way randomness) through techniques analogous to Liu and Pass.","We are able to obtain an equivalence between a similar ''leakage-resilient hardness'' assumption and a slightly stronger statement than derandomization of $\\mathsf{prBP{\\cdot}L}$, that of finding ''non-no'' instances of ''promise search problems.''"],"url":"http://arxiv.org/abs/2312.14023v1"}
{"created":"2023-12-21 16:52:41","title":"BANSpEmo: A Bangla Emotional Speech Recognition Dataset","abstract":"In the field of audio and speech analysis, the ability to identify emotions from acoustic signals is essential. Human-computer interaction (HCI) and behavioural analysis are only a few of the many areas where the capacity to distinguish emotions from speech signals has an extensive range of applications. Here, we are introducing BanSpEmo, a corpus of emotional speech that only consists of audio recordings and has been created specifically for the Bangla language. This corpus contains 792 audio recordings over a duration of more than 1 hour and 23 minutes. 22 native speakers took part in the recording of two sets of sentences that represent the six desired emotions. The data set consists of 12 Bangla sentences which are uttered in 6 emotions as Disgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender balanced. Ten individuals who either have experience in related field or have acting experience took part in the assessment of this corpus. It has a balanced number of audio recordings in each emotion class. BanSpEmo can be considered as a useful resource to promote emotion and speech recognition research and related applications in the Bangla language. The dataset can be found here: https://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for academic research.","sentences":["In the field of audio and speech analysis, the ability to identify emotions from acoustic signals is essential.","Human-computer interaction (HCI) and behavioural analysis are only a few of the many areas where the capacity to distinguish emotions from speech signals has an extensive range of applications.","Here, we are introducing BanSpEmo, a corpus of emotional speech that only consists of audio recordings and has been created specifically for the Bangla language.","This corpus contains 792 audio recordings over a duration of more than 1 hour and 23 minutes.","22 native speakers took part in the recording of two sets of sentences that represent the six desired emotions.","The data set consists of 12 Bangla sentences which are uttered in 6 emotions as Disgust, Happy, Sad, Surprised, Anger, and Fear.","This corpus is not also gender balanced.","Ten individuals who either have experience in related field or have acting experience took part in the assessment of this corpus.","It has a balanced number of audio recordings in each emotion class.","BanSpEmo can be considered as a useful resource to promote emotion and speech recognition research and related applications in the Bangla language.","The dataset can be found here: https://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for academic research."],"url":"http://arxiv.org/abs/2312.14020v1"}
{"created":"2023-12-21 16:36:33","title":"On the choice of the optimal temporal support for audio classification with Pre-trained embeddings","abstract":"Current state-of-the-art audio analysis systems rely on pre-trained embedding models, often used off-the-shelf as (frozen) feature extractors. Choosing the best one for a set of tasks is the subject of many recent publications. However, one aspect often overlooked in these works is the influence of the duration of audio input considered to extract an embedding, which we refer to as Temporal Support (TS). In this work, we study the influence of the TS for well-established or emerging pre-trained embeddings, chosen to represent different types of architectures and learning paradigms. We conduct this evaluation using both musical instrument and environmental sound datasets, namely OpenMIC, TAU Urban Acoustic Scenes 2020 Mobile, and ESC-50. We especially highlight that Audio Spectrogram Transformer-based systems (PaSST and BEATs) remain effective with smaller TS, which therefore allows for a drastic reduction in memory and computational cost. Moreover, we show that by choosing the optimal TS we reach competitive results across all tasks. In particular, we improve the state-of-the-art results on OpenMIC, using BEATs and PaSST without any fine-tuning.","sentences":["Current state-of-the-art audio analysis systems rely on pre-trained embedding models, often used off-the-shelf as (frozen) feature extractors.","Choosing the best one for a set of tasks is the subject of many recent publications.","However, one aspect often overlooked in these works is the influence of the duration of audio input considered to extract an embedding, which we refer to as Temporal Support (TS).","In this work, we study the influence of the TS for well-established or emerging pre-trained embeddings, chosen to represent different types of architectures and learning paradigms.","We conduct this evaluation using both musical instrument and environmental sound datasets, namely OpenMIC, TAU Urban Acoustic Scenes 2020 Mobile, and ESC-50.","We especially highlight that Audio Spectrogram Transformer-based systems (PaSST and BEATs) remain effective with smaller TS, which therefore allows for a drastic reduction in memory and computational cost.","Moreover, we show that by choosing the optimal TS we reach competitive results across all tasks.","In particular, we improve the state-of-the-art results on OpenMIC, using BEATs and PaSST without any fine-tuning."],"url":"http://arxiv.org/abs/2312.14005v1"}
{"created":"2023-12-21 16:35:11","title":"Deep Learning Based Face Recognition Method using Siamese Network","abstract":"Achieving state-of-the-art results in face verification systems typically hinges on the availability of labeled face training data, a resource that often proves challenging to acquire in substantial quantities. In this research endeavor, we proposed employing Siamese networks for face recognition, eliminating the need for labeled face images. We achieve this by strategically leveraging negative samples alongside nearest neighbor counterparts, thereby establishing positive and negative pairs through an unsupervised methodology. The architectural framework adopts a VGG encoder, trained as a double branch siamese network. Our primary aim is to circumvent the necessity for labeled face image data, thus proposing the generation of training pairs in an entirely unsupervised manner. Positive training data are selected within a dataset based on their highest cosine similarity scores with a designated anchor, while negative training data are culled in a parallel fashion, though drawn from an alternate dataset. During training, the proposed siamese network conducts binary classification via cross-entropy loss. Subsequently, during the testing phase, we directly extract face verification scores from the network's output layer. Experimental results reveal that the proposed unsupervised system delivers a performance on par with a similar but fully supervised baseline.","sentences":["Achieving state-of-the-art results in face verification systems typically hinges on the availability of labeled face training data, a resource that often proves challenging to acquire in substantial quantities.","In this research endeavor, we proposed employing Siamese networks for face recognition, eliminating the need for labeled face images.","We achieve this by strategically leveraging negative samples alongside nearest neighbor counterparts, thereby establishing positive and negative pairs through an unsupervised methodology.","The architectural framework adopts a VGG encoder, trained as a double branch siamese network.","Our primary aim is to circumvent the necessity for labeled face image data, thus proposing the generation of training pairs in an entirely unsupervised manner.","Positive training data are selected within a dataset based on their highest cosine similarity scores with a designated anchor, while negative training data are culled in a parallel fashion, though drawn from an alternate dataset.","During training, the proposed siamese network conducts binary classification via cross-entropy loss.","Subsequently, during the testing phase, we directly extract face verification scores from the network's output layer.","Experimental results reveal that the proposed unsupervised system delivers a performance on par with a similar but fully supervised baseline."],"url":"http://arxiv.org/abs/2312.14001v1"}
{"created":"2023-12-21 16:34:03","title":"Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian Score Climbing","abstract":"Stochastic optimal control of dynamical systems is a crucial challenge in sequential decision-making. Recently, control-as-inference approaches have had considerable success, providing a viable risk-sensitive framework to address the exploration-exploitation dilemma. Nonetheless, a majority of these techniques only invoke the inference-control duality to derive a modified risk objective that is then addressed within a reinforcement learning framework. This paper introduces a novel perspective by framing risk-sensitive stochastic control as Markovian score climbing under samples drawn from a conditional particle filter. Our approach, while purely inference-centric, provides asymptotically unbiased estimates for gradient-based policy optimization with optimal importance weighting and no explicit value function learning. To validate our methodology, we apply it to the task of learning neural non-Gaussian feedback policies, showcasing its efficacy on numerical benchmarks of stochastic dynamical systems.","sentences":["Stochastic optimal control of dynamical systems is a crucial challenge in sequential decision-making.","Recently, control-as-inference approaches have had considerable success, providing a viable risk-sensitive framework to address the exploration-exploitation dilemma.","Nonetheless, a majority of these techniques only invoke the inference-control duality to derive a modified risk objective that is then addressed within a reinforcement learning framework.","This paper introduces a novel perspective by framing risk-sensitive stochastic control as Markovian score climbing under samples drawn from a conditional particle filter.","Our approach, while purely inference-centric, provides asymptotically unbiased estimates for gradient-based policy optimization with optimal importance weighting and no explicit value function learning.","To validate our methodology, we apply it to the task of learning neural non-Gaussian feedback policies, showcasing its efficacy on numerical benchmarks of stochastic dynamical systems."],"url":"http://arxiv.org/abs/2312.14000v1"}
{"created":"2023-12-21 16:28:08","title":"Open-Set: ID Card Presentation Attack Detection using Neural Transfer Style","abstract":"The accurate detection of ID card Presentation Attacks (PA) is becoming increasingly important due to the rising number of online/remote services that require the presentation of digital photographs of ID cards for digital onboarding or authentication. Furthermore, cybercriminals are continuously searching for innovative ways to fool authentication systems to gain unauthorized access to these services. Although advances in neural network design and training have pushed image classification to the state of the art, one of the main challenges faced by the development of fraud detection systems is the curation of representative datasets for training and evaluation. The handcrafted creation of representative presentation attack samples often requires expertise and is very time-consuming, thus an automatic process of obtaining high-quality data is highly desirable. This work explores ID card Presentation Attack Instruments (PAI) in order to improve the generation of samples with four Generative Adversarial Networks (GANs) based image translation models and analyses the effectiveness of the generated data for training fraud detection systems. Using open-source data, we show that synthetic attack presentations are an adequate complement for additional real attack presentations, where we obtain an EER performance increase of 0.63% points for print attacks and a loss of 0.29% for screen capture attacks.","sentences":["The accurate detection of ID card Presentation Attacks (PA) is becoming increasingly important due to the rising number of online/remote services that require the presentation of digital photographs of ID cards for digital onboarding or authentication.","Furthermore, cybercriminals are continuously searching for innovative ways to fool authentication systems to gain unauthorized access to these services.","Although advances in neural network design and training have pushed image classification to the state of the art, one of the main challenges faced by the development of fraud detection systems is the curation of representative datasets for training and evaluation.","The handcrafted creation of representative presentation attack samples often requires expertise and is very time-consuming, thus an automatic process of obtaining high-quality data is highly desirable.","This work explores ID card Presentation Attack Instruments (PAI) in order to improve the generation of samples with four Generative Adversarial Networks (GANs) based image translation models and analyses the effectiveness of the generated data for training fraud detection systems.","Using open-source data, we show that synthetic attack presentations are an adequate complement for additional real attack presentations, where we obtain an EER performance increase of 0.63% points for print attacks and a loss of 0.29% for screen capture attacks."],"url":"http://arxiv.org/abs/2312.13993v1"}
{"created":"2023-12-21 16:20:12","title":"Modular Neural Network Policies for Learning In-Flight Object Catching with a Robot Hand-Arm System","abstract":"We present a modular framework designed to enable a robot hand-arm system to learn how to catch flying objects, a task that requires fast, reactive, and accurately-timed robot motions. Our framework consists of five core modules: (i) an object state estimator that learns object trajectory prediction, (ii) a catching pose quality network that learns to score and rank object poses for catching, (iii) a reaching control policy trained to move the robot hand to pre-catch poses, (iv) a grasping control policy trained to perform soft catching motions for safe and robust grasping, and (v) a gating network trained to synthesize the actions given by the reaching and grasping policy. The former two modules are trained via supervised learning and the latter three use deep reinforcement learning in a simulated environment. We conduct extensive evaluations of our framework in simulation for each module and the integrated system, to demonstrate high success rates of in-flight catching and robustness to perturbations and sensory noise. Whilst only simple cylindrical and spherical objects are used for training, the integrated system shows successful generalization to a variety of household objects that are not used in training.","sentences":["We present a modular framework designed to enable a robot hand-arm system to learn how to catch flying objects, a task that requires fast, reactive, and accurately-timed robot motions.","Our framework consists of five core modules: (i) an object state estimator that learns object trajectory prediction, (ii) a catching pose quality network that learns to score and rank object poses for catching, (iii) a reaching control policy trained to move the robot hand to pre-catch poses, (iv) a grasping control policy trained to perform soft catching motions for safe and robust grasping, and (v) a gating network trained to synthesize the actions given by the reaching and grasping policy.","The former two modules are trained via supervised learning and the latter three use deep reinforcement learning in a simulated environment.","We conduct extensive evaluations of our framework in simulation for each module and the integrated system, to demonstrate high success rates of in-flight catching and robustness to perturbations and sensory noise.","Whilst only simple cylindrical and spherical objects are used for training, the integrated system shows successful generalization to a variety of household objects that are not used in training."],"url":"http://arxiv.org/abs/2312.13987v1"}
{"created":"2023-12-21 16:18:33","title":"R\u00e9nyi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration","abstract":"Pufferfish privacy is a flexible generalization of differential privacy that allows to model arbitrary secrets and adversary's prior knowledge about the data. Unfortunately, designing general and tractable Pufferfish mechanisms that do not compromise utility is challenging. Furthermore, this framework does not provide the composition guarantees needed for a direct use in iterative machine learning algorithms. To mitigate these issues, we introduce a R\\'enyi divergence-based variant of Pufferfish and show that it allows us to extend the applicability of the Pufferfish framework. We first generalize the Wasserstein mechanism to cover a wide range of noise distributions and introduce several ways to improve its utility. We also derive stronger guarantees against out-of-distribution adversaries. Finally, as an alternative to composition, we prove privacy amplification results for contractive noisy iterations and showcase the first use of Pufferfish in private convex optimization. A common ingredient underlying our results is the use and extension of shift reduction lemmas.","sentences":["Pufferfish privacy is a flexible generalization of differential privacy that allows to model arbitrary secrets and adversary's prior knowledge about the data.","Unfortunately, designing general and tractable Pufferfish mechanisms that do not compromise utility is challenging.","Furthermore, this framework does not provide the composition guarantees needed for a direct use in iterative machine learning algorithms.","To mitigate these issues, we introduce a R\\'enyi divergence-based variant of Pufferfish and show that it allows us to extend the applicability of the Pufferfish framework.","We first generalize the Wasserstein mechanism to cover a wide range of noise distributions and introduce several ways to improve its utility.","We also derive stronger guarantees against out-of-distribution adversaries.","Finally, as an alternative to composition, we prove privacy amplification results for contractive noisy iterations and showcase the first use of Pufferfish in private convex optimization.","A common ingredient underlying our results is the use and extension of shift reduction lemmas."],"url":"http://arxiv.org/abs/2312.13985v1"}
{"created":"2023-12-21 16:12:22","title":"Understanding Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) with Real-World Packet Traces","abstract":"Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) is a new physical layer option that has been recently added to the LoRa family with the promise of achieving much higher network capacity than the previous versions of LoRa. In this paper, we present our evaluation of LR-FHSS based on real-world packet traces collected with an LR-FHSS device and a receiver we designed and implemented in software. We overcame challenges due to the lack of documentations of LR-FHSS and our study is the first of its kind that processes signals transmitted by an actual LR-FHSS device with practical issues such as frequency error. Our results show that LR-FHSS meets its expectations in communication range and network capacity. We also propose customized methods for LR-FHSS that improve its performance significantly, allowing our receiver to achieve higher network capacity than those reported earlier.","sentences":["Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) is a new physical layer option that has been recently added to the LoRa family with the promise of achieving much higher network capacity than the previous versions of LoRa.","In this paper, we present our evaluation of LR-FHSS based on real-world packet traces collected with an LR-FHSS device and a receiver we designed and implemented in software.","We overcame challenges due to the lack of documentations of LR-FHSS and our study is the first of its kind that processes signals transmitted by an actual LR-FHSS device with practical issues such as frequency error.","Our results show that LR-FHSS meets its expectations in communication range and network capacity.","We also propose customized methods for LR-FHSS that improve its performance significantly, allowing our receiver to achieve higher network capacity than those reported earlier."],"url":"http://arxiv.org/abs/2312.13981v1"}
{"created":"2023-12-21 16:10:33","title":"Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning","abstract":"Recent advancements in the text-to-3D task leverage finetuned text-to-image diffusion models to generate multi-view images, followed by NeRF reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still suffer from multi-view inconsistency and the resulting NeRF artifacts. Although training longer with SFT improves consistency, it also causes distribution shift, which reduces diversity and realistic details. We argue that the SFT of multi-view diffusion models resembles the instruction finetuning stage of the LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods. Essentially, RLFT methods optimize models beyond their SFT data distribution by using their own outputs, effectively mitigating distribution shift. To this end, we introduce Carve3D, a RLFT method coupled with the Multi-view Reconstruction Consistency (MRC) metric, to improve the consistency of multi-view diffusion models. To compute MRC on a set of multi-view images, we compare them with their corresponding renderings of the reconstructed NeRF at the same viewpoints. We validate the robustness of MRC with extensive experiments conducted under controlled inconsistency levels. We enhance the base RLFT algorithm to stabilize the training process, reduce distribution shift, and identify scaling laws. Through qualitative and quantitative experiments, along with a user study, we demonstrate Carve3D's improved multi-view consistency, the resulting superior NeRF reconstruction quality, and minimal distribution shift compared to longer SFT. Project webpage: https://desaixie.github.io/carve-3d.","sentences":["Recent advancements in the text-to-3D task leverage finetuned text-to-image diffusion models to generate multi-view images, followed by NeRF reconstruction.","Yet, existing supervised finetuned (SFT) diffusion models still suffer from multi-view inconsistency and the resulting NeRF artifacts.","Although training longer with SFT improves consistency, it also causes distribution shift, which reduces diversity and realistic details.","We argue that the SFT of multi-view diffusion models resembles the instruction finetuning stage of the LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.","Essentially, RLFT methods optimize models beyond their SFT data distribution by using their own outputs, effectively mitigating distribution shift.","To this end, we introduce Carve3D, a RLFT method coupled with the Multi-view Reconstruction Consistency (MRC) metric, to improve the consistency of multi-view diffusion models.","To compute MRC on a set of multi-view images, we compare them with their corresponding renderings of the reconstructed NeRF at the same viewpoints.","We validate the robustness of MRC with extensive experiments conducted under controlled inconsistency levels.","We enhance the base RLFT algorithm to stabilize the training process, reduce distribution shift, and identify scaling laws.","Through qualitative and quantitative experiments, along with a user study, we demonstrate Carve3D's improved multi-view consistency, the resulting superior NeRF reconstruction quality, and minimal distribution shift compared to longer SFT.","Project webpage: https://desaixie.github.io/carve-3d."],"url":"http://arxiv.org/abs/2312.13980v1"}
{"created":"2023-12-21 16:06:44","title":"Metalearning with Very Few Samples Per Task","abstract":"Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new, previously unseen tasks from the metadistribution.   In this work, we consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ of interest can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in H$ is a map from features to some representation space that is shared across tasks, and $f_{P} \\in F$ is a task-specific classifier from the representation space to labels. The main question we ask in this work is how much data do we need to metalearn a good representation? Here, the amount of data is measured in terms of both the number of tasks $t$ that we need to see and the number of samples $n$ per task. We focus on the regime where the number of samples per task is extremely small. Our main result shows that, in a distribution-free setting where the feature vectors are in $\\mathbb{R}^d$, the representation is a linear map from $\\mathbb{R}^d \\to \\mathbb{R}^k$, and the task-specific classifiers are halfspaces in $\\mathbb{R}^k$, we can metalearn a representation with error $\\varepsilon$ using just $n = k+2$ samples per task, and $d \\cdot (1/\\varepsilon)^{O(k)}$ tasks. Learning with so few samples per task is remarkable because metalearning would be impossible with $k+1$ samples per task, and because we cannot even hope to learn an accurate task-specific classifier with just $k+2$ samples per task.","sentences":["Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own.","In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new, previously unseen tasks from the metadistribution.   ","In this work, we consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ of interest can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in H$ is a map from features to some representation space that is shared across tasks, and $f_{P} \\in F$ is a task-specific classifier from the representation space to labels.","The main question we ask in this work is how much data do we need to metalearn a good representation?","Here, the amount of data is measured in terms of both the number of tasks $t$ that we need to see and the number of samples $n$ per task.","We focus on the regime where the number of samples per task is extremely small.","Our main result shows that, in a distribution-free setting where the feature vectors are in $\\mathbb{R}^d$, the representation is a linear map from $\\mathbb{R}^d \\to \\mathbb{R}^k$, and the task-specific classifiers are halfspaces in $\\mathbb{R}^k$, we can metalearn a representation with error $\\varepsilon$ using just $n = k+2$ samples per task, and $d \\cdot (1/\\varepsilon)^{O(k)}$ tasks.","Learning with so few samples per task is remarkable because metalearning would be impossible with $k+1$ samples per task, and because we cannot even hope to learn an accurate task-specific classifier with just $k+2$ samples per task."],"url":"http://arxiv.org/abs/2312.13978v1"}
{"created":"2023-12-21 16:04:45","title":"NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views","abstract":"Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods.","sentences":["Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction.","However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views.","Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives.","In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction.","Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details.","To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint.","To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint.","The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.13977v1"}
{"created":"2023-12-21 16:03:07","title":"A Joint Communication and Computation Design for Semantic Wireless Communication with Probability Graph","abstract":"In this paper, we delve into the challenge of optimizing joint communication and computation for semantic communication over wireless networks using a probability graph framework. In the considered model, the base station (BS) extracts the small-sized compressed semantic information through removing redundant messages based on the stored knowledge base. Specifically, the knowledge base is encapsulated in a probability graph that encapsulates statistical relations. At the user side, the compressed information is accurately deduced using the same probability graph employed by the BS. While this approach introduces an additional computational overhead for semantic information extraction, it significantly curtails communication resource consumption by transmitting concise data. We derive both communication and computation cost models based on the inference process of the probability graph. Building upon these models, we introduce a joint communication and computation resource allocation problem aimed at minimizing the overall energy consumption of the network, while accounting for latency, power, and semantic constraints. To address this problem, we obtain a closed-form solution for transmission power under a fixed semantic compression ratio. Subsequently, we propose an efficient linear search-based algorithm to attain the optimal solution for the considered problem with low computational complexity. Simulation results underscore the effectiveness of our proposed system, showcasing notable improvements compared to conventional non-semantic schemes.","sentences":["In this paper, we delve into the challenge of optimizing joint communication and computation for semantic communication over wireless networks using a probability graph framework.","In the considered model, the base station (BS) extracts the small-sized compressed semantic information through removing redundant messages based on the stored knowledge base.","Specifically, the knowledge base is encapsulated in a probability graph that encapsulates statistical relations.","At the user side, the compressed information is accurately deduced using the same probability graph employed by the BS.","While this approach introduces an additional computational overhead for semantic information extraction, it significantly curtails communication resource consumption by transmitting concise data.","We derive both communication and computation cost models based on the inference process of the probability graph.","Building upon these models, we introduce a joint communication and computation resource allocation problem aimed at minimizing the overall energy consumption of the network, while accounting for latency, power, and semantic constraints.","To address this problem, we obtain a closed-form solution for transmission power under a fixed semantic compression ratio.","Subsequently, we propose an efficient linear search-based algorithm to attain the optimal solution for the considered problem with low computational complexity.","Simulation results underscore the effectiveness of our proposed system, showcasing notable improvements compared to conventional non-semantic schemes."],"url":"http://arxiv.org/abs/2312.13975v1"}
{"created":"2023-12-21 15:56:09","title":"On Partial Optimal Transport: Revising the Infeasibility of Sinkhorn and Efficient Gradient Methods","abstract":"This paper studies the Partial Optimal Transport (POT) problem between two unbalanced measures with at most $n$ supports and its applications in various AI tasks such as color transfer or domain adaptation. There is hence the need for fast approximations of POT with increasingly large problem sizes in arising applications. We first theoretically and experimentally investigate the infeasibility of the state-of-the-art Sinkhorn algorithm for POT due to its incompatible rounding procedure, which consequently degrades its qualitative performance in real world applications like point-cloud registration. To this end, we propose a novel rounding algorithm for POT, and then provide a feasible Sinkhorn procedure with a revised computation complexity of $\\mathcal{\\widetilde O}(n^2/\\varepsilon^4)$. Our rounding algorithm also permits the development of two first-order methods to approximate the POT problem. The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD), finds an $\\varepsilon$-approximate solution to the POT problem in $\\mathcal{\\widetilde O}(n^{2.5}/\\varepsilon)$, which is better in $\\varepsilon$ than revised Sinkhorn. The second method, Dual Extrapolation, achieves the computation complexity of $\\mathcal{\\widetilde O}(n^2/\\varepsilon)$, thereby being the best in the literature. We further demonstrate the flexibility of POT compared to standard OT as well as the practicality of our algorithms on real applications where two marginal distributions are unbalanced.","sentences":["This paper studies the Partial Optimal Transport (POT) problem between two unbalanced measures with at most $n$ supports and its applications in various AI tasks such as color transfer or domain adaptation.","There is hence the need for fast approximations of POT with increasingly large problem sizes in arising applications.","We first theoretically and experimentally investigate the infeasibility of the state-of-the-art Sinkhorn algorithm for POT due to its incompatible rounding procedure, which consequently degrades its qualitative performance in real world applications like point-cloud registration.","To this end, we propose a novel rounding algorithm for POT, and then provide a feasible Sinkhorn procedure with a revised computation complexity of $\\mathcal{\\widetilde O}(n^2/\\varepsilon^4)$. Our rounding algorithm also permits the development of two first-order methods to approximate the POT problem.","The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD), finds an $\\varepsilon$-approximate solution to the POT problem in $\\mathcal{\\widetilde O}(n^{2.5}/\\varepsilon)$, which is better in $\\varepsilon$ than revised Sinkhorn.","The second method, Dual Extrapolation, achieves the computation complexity of $\\mathcal{\\widetilde O}(n^2/\\varepsilon)$, thereby being the best in the literature.","We further demonstrate the flexibility of POT compared to standard OT as well as the practicality of our algorithms on real applications where two marginal distributions are unbalanced."],"url":"http://arxiv.org/abs/2312.13970v1"}
{"created":"2023-12-21 15:56:02","title":"An event-driven link-level simulator for validation of AFDX and Ethernet avionics networks","abstract":"Aircraft are composed of many electronic systems: sensors, displays, navigation equipment and communication elements. These elements require a reliable interconnection, which is a major challenge for communication networks as high reliability and predictability requirements must be verified for safe operation. In addition, their verification via hardware deployments is limited because these are costly and make difficult to try different architectures and configurations, thus delaying the design and development in this area. Therefore, verification at early stages in the design process is of great importance and must be supported by simulation. In this context, this work presents an event-driven link level framework and simulator for the validation of avionics networks. The presented tool supports communication protocols such as Avionics Full-Duplex Switched Ethernet (AFDX), which is a common protocol in avionics, as well as Ethernet, used with static routing. Alsa, accurate results are facilitated by the simulator through the utilization of realistic models for the different devices. The proposed platform is evaluated in Clean Sky's Disruptive Cockpit for Large Passenger Aircraft architecture scenario showing capabilities of the simulator. The speed of the verification is a key factor in its application, so the computational cost is analysed, proving that the execution time is linearly dependent on the number of messages sent.","sentences":["Aircraft are composed of many electronic systems: sensors, displays, navigation equipment and communication elements.","These elements require a reliable interconnection, which is a major challenge for communication networks as high reliability and predictability requirements must be verified for safe operation.","In addition, their verification via hardware deployments is limited because these are costly and make difficult to try different architectures and configurations, thus delaying the design and development in this area.","Therefore, verification at early stages in the design process is of great importance and must be supported by simulation.","In this context, this work presents an event-driven link level framework and simulator for the validation of avionics networks.","The presented tool supports communication protocols such as Avionics Full-Duplex Switched Ethernet (AFDX), which is a common protocol in avionics, as well as Ethernet, used with static routing.","Alsa, accurate results are facilitated by the simulator through the utilization of realistic models for the different devices.","The proposed platform is evaluated in Clean Sky's Disruptive Cockpit for Large Passenger Aircraft architecture scenario showing capabilities of the simulator.","The speed of the verification is a key factor in its application, so the computational cost is analysed, proving that the execution time is linearly dependent on the number of messages sent."],"url":"http://arxiv.org/abs/2312.13969v1"}
{"created":"2023-12-21 15:53:54","title":"Asynchronous Authentication","abstract":"A myriad of authentication mechanisms embody a continuous evolution from verbal passwords in ancient times to contemporary multi-factor authentication. Nevertheless, digital asset heists and numerous identity theft cases illustrate the urgent need to revisit the fundamentals of user authentication. We abstract away credential details and formalize the general, common case of asynchronous authentication, with unbounded message propagation time. Our model, which might be of independent interest, allows for eventual message delivery, while bounding execution time to maintain cryptographic guarantees. Given credentials' fault probabilities (e.g., loss or leak), we seek mechanisms with the highest success probability. We show that every mechanism is dominated by some Boolean mechanism -- defined by a monotonic Boolean function on presented credentials. We present an algorithm for finding approximately optimal mechanisms. Previous work analyzed Boolean mechanisms specifically, but used brute force, which quickly becomes prohibitively complex. We leverage the problem structure to reduce complexity by orders of magnitude. The algorithm is readily applicable to practical settings. For example, we revisit the common approach in cryptocurrency wallets that use a handful of high-quality credentials. We show that adding low-quality credentials improves security by orders of magnitude.","sentences":["A myriad of authentication mechanisms embody a continuous evolution from verbal passwords in ancient times to contemporary multi-factor authentication.","Nevertheless, digital asset heists and numerous identity theft cases illustrate the urgent need to revisit the fundamentals of user authentication.","We abstract away credential details and formalize the general, common case of asynchronous authentication, with unbounded message propagation time.","Our model, which might be of independent interest, allows for eventual message delivery, while bounding execution time to maintain cryptographic guarantees.","Given credentials' fault probabilities (e.g., loss or leak), we seek mechanisms with the highest success probability.","We show that every mechanism is dominated by some Boolean mechanism -- defined by a monotonic Boolean function on presented credentials.","We present an algorithm for finding approximately optimal mechanisms.","Previous work analyzed Boolean mechanisms specifically, but used brute force, which quickly becomes prohibitively complex.","We leverage the problem structure to reduce complexity by orders of magnitude.","The algorithm is readily applicable to practical settings.","For example, we revisit the common approach in cryptocurrency wallets that use a handful of high-quality credentials.","We show that adding low-quality credentials improves security by orders of magnitude."],"url":"http://arxiv.org/abs/2312.13967v1"}
{"created":"2023-12-21 15:51:12","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models","abstract":"Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame affinity as input to transfer appearance information guided by the affinity hint for individual frame synthesis in the latent space. This design mitigates the challenges of appearance-related image alignment within and allows for a stronger focus on aligning with motion-related guidance.","sentences":["Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles.","While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text.","In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning.","To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model.","A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame affinity as input to transfer appearance information guided by the affinity hint for individual frame synthesis in the latent space.","This design mitigates the challenges of appearance-related image alignment within and allows for a stronger focus on aligning with motion-related guidance."],"url":"http://arxiv.org/abs/2312.13964v1"}
{"created":"2023-12-21 15:46:36","title":"ChatGPT as a commenter to the news: can LLMs generate human-like opinions?","abstract":"ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn significant attention since their release, and the abilities of these models have been investigated for a wide variety of tasks. In this research we investigate to what extent GPT-3.5 can generate human-like comments on Dutch news articles. We define human likeness as `not distinguishable from human comments', approximated by the difficulty of automatic classification between human and GPT comments. We analyze human likeness across multiple prompting techniques. In particular, we utilize zero-shot, few-shot and context prompts, for two generated personas. We found that our fine-tuned BERT models can easily distinguish human-written comments from GPT-3.5 generated comments, with none of the used prompting methods performing noticeably better. We further analyzed that human comments consistently showed higher lexical diversity than GPT-generated comments. This indicates that although generative LLMs can generate fluent text, their capability to create human-like opinionated comments is still limited.","sentences":["ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn significant attention since their release, and the abilities of these models have been investigated for a wide variety of tasks.","In this research we investigate to what extent GPT-3.5 can generate human-like comments on Dutch news articles.","We define human likeness as `not distinguishable from human comments', approximated by the difficulty of automatic classification between human and GPT comments.","We analyze human likeness across multiple prompting techniques.","In particular, we utilize zero-shot, few-shot and context prompts, for two generated personas.","We found that our fine-tuned BERT models can easily distinguish human-written comments from GPT-3.5 generated comments, with none of the used prompting methods performing noticeably better.","We further analyzed that human comments consistently showed higher lexical diversity than GPT-generated comments.","This indicates that although generative LLMs can generate fluent text, their capability to create human-like opinionated comments is still limited."],"url":"http://arxiv.org/abs/2312.13961v1"}
