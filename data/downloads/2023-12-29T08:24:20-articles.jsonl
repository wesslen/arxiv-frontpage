{"created":"2023-12-28 18:59:57","title":"iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views","abstract":"We present iFusion, a novel 3D object reconstruction framework that requires only two views with unknown camera poses. While single-view reconstruction yields visually appealing results, it can deviate significantly from the actual object, especially on unseen sides. Additional views improve reconstruction fidelity but necessitate known camera poses. However, assuming the availability of pose may be unrealistic, and existing pose estimators fail in sparse view scenarios. To address this, we harness a pre-trained novel view synthesis diffusion model, which embeds implicit knowledge about the geometry and appearance of diverse objects. Our strategy unfolds in three steps: (1) We invert the diffusion model for camera pose estimation instead of synthesizing novel views. (2) The diffusion model is fine-tuned using provided views and estimated poses, turned into a novel view synthesizer tailored for the target object. (3) Leveraging registered views and the fine-tuned diffusion model, we reconstruct the 3D object. Experiments demonstrate strong performance in both pose estimation and novel view synthesis. Moreover, iFusion seamlessly integrates with various reconstruction methods and enhances them.","sentences":["We present iFusion, a novel 3D object reconstruction framework that requires only two views with unknown camera poses.","While single-view reconstruction yields visually appealing results, it can deviate significantly from the actual object, especially on unseen sides.","Additional views improve reconstruction fidelity but necessitate known camera poses.","However, assuming the availability of pose may be unrealistic, and existing pose estimators fail in sparse view scenarios.","To address this, we harness a pre-trained novel view synthesis diffusion model, which embeds implicit knowledge about the geometry and appearance of diverse objects.","Our strategy unfolds in three steps: (1) We invert the diffusion model for camera pose estimation instead of synthesizing novel views.","(2) The diffusion model is fine-tuned using provided views and estimated poses, turned into a novel view synthesizer tailored for the target object.","(3) Leveraging registered views and the fine-tuned diffusion model, we reconstruct the 3D object.","Experiments demonstrate strong performance in both pose estimation and novel view synthesis.","Moreover, iFusion seamlessly integrates with various reconstruction methods and enhances them."],"url":"http://arxiv.org/abs/2312.17250v1"}
{"created":"2023-12-28 18:59:50","title":"Do Androids Know They're Only Dreaming of Electric Sheep?","abstract":"We design probes trained on the internal representations of a transformer language model that are predictive of its hallucinatory behavior on in-context generation tasks. To facilitate this detection, we create a span-annotated dataset of organic and synthetic hallucinations over several tasks. We find that probes trained on the force-decoded states of synthetic hallucinations are generally ecologically invalid in organic hallucination detection. Furthermore, hidden state information about hallucination appears to be task and distribution-dependent. Intrinsic and extrinsic hallucination saliency varies across layers, hidden state types, and tasks; notably, extrinsic hallucinations tend to be more salient in a transformer's internal representations. Outperforming multiple contemporary baselines, we show that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available.","sentences":["We design probes trained on the internal representations of a transformer language model that are predictive of its hallucinatory behavior on in-context generation tasks.","To facilitate this detection, we create a span-annotated dataset of organic and synthetic hallucinations over several tasks.","We find that probes trained on the force-decoded states of synthetic hallucinations are generally ecologically invalid in organic hallucination detection.","Furthermore, hidden state information about hallucination appears to be task and distribution-dependent.","Intrinsic and extrinsic hallucination saliency varies across layers, hidden state types, and tasks; notably, extrinsic hallucinations tend to be more salient in a transformer's internal representations.","Outperforming multiple contemporary baselines, we show that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available."],"url":"http://arxiv.org/abs/2312.17249v1"}
{"created":"2023-12-28 18:59:49","title":"Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity","abstract":"Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity -- the complexity of functions to be represented -- among these RL paradigms. We first demonstrate that, for a broad class of Markov decision processes (MDPs), the model can be represented by constant-depth circuits with polynomial size or Multi-Layer Perceptrons (MLPs) with constant layers and polynomial hidden dimension. However, the representation of the optimal policy and optimal value proves to be $\\mathsf{NP}$-complete and unattainable by constant-layer MLPs with polynomial size. This demonstrates a significant representation complexity gap between model-based RL and model-free RL, which includes policy-based RL and value-based RL. To further explore the representation complexity hierarchy between policy-based RL and value-based RL, we introduce another general class of MDPs where both the model and optimal policy can be represented by constant-depth circuits with polynomial size or constant-layer MLPs with polynomial size. In contrast, representing the optimal value is $\\mathsf{P}$-complete and intractable via a constant-layer MLP with polynomial hidden dimension. This accentuates the intricate representation complexity associated with value-based RL compared to policy-based RL. In summary, we unveil a potential representation complexity hierarchy within RL -- representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge.","sentences":["Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively.","This work investigates the potential hierarchy of representation complexity -- the complexity of functions to be represented -- among these RL paradigms.","We first demonstrate that, for a broad class of Markov decision processes (MDPs), the model can be represented by constant-depth circuits with polynomial size or Multi-Layer Perceptrons (MLPs) with constant layers and polynomial hidden dimension.","However, the representation of the optimal policy and optimal value proves to be $\\mathsf{NP}$-complete and unattainable by constant-layer MLPs with polynomial size.","This demonstrates a significant representation complexity gap between model-based RL and model-free RL, which includes policy-based RL and value-based RL.","To further explore the representation complexity hierarchy between policy-based RL and value-based RL, we introduce another general class of MDPs where both the model and optimal policy can be represented by constant-depth circuits with polynomial size or constant-layer MLPs with polynomial size.","In contrast, representing the optimal value is $\\mathsf{P}$-complete and intractable via a constant-layer MLP with polynomial hidden dimension.","This accentuates the intricate representation complexity associated with value-based RL compared to policy-based RL.","In summary, we unveil a potential representation complexity hierarchy within RL -- representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge."],"url":"http://arxiv.org/abs/2312.17248v1"}
{"created":"2023-12-28 18:59:41","title":"Amodal Ground Truth and Completion in the Wild","abstract":"The problem we study in this paper is amodal image segmentation: predicting entire object segmentation masks including both visible and invisible (occluded) parts. In previous work, the amodal segmentation ground truth on real images is usually predicted by manual annotaton and thus is subjective. In contrast, we use 3D data to establish an automatic pipeline to determine authentic ground truth amodal masks for partially occluded objects in real images. This pipeline is used to construct an amodal completion evaluation benchmark, MP3D-Amodal, consisting of a variety of object categories and labels. To better handle the amodal completion task in the wild, we explore two architecture variants: a two-stage model that first infers the occluder, followed by amodal mask completion; and a one-stage model that exploits the representation power of Stable Diffusion for amodal segmentation across many categories. Without bells and whistles, our method achieves a new state-of-the-art performance on Amodal segmentation datasets that cover a large variety of objects, including COCOA and our new MP3D-Amodal dataset. The dataset, model, and code are available at https://www.robots.ox.ac.uk/~vgg/research/amodal/.","sentences":["The problem we study in this paper is amodal image segmentation: predicting entire object segmentation masks including both visible and invisible (occluded) parts.","In previous work, the amodal segmentation ground truth on real images is usually predicted by manual annotaton and thus is subjective.","In contrast, we use 3D data to establish an automatic pipeline to determine authentic ground truth amodal masks for partially occluded objects in real images.","This pipeline is used to construct an amodal completion evaluation benchmark, MP3D-Amodal, consisting of a variety of object categories and labels.","To better handle the amodal completion task in the wild, we explore two architecture variants: a two-stage model that first infers the occluder, followed by amodal mask completion; and a one-stage model that exploits the representation power of Stable Diffusion for amodal segmentation across many categories.","Without bells and whistles, our method achieves a new state-of-the-art performance on Amodal segmentation datasets that cover a large variety of objects, including COCOA and our new MP3D-Amodal dataset.","The dataset, model, and code are available at https://www.robots.ox.ac.uk/~vgg/research/amodal/."],"url":"http://arxiv.org/abs/2312.17247v1"}
{"created":"2023-12-28 18:59:09","title":"The LLM Surgeon","abstract":"State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models.","sentences":["State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data.","However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints.","We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch.","To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models.","In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal.","We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient.","Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models."],"url":"http://arxiv.org/abs/2312.17244v1"}
{"created":"2023-12-28 18:59:04","title":"Unsupervised Universal Image Segmentation","abstract":"Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation). We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified framework. U2Seg generates pseudo semantic labels for these segmentation tasks via leveraging self-supervised models followed by clustering; each cluster represents different semantic and/or instance membership of pixels. We then self-train the model on these pseudo semantic labels, yielding substantial performance gains over specialized methods tailored to each task: a +2.6 AP$^{\\text{box}}$ boost vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff. Moreover, our method sets up a new baseline for unsupervised panoptic segmentation, which has not been previously explored. U2Seg is also a strong pretrained model for few-shot segmentation, surpassing CutLER by +5.0 AP$^{\\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO labels. We hope our simple yet effective method can inspire more research on unsupervised universal image segmentation.","sentences":["Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation).","We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified framework.","U2Seg generates pseudo semantic labels for these segmentation tasks via leveraging self-supervised models followed by clustering; each cluster represents different semantic and/or instance membership of pixels.","We then self-train the model on these pseudo semantic labels, yielding substantial performance gains over specialized methods tailored to each task: a +2.6 AP$^{\\text{box}}$ boost vs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc increase (vs. STEGO) in unsupervised semantic segmentation on COCOStuff.","Moreover, our method sets up a new baseline for unsupervised panoptic segmentation, which has not been previously explored.","U2Seg is also a strong pretrained model for few-shot segmentation, surpassing CutLER by +5.0","AP$^{\\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO labels.","We hope our simple yet effective method can inspire more research on unsupervised universal image segmentation."],"url":"http://arxiv.org/abs/2312.17243v1"}
{"created":"2023-12-28 18:58:52","title":"Learning to Generate Text in Arbitrary Writing Styles","abstract":"Prior work in style-controlled text generation has focused on tasks such as emulating the style of prolific literary authors, producing formal or informal text, and the degree of toxicity of generated text. Plentiful demonstrations of these styles are available, and as a result modern language models are often able to emulate them, either via prompting or discriminative control. However, in applications such as writing assistants, it is desirable for language models to produce text in an author-specific style on the basis of a small writing sample. We find that instruction-tuned language models can struggle to reproduce author-specific style demonstrated in a prompt. Instead, we propose to guide a language model to generate text in a target style using contrastively-trained representations that capture stylometric features. A central challenge in doing so is that an author's writing is characterized by surprising token choices under a generic language model. To reconcile this tension, we combine generative re-scoring to achieve an author-specific model, with discriminative control to ensure style consistency at the sequence-level. The combination of these approaches is found to be particularly effective at adhering to an author-specific style in a variety of conditions, including unconditional generation and style transfer, and is applicable to any underlying language model without requiring fine-tuning.","sentences":["Prior work in style-controlled text generation has focused on tasks such as emulating the style of prolific literary authors, producing formal or informal text, and the degree of toxicity of generated text.","Plentiful demonstrations of these styles are available, and as a result modern language models are often able to emulate them, either via prompting or discriminative control.","However, in applications such as writing assistants, it is desirable for language models to produce text in an author-specific style on the basis of a small writing sample.","We find that instruction-tuned language models can struggle to reproduce author-specific style demonstrated in a prompt.","Instead, we propose to guide a language model to generate text in a target style using contrastively-trained representations that capture stylometric features.","A central challenge in doing so is that an author's writing is characterized by surprising token choices under a generic language model.","To reconcile this tension, we combine generative re-scoring to achieve an author-specific model, with discriminative control to ensure style consistency at the sequence-level.","The combination of these approaches is found to be particularly effective at adhering to an author-specific style in a variety of conditions, including unconditional generation and style transfer, and is applicable to any underlying language model without requiring fine-tuning."],"url":"http://arxiv.org/abs/2312.17242v1"}
{"created":"2023-12-28 18:58:45","title":"Compact Neural Graphics Primitives with Learned Hash Probing","abstract":"Neural graphics primitives are faster and achieve higher quality when their neural networks are augmented by spatial data structures that hold trainable features arranged in a grid. However, existing feature grids either come with a large memory footprint (dense or factorized grids, trees, and hash tables) or slow performance (index learning and vector quantization). In this paper, we show that a hash table with learned probes has neither disadvantage, resulting in a favorable combination of size and speed. Inference is faster than unprobed hash tables at equal quality while training is only 1.2-2.6x slower, significantly outperforming prior index learning approaches. We arrive at this formulation by casting all feature grids into a common framework: they each correspond to a lookup function that indexes into a table of feature vectors. In this framework, the lookup functions of existing data structures can be combined by simple arithmetic combinations of their indices, resulting in Pareto optimal compression and speed.","sentences":["Neural graphics primitives are faster and achieve higher quality when their neural networks are augmented by spatial data structures that hold trainable features arranged in a grid.","However, existing feature grids either come with a large memory footprint (dense or factorized grids, trees, and hash tables) or slow performance (index learning and vector quantization).","In this paper, we show that a hash table with learned probes has neither disadvantage, resulting in a favorable combination of size and speed.","Inference is faster than unprobed hash tables at equal quality while training is only 1.2-2.6x slower, significantly outperforming prior index learning approaches.","We arrive at this formulation by casting all feature grids into a common framework: they each correspond to a lookup function that indexes into a table of feature vectors.","In this framework, the lookup functions of existing data structures can be combined by simple arithmetic combinations of their indices, resulting in Pareto optimal compression and speed."],"url":"http://arxiv.org/abs/2312.17241v1"}
{"created":"2023-12-28 18:58:33","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model","abstract":"While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats. In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact. The main enhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}: The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation. \\textbf{2) More Natural Conversation}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD). These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources. Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction. LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications.","sentences":["While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats.","In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact.","The main enhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}:","The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation.","\\textbf{2) More Natural Conversation}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD).","These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources.","Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction.","LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications."],"url":"http://arxiv.org/abs/2312.17240v1"}
{"created":"2023-12-28 18:58:13","title":"Fast Inference of Mixture-of-Experts Language Models with Offloading","abstract":"With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their dense counterparts, but it also increases model size due to having multiple experts. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.","sentences":["With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently.","One such strategy is to use sparse Mixture-of-Experts (MoE) - a type of model architectures where only a fraction of model layers are active for any given input.","This property allows MoE-based language models to generate tokens faster than their dense counterparts, but it also increases model size due to having multiple experts.","Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs.","In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory.","We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs.","Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances."],"url":"http://arxiv.org/abs/2312.17238v1"}
{"created":"2023-12-28 18:58:06","title":"Factoring Expertise, Workload, and Turnover into Code Review Recommendation","abstract":"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload.   We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review. Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover.   Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover. Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender.   Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer. In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge. For the projects we study, we are able to globally increase expertise during reviews, +3%, reduce workload concentration, -12%, and reduce the files at risk, -28%. We make our scripts and data available in our replication package. Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes.","sentences":["Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects.","Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers.","In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover while more evenly distributing review workload.   ","We conduct historical analyses to understand the natural concentration of review workload and the degree of knowledge spreading that is inherent in code review.","Even though review workload is highly concentrated, we show that code review natural spreads knowledge thereby reducing the files at risk to turnover.   ","Using simulation, we evaluate existing code review recommenders and develop novel recommenders to understand their impact on the level of expertise during review, the workload of reviewers, and the files at risk to turnover.","Our simulations use seeded random replacement of reviewers to allow us to compare the reviewer recommenders without the confounding variation of different reviewers being replaced for each recommender.   ","Combining recommenders, we develop the SofiaWL recommender that suggests experts with low active review workload when none of the files under review are known by only one developer.","In contrast, when knowledge is concentrated on one developer, it sends the review to other reviewers to spread knowledge.","For the projects we study, we are able to globally increase expertise during reviews, +3%, reduce workload concentration, -12%, and reduce the files at risk, -28%.","We make our scripts and data available in our replication package.","Developers can optimize for a particular outcome measure based on the needs of their project, or use our GitHub bot to automatically balance the outcomes."],"url":"http://arxiv.org/abs/2312.17236v1"}
{"created":"2023-12-28 18:58:01","title":"A Simple LLM Framework for Long-Range Video Question-Answering","abstract":"We present LLoVi, a language-based framework for long-range video question-answering (LVQA). Unlike prior long-range video understanding methods, which are often costly and require specialized long-range video modeling design (e.g., memory queues, state-space layers, etc.), our approach uses a frame/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with a Large Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisingly effective LVQA framework. Specifically, we decompose short and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8s in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to perform long-range temporal reasoning needed to understand the whole video and answer a question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our system. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost. On EgoSchema, which is best known as a very long-form video question-answering benchmark, our method achieves 50.3% accuracy, outperforming the previous best-performing approach by 18.1% (absolute gain). In addition, our approach outperforms the previous state-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA. We also extend LLoVi to grounded LVQA and show that it outperforms all prior methods on the NeXT-GQA dataset. We will release our code at https://github.com/CeeZh/LLoVi.","sentences":["We present LLoVi, a language-based framework for long-range video question-answering (LVQA).","Unlike prior long-range video understanding methods, which are often costly and require specialized long-range video modeling design (e.g., memory queues, state-space layers, etc.), our approach uses a frame/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with a Large Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisingly effective LVQA framework.","Specifically, we decompose short and long-range modeling aspects of LVQA into two stages.","First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8s in length) densely sampled from a long input video.","Afterward, an LLM aggregates the densely extracted short-term captions to perform long-range temporal reasoning needed to understand the whole video and answer a question.","To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our system.","Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance.","Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost.","On EgoSchema, which is best known as a very long-form video question-answering benchmark, our method achieves 50.3% accuracy, outperforming the previous best-performing approach by 18.1% (absolute gain).","In addition, our approach outperforms the previous state-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA.","We also extend LLoVi to grounded LVQA and show that it outperforms all prior methods on the NeXT-GQA dataset.","We will release our code at https://github.com/CeeZh/LLoVi."],"url":"http://arxiv.org/abs/2312.17235v1"}
{"created":"2023-12-28 18:57:49","title":"Personalized Restoration via Dual-Pivot Tuning","abstract":"Generative diffusion models can serve as a prior which ensures that solutions of image restoration systems adhere to the manifold of natural images. However, for restoring facial images, a personalized prior is necessary to accurately represent and reconstruct unique facial features of a given individual. In this paper, we propose a simple, yet effective, method for personalized restoration, called Dual-Pivot Tuning - a two-stage approach that personalize a blind restoration system while maintaining the integrity of the general prior and the distinct role of each component. Our key observation is that for optimal personalization, the generative model should be tuned around a fixed text pivot, while the guiding network should be tuned in a generic (non-personalized) manner, using the personalized generative model as a fixed ``pivot\". This approach ensures that personalization does not interfere with the restoration process, resulting in a natural appearance with high fidelity to the person's identity and the attributes of the degraded image. We evaluated our approach both qualitatively and quantitatively through extensive experiments with images of widely recognized individuals, comparing it against relevant baselines. Surprisingly, we found that our personalized prior not only achieves higher fidelity to identity with respect to the person's identity, but also outperforms state-of-the-art generic priors in terms of general image quality. Project webpage: https://personalized-restoration.github.io","sentences":["Generative diffusion models can serve as a prior which ensures that solutions of image restoration systems adhere to the manifold of natural images.","However, for restoring facial images, a personalized prior is necessary to accurately represent and reconstruct unique facial features of a given individual.","In this paper, we propose a simple, yet effective, method for personalized restoration, called Dual-Pivot Tuning - a two-stage approach that personalize a blind restoration system while maintaining the integrity of the general prior and the distinct role of each component.","Our key observation is that for optimal personalization, the generative model should be tuned around a fixed text pivot, while the guiding network should be tuned in a generic (non-personalized) manner, using the personalized generative model as a fixed ``pivot\".","This approach ensures that personalization does not interfere with the restoration process, resulting in a natural appearance with high fidelity to the person's identity and the attributes of the degraded image.","We evaluated our approach both qualitatively and quantitatively through extensive experiments with images of widely recognized individuals, comparing it against relevant baselines.","Surprisingly, we found that our personalized prior not only achieves higher fidelity to identity with respect to the person's identity, but also outperforms state-of-the-art generic priors in terms of general image quality.","Project webpage: https://personalized-restoration.github.io"],"url":"http://arxiv.org/abs/2312.17234v1"}
{"created":"2023-12-28 18:57:11","title":"Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels","abstract":"Current 3D scene segmentation methods are heavily dependent on manually annotated 3D training datasets. Such manual annotations are labor-intensive, and often lack fine-grained details. Importantly, models trained on this data typically struggle to recognize object classes beyond the annotated classes, i.e., they do not generalize well to unseen domains and require additional domain-specific annotations. In contrast, 2D foundation models demonstrate strong generalization and impressive zero-shot abilities, inspiring us to incorporate these characteristics from 2D models into 3D models. Therefore, we explore the use of image segmentation foundation models to automatically generate training labels for 3D segmentation. We propose Segment3D, a method for class-agnostic 3D scene segmentation that produces high-quality 3D segmentation masks. It improves over existing 3D segmentation models (especially on fine-grained masks), and enables easily adding new training data to further boost the segmentation performance -- all without the need for manual training labels.","sentences":["Current 3D scene segmentation methods are heavily dependent on manually annotated 3D training datasets.","Such manual annotations are labor-intensive, and often lack fine-grained details.","Importantly, models trained on this data typically struggle to recognize object classes beyond the annotated classes, i.e., they do not generalize well to unseen domains and require additional domain-specific annotations.","In contrast, 2D foundation models demonstrate strong generalization and impressive zero-shot abilities, inspiring us to incorporate these characteristics from 2D models into 3D models.","Therefore, we explore the use of image segmentation foundation models to automatically generate training labels for 3D segmentation.","We propose Segment3D, a method for class-agnostic 3D scene segmentation that produces high-quality 3D segmentation masks.","It improves over existing 3D segmentation models (especially on fine-grained masks), and enables easily adding new training data to further boost the segmentation performance -- all without the need for manual training labels."],"url":"http://arxiv.org/abs/2312.17232v1"}
{"created":"2023-12-28 18:55:09","title":"Think Before You Duel: Understanding Complexities of Preference Learning under Constrained Resources","abstract":"We consider the problem of reward maximization in the dueling bandit setup along with constraints on resource consumption. As in the classic dueling bandits, at each round the learner has to choose a pair of items from a set of $K$ items and observe a relative feedback for the current pair. Additionally, for both items, the learner also observes a vector of resource consumptions. The objective of the learner is to maximize the cumulative reward, while ensuring that the total consumption of any resource is within the allocated budget. We show that due to the relative nature of the feedback, the problem is more difficult than its bandit counterpart and that without further assumptions the problem is not learnable from a regret minimization perspective. Thereafter, by exploiting assumptions on the available budget, we provide an EXP3 based dueling algorithm that also considers the associated consumptions and show that it achieves an $\\tilde{\\mathcal{O}}\\left({\\frac{OPT^{(b)}}{B}}K^{1/3}T^{2/3}\\right)$ regret, where $OPT^{(b)}$ is the optimal value and $B$ is the available budget. Finally, we provide numerical simulations to demonstrate the efficacy of our proposed method.","sentences":["We consider the problem of reward maximization in the dueling bandit setup along with constraints on resource consumption.","As in the classic dueling bandits, at each round the learner has to choose a pair of items from a set of $K$ items and observe a relative feedback for the current pair.","Additionally, for both items, the learner also observes a vector of resource consumptions.","The objective of the learner is to maximize the cumulative reward, while ensuring that the total consumption of any resource is within the allocated budget.","We show that due to the relative nature of the feedback, the problem is more difficult than its bandit counterpart and that without further assumptions the problem is not learnable from a regret minimization perspective.","Thereafter, by exploiting assumptions on the available budget, we provide an EXP3 based dueling algorithm that also considers the associated consumptions and show that it achieves an $\\tilde{\\mathcal{O}}\\left({\\frac{OPT^{(b)}}{B}}K^{1/3}T^{2/3}\\right)$ regret, where $OPT^{(b)}$ is the optimal value and $B$ is the available budget.","Finally, we provide numerical simulations to demonstrate the efficacy of our proposed method."],"url":"http://arxiv.org/abs/2312.17229v1"}
{"created":"2023-12-28 18:54:21","title":"Gradient-based Planning with World Models","abstract":"The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours. While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations. Consequently, these models must be learned from data using neural networks. Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning. However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model. In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms. In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks. Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks.","sentences":["The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours.","While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations.","Consequently, these models must be learned from data using neural networks.","Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning.","However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model.","In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms.","In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks.","Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks."],"url":"http://arxiv.org/abs/2312.17227v1"}
{"created":"2023-12-28 18:53:39","title":"4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency","abstract":"Aided by text-to-image and text-to-video diffusion models, existing 4D content creation pipelines utilize score distillation sampling to optimize the entire dynamic 3D scene. However, as these pipelines generate 4D content from text or image inputs, they incur significant time and effort in prompt engineering through trial and error. This work introduces 4DGen, a novel, holistic framework for grounded 4D content creation that decomposes the 4D generation task into multiple stages. We identify static 3D assets and monocular video sequences as key components in constructing the 4D content. Our pipeline facilitates conditional 4D generation, enabling users to specify geometry (3D assets) and motion (monocular videos), thus offering superior control over content creation. Furthermore, we construct our 4D representation using dynamic 3D Gaussians, which permits efficient, high-resolution supervision through rendering during training, thereby facilitating high-quality 4D generation. Additionally, we employ spatial-temporal pseudo labels on anchor frames, along with seamless consistency priors implemented through 3D-aware score distillation sampling and smoothness regularizations. Compared to existing baselines, our approach yields competitive results in faithfully reconstructing input signals and realistically inferring renderings from novel viewpoints and timesteps. Most importantly, our method supports grounded generation, offering users enhanced control, a feature difficult to achieve with previous methods. Project page: https://vita-group.github.io/4DGen/","sentences":["Aided by text-to-image and text-to-video diffusion models, existing 4D content creation pipelines utilize score distillation sampling to optimize the entire dynamic 3D scene.","However, as these pipelines generate 4D content from text or image inputs, they incur significant time and effort in prompt engineering through trial and error.","This work introduces 4DGen, a novel, holistic framework for grounded 4D content creation that decomposes the 4D generation task into multiple stages.","We identify static 3D assets and monocular video sequences as key components in constructing the 4D content.","Our pipeline facilitates conditional 4D generation, enabling users to specify geometry (3D assets) and motion (monocular videos), thus offering superior control over content creation.","Furthermore, we construct our 4D representation using dynamic 3D Gaussians, which permits efficient, high-resolution supervision through rendering during training, thereby facilitating high-quality 4D generation.","Additionally, we employ spatial-temporal pseudo labels on anchor frames, along with seamless consistency priors implemented through 3D-aware score distillation sampling and smoothness regularizations.","Compared to existing baselines, our approach yields competitive results in faithfully reconstructing input signals and realistically inferring renderings from novel viewpoints and timesteps.","Most importantly, our method supports grounded generation, offering users enhanced control, a feature difficult to achieve with previous methods.","Project page: https://vita-group.github.io/4DGen/"],"url":"http://arxiv.org/abs/2312.17225v1"}
{"created":"2023-12-28 18:53:21","title":"Complexity-Theoretic Implications of Multicalibration","abstract":"We present connections between the recent literature on multigroup fairness for prediction algorithms and classical results in computational complexity. Multiaccurate predictors are correct in expectation on each member of an arbitrary collection of pre-specified sets. Multicalibrated predictors satisfy a stronger condition: they are calibrated on each set in the collection.   Multiaccuracy is equivalent to a regularity notion for functions defined by Trevisan, Tulsiani, and Vadhan (2009). They showed that, given a class $F$ of (possibly simple) functions, an arbitrarily complex function $g$ can be approximated by a low-complexity function $h$ that makes a small number of oracle calls to members of $F$, where the notion of approximation requires that $h$ cannot be distinguished from $g$ by members of $F$. This complexity-theoretic Regularity Lemma is known to have implications in different areas, including in complexity theory, additive number theory, information theory, graph theory, and cryptography. Starting from the stronger notion of multicalibration, we obtain stronger and more general versions of a number of applications of the Regularity Lemma, including the Hardcore Lemma, the Dense Model Theorem, and the equivalence of conditional pseudo-min-entropy and unpredictability. For example, we show that every boolean function (regardless of its hardness) has a small collection of disjoint hardcore sets, where the sizes of those hardcore sets are related to how balanced the function is on corresponding pieces of an efficient partition of the domain.","sentences":["We present connections between the recent literature on multigroup fairness for prediction algorithms and classical results in computational complexity.","Multiaccurate predictors are correct in expectation on each member of an arbitrary collection of pre-specified sets.","Multicalibrated predictors satisfy a stronger condition: they are calibrated on each set in the collection.   ","Multiaccuracy is equivalent to a regularity notion for functions defined by Trevisan, Tulsiani, and Vadhan (2009).","They showed that, given a class $F$ of (possibly simple) functions, an arbitrarily complex function $g$ can be approximated by a low-complexity function $h$ that makes a small number of oracle calls to members of $F$, where the notion of approximation requires that $h$ cannot be distinguished from $g$ by members of $F$. This complexity-theoretic Regularity Lemma is known to have implications in different areas, including in complexity theory, additive number theory, information theory, graph theory, and cryptography.","Starting from the stronger notion of multicalibration, we obtain stronger and more general versions of a number of applications of the Regularity Lemma, including the Hardcore Lemma, the Dense Model Theorem, and the equivalence of conditional pseudo-min-entropy and unpredictability.","For example, we show that every boolean function (regardless of its hardness) has a small collection of disjoint hardcore sets, where the sizes of those hardcore sets are related to how balanced the function is on corresponding pieces of an efficient partition of the domain."],"url":"http://arxiv.org/abs/2312.17223v1"}
{"created":"2023-12-28 18:51:25","title":"Scalable and automated Evaluation of Blue Team cyber posture in Cyber Ranges","abstract":"Cyber ranges are virtual training ranges that have emerged as indispensable environments for conducting secure exercises and simulating real or hypothetical scenarios. These complex computational infrastructures enable the simulation of attacks, facilitating the evaluation of defense tools and methodologies and developing novel countermeasures against threats. One of the main challenges of cyber range scalability is the exercise evaluation that often requires the manual intervention of human operators, the White team. This paper proposes a novel approach that uses Blue and Red team reports and well-known databases to automate the evaluation and assessment of the exercise outcomes, overcoming the limitations of existing assessment models. Our proposal encompasses evaluating various aspects and metrics, explicitly emphasizing Blue Teams' actions and strategies and allowing the automated generation of their cyber posture.","sentences":["Cyber ranges are virtual training ranges that have emerged as indispensable environments for conducting secure exercises and simulating real or hypothetical scenarios.","These complex computational infrastructures enable the simulation of attacks, facilitating the evaluation of defense tools and methodologies and developing novel countermeasures against threats.","One of the main challenges of cyber range scalability is the exercise evaluation that often requires the manual intervention of human operators, the White team.","This paper proposes a novel approach that uses Blue and Red team reports and well-known databases to automate the evaluation and assessment of the exercise outcomes, overcoming the limitations of existing assessment models.","Our proposal encompasses evaluating various aspects and metrics, explicitly emphasizing Blue Teams' actions and strategies and allowing the automated generation of their cyber posture."],"url":"http://arxiv.org/abs/2312.17221v1"}
{"created":"2023-12-28 18:50:30","title":"Timeliness: A New Design Metric and a New Attack Surface","abstract":"As the landscape of time-sensitive applications gains prominence in 5G/6G communications, timeliness of information updates at network nodes has become crucial, which is popularly quantified in the literature by the age of information metric. However, as we devise policies to improve age of information of our systems, we inadvertently introduce a new vulnerability for adversaries to exploit. In this article, we comprehensively discuss the diverse threats that age-based systems are vulnerable to. We begin with discussion on densely interconnected networks that employ gossiping between nodes to expedite dissemination of dynamic information in the network, and show how the age-based nature of gossiping renders these networks uniquely susceptible to threats such as timestomping attacks, jamming attacks, and the propagation of misinformation. Later, we survey adversarial works within simpler network settings, specifically in one-hop and two-hop configurations, and delve into adversarial robustness concerning challenges posed by jamming, timestomping, and issues related to privacy leakage. We conclude this article with future directions that aim to address challenges posed by more intelligent adversaries and robustness of networks to them.","sentences":["As the landscape of time-sensitive applications gains prominence in 5G/6G communications, timeliness of information updates at network nodes has become crucial, which is popularly quantified in the literature by the age of information metric.","However, as we devise policies to improve age of information of our systems, we inadvertently introduce a new vulnerability for adversaries to exploit.","In this article, we comprehensively discuss the diverse threats that age-based systems are vulnerable to.","We begin with discussion on densely interconnected networks that employ gossiping between nodes to expedite dissemination of dynamic information in the network, and show how the age-based nature of gossiping renders these networks uniquely susceptible to threats such as timestomping attacks, jamming attacks, and the propagation of misinformation.","Later, we survey adversarial works within simpler network settings, specifically in one-hop and two-hop configurations, and delve into adversarial robustness concerning challenges posed by jamming, timestomping, and issues related to privacy leakage.","We conclude this article with future directions that aim to address challenges posed by more intelligent adversaries and robustness of networks to them."],"url":"http://arxiv.org/abs/2312.17220v1"}
{"created":"2023-12-28 18:46:21","title":"Control Barrier Function Based UAV Safety Controller in Autonomous Airborne Tracking and Following Systems","abstract":"Safe operations of UAVs are of paramount importance for various mission-critical and safety-critical UAV applications. In context of airborne target tracking and following, UAVs need to track a flying target avoiding collision and also closely follow its trajectory. The safety situation becomes critical and more complex when the flying target is non-cooperative and has erratic movements. This paper proposes a method for collision avoidance in an autonomous fast moving dynamic quadrotor UAV tracking and following another target UAV. This is achieved by designing a safety controller that minimally modifies the control input from a trajectory tracking controller and guarantees safety. This method enables pairing our proposed safety controller with already existing flight controllers. Our safety controller uses a control barrier function based quadratic program (CBF-QP) to produce an optimal control input enabling safe operation while also follow the trajectory of the target closely. We implement our solution on AirSim simulator over PX4 flight controller and with numerical results, we validate our approach through several simulation experiments with multiple scenarios and trajectories.","sentences":["Safe operations of UAVs are of paramount importance for various mission-critical and safety-critical UAV applications.","In context of airborne target tracking and following, UAVs need to track a flying target avoiding collision and also closely follow its trajectory.","The safety situation becomes critical and more complex when the flying target is non-cooperative and has erratic movements.","This paper proposes a method for collision avoidance in an autonomous fast moving dynamic quadrotor UAV tracking and following another target UAV.","This is achieved by designing a safety controller that minimally modifies the control input from a trajectory tracking controller and guarantees safety.","This method enables pairing our proposed safety controller with already existing flight controllers.","Our safety controller uses a control barrier function based quadratic program (CBF-QP) to produce an optimal control input enabling safe operation while also follow the trajectory of the target closely.","We implement our solution on AirSim simulator over PX4 flight controller and with numerical results, we validate our approach through several simulation experiments with multiple scenarios and trajectories."],"url":"http://arxiv.org/abs/2312.17215v1"}
{"created":"2023-12-28 18:40:31","title":"EFHQ: Multi-purpose ExtremePose-Face-HQ dataset","abstract":"The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with profile or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high-quality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation, and face reenactment. Specifically, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, confirmed by extensive experiments. Additionally, we utilize EFHQ to define a challenging cross-view face verification benchmark, in which the performance of SOTA face recognition models drops 5-37\\% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.","sentences":["The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with profile or pitched faces.","This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high-quality images of faces at extreme poses.","To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings.","Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation, and face reenactment.","Specifically, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, confirmed by extensive experiments.","Additionally, we utilize EFHQ to define a challenging cross-view face verification benchmark, in which the performance of SOTA face recognition models drops 5-37\\% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild."],"url":"http://arxiv.org/abs/2312.17205v1"}
{"created":"2023-12-28 18:29:40","title":"Navigating the Research Landscape of Decentralized Autonomous Organizations: A Research Note and Agenda","abstract":"This note and agenda serve as a cause for thought for scholars interested in researching Decentralized Autonomous Organizations (DAOs), addressing both the opportunities and challenges posed by this phenomenon. It covers key aspects of data retrieval, data selection criteria, issues in data reliability and validity such as governance token pricing complexities, discrepancy in treasuries, Mainnet and Testnet data, understanding the variety of DAO types and proposal categories, airdrops affecting governance, and the Sybil problem. The agenda aims to equip scholars with the essential knowledge required to conduct nuanced and rigorous academic studies on DAOs by illuminating these various aspects and proposing directions for future research.","sentences":["This note and agenda serve as a cause for thought for scholars interested in researching Decentralized Autonomous Organizations (DAOs), addressing both the opportunities and challenges posed by this phenomenon.","It covers key aspects of data retrieval, data selection criteria, issues in data reliability and validity such as governance token pricing complexities, discrepancy in treasuries, Mainnet and Testnet data, understanding the variety of DAO types and proposal categories, airdrops affecting governance, and the Sybil problem.","The agenda aims to equip scholars with the essential knowledge required to conduct nuanced and rigorous academic studies on DAOs by illuminating these various aspects and proposing directions for future research."],"url":"http://arxiv.org/abs/2312.17197v1"}
{"created":"2023-12-28 18:24:57","title":"HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human Reconstruction","abstract":"Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details. Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes. To this aim, we present a new hybrid implicit surface representation to model human shapes. This representation is composed of two surface layers that represent opaque and translucent regions on the clothed human body. We segment different regions automatically using visual cues and learn to reconstruct two signed distance functions (SDFs). We perform surface-based rendering on opaque regions (e.g., body, face, clothes) to preserve high-fidelity surface normals and volume rendering on translucent regions (e.g., hair). Experiments demonstrate that our approach obtains state-of-the-art results on 3D human reconstructions, and also shows competitive performances on other objects.","sentences":["Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details.","Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes.","To this aim, we present a new hybrid implicit surface representation to model human shapes.","This representation is composed of two surface layers that represent opaque and translucent regions on the clothed human body.","We segment different regions automatically using visual cues and learn to reconstruct two signed distance functions (SDFs).","We perform surface-based rendering on opaque regions (e.g., body, face, clothes) to preserve high-fidelity surface normals and volume rendering on translucent regions (e.g., hair).","Experiments demonstrate that our approach obtains state-of-the-art results on 3D human reconstructions, and also shows competitive performances on other objects."],"url":"http://arxiv.org/abs/2312.17192v1"}
{"created":"2023-12-28 18:14:17","title":"Geometric Guidance for the Deployment of Elastic Geodesic Grids","abstract":"Elastic gridshells are advanced free-form structures enabling curved target shapes and material-efficient large spans. This paper focuses on a novel type of gridshells recently proposed employing a scissor-like deployment mechanism. While recent form-finding advancements have produced fascinating outcomes, a significant challenge arises when architecturally implementing such mechanisms: for the realization of real-world structures, professional FEA is necessary. However, performing Finite Element simulations of these structures proves surprisingly complex due to the requirement of simulating the deployment -- a task nearly unachievable using uninformed approaches. Therefore, geometric guidance of the highly elastic gridshells while simulating the expansion is essential. Present solutions to this predicament primarily involve rudimentary trial-and-error methods, suitable only for the most basic shapes. We propose a solution involving the provision of geometric guidance via sequences of linear displacements synchronized with a universal time parameter. When applied to chosen positions, this allows for multi-step gridshell deployment and successfully avoids undesirable buckling issues. We conclude with successful demonstrations of our method, anticipating our work to pave the way for further quantitative explorations of these intriguing structures.","sentences":["Elastic gridshells are advanced free-form structures enabling curved target shapes and material-efficient large spans.","This paper focuses on a novel type of gridshells recently proposed employing a scissor-like deployment mechanism.","While recent form-finding advancements have produced fascinating outcomes, a significant challenge arises when architecturally implementing such mechanisms: for the realization of real-world structures, professional FEA is necessary.","However, performing Finite Element simulations of these structures proves surprisingly complex due to the requirement of simulating the deployment -- a task nearly unachievable using uninformed approaches.","Therefore, geometric guidance of the highly elastic gridshells while simulating the expansion is essential.","Present solutions to this predicament primarily involve rudimentary trial-and-error methods, suitable only for the most basic shapes.","We propose a solution involving the provision of geometric guidance via sequences of linear displacements synchronized with a universal time parameter.","When applied to chosen positions, this allows for multi-step gridshell deployment and successfully avoids undesirable buckling issues.","We conclude with successful demonstrations of our method, anticipating our work to pave the way for further quantitative explorations of these intriguing structures."],"url":"http://arxiv.org/abs/2312.17181v1"}
{"created":"2023-12-28 18:12:42","title":"Virtual Scientific Companion for Synchrotron Beamlines: A Prototype","abstract":"The extraordinarily high X-ray flux and specialized instrumentation at synchrotron beamlines have enabled versatile in-situ and high throughput studies that are impossible elsewhere. Dexterous and efficient control of experiments are thus crucial for efficient beamline operation. Artificial intelligence and machine learning methods are constantly being developed to enhance facility performance, but the full potential of these developments can only be reached with efficient human-computer-interaction. Natural language is the most intuitive and efficient way for humans to communicate. However, the low credibility and reproducibility of existing large language models and tools demand extensive development to be made for robust and reliable performance for scientific purposes. In this work, we introduce the prototype of virtual scientific companion (VISION) and demonstrate that it is possible to control basic beamline operations through natural language with open-source language model and the limited computational resources at beamline. The human-AI nature of VISION leverages existing automation systems and data framework at synchrotron beamlines.","sentences":["The extraordinarily high X-ray flux and specialized instrumentation at synchrotron beamlines have enabled versatile in-situ and high throughput studies that are impossible elsewhere.","Dexterous and efficient control of experiments are thus crucial for efficient beamline operation.","Artificial intelligence and machine learning methods are constantly being developed to enhance facility performance, but the full potential of these developments can only be reached with efficient human-computer-interaction.","Natural language is the most intuitive and efficient way for humans to communicate.","However, the low credibility and reproducibility of existing large language models and tools demand extensive development to be made for robust and reliable performance for scientific purposes.","In this work, we introduce the prototype of virtual scientific companion (VISION) and demonstrate that it is possible to control basic beamline operations through natural language with open-source language model and the limited computational resources at beamline.","The human-AI nature of VISION leverages existing automation systems and data framework at synchrotron beamlines."],"url":"http://arxiv.org/abs/2312.17180v1"}
{"created":"2023-12-28 18:12:41","title":"Going Green in RAN Slicing","abstract":"Network slicing is essential for transforming future telecommunication networks into versatile service platforms, but it also presents challenges for sustainable network operations. While meeting the requirements of network slices incurs additional energy consumption compared to non-sliced networks, operators strive to offer diverse 5G and beyond services while maintaining energy efficiency. In this study, we address the issue of slice activation/deactivation to reduce energy consumption while maintaining the user quality of service (QoS). We employ Deep Contextual Multi-Armed Bandit and Thompson Sampling Contextual Multi-Armed Bandit agents to make activation/deactivation decisions for individual clusters. Evaluations are performed using the NetMob23 dataset, which captures the spatio-temporal consumption of various mobile services in France. Our simulation results demonstrate that our proposed solutions provide significant reductions in network energy consumption while ensuring the QoS remains at a similar level compared to a scenario where all slice instances are active.","sentences":["Network slicing is essential for transforming future telecommunication networks into versatile service platforms, but it also presents challenges for sustainable network operations.","While meeting the requirements of network slices incurs additional energy consumption compared to non-sliced networks, operators strive to offer diverse 5G and beyond services while maintaining energy efficiency.","In this study, we address the issue of slice activation/deactivation to reduce energy consumption while maintaining the user quality of service (QoS).","We employ Deep Contextual Multi-Armed Bandit and Thompson Sampling Contextual Multi-Armed Bandit agents to make activation/deactivation decisions for individual clusters.","Evaluations are performed using the NetMob23 dataset, which captures the spatio-temporal consumption of various mobile services in France.","Our simulation results demonstrate that our proposed solutions provide significant reductions in network energy consumption while ensuring the QoS remains at a similar level compared to a scenario where all slice instances are active."],"url":"http://arxiv.org/abs/2312.17179v1"}
{"created":"2023-12-28 18:02:22","title":"Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution","abstract":"Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability. To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features. We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare. Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available. Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively.","sentences":["Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability.","To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features.","We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare.","Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available.","Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2312.17174v1"}
{"created":"2023-12-28 17:57:06","title":"Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action","abstract":"We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community.","sentences":["We present Unified-IO 2, the first autoregressive multimodal model that is capable of understanding and generating image, text, audio, and action.","To unify different modalities, we tokenize inputs and outputs -- images, text, audio, action, bounding boxes, etc., into a shared semantic space and then process them with a single encoder-decoder transformer model.","Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training.","We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective.","To learn an expansive set of skills, such as following multimodal instructions, we construct and finetune on an ensemble of 120 datasets with prompts and augmentations.","With a single unified model, Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation.","We release all our models to the research community."],"url":"http://arxiv.org/abs/2312.17172v1"}
{"created":"2023-12-28 17:55:13","title":"Improving Code Reviewer Recommendation: Accuracy, Latency, Workload, and Bystanders","abstract":"Code review ensures that a peer engineer manually examines the code before it is integrated and released into production. At Meta, we develop a wide range of software at scale, from social networking to software development infrastructure, such as calendar and meeting tools to continuous integration. We are constantly improving our code review system, and in this work we describe a series of experiments that were conducted across 10's of thousands of engineers and 100's of thousands of reviews.   We build upon the recommender that has been in production since 2018, RevRecV1. We found that reviewers were being assigned based on prior authorship of files. We reviewed the literature for successful features and experimented with them with RevRecV2 in production. The most important feature in our new model was the familiarity of the author and reviewer, we saw an overall improvement in accuracy of 14 percentage points.   Prior research has shown that reviewer workload is skewed. To balance workload, we divide the reviewer score from RevRecV2 by each candidate reviewers workload. We experimented with multiple types of workload to develop RevRecWL. We find that reranking candidate reviewers by workload often leads to a reviewers with lower workload being selected by authors.   The bystander effect can occur when a team of reviewers is assigned the review. We mitigate the bystander effect by randomly assigning one of the recommended reviewers. Having an individual who is responsible for the review, reduces the time take for reviews by -11%.","sentences":["Code review ensures that a peer engineer manually examines the code before it is integrated and released into production.","At Meta, we develop a wide range of software at scale, from social networking to software development infrastructure, such as calendar and meeting tools to continuous integration.","We are constantly improving our code review system, and in this work we describe a series of experiments that were conducted across 10's of thousands of engineers and 100's of thousands of reviews.   ","We build upon the recommender that has been in production since 2018, RevRecV1.","We found that reviewers were being assigned based on prior authorship of files.","We reviewed the literature for successful features and experimented with them with RevRecV2 in production.","The most important feature in our new model was the familiarity of the author and reviewer, we saw an overall improvement in accuracy of 14 percentage points.   ","Prior research has shown that reviewer workload is skewed.","To balance workload, we divide the reviewer score from RevRecV2 by each candidate reviewers workload.","We experimented with multiple types of workload to develop RevRecWL.","We find that reranking candidate reviewers by workload often leads to a reviewers with lower workload being selected by authors.   ","The bystander effect can occur when a team of reviewers is assigned the review.","We mitigate the bystander effect by randomly assigning one of the recommended reviewers.","Having an individual who is responsible for the review, reduces the time take for reviews by -11%."],"url":"http://arxiv.org/abs/2312.17169v1"}
{"created":"2023-12-28 17:54:56","title":"Can Active Sampling Reduce Causal Confusion in Offline Reinforcement Learning?","abstract":"Causal confusion is a phenomenon where an agent learns a policy that reflects imperfect spurious correlations in the data. Such a policy may falsely appear to be optimal during training if most of the training data contain such spurious correlations. This phenomenon is particularly pronounced in domains such as robotics, with potentially large gaps between the open- and closed-loop performance of an agent. In such settings, causally confused models may appear to perform well according to open-loop metrics during training but fail catastrophically when deployed in the real world. In this paper, we study causal confusion in offline reinforcement learning. We investigate whether selectively sampling appropriate points from a dataset of demonstrations may enable offline reinforcement learning agents to disambiguate the underlying causal mechanisms of the environment, alleviate causal confusion in offline reinforcement learning, and produce a safer model for deployment. To answer this question, we consider a set of tailored offline reinforcement learning datasets that exhibit causal ambiguity and assess the ability of active sampling techniques to reduce causal confusion at evaluation. We provide empirical evidence that uniform and active sampling techniques are able to consistently reduce causal confusion as training progresses and that active sampling is able to do so significantly more efficiently than uniform sampling.","sentences":["Causal confusion is a phenomenon where an agent learns a policy that reflects imperfect spurious correlations in the data.","Such a policy may falsely appear to be optimal during training if most of the training data contain such spurious correlations.","This phenomenon is particularly pronounced in domains such as robotics, with potentially large gaps between the open- and closed-loop performance of an agent.","In such settings, causally confused models may appear to perform well according to open-loop metrics during training but fail catastrophically when deployed in the real world.","In this paper, we study causal confusion in offline reinforcement learning.","We investigate whether selectively sampling appropriate points from a dataset of demonstrations may enable offline reinforcement learning agents to disambiguate the underlying causal mechanisms of the environment, alleviate causal confusion in offline reinforcement learning, and produce a safer model for deployment.","To answer this question, we consider a set of tailored offline reinforcement learning datasets that exhibit causal ambiguity and assess the ability of active sampling techniques to reduce causal confusion at evaluation.","We provide empirical evidence that uniform and active sampling techniques are able to consistently reduce causal confusion as training progresses and that active sampling is able to do so significantly more efficiently than uniform sampling."],"url":"http://arxiv.org/abs/2312.17168v1"}
{"created":"2023-12-28 17:52:21","title":"Securing NextG Systems against Poisoning Attacks on Federated Learning: A Game-Theoretic Solution","abstract":"This paper studies the poisoning attack and defense interactions in a federated learning (FL) system, specifically in the context of wireless signal classification using deep learning for next-generation (NextG) communications. FL collectively trains a global model without the need for clients to exchange their data samples. By leveraging geographically dispersed clients, the trained global model can be used for incumbent user identification, facilitating spectrum sharing. However, in this distributed learning system, the presence of malicious clients introduces the risk of poisoning the training data to manipulate the global model through falsified local model exchanges. To address this challenge, a proactive defense mechanism is employed in this paper to make informed decisions regarding the admission or rejection of clients participating in FL systems. Consequently, the attack-defense interactions are modeled as a game, centered around the underlying admission and poisoning decisions. First, performance bounds are established, encompassing the best and worst strategies for attackers and defenders. Subsequently, the attack and defense utilities are characterized within the Nash equilibrium, where no player can unilaterally improve its performance given the fixed strategies of others. The results offer insights into novel operational modes that safeguard FL systems against poisoning attacks by quantifying the performance of both attacks and defenses in the context of NextG communications.","sentences":["This paper studies the poisoning attack and defense interactions in a federated learning (FL) system, specifically in the context of wireless signal classification using deep learning for next-generation (NextG) communications.","FL collectively trains a global model without the need for clients to exchange their data samples.","By leveraging geographically dispersed clients, the trained global model can be used for incumbent user identification, facilitating spectrum sharing.","However, in this distributed learning system, the presence of malicious clients introduces the risk of poisoning the training data to manipulate the global model through falsified local model exchanges.","To address this challenge, a proactive defense mechanism is employed in this paper to make informed decisions regarding the admission or rejection of clients participating in FL systems.","Consequently, the attack-defense interactions are modeled as a game, centered around the underlying admission and poisoning decisions.","First, performance bounds are established, encompassing the best and worst strategies for attackers and defenders.","Subsequently, the attack and defense utilities are characterized within the Nash equilibrium, where no player can unilaterally improve its performance given the fixed strategies of others.","The results offer insights into novel operational modes that safeguard FL systems against poisoning attacks by quantifying the performance of both attacks and defenses in the context of NextG communications."],"url":"http://arxiv.org/abs/2312.17164v1"}
{"created":"2023-12-28 17:52:09","title":"FENet: Focusing Enhanced Network for Lane Detection","abstract":"Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving. Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety. While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis. Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures. Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perception principles. Code will be made available.","sentences":["Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving.","Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety.","While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis.","Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures.","Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perception principles.","Code will be made available."],"url":"http://arxiv.org/abs/2312.17163v1"}
{"created":"2023-12-28 17:50:54","title":"Restoration by Generation with Constrained Priors","abstract":"The inherent generative power of denoising diffusion models makes them well-suited for image restoration tasks where the objective is to find the optimal high-quality image within the generative space that closely resembles the input image. We propose a method to adapt a pretrained diffusion model for image restoration by simply adding noise to the input image to be restored and then denoise. Our method is based on the observation that the space of a generative model needs to be constrained. We impose this constraint by finetuning the generative model with a set of anchor images that capture the characteristics of the input image. With the constrained space, we can then leverage the sampling strategy used for generation to do image restoration. We evaluate against previous methods and show superior performances on multiple real-world restoration datasets in preserving identity and image quality. We also demonstrate an important and practical application on personalized restoration, where we use a personal album as the anchor images to constrain the generative space. This approach allows us to produce results that accurately preserve high-frequency details, which previous works are unable to do. Project webpage: https://gen2res.github.io.","sentences":["The inherent generative power of denoising diffusion models makes them well-suited for image restoration tasks where the objective is to find the optimal high-quality image within the generative space that closely resembles the input image.","We propose a method to adapt a pretrained diffusion model for image restoration by simply adding noise to the input image to be restored and then denoise.","Our method is based on the observation that the space of a generative model needs to be constrained.","We impose this constraint by finetuning the generative model with a set of anchor images that capture the characteristics of the input image.","With the constrained space, we can then leverage the sampling strategy used for generation to do image restoration.","We evaluate against previous methods and show superior performances on multiple real-world restoration datasets in preserving identity and image quality.","We also demonstrate an important and practical application on personalized restoration, where we use a personal album as the anchor images to constrain the generative space.","This approach allows us to produce results that accurately preserve high-frequency details, which previous works are unable to do.","Project webpage: https://gen2res.github.io."],"url":"http://arxiv.org/abs/2312.17161v1"}
{"created":"2023-12-28 17:47:25","title":"Replica Tree-based Federated Learning using Limited Data","abstract":"Learning from limited data has been extensively studied in machine learning, considering that deep neural networks achieve optimal performance when trained using a large amount of samples. Although various strategies have been proposed for centralized training, the topic of federated learning with small datasets remains largely unexplored. Moreover, in realistic scenarios, such as settings where medical institutions are involved, the number of participating clients is also constrained. In this work, we propose a novel federated learning framework, named RepTreeFL. At the core of the solution is the concept of a replica, where we replicate each participating client by copying its model architecture and perturbing its local data distribution. Our approach enables learning from limited data and a small number of clients by aggregating a larger number of models with diverse data distributions. Furthermore, we leverage the hierarchical structure of the client network (both original and virtual), alongside the model diversity across replicas, and introduce a diversity-based tree aggregation, where replicas are combined in a tree-like manner and the aggregation weights are dynamically updated based on the model discrepancy. We evaluated our method on two tasks and two types of data, graph generation and image classification (binary and multi-class), with both homogeneous and heterogeneous model architectures. Experimental results demonstrate the effectiveness and outperformance of RepTreeFL in settings where both data and clients are limited. Our code is available at https://github.com/basiralab/RepTreeFL.","sentences":["Learning from limited data has been extensively studied in machine learning, considering that deep neural networks achieve optimal performance when trained using a large amount of samples.","Although various strategies have been proposed for centralized training, the topic of federated learning with small datasets remains largely unexplored.","Moreover, in realistic scenarios, such as settings where medical institutions are involved, the number of participating clients is also constrained.","In this work, we propose a novel federated learning framework, named RepTreeFL.","At the core of the solution is the concept of a replica, where we replicate each participating client by copying its model architecture and perturbing its local data distribution.","Our approach enables learning from limited data and a small number of clients by aggregating a larger number of models with diverse data distributions.","Furthermore, we leverage the hierarchical structure of the client network (both original and virtual), alongside the model diversity across replicas, and introduce a diversity-based tree aggregation, where replicas are combined in a tree-like manner and the aggregation weights are dynamically updated based on the model discrepancy.","We evaluated our method on two tasks and two types of data, graph generation and image classification (binary and multi-class), with both homogeneous and heterogeneous model architectures.","Experimental results demonstrate the effectiveness and outperformance of RepTreeFL in settings where both data and clients are limited.","Our code is available at https://github.com/basiralab/RepTreeFL."],"url":"http://arxiv.org/abs/2312.17159v1"}
{"created":"2023-12-28 17:43:39","title":"BEAST: Online Joint Beat and Downbeat Tracking Based on Streaming Transformer","abstract":"Many deep learning models have achieved dominant performance on the offline beat tracking task. However, online beat tracking, in which only the past and present input features are available, still remains challenging. In this paper, we propose BEAt tracking Streaming Transformer (BEAST), an online joint beat and downbeat tracking system based on the streaming Transformer. To deal with online scenarios, BEAST applies contextual block processing in the Transformer encoder. Moreover, we adopt relative positional encoding in the attention layer of the streaming Transformer encoder to capture relative timing position which is critically important information in music. Carrying out beat and downbeat experiments on benchmark datasets for a low latency scenario with maximum latency under 50 ms, BEAST achieves an F1-measure of 80.04% in beat and 52.73% in downbeat, which is a substantial improvement of about 5 and 13 percentage points over the state-of-the-art online beat and downbeat tracking model.","sentences":["Many deep learning models have achieved dominant performance on the offline beat tracking task.","However, online beat tracking, in which only the past and present input features are available, still remains challenging.","In this paper, we propose BEAt tracking Streaming Transformer (BEAST), an online joint beat and downbeat tracking system based on the streaming Transformer.","To deal with online scenarios, BEAST applies contextual block processing in the Transformer encoder.","Moreover, we adopt relative positional encoding in the attention layer of the streaming Transformer encoder to capture relative timing position which is critically important information in music.","Carrying out beat and downbeat experiments on benchmark datasets for a low latency scenario with maximum latency under 50 ms, BEAST achieves an F1-measure of 80.04% in beat and 52.73% in downbeat, which is a substantial improvement of about 5 and 13 percentage points over the state-of-the-art online beat and downbeat tracking model."],"url":"http://arxiv.org/abs/2312.17156v1"}
{"created":"2023-12-28 17:30:25","title":"On-Demand JSON: A Better Way to Parse Documents?","abstract":"JSON is a popular standard for data interchange on the Internet. Ingesting JSON documents can be a performance bottleneck. A popular parsing strategy consists in converting the input text into a tree-based data structure -- sometimes called a Document Object Model or DOM. We designed and implemented a novel JSON parsing interface -- called On-Demand -- that appears to the programmer like a conventional DOM-based approach. However, the underlying implementation is a pointer iterating through the content, only materializing the results (objects, arrays, strings, numbers) lazily.On recent commodity processors, an implementation of our approach provides superior performance in multiple benchmarks. To ensure reproducibility, our work is freely available as open source software. Several systems use On Demand: e.g., Apache Doris, the Node.js JavaScript runtime, Milvus, and Velox.","sentences":["JSON is a popular standard for data interchange on the Internet.","Ingesting JSON documents can be a performance bottleneck.","A popular parsing strategy consists in converting the input text into a tree-based data structure -- sometimes called a Document Object Model or DOM.","We designed and implemented a novel JSON parsing interface -- called On-Demand -- that appears to the programmer like a conventional DOM-based approach.","However, the underlying implementation is a pointer iterating through the content, only materializing the results (objects, arrays, strings, numbers) lazily.","On recent commodity processors, an implementation of our approach provides superior performance in multiple benchmarks.","To ensure reproducibility, our work is freely available as open source software.","Several systems use On Demand: e.g., Apache Doris, the Node.js JavaScript runtime, Milvus, and Velox."],"url":"http://arxiv.org/abs/2312.17149v1"}
{"created":"2023-12-28 17:16:44","title":"DreamGaussian4D: Generative 4D Gaussian Splatting","abstract":"Remarkable progress has been made in 4D content generation recently. However, existing methods suffer from long optimization time, lack of motion controllability, and a low level of detail. In this paper, we introduce DreamGaussian4D, an efficient 4D generation framework that builds on 4D Gaussian Splatting representation. Our key insight is that the explicit modeling of spatial transformations in Gaussian Splatting makes it more suitable for the 4D generation setting compared with implicit representations. DreamGaussian4D reduces the optimization time from several hours to just a few minutes, allows flexible control of the generated 3D motion, and produces animated meshes that can be efficiently rendered in 3D engines.","sentences":["Remarkable progress has been made in 4D content generation recently.","However, existing methods suffer from long optimization time, lack of motion controllability, and a low level of detail.","In this paper, we introduce DreamGaussian4D, an efficient 4D generation framework that builds on 4D Gaussian Splatting representation.","Our key insight is that the explicit modeling of spatial transformations in Gaussian Splatting makes it more suitable for the 4D generation setting compared with implicit representations.","DreamGaussian4D reduces the optimization time from several hours to just a few minutes, allows flexible control of the generated 3D motion, and produces animated meshes that can be efficiently rendered in 3D engines."],"url":"http://arxiv.org/abs/2312.17142v1"}
{"created":"2023-12-28 17:16:14","title":"Probabilistic Programming with Exact Conditions","abstract":"We spell out the paradigm of exact conditioning as an intuitive and powerful way of conditioning on observations in probabilistic programs. This is contrasted with likelihood-based scoring known from languages such as Stan. We study exact conditioning in the cases of discrete and Gaussian probability, presenting prototypical languages for each case and giving semantics to them. We make use of categorical probability (namely Markov and CD categories) to give a general account of exact conditioning which avoids limits and measure theory, instead focusing on restructuring dataflow and program equations. The correspondence between such categories and a class of programming languages is made precise by defining the internal language of a CD category.","sentences":["We spell out the paradigm of exact conditioning as an intuitive and powerful way of conditioning on observations in probabilistic programs.","This is contrasted with likelihood-based scoring known from languages such as Stan.","We study exact conditioning in the cases of discrete and Gaussian probability, presenting prototypical languages for each case and giving semantics to them.","We make use of categorical probability (namely Markov and CD categories) to give a general account of exact conditioning which avoids limits and measure theory, instead focusing on restructuring dataflow and program equations.","The correspondence between such categories and a class of programming languages is made precise by defining the internal language of a CD category."],"url":"http://arxiv.org/abs/2312.17141v1"}
{"created":"2023-12-28 17:14:28","title":"On Inapproximability of Reconfiguration Problems: PSPACE-Hardness and some Tight NP-Hardness Results","abstract":"The field of combinatorial reconfiguration studies search problems with a focus on transforming one feasible solution into another.   Recently, Ohsaka [STACS'23] put forth the Reconfiguration Inapproximability Hypothesis (RIH), which roughly asserts that there is some $\\varepsilon>0$ such that given as input a $k$-CSP instance (for some constant $k$) over some constant sized alphabet, and two satisfying assignments $\\psi_s$ and $\\psi_t$, it is PSPACE-hard to find a sequence of assignments starting from $\\psi_s$ and ending at $\\psi_t$ such that every assignment in the sequence satisfies at least $(1-\\varepsilon)$ fraction of the constraints and also that every assignment in the sequence is obtained by changing its immediately preceding assignment (in the sequence) on exactly one variable. Assuming RIH, many important reconfiguration problems have been shown to be PSPACE-hard to approximate by Ohsaka [STACS'23; SODA'24].   In this paper, we prove RIH, thus establishing the first (constant factor) PSPACE-hardness of approximation results for many reconfiguration problems, resolving an open question posed by Ito et al. [TCS'11]. Our proof uses known constructions of Probabilistically Checkable Proofs of Proximity (in a black-box manner) to create the gap.   We also prove that the aforementioned $k$-CSP Reconfiguration problem is NP-hard to approximate to within a factor of $1/2 + \\varepsilon$ (for any $\\varepsilon>0$) when $k=2$. We complement this with a $(1/2 - \\varepsilon)$-approximation polynomial time algorithm, which improves upon a $(1/4 - \\varepsilon)$-approximation algorithm of Ohsaka [2023] (again for any $\\varepsilon>0$).   Finally, we show that Set Cover Reconfiguration is NP-hard to approximate to within a factor of $2 - \\varepsilon$ for any constant $\\varepsilon > 0$, which matches the simple linear-time 2-approximation algorithm by Ito et al. [TCS'11].","sentences":["The field of combinatorial reconfiguration studies search problems with a focus on transforming one feasible solution into another.   ","Recently, Ohsaka [STACS'23] put forth the Reconfiguration Inapproximability Hypothesis (RIH), which roughly asserts that there is some $\\varepsilon>0$ such that given as input a $k$-CSP instance (for some constant $k$) over some constant sized alphabet, and two satisfying assignments $\\psi_s$ and $\\psi_t$, it is PSPACE-hard to find a sequence of assignments starting from $\\psi_s$ and ending at $\\psi_t$ such that every assignment in the sequence satisfies at least $(1-\\varepsilon)$ fraction of the constraints and also that every assignment in the sequence is obtained by changing its immediately preceding assignment (in the sequence) on exactly one variable.","Assuming RIH, many important reconfiguration problems have been shown to be PSPACE-hard to approximate by Ohsaka","[STACS'23; SODA'24].   ","In this paper, we prove RIH, thus establishing the first (constant factor) PSPACE-hardness of approximation results for many reconfiguration problems, resolving an open question posed by Ito et al.","[TCS'11].","Our proof uses known constructions of Probabilistically Checkable Proofs of Proximity (in a black-box manner) to create the gap.   ","We also prove that the aforementioned $k$-CSP Reconfiguration problem is NP-hard to approximate to within a factor of $1/2 + \\varepsilon$ (for any $\\varepsilon>0$) when $k=2$. We complement this with a $(1/2 - \\varepsilon)$-approximation polynomial time algorithm, which improves upon a $(1/4 - \\varepsilon)$-approximation algorithm of Ohsaka [2023] (again for any $\\varepsilon>0$).   ","Finally, we show that Set Cover Reconfiguration is NP-hard to approximate to within a factor of $2 - \\varepsilon$ for any constant $\\varepsilon > 0$, which matches the simple linear-time 2-approximation algorithm by Ito et al.","[TCS'11]."],"url":"http://arxiv.org/abs/2312.17140v1"}
{"created":"2023-12-28 17:10:31","title":"InsActor: Instruction-driven Physics-based Characters","abstract":"Generating animation of physics-based characters with intuitive control has long been a desirable task with numerous applications. However, generating physically simulated animations that reflect high-level human instructions remains a difficult problem due to the complexity of physical environments and the richness of human language. In this paper, we present InsActor, a principled generative framework that leverages recent advancements in diffusion-based human motion models to produce instruction-driven animations of physics-based characters. Our framework empowers InsActor to capture complex relationships between high-level human instructions and character motions by employing diffusion policies for flexibly conditioned motion planning. To overcome invalid states and infeasible state transitions in planned motions, InsActor discovers low-level skills and maps plans to latent skill sequences in a compact latent space. Extensive experiments demonstrate that InsActor achieves state-of-the-art results on various tasks, including instruction-driven motion generation and instruction-driven waypoint heading. Notably, the ability of InsActor to generate physically simulated animations using high-level human instructions makes it a valuable tool, particularly in executing long-horizon tasks with a rich set of instructions.","sentences":["Generating animation of physics-based characters with intuitive control has long been a desirable task with numerous applications.","However, generating physically simulated animations that reflect high-level human instructions remains a difficult problem due to the complexity of physical environments and the richness of human language.","In this paper, we present InsActor, a principled generative framework that leverages recent advancements in diffusion-based human motion models to produce instruction-driven animations of physics-based characters.","Our framework empowers InsActor to capture complex relationships between high-level human instructions and character motions by employing diffusion policies for flexibly conditioned motion planning.","To overcome invalid states and infeasible state transitions in planned motions, InsActor discovers low-level skills and maps plans to latent skill sequences in a compact latent space.","Extensive experiments demonstrate that InsActor achieves state-of-the-art results on various tasks, including instruction-driven motion generation and instruction-driven waypoint heading.","Notably, the ability of InsActor to generate physically simulated animations using high-level human instructions makes it a valuable tool, particularly in executing long-horizon tasks with a rich set of instructions."],"url":"http://arxiv.org/abs/2312.17135v1"}
{"created":"2023-12-28 17:08:11","title":"ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe","abstract":"We present ARTrackV2, which integrates two pivotal aspects of tracking: determining where to look (localization) and how to describe (appearance analysis) the target object across video frames. Building on the foundation of its predecessor, ARTrackV2 extends the concept by introducing a unified generative framework to \"read out\" object's trajectory and \"retell\" its appearance in an autoregressive manner. This approach fosters a time-continuous methodology that models the joint evolution of motion and visual features, guided by previous estimates. Furthermore, ARTrackV2 stands out for its efficiency and simplicity, obviating the less efficient intra-frame autoregression and hand-tuned parameters for appearance updates. Despite its simplicity, ARTrackV2 achieves state-of-the-art performance on prevailing benchmark datasets while demonstrating remarkable efficiency improvement. In particular, ARTrackV2 achieves AO score of 79.5\\% on GOT-10k, and AUC of 86.1\\% on TrackingNet while being $3.6 \\times$ faster than ARTrack. The code will be released.","sentences":["We present ARTrackV2, which integrates two pivotal aspects of tracking: determining where to look (localization) and how to describe (appearance analysis) the target object across video frames.","Building on the foundation of its predecessor, ARTrackV2 extends the concept by introducing a unified generative framework to \"read out\" object's trajectory and \"retell\" its appearance in an autoregressive manner.","This approach fosters a time-continuous methodology that models the joint evolution of motion and visual features, guided by previous estimates.","Furthermore, ARTrackV2 stands out for its efficiency and simplicity, obviating the less efficient intra-frame autoregression and hand-tuned parameters for appearance updates.","Despite its simplicity, ARTrackV2 achieves state-of-the-art performance on prevailing benchmark datasets while demonstrating remarkable efficiency improvement.","In particular, ARTrackV2 achieves AO score of 79.5\\% on GOT-10k, and AUC of 86.1\\% on TrackingNet while being $3.6 \\times$ faster than ARTrack.","The code will be released."],"url":"http://arxiv.org/abs/2312.17133v1"}
{"created":"2023-12-28 17:04:50","title":"Probabilistic programming interfaces for random graphs: Markov categories, graphons, and nominal sets","abstract":"We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.   We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi graphons. In this way, we build new models of graph probabilistic programming from graphons.","sentences":["We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics.","We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.   ","We provide three constructions for showing that every graphon arises from an equational theory.","The first is an abstract construction, using Markov categories and monoidal indeterminates.","The second and third are more concrete.","The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons.","The third is in terms of probability monads on the nominal sets of Gabbay and Pitts.","Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi graphons.","In this way, we build new models of graph probabilistic programming from graphons."],"url":"http://arxiv.org/abs/2312.17127v1"}
{"created":"2023-12-28 16:59:06","title":"Large Language Model for Causal Decision Making","abstract":"Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics. However, their capability to inference based on user-specified structured data and knowledge in corpus-rare concepts like causal decision-making is still limited. In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset. Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation. With three case studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers. Numerical studies also reveal that it has a remarkable ability to identify the correct causal task given a query.","sentences":["Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics.","However, their capability to inference based on user-specified structured data and knowledge in corpus-rare concepts like causal decision-making is still limited.","In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset.","Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation.","With three case studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers.","Numerical studies also reveal that it has a remarkable ability to identify the correct causal task given a query."],"url":"http://arxiv.org/abs/2312.17122v1"}
{"created":"2023-12-28 16:55:40","title":"Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math","abstract":"High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce \\textsc{MathPile}, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of ``\\emph{less is more}'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates. We hope our \\textsc{MathPile} can help to enhance the mathematical reasoning abilities of language models. We plan to open-source different versions of \\mathpile with the scripts used for processing, to facilitate future developments in this field.","sentences":["High-quality, large-scale corpora are the cornerstone of building foundation models.","In this work, we introduce \\textsc{MathPile}, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens.","Throughout its creation, we adhered to the principle of ``\\emph{less is more}'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase.","Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus.","Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates.","We hope our \\textsc{MathPile} can help to enhance the mathematical reasoning abilities of language models.","We plan to open-source different versions of \\mathpile with the scripts used for processing, to facilitate future developments in this field."],"url":"http://arxiv.org/abs/2312.17120v1"}
{"created":"2023-12-28 16:54:53","title":"Fully Sparse 3D Panoptic Occupancy Prediction","abstract":"Occupancy prediction plays a pivotal role in the realm of autonomous driving. Previous methods typically constructs a dense 3D volume, neglecting the inherent sparsity of the scene, which results in a high computational cost. Furthermore, these methods are limited to semantic occupancy and fail to differentiate between distinct instances. To exploit the sparsity property and ensure instance-awareness, we introduce a novel fully sparse panoptic occupancy network, termed SparseOcc. SparseOcc initially reconstructs a sparse 3D representation from visual inputs. Subsequently, it employs sparse instance queries to predict each object instance from the sparse 3D representation. These instance queries interact with 2D features via mask-guided sparse sampling, thereby circumventing the need for costly dense features or global attention. Additionally, we have established the first-ever vision-centric panoptic occupancy benchmark. SparseOcc demonstrates its efficacy on the Occ3D-nus dataset by achieving a mean Intersection over Union (mIoU) of 26.0, while maintaining a real-time inference speed of 25.4 FPS. By incorporating temporal modeling from the preceding 8 frames, SparseOcc further improves its performance, achieving 30.9 mIoU without whistles and bells. Code will be made available.","sentences":["Occupancy prediction plays a pivotal role in the realm of autonomous driving.","Previous methods typically constructs a dense 3D volume, neglecting the inherent sparsity of the scene, which results in a high computational cost.","Furthermore, these methods are limited to semantic occupancy and fail to differentiate between distinct instances.","To exploit the sparsity property and ensure instance-awareness, we introduce a novel fully sparse panoptic occupancy network, termed SparseOcc.","SparseOcc initially reconstructs a sparse 3D representation from visual inputs.","Subsequently, it employs sparse instance queries to predict each object instance from the sparse 3D representation.","These instance queries interact with 2D features via mask-guided sparse sampling, thereby circumventing the need for costly dense features or global attention.","Additionally, we have established the first-ever vision-centric panoptic occupancy benchmark.","SparseOcc demonstrates its efficacy on the Occ3D-nus dataset by achieving a mean Intersection over Union (mIoU) of 26.0, while maintaining a real-time inference speed of 25.4 FPS.","By incorporating temporal modeling from the preceding 8 frames, SparseOcc further improves its performance, achieving 30.9 mIoU without whistles and bells.","Code will be made available."],"url":"http://arxiv.org/abs/2312.17118v1"}
{"created":"2023-12-28 16:54:21","title":"Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos","abstract":"Temporal Sentence Grounding (TSG), which aims to localize moments from videos based on the given natural language queries, has attracted widespread attention. Existing works are mainly designed for short videos, failing to handle TSG in long videos, which poses two challenges: i) complicated contexts in long videos require temporal reasoning over longer moment sequences, and ii) multiple modalities including textual speech with rich information require special designs for content understanding in long videos. To tackle these challenges, in this work we propose a Grounding-Prompter method, which is capable of conducting TSG in long videos through prompting LLM with multimodal information. In detail, we first transform the TSG task and its multimodal inputs including speech and visual, into compressed task textualization. Furthermore, to enhance temporal reasoning under complicated contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains three folds: i) we design a novel Multiscale Denoising Chain-of-Thought (CoT) to combine global and local semantics with noise filtering step by step, ii) we set up validity principles capable of constraining LLM to generate reasonable predictions following specific formats, and iii) we introduce one-shot In-Context-Learning (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding. Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter method, revealing the benefits of prompting LLM with multimodal information for TSG in long videos.","sentences":["Temporal Sentence Grounding (TSG), which aims to localize moments from videos based on the given natural language queries, has attracted widespread attention.","Existing works are mainly designed for short videos, failing to handle TSG in long videos, which poses two challenges: i) complicated contexts in long videos require temporal reasoning over longer moment sequences, and ii) multiple modalities including textual speech with rich information require special designs for content understanding in long videos.","To tackle these challenges, in this work we propose a Grounding-Prompter method, which is capable of conducting TSG in long videos through prompting LLM with multimodal information.","In detail, we first transform the TSG task and its multimodal inputs including speech and visual, into compressed task textualization.","Furthermore, to enhance temporal reasoning under complicated contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains three folds: i) we design a novel Multiscale Denoising Chain-of-Thought (CoT) to combine global and local semantics with noise filtering step by step, ii) we set up validity principles capable of constraining LLM to generate reasonable predictions following specific formats, and iii) we introduce one-shot In-Context-Learning (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding.","Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter method, revealing the benefits of prompting LLM with multimodal information for TSG in long videos."],"url":"http://arxiv.org/abs/2312.17117v1"}
{"created":"2023-12-28 16:53:23","title":"Generalizable Visual Reinforcement Learning with Segment Anything Model","abstract":"Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/","sentences":["Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL).","While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged.","In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents.","We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly.","Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations.","Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods.","Video and code:","https://yanjieze.com/SAM-G/"],"url":"http://arxiv.org/abs/2312.17116v1"}
{"created":"2023-12-28 16:51:11","title":"How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation","abstract":"Human behavior simulation of AI agents necessitates the agents to possess a quality of believability, which is crucial as it facilitates users in establishing trust toward the agents and streamlines the fulfillment of the agents' goal. While recent advancements in Large Language Model (LLM) based agents have improved human behavior simulation, challenges inherent to LLMs (e.g., long context modeling) can undermine their believability. Consequently, evaluating AI agent believability becomes imperative. Unfortunately, prior research often neglects the negative impacts of LLM deficiencies. To address these gaps, we introduce two metrics for assessing LLM-based agent believability: consistency, and robustness, together with a benchmark, SimulateBench, with which, we evaluate the consistency and robustness of agents implemented with popular LLMs. We find that agents (i) struggle to accurately depict character information when presented with lengthy profile inputs; (ii) exhibit vulnerability to profile perturbations; and (iii) are significantly affected by certain key factors that impact their overall believability. Code and SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.","sentences":["Human behavior simulation of AI agents necessitates the agents to possess a quality of believability, which is crucial as it facilitates users in establishing trust toward the agents and streamlines the fulfillment of the agents' goal.","While recent advancements in Large Language Model (LLM) based agents have improved human behavior simulation, challenges inherent to LLMs (e.g., long context modeling) can undermine their believability.","Consequently, evaluating AI agent believability becomes imperative.","Unfortunately, prior research often neglects the negative impacts of LLM deficiencies.","To address these gaps, we introduce two metrics for assessing LLM-based agent believability: consistency, and robustness, together with a benchmark, SimulateBench, with which, we evaluate the consistency and robustness of agents implemented with popular LLMs.","We find that agents (i) struggle to accurately depict character information when presented with lengthy profile inputs; (ii) exhibit vulnerability to profile perturbations; and (iii) are significantly affected by certain key factors that impact their overall believability.","Code and SimulateBench are public at https://github.com/GAIR-NLP/GPTMan."],"url":"http://arxiv.org/abs/2312.17115v1"}
{"created":"2023-12-28 16:39:25","title":"Kirchhoff-Law Johnson Noise Meets Web 3.0: A Statistical Physical Method of Random Key Generation for Decentralized Identity Protocols","abstract":"This paper presents a statistical physical generation of random keys for a decentralized identity ecosystem that uses Web 3.0 protocols. Web 3.0 is driven by secure keys, typically represented in hexadecimal, that are pseudo-randomly generated by an initialization vector and complex computational algorithms. We demonstrate that the statistical physical Kirchhoff-law-Johnson-noise (KLJN) scheme eliminates the additional computational power by naturally generating truly random binary keys to drive the creation of decentralized identifiers (DIDs) that are appended to an Ethereum blockchain.","sentences":["This paper presents a statistical physical generation of random keys for a decentralized identity ecosystem that uses Web 3.0 protocols.","Web 3.0 is driven by secure keys, typically represented in hexadecimal, that are pseudo-randomly generated by an initialization vector and complex computational algorithms.","We demonstrate that the statistical physical Kirchhoff-law-Johnson-noise (KLJN) scheme eliminates the additional computational power by naturally generating truly random binary keys to drive the creation of decentralized identifiers (DIDs) that are appended to an Ethereum blockchain."],"url":"http://arxiv.org/abs/2312.17113v1"}
{"created":"2023-12-28 16:35:33","title":"Toward Semantic Scene Understanding for Fine-Grained 3D Modeling of Plants","abstract":"Agricultural robotics is an active research area due to global population growth and expectations of food and labor shortages. Robots can potentially help with tasks such as pruning, harvesting, phenotyping, and plant modeling. However, agricultural automation is hampered by the difficulty in creating high resolution 3D semantic maps in the field that would allow for safe manipulation and navigation. In this paper, we build toward solutions for this issue and showcase how the use of semantics and environmental priors can help in constructing accurate 3D maps for the target application of sorghum. Specifically, we 1) use sorghum seeds as semantic landmarks to build a visual Simultaneous Localization and Mapping (SLAM) system that enables us to map 78\\\\% of a sorghum range on average, compared to 38% with ORB-SLAM2; and 2) use seeds as semantic features to improve 3D reconstruction of a full sorghum panicle from images taken by a robotic in-hand camera.","sentences":["Agricultural robotics is an active research area due to global population growth and expectations of food and labor shortages.","Robots can potentially help with tasks such as pruning, harvesting, phenotyping, and plant modeling.","However, agricultural automation is hampered by the difficulty in creating high resolution 3D semantic maps in the field that would allow for safe manipulation and navigation.","In this paper, we build toward solutions for this issue and showcase how the use of semantics and environmental priors can help in constructing accurate 3D maps for the target application of sorghum.","Specifically, we 1) use sorghum seeds as semantic landmarks to build a visual Simultaneous Localization and Mapping (SLAM) system that enables us to map 78\\\\% of a sorghum range on average, compared to 38% with ORB-SLAM2; and 2) use seeds as semantic features to improve 3D reconstruction of a full sorghum panicle from images taken by a robotic in-hand camera."],"url":"http://arxiv.org/abs/2312.17110v1"}
{"created":"2023-12-28 16:33:32","title":"MIVC: Multiple Instance Visual Component for Visual-Language Models","abstract":"Vision-language models have been widely explored across a wide range of tasks and achieve satisfactory performance. However, it's under-explored how to consolidate entity understanding through a varying number of images and to align it with the pre-trained language models for generative tasks. In this paper, we propose MIVC, a general multiple instance visual component to bridge the gap between various image inputs with off-the-shelf vision-language models by aggregating visual representations in a permutation-invariant fashion through a neural network. We show that MIVC could be plugged into the visual-language models to improve the model performance consistently on visual question answering, classification and captioning tasks on a public available e-commerce dataset with multiple images per product. Furthermore, we show that the component provides insight into the contribution of each image to the downstream tasks.","sentences":["Vision-language models have been widely explored across a wide range of tasks and achieve satisfactory performance.","However, it's under-explored how to consolidate entity understanding through a varying number of images and to align it with the pre-trained language models for generative tasks.","In this paper, we propose MIVC, a general multiple instance visual component to bridge the gap between various image inputs with off-the-shelf vision-language models by aggregating visual representations in a permutation-invariant fashion through a neural network.","We show that MIVC could be plugged into the visual-language models to improve the model performance consistently on visual question answering, classification and captioning tasks on a public available e-commerce dataset with multiple images per product.","Furthermore, we show that the component provides insight into the contribution of each image to the downstream tasks."],"url":"http://arxiv.org/abs/2312.17109v1"}
{"created":"2023-12-28 16:30:48","title":"The Intelligence College in Europe (ICE): An Effort to Create a European Intelligence Community","abstract":"In fulfilling the European security commitment, the actors of the so-called \"Intelligence Community\" play a central role. They provide political and military decision-makers with important analyses and information. The Intelligence College in Europe (ICE) is the first entity to offer professional intelligence training as well as postgraduate level academic education in intelligence and security studies at a pan-European level. In developing its postgraduate provision, ICE has benefited from the experience of the German Master of Intelligence and Security Studies (MISS), which is a joint effort of the University of the Bundeswehr Munich and the Department of Intelligence at the Federal University of Administrative Sciences in Berlin. As a main contribution of this paper, the module Counterterrorism (adapted from the MISS) is examined in more detail as a case study of how postgraduate modules can be modified to speak to a pan-European audience of intelligence professionals.","sentences":["In fulfilling the European security commitment, the actors of the so-called \"Intelligence Community\" play a central role.","They provide political and military decision-makers with important analyses and information.","The Intelligence College in Europe (ICE) is the first entity to offer professional intelligence training as well as postgraduate level academic education in intelligence and security studies at a pan-European level.","In developing its postgraduate provision, ICE has benefited from the experience of the German Master of Intelligence and Security Studies (MISS), which is a joint effort of the University of the Bundeswehr Munich and the Department of Intelligence at the Federal University of Administrative Sciences in Berlin.","As a main contribution of this paper, the module Counterterrorism (adapted from the MISS) is examined in more detail as a case study of how postgraduate modules can be modified to speak to a pan-European audience of intelligence professionals."],"url":"http://arxiv.org/abs/2312.17107v1"}
{"created":"2023-12-28 16:30:05","title":"Geometry-Biased Transformer for Robust Multi-View 3D Human Pose Reconstruction","abstract":"We address the challenges in estimating 3D human poses from multiple views under occlusion and with limited overlapping views. We approach multi-view, single-person 3D human pose reconstruction as a regression problem and propose a novel encoder-decoder Transformer architecture to estimate 3D poses from multi-view 2D pose sequences. The encoder refines 2D skeleton joints detected across different views and times, fusing multi-view and temporal information through global self-attention. We enhance the encoder by incorporating a geometry-biased attention mechanism, effectively leveraging geometric relationships between views. Additionally, we use detection scores provided by the 2D pose detector to further guide the encoder's attention based on the reliability of the 2D detections. The decoder subsequently regresses the 3D pose sequence from these refined tokens, using pre-defined queries for each joint. To enhance the generalization of our method to unseen scenes and improve resilience to missing joints, we implement strategies including scene centering, synthetic views, and token dropout. We conduct extensive experiments on three benchmark public datasets, Human3.6M, CMU Panoptic and Occlusion-Persons. Our results demonstrate the efficacy of our approach, particularly in occluded scenes and when few views are available, which are traditionally challenging scenarios for triangulation-based methods.","sentences":["We address the challenges in estimating 3D human poses from multiple views under occlusion and with limited overlapping views.","We approach multi-view, single-person 3D human pose reconstruction as a regression problem and propose a novel encoder-decoder Transformer architecture to estimate 3D poses from multi-view 2D pose sequences.","The encoder refines 2D skeleton joints detected across different views and times, fusing multi-view and temporal information through global self-attention.","We enhance the encoder by incorporating a geometry-biased attention mechanism, effectively leveraging geometric relationships between views.","Additionally, we use detection scores provided by the 2D pose detector to further guide the encoder's attention based on the reliability of the 2D detections.","The decoder subsequently regresses the 3D pose sequence from these refined tokens, using pre-defined queries for each joint.","To enhance the generalization of our method to unseen scenes and improve resilience to missing joints, we implement strategies including scene centering, synthetic views, and token dropout.","We conduct extensive experiments on three benchmark public datasets, Human3.6M, CMU Panoptic and Occlusion-Persons.","Our results demonstrate the efficacy of our approach, particularly in occluded scenes and when few views are available, which are traditionally challenging scenarios for triangulation-based methods."],"url":"http://arxiv.org/abs/2312.17106v1"}
{"created":"2023-12-28 16:23:58","title":"TSPP: A Unified Benchmarking Tool for Time-series Forecasting","abstract":"Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many tasks, such as fraud detection and recommender systems. Albeit, there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications or are limited in their application domain. This work tackles this shortcoming by proposing a scalable synthetic graph generation tool to scale the datasets to production-size graphs with trillions of edges and billions of nodes. The tool learns a series of parametric models from proprietary datasets that can be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications. We demonstrate the generalizability of the framework across a series of datasets, mimicking structural and feature distributions as well as the ability to scale them across varying sizes demonstrating their usefulness for benchmarking and model development.","sentences":["Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many tasks, such as fraud detection and recommender systems.","Albeit, there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications or are limited in their application domain.","This work tackles this shortcoming by proposing a scalable synthetic graph generation tool to scale the datasets to production-size graphs with trillions of edges and billions of nodes.","The tool learns a series of parametric models from proprietary datasets that can be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications.","We demonstrate the generalizability of the framework across a series of datasets, mimicking structural and feature distributions as well as the ability to scale them across varying sizes demonstrating their usefulness for benchmarking and model development."],"url":"http://arxiv.org/abs/2312.17100v1"}
{"created":"2023-12-28 16:20:17","title":"Tighter List-Size Bounds for List-Decoding and Recovery of Folded Reed-Solomon and Multiplicity Codes","abstract":"Folded Reed-Solomon (FRS) and univariate multiplicity codes are prominent polynomial codes over finite fields, renowned for achieving list decoding capacity. These codes have found a wide range of applications beyond the traditional scope of coding theory. In this paper, we introduce improved bounds on the list size for list decoding of these codes, achieved through a more streamlined proof method. Additionally, we refine an existing randomized algorithm to output the codewords on the list, enhancing its success probability and reducing its running time. Lastly, we establish list-size bounds for a fixed decoding parameter. Notably, our results demonstrate that FRS codes asymptotically attain the generalized Singleton bound for a list of size $2$ over a relatively small alphabet, marking the first explicit instance of a code with this property.","sentences":["Folded Reed-Solomon (FRS) and univariate multiplicity codes are prominent polynomial codes over finite fields, renowned for achieving list decoding capacity.","These codes have found a wide range of applications beyond the traditional scope of coding theory.","In this paper, we introduce improved bounds on the list size for list decoding of these codes, achieved through a more streamlined proof method.","Additionally, we refine an existing randomized algorithm to output the codewords on the list, enhancing its success probability and reducing its running time.","Lastly, we establish list-size bounds for a fixed decoding parameter.","Notably, our results demonstrate that FRS codes asymptotically attain the generalized Singleton bound for a list of size $2$ over a relatively small alphabet, marking the first explicit instance of a code with this property."],"url":"http://arxiv.org/abs/2312.17097v1"}
{"created":"2023-12-28 16:10:25","title":"Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels","abstract":"The explosion of visual content available online underscores the requirement for an accurate machine assessor to robustly evaluate scores across diverse types of visual contents. While recent studies have demonstrated the exceptional potentials of large multi-modality models (LMMs) on a wide range of related fields, in this work, we explore how to teach them for visual rating aligned with human opinions. Observing that human raters only learn and judge discrete text-defined levels in subjective studies, we propose to emulate this subjective process and teach LMMs with text-defined rating levels instead of scores. The proposed Q-Align achieves state-of-the-art performance on image quality assessment (IQA), image aesthetic assessment (IAA), as well as video quality assessment (VQA) tasks under the original LMM structure. With the syllabus, we further unify the three tasks into one model, termed the OneAlign. In our experiments, we demonstrate the advantage of the discrete-level-based syllabus over direct-score-based variants for LMMs. Our code and the pre-trained weights are released at https://github.com/Q-Future/Q-Align.","sentences":["The explosion of visual content available online underscores the requirement for an accurate machine assessor to robustly evaluate scores across diverse types of visual contents.","While recent studies have demonstrated the exceptional potentials of large multi-modality models (LMMs) on a wide range of related fields, in this work, we explore how to teach them for visual rating aligned with human opinions.","Observing that human raters only learn and judge discrete text-defined levels in subjective studies, we propose to emulate this subjective process and teach LMMs with text-defined rating levels instead of scores.","The proposed Q-Align achieves state-of-the-art performance on image quality assessment (IQA), image aesthetic assessment (IAA), as well as video quality assessment (VQA) tasks under the original LMM structure.","With the syllabus, we further unify the three tasks into one model, termed the OneAlign.","In our experiments, we demonstrate the advantage of the discrete-level-based syllabus over direct-score-based variants for LMMs.","Our code and the pre-trained weights are released at https://github.com/Q-Future/Q-Align."],"url":"http://arxiv.org/abs/2312.17090v1"}
{"created":"2023-12-28 15:52:05","title":"When Metaverses Meet Vehicle Road Cooperation: Multi-Agent DRL-Based Stackelberg Game for Vehicular Twins Migration","abstract":"Vehicular Metaverses represent emerging paradigms arising from the convergence of vehicle road cooperation, Metaverse, and augmented intelligence of things. Users engaging with Vehicular Metaverses (VMUs) gain entry by consistently updating their Vehicular Twins (VTs), which are deployed on RoadSide Units (RSUs) in proximity. The constrained RSU coverage and the consistently moving vehicles necessitate the continuous migration of VTs between RSUs through vehicle road cooperation, ensuring uninterrupted immersion services for VMUs. Nevertheless, the VT migration process faces challenges in obtaining adequate bandwidth resources from RSUs for timely migration, posing a resource trading problem among RSUs. In this paper, we tackle this challenge by formulating a game-theoretic incentive mechanism with multi-leader multi-follower, incorporating insights from social-awareness and queueing theory to optimize VT migration. To validate the existence and uniqueness of the Stackelberg Equilibrium, we apply the backward induction method. Theoretical solutions for this equilibrium are then obtained through the Alternating Direction Method of Multipliers (ADMM) algorithm. Moreover, owing to incomplete information caused by the requirements for privacy protection, we proposed a multi-agent deep reinforcement learning algorithm named MALPPO. MALPPO facilitates learning the Stackelberg Equilibrium without requiring private information from others, relying solely on past experiences. Comprehensive experimental results demonstrate that our MALPPO-based incentive mechanism outperforms baseline approaches significantly, showcasing rapid convergence and achieving the highest reward.","sentences":["Vehicular Metaverses represent emerging paradigms arising from the convergence of vehicle road cooperation, Metaverse, and augmented intelligence of things.","Users engaging with Vehicular Metaverses (VMUs) gain entry by consistently updating their Vehicular Twins (VTs), which are deployed on RoadSide Units (RSUs) in proximity.","The constrained RSU coverage and the consistently moving vehicles necessitate the continuous migration of VTs between RSUs through vehicle road cooperation, ensuring uninterrupted immersion services for VMUs.","Nevertheless, the VT migration process faces challenges in obtaining adequate bandwidth resources from RSUs for timely migration, posing a resource trading problem among RSUs.","In this paper, we tackle this challenge by formulating a game-theoretic incentive mechanism with multi-leader multi-follower, incorporating insights from social-awareness and queueing theory to optimize VT migration.","To validate the existence and uniqueness of the Stackelberg Equilibrium, we apply the backward induction method.","Theoretical solutions for this equilibrium are then obtained through the Alternating Direction Method of Multipliers (ADMM) algorithm.","Moreover, owing to incomplete information caused by the requirements for privacy protection, we proposed a multi-agent deep reinforcement learning algorithm named MALPPO.","MALPPO facilitates learning the Stackelberg Equilibrium without requiring private information from others, relying solely on past experiences.","Comprehensive experimental results demonstrate that our MALPPO-based incentive mechanism outperforms baseline approaches significantly, showcasing rapid convergence and achieving the highest reward."],"url":"http://arxiv.org/abs/2312.17081v1"}
{"created":"2023-12-28 15:49:43","title":"Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs","abstract":"In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance ten times more accurate than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering fundamental deficiencies in their training and evaluation approaches. This paper not only advocates for a paradigm shift in the assessment of LLMs but also contributes to the ongoing discourse on the trajectory towards Artificial General Intelligence (AGI). By promoting the adoption of meta-reasoning evaluation methods similar to ours, we aim to facilitate a more accurate assessment of the true cognitive abilities of LLMs.","sentences":["In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning.","This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents.","Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models.","For example, in our benchmark, GPT-4 demonstrates a performance ten times more accurate than GPT3-5.","The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities.","Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering fundamental deficiencies in their training and evaluation approaches.","This paper not only advocates for a paradigm shift in the assessment of LLMs but also contributes to the ongoing discourse on the trajectory towards Artificial General Intelligence (AGI).","By promoting the adoption of meta-reasoning evaluation methods similar to ours, we aim to facilitate a more accurate assessment of the true cognitive abilities of LLMs."],"url":"http://arxiv.org/abs/2312.17080v1"}
{"created":"2023-12-28 15:41:17","title":"Minimally-intrusive Navigation in Dense Crowds with Integrated Macro and Micro-level Dynamics","abstract":"In mobile robot navigation, despite advancements, the generation of optimal paths often disrupts pedestrian areas. To tackle this, we propose three key contributions to improve human-robot coexistence in shared spaces. Firstly, we have established a comprehensive framework to understand disturbances at individual and flow levels. Our framework provides specialized computational strategies for in-depth studies of human-robot interactions from both micro and macro perspectives. By employing novel penalty terms, namely Flow Disturbance Penalty (FDP) and Individual Disturbance Penalty (IDP), our framework facilitates a more nuanced assessment and analysis of the robot navigation's impact on pedestrians. Secondly, we introduce an innovative sampling-based navigation system that adeptly integrates a suite of safety measures with the predictability of robotic movements. This system not only accounts for traditional factors such as trajectory length and travel time but also actively incorporates pedestrian awareness. Our navigation system aims to minimize disturbances and promote harmonious coexistence by considering safety protocols, trajectory clarity, and pedestrian engagement. Lastly, we validate our algorithm's effectiveness and real-time performance through simulations and real-world tests, demonstrating its ability to navigate with minimal pedestrian disturbance in various environments.","sentences":["In mobile robot navigation, despite advancements, the generation of optimal paths often disrupts pedestrian areas.","To tackle this, we propose three key contributions to improve human-robot coexistence in shared spaces.","Firstly, we have established a comprehensive framework to understand disturbances at individual and flow levels.","Our framework provides specialized computational strategies for in-depth studies of human-robot interactions from both micro and macro perspectives.","By employing novel penalty terms, namely Flow Disturbance Penalty (FDP) and Individual Disturbance Penalty (IDP), our framework facilitates a more nuanced assessment and analysis of the robot navigation's impact on pedestrians.","Secondly, we introduce an innovative sampling-based navigation system that adeptly integrates a suite of safety measures with the predictability of robotic movements.","This system not only accounts for traditional factors such as trajectory length and travel time but also actively incorporates pedestrian awareness.","Our navigation system aims to minimize disturbances and promote harmonious coexistence by considering safety protocols, trajectory clarity, and pedestrian engagement.","Lastly, we validate our algorithm's effectiveness and real-time performance through simulations and real-world tests, demonstrating its ability to navigate with minimal pedestrian disturbance in various environments."],"url":"http://arxiv.org/abs/2312.17076v1"}
{"created":"2023-12-28 15:34:54","title":"An Adaptive Framework of Geographical Group-Specific Network on O2O Recommendation","abstract":"Online to offline recommendation strongly correlates with the user and service's spatiotemporal information, therefore calling for a higher degree of model personalization. The traditional methodology is based on a uniform model structure trained by collected centralized data, which is unlikely to capture all user patterns over different geographical areas or time periods. To tackle this challenge, we propose a geographical group-specific modeling method called GeoGrouse, which simultaneously studies the common knowledge as well as group-specific knowledge of user preferences. An automatic grouping paradigm is employed and verified based on users' geographical grouping indicators. Offline and online experiments are conducted to verify the effectiveness of our approach, and substantial business improvement is achieved.","sentences":["Online to offline recommendation strongly correlates with the user and service's spatiotemporal information, therefore calling for a higher degree of model personalization.","The traditional methodology is based on a uniform model structure trained by collected centralized data, which is unlikely to capture all user patterns over different geographical areas or time periods.","To tackle this challenge, we propose a geographical group-specific modeling method called GeoGrouse, which simultaneously studies the common knowledge as well as group-specific knowledge of user preferences.","An automatic grouping paradigm is employed and verified based on users' geographical grouping indicators.","Offline and online experiments are conducted to verify the effectiveness of our approach, and substantial business improvement is achieved."],"url":"http://arxiv.org/abs/2312.17072v1"}
{"created":"2023-12-28 15:33:16","title":"SCTNet: Single-Branch CNN with Transformer Semantic Information for Real-Time Segmentation","abstract":"Recent real-time semantic segmentation methods usually adopt an additional semantic branch to pursue rich long-range context. However, the additional branch incurs undesirable computational overhead and slows inference speed. To eliminate this dilemma, we propose SCTNet, a single branch CNN with transformer semantic information for real-time segmentation. SCTNet enjoys the rich semantic representations of an inference-free semantic branch while retaining the high efficiency of lightweight single branch CNN. SCTNet utilizes a transformer as the training-only semantic branch considering its superb ability to extract long-range context. With the help of the proposed transformer-like CNN block CFBlock and the semantic information alignment module, SCTNet could capture the rich semantic information from the transformer branch in training. During the inference, only the single branch CNN needs to be deployed. We conduct extensive experiments on Cityscapes, ADE20K, and COCO-Stuff-10K, and the results show that our method achieves the new state-of-the-art performance. The code and model is available at https://github.com/xzz777/SCTNet","sentences":["Recent real-time semantic segmentation methods usually adopt an additional semantic branch to pursue rich long-range context.","However, the additional branch incurs undesirable computational overhead and slows inference speed.","To eliminate this dilemma, we propose SCTNet, a single branch CNN with transformer semantic information for real-time segmentation.","SCTNet enjoys the rich semantic representations of an inference-free semantic branch while retaining the high efficiency of lightweight single branch CNN.","SCTNet utilizes a transformer as the training-only semantic branch considering its superb ability to extract long-range context.","With the help of the proposed transformer-like CNN block CFBlock and the semantic information alignment module, SCTNet could capture the rich semantic information from the transformer branch in training.","During the inference, only the single branch CNN needs to be deployed.","We conduct extensive experiments on Cityscapes, ADE20K, and COCO-Stuff-10K, and the results show that our method achieves the new state-of-the-art performance.","The code and model is available at https://github.com/xzz777/SCTNet"],"url":"http://arxiv.org/abs/2312.17071v1"}
{"created":"2023-12-28 15:11:53","title":"On the optimality of Shapley mechanism under Sybil strategies","abstract":"In the realm of cost-sharing mechanisms, the vulnerability to Sybil strategies, where agents can create fake identities to manipulate outcomes, has not yet been studied. In this paper, we delve into the intricacies of different cost-sharing mechanisms proposed in the literature highlighting its non Sybil-resistance nature. Furthermore, we prove that under mild conditions, a Sybil-proof cost-sharing mechanism for public excludable goods is at least $(n/2+1)-$approximate. This finding reveals an actual exponential increase in the worst-case social cost in environments where agents are restricted from using Sybil strategies. We introduce the concept of \\textit{Sybil Welfare Invariant} mechanisms, where a mechanism maintains its worst-case welfare under Sybil-strategies for every set of prior beliefs with full support even when the mechanism is not Sybil-proof. Finally, we prove that the Shapley value mechanism for public excludable goods holds this property, and so deduce that the worst-case social cost of this mechanism is the $n$th harmonic number $\\mathcal H_n$ even under equilibrium of the game with Sybil strategies, matching the worst-case social cost bound for cost-sharing mechanisms. This finding carries important implications for decentralized autonomous organizations (DAOs), indicating that they are capable of funding public excludable goods efficiently, even when the total number of agents in the DAO is unknown.","sentences":["In the realm of cost-sharing mechanisms, the vulnerability to Sybil strategies, where agents can create fake identities to manipulate outcomes, has not yet been studied.","In this paper, we delve into the intricacies of different cost-sharing mechanisms proposed in the literature highlighting its non Sybil-resistance nature.","Furthermore, we prove that under mild conditions, a Sybil-proof cost-sharing mechanism for public excludable goods is at least $(n/2+1)-$approximate.","This finding reveals an actual exponential increase in the worst-case social cost in environments where agents are restricted from using Sybil strategies.","We introduce the concept of \\textit{Sybil Welfare Invariant} mechanisms, where a mechanism maintains its worst-case welfare under Sybil-strategies for every set of prior beliefs with full support even when the mechanism is not Sybil-proof.","Finally, we prove that the Shapley value mechanism for public excludable goods holds this property, and so deduce that the worst-case social cost of this mechanism is the $n$th harmonic number $\\mathcal H_n$ even under equilibrium of the game with Sybil strategies, matching the worst-case social cost bound for cost-sharing mechanisms.","This finding carries important implications for decentralized autonomous organizations (DAOs), indicating that they are capable of funding public excludable goods efficiently, even when the total number of agents in the DAO is unknown."],"url":"http://arxiv.org/abs/2312.17058v1"}
{"created":"2023-12-28 15:02:03","title":"Improving In-context Learning via Bidirectional Alignment","abstract":"Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL). Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges. In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller models with that of larger models. Existing methods either train smaller models on the generated outputs of larger models or to imitate their token-level probability distributions. However, these distillation methods pay little to no attention to the input part, which also plays a crucial role in ICL. Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of smaller models. Specifically, we introduce the alignment of input preferences between smaller and larger models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution. With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks including language understanding, reasoning, and coding.","sentences":["Large language models (LLMs) have shown impressive few-shot generalization on many tasks via in-context learning (ICL).","Despite their success in showing such emergent abilities, the scale and complexity of larger models also lead to unprecedentedly high computational demands and deployment challenges.","In reaction, researchers explore transferring the powerful capabilities of larger models to more efficient and compact models by typically aligning the output of smaller models with that of larger models.","Existing methods either train smaller models on the generated outputs of larger models or to imitate their token-level probability distributions.","However, these distillation methods pay little to no attention to the input part, which also plays a crucial role in ICL.","Based on the finding that the performance of ICL is highly sensitive to the selection of demonstration examples, we propose Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for ICL examples to improve the ICL abilities of smaller models.","Specifically, we introduce the alignment of input preferences between smaller and larger models by incorporating a novel ranking loss, in addition to aligning the token-level output distribution.","With extensive experiments and analysis, we demonstrate that BiAlign can consistently outperform existing baselines on a variety of tasks including language understanding, reasoning, and coding."],"url":"http://arxiv.org/abs/2312.17055v1"}
{"created":"2023-12-28 14:53:32","title":"Multi-Attention Fusion Drowsy Driving Detection Model","abstract":"Drowsy driving represents a major contributor to traffic accidents, and the implementation of driver drowsy driving detection systems has been proven to significantly reduce the occurrence of such accidents. Despite the development of numerous drowsy driving detection algorithms, many of them impose specific prerequisites such as the availability of complete facial images, optimal lighting conditions, and the use of RGB images. In our study, we introduce a novel approach called the Multi-Attention Fusion Drowsy Driving Detection Model (MAF). MAF is aimed at significantly enhancing classification performance, especially in scenarios involving partial facial occlusion and low lighting conditions. It accomplishes this by capitalizing on the local feature extraction capabilities provided by multi-attention fusion, thereby enhancing the algorithm's overall robustness. To enhance our dataset, we collected real-world data that includes both occluded and unoccluded faces captured under nighttime and daytime lighting conditions. We conducted a comprehensive series of experiments using both publicly available datasets and our self-built data. The results of these experiments demonstrate that our proposed model achieves an impressive driver drowsiness detection accuracy of 96.8%.","sentences":["Drowsy driving represents a major contributor to traffic accidents, and the implementation of driver drowsy driving detection systems has been proven to significantly reduce the occurrence of such accidents.","Despite the development of numerous drowsy driving detection algorithms, many of them impose specific prerequisites such as the availability of complete facial images, optimal lighting conditions, and the use of RGB images.","In our study, we introduce a novel approach called the Multi-Attention Fusion Drowsy Driving Detection Model (MAF).","MAF is aimed at significantly enhancing classification performance, especially in scenarios involving partial facial occlusion and low lighting conditions.","It accomplishes this by capitalizing on the local feature extraction capabilities provided by multi-attention fusion, thereby enhancing the algorithm's overall robustness.","To enhance our dataset, we collected real-world data that includes both occluded and unoccluded faces captured under nighttime and daytime lighting conditions.","We conducted a comprehensive series of experiments using both publicly available datasets and our self-built data.","The results of these experiments demonstrate that our proposed model achieves an impressive driver drowsiness detection accuracy of 96.8%."],"url":"http://arxiv.org/abs/2312.17052v1"}
{"created":"2023-12-28 14:52:07","title":"FILP-3D: Enhancing 3D Few-shot Class-incremental Learning with Pre-trained Vision-Language Models","abstract":"Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophic forgetting issue when a model is incrementally trained on limited data. While the Contrastive Vision-Language Pre-Training (CLIP) model has been effective in addressing 2D few/zero-shot learning tasks, its direct application to 3D FSCIL faces limitations. These limitations arise from feature space misalignment and significant noise in real-world scanned 3D data. To address these challenges, we introduce two novel components: the Redundant Feature Eliminator (RFE) and the Spatial Noise Compensator (SNC). RFE aligns the feature spaces of input point clouds and their embeddings by performing a unique dimensionality reduction on the feature space of pre-trained models (PTMs), effectively eliminating redundant information without compromising semantic integrity. On the other hand, SNC is a graph-based 3D model designed to capture robust geometric information within point clouds, thereby augmenting the knowledge lost due to projection, particularly when processing real-world scanned data. Considering the imbalance in existing 3D datasets, we also propose new evaluation metrics that offer a more nuanced assessment of a 3D FSCIL model. Traditional accuracy metrics are proved to be biased; thus, our metrics focus on the model's proficiency in learning new classes while maintaining the balance between old and new classes. Experimental results on both established 3D FSCIL benchmarks and our dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods.","sentences":["Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophic forgetting issue when a model is incrementally trained on limited data.","While the Contrastive Vision-Language Pre-Training (CLIP) model has been effective in addressing 2D few/zero-shot learning tasks, its direct application to 3D FSCIL faces limitations.","These limitations arise from feature space misalignment and significant noise in real-world scanned 3D data.","To address these challenges, we introduce two novel components: the Redundant Feature Eliminator (RFE) and the Spatial Noise Compensator (SNC).","RFE aligns the feature spaces of input point clouds and their embeddings by performing a unique dimensionality reduction on the feature space of pre-trained models (PTMs), effectively eliminating redundant information without compromising semantic integrity.","On the other hand, SNC is a graph-based 3D model designed to capture robust geometric information within point clouds, thereby augmenting the knowledge lost due to projection, particularly when processing real-world scanned data.","Considering the imbalance in existing 3D datasets, we also propose new evaluation metrics that offer a more nuanced assessment of a 3D FSCIL model.","Traditional accuracy metrics are proved to be biased; thus, our metrics focus on the model's proficiency in learning new classes while maintaining the balance between old and new classes.","Experimental results on both established 3D FSCIL benchmarks and our dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2312.17051v1"}
{"created":"2023-12-28 14:51:45","title":"KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching","abstract":"Dual-lens super-resolution (SR) is a practical scenario for reference (Ref) based SR by utilizing the telephoto image (Ref) to assist the super-resolution of the low-resolution wide-angle image (LR input). Different from general RefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV) area. However, current dual-lens SR methods rarely utilize these specific characteristics and directly perform dense matching between the LR input and Ref. Due to the resolution gap between LR and Ref, the matching may miss the best-matched candidate and destroy the consistent structures in the overlapped FoV area. Different from them, we propose to first align the Ref with the center region (namely the overlapped FoV area) of the LR input by combining global warping and local warping to make the aligned Ref be sharp and consistent. Then, we formulate the aligned Ref and LR center as value-key pairs, and the corner region of the LR is formulated as queries. In this way, we propose a kernel-free matching strategy by matching between the LR-corner (query) and LR-center (key) regions, and the corresponding aligned Ref (value) can be warped to the corner region of the target. Our kernel-free matching strategy avoids the resolution gap between LR and Ref, which makes our network have better generalization ability. In addition, we construct a DuSR-Real dataset with (LR, Ref, HR) triples, where the LR and HR are well aligned. Experiments on three datasets demonstrate that our method outperforms the second-best method by a large margin. Our code and dataset are available at https://github.com/Craigie-Hill/KeDuSR.","sentences":["Dual-lens super-resolution (SR) is a practical scenario for reference (Ref) based SR by utilizing the telephoto image (Ref) to assist the super-resolution of the low-resolution wide-angle image (LR input).","Different from general RefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV) area.","However, current dual-lens SR methods rarely utilize these specific characteristics and directly perform dense matching between the LR input and Ref.","Due to the resolution gap between LR and Ref, the matching may miss the best-matched candidate and destroy the consistent structures in the overlapped FoV area.","Different from them, we propose to first align the Ref with the center region (namely the overlapped FoV area) of the LR input by combining global warping and local warping to make the aligned Ref be sharp and consistent.","Then, we formulate the aligned Ref and LR center as value-key pairs, and the corner region of the LR is formulated as queries.","In this way, we propose a kernel-free matching strategy by matching between the LR-corner (query) and LR-center (key) regions, and the corresponding aligned Ref (value) can be warped to the corner region of the target.","Our kernel-free matching strategy avoids the resolution gap between LR and Ref, which makes our network have better generalization ability.","In addition, we construct a DuSR-Real dataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.","Experiments on three datasets demonstrate that our method outperforms the second-best method by a large margin.","Our code and dataset are available at https://github.com/Craigie-Hill/KeDuSR."],"url":"http://arxiv.org/abs/2312.17050v1"}
