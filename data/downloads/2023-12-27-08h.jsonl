{"created":"2023-12-26 18:59:33","title":"Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4","abstract":"This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work provides a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.","sentences":["This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models.","Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts.","Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design.","We hope that this work provides a better guide for researchers working on the prompting of large language models.","Project page is available at https://github.com/VILA-Lab/ATLAS."],"url":"http://arxiv.org/abs/2312.16171v1"}
{"created":"2023-12-26 18:59:11","title":"EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI","abstract":"In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions. This necessitates the ability to fully understand 3D scenes given their first-person observations and contextualize them into language for interaction. However, traditional research focuses more on scene-level input and output setups from a global view. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding. It encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories. Building upon this database, we introduce a baseline framework named Embodied Perceptron. It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and in the wild. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.","sentences":["In the realm of computer vision and robotics, embodied agents are expected to explore their environment and carry out human instructions.","This necessitates the ability to fully understand 3D scenes given their first-person observations and contextualize them into language for interaction.","However, traditional research focuses more on scene-level input and output setups from a global view.","To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric 3D perception dataset and benchmark for holistic 3D scene understanding.","It encompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language prompts, 160k 3D-oriented boxes spanning over 760 categories, some of which partially align with LVIS, and dense semantic occupancy with 80 common categories.","Building upon this database, we introduce a baseline framework named Embodied Perceptron.","It is capable of processing an arbitrary number of multi-modal inputs and demonstrates remarkable 3D perception capabilities, both within the two series of benchmarks we set up, i.e., fundamental 3D perception tasks and language-grounded tasks, and in the wild.","Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan."],"url":"http://arxiv.org/abs/2312.16170v1"}
{"created":"2023-12-26 18:56:49","title":"Social-Transmotion: Promptable Human Trajectory Prediction","abstract":"Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space. To address this, we introduce Social-Transmotion, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses. This, in turn, augments trajectory data, leading to enhanced human trajectory prediction. Our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedestrians based on the available visual cues, whether they are poses, bounding boxes, or a combination thereof. By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data. We delve into the merits of using 2d versus 3d poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction. Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY. The code is publicly available: https://github.com/vita-epfl/social-transmotion","sentences":["Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems.","Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.","To address this, we introduce Social-Transmotion, a generic model that exploits the power of transformers to handle diverse and numerous visual cues, capturing the multi-modal nature of human behavior.","We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes or body poses.","This, in turn, augments trajectory data, leading to enhanced human trajectory prediction.","Our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between pedestrians based on the available visual cues, whether they are poses, bounding boxes, or a combination thereof.","By the masking technique, we ensure our model's effectiveness even when certain visual cues are unavailable, although performance is further boosted with the presence of comprehensive visual data.","We delve into the merits of using 2d versus 3d poses, and a limited set of poses.","Additionally, we investigate the spatial and temporal attention map to identify which keypoints and frames of poses are vital for optimizing human trajectory prediction.","Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY.","The code is publicly available: https://github.com/vita-epfl/social-transmotion"],"url":"http://arxiv.org/abs/2312.16168v1"}
{"created":"2023-12-26 18:51:40","title":"Age of Information in Gossip Networks: A Friendly Introduction and Literature Survey","abstract":"Gossiping is a communication mechanism, used for fast information dissemination in a network, where each node of the network randomly shares its information with the neighboring nodes. To characterize the notion of fastness in the context of gossip networks, age of information (AoI) is used as a timeliness metric. In this article, we summarize the recent works related to timely gossiping in a network. We start with the introduction of randomized gossip algorithms as an epidemic algorithm for database maintenance, and how the gossiping literature was later developed in the context of rumor spreading, message passing and distributed mean estimation. Then, we motivate the need for timely gossiping in applications such as source tracking and decentralized learning. We evaluate timeliness scaling of gossiping in various network topologies, such as, fully connected, ring, grid, generalized ring, hierarchical, and sparse asymmetric networks. We discuss age-aware gossiping and the higher order moments of the age process. We also consider different variations of gossiping in networks, such as, file slicing and network coding, reliable and unreliable sources, information mutation, different adversarial actions in gossiping, and energy harvesting sensors. Finally, we conclude this article with a few open problems and future directions in timely gossiping.","sentences":["Gossiping is a communication mechanism, used for fast information dissemination in a network, where each node of the network randomly shares its information with the neighboring nodes.","To characterize the notion of fastness in the context of gossip networks, age of information (AoI) is used as a timeliness metric.","In this article, we summarize the recent works related to timely gossiping in a network.","We start with the introduction of randomized gossip algorithms as an epidemic algorithm for database maintenance, and how the gossiping literature was later developed in the context of rumor spreading, message passing and distributed mean estimation.","Then, we motivate the need for timely gossiping in applications such as source tracking and decentralized learning.","We evaluate timeliness scaling of gossiping in various network topologies, such as, fully connected, ring, grid, generalized ring, hierarchical, and sparse asymmetric networks.","We discuss age-aware gossiping and the higher order moments of the age process.","We also consider different variations of gossiping in networks, such as, file slicing and network coding, reliable and unreliable sources, information mutation, different adversarial actions in gossiping, and energy harvesting sensors.","Finally, we conclude this article with a few open problems and future directions in timely gossiping."],"url":"http://arxiv.org/abs/2312.16163v1"}
{"created":"2023-12-26 18:38:54","title":"Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages","abstract":"Large language models (LLMs) have shown impressive zero-shot capabilities in various document reranking tasks. Despite their successful implementations, there is still a gap in existing literature on their effectiveness in low-resource languages. To address this gap, we investigate how LLMs function as rerankers in cross-lingual information retrieval (CLIR) systems for African languages. Our implementation covers English and four African languages (Hausa, Somali, Swahili, and Yoruba) and we examine cross-lingual reranking with queries in English and passages in the African languages. Additionally, we analyze and compare the effectiveness of monolingual reranking using both query and document translations. We also evaluate the effectiveness of LLMs when leveraging their own generated translations. To get a grasp of the effectiveness of multiple LLMs, our study focuses on the proprietary models RankGPT-4 and RankGPT-3.5, along with the open-source model, RankZephyr. While reranking remains most effective in English, our results reveal that cross-lingual reranking may be competitive with reranking in African languages depending on the multilingual capability of the LLM.","sentences":["Large language models (LLMs) have shown impressive zero-shot capabilities in various document reranking tasks.","Despite their successful implementations, there is still a gap in existing literature on their effectiveness in low-resource languages.","To address this gap, we investigate how LLMs function as rerankers in cross-lingual information retrieval (CLIR) systems for African languages.","Our implementation covers English and four African languages (Hausa, Somali, Swahili, and Yoruba) and we examine cross-lingual reranking with queries in English and passages in the African languages.","Additionally, we analyze and compare the effectiveness of monolingual reranking using both query and document translations.","We also evaluate the effectiveness of LLMs when leveraging their own generated translations.","To get a grasp of the effectiveness of multiple LLMs, our study focuses on the proprietary models RankGPT-4 and RankGPT-3.5, along with the open-source model, RankZephyr.","While reranking remains most effective in English, our results reveal that cross-lingual reranking may be competitive with reranking in African languages depending on the multilingual capability of the LLM."],"url":"http://arxiv.org/abs/2312.16159v1"}
{"created":"2023-12-26 18:36:01","title":"Association rule mining with earthquake data collected from Turkiye region","abstract":"Earthquakes are evaluated among the most destructive disasters for human beings, as also experienced for Turkiye region. Data science has the property of discovering hidden patterns in case a sufficient volume of data is supplied. Time dependency of events, specifically being defined by co-occurrence in a specific time window, may be handled as an associate rule mining task such as a market-basket analysis application. In this regard, we assumed each day's seismic activity as a single basket of events, leading to discovering the association patterns between these events. Consequently, this study presents the most prominent association rules for the earthquakes recorded in Turkiye region in the last 5 years, each year presented separately. Results indicate statistical inference with events recorded from regions of various distances, which could be further verified with geologic evidence from the field. As a result, we believe that the current study may form a statistical basis for the future works with the aid of machine learning algorithm performed for associate rule mining.","sentences":["Earthquakes are evaluated among the most destructive disasters for human beings, as also experienced for Turkiye region.","Data science has the property of discovering hidden patterns in case a sufficient volume of data is supplied.","Time dependency of events, specifically being defined by co-occurrence in a specific time window, may be handled as an associate rule mining task such as a market-basket analysis application.","In this regard, we assumed each day's seismic activity as a single basket of events, leading to discovering the association patterns between these events.","Consequently, this study presents the most prominent association rules for the earthquakes recorded in Turkiye region in the last 5 years, each year presented separately.","Results indicate statistical inference with events recorded from regions of various distances, which could be further verified with geologic evidence from the field.","As a result, we believe that the current study may form a statistical basis for the future works with the aid of machine learning algorithm performed for associate rule mining."],"url":"http://arxiv.org/abs/2312.16158v1"}
{"created":"2023-12-26 18:30:29","title":"From Text to Multimodal: A Comprehensive Survey of Adversarial Example Generation in Question Answering Systems","abstract":"Integrating adversarial machine learning with Question Answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robustness of these systems. This article aims to comprehensively review adversarial example-generation techniques in the QA field, including textual and multimodal contexts. We examine the techniques employed through systematic categorization, providing a comprehensive, structured review. Beginning with an overview of traditional QA models, we traverse the adversarial example generation by exploring rule-based perturbations and advanced generative models. We then extend our research to include multimodal QA systems, analyze them across various methods, and examine generative models, seq2seq architectures, and hybrid methodologies. Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the comprehensive literature on adversarial QA. Finally, the paper considers the future landscape of adversarial question generation, highlighting potential research directions that can advance textual and multimodal QA systems in the context of adversarial challenges.","sentences":["Integrating adversarial machine learning with Question Answering (QA) systems has emerged as a critical area for understanding the vulnerabilities and robustness of these systems.","This article aims to comprehensively review adversarial example-generation techniques in the QA field, including textual and multimodal contexts.","We examine the techniques employed through systematic categorization, providing a comprehensive, structured review.","Beginning with an overview of traditional QA models, we traverse the adversarial example generation by exploring rule-based perturbations and advanced generative models.","We then extend our research to include multimodal QA systems, analyze them across various methods, and examine generative models, seq2seq architectures, and hybrid methodologies.","Our research grows to different defense strategies, adversarial datasets, and evaluation metrics and illustrates the comprehensive literature on adversarial QA.","Finally, the paper considers the future landscape of adversarial question generation, highlighting potential research directions that can advance textual and multimodal QA systems in the context of adversarial challenges."],"url":"http://arxiv.org/abs/2312.16156v1"}
{"created":"2023-12-26 18:28:25","title":"The Clustered Orienteering Problem with Subgroups","abstract":"This paper introduces an extension to the Orienteering Problem (OP), called Clustered Orienteering Problem with Subgroups (COPS). In this variant, nodes are arranged into subgroups, and the subgroups are organized into clusters. A reward is associated with each subgroup and is gained only if all of its nodes are visited; however, at most one subgroup can be visited per cluster. The objective is to maximize the total collected reward while attaining a travel budget. We show that our new formulation has the ability to model and solve two previous well-known variants, the Clustered Orienteering Problem (COP) and the Set Orienteering Problem (SOP), in addition to other scenarios introduced here. An Integer Linear Programming (ILP) formulation and a Tabu Search-based heuristic are proposed to solve the problem. Experimental results indicate that the ILP method can yield optimal solutions at the cost of time, whereas the metaheuristic produces comparable solutions within a more reasonable computational cost.","sentences":["This paper introduces an extension to the Orienteering Problem (OP), called Clustered Orienteering Problem with Subgroups (COPS).","In this variant, nodes are arranged into subgroups, and the subgroups are organized into clusters.","A reward is associated with each subgroup and is gained only if all of its nodes are visited; however, at most one subgroup can be visited per cluster.","The objective is to maximize the total collected reward while attaining a travel budget.","We show that our new formulation has the ability to model and solve two previous well-known variants, the Clustered Orienteering Problem (COP) and the Set Orienteering Problem (SOP), in addition to other scenarios introduced here.","An Integer Linear Programming (ILP) formulation and a Tabu Search-based heuristic are proposed to solve the problem.","Experimental results indicate that the ILP method can yield optimal solutions at the cost of time, whereas the metaheuristic produces comparable solutions within a more reasonable computational cost."],"url":"http://arxiv.org/abs/2312.16154v1"}
{"created":"2023-12-26 18:20:48","title":"Large-scale Long-tailed Disease Diagnosis on Radiology Images","abstract":"In this study, we aim to investigate the problem of large-scale, large-vocabulary disease classification for radiologic images, which can be formulated as a multi-modal, multi-anatomy, multi-label, long-tailed classification. Our main contributions are three folds: (i), on dataset construction, we build up an academically accessible, large-scale diagnostic dataset that encompasses 5568 disorders linked with 930 unique ICD-10-CM codes, containing 39,026 cases (192,675 scans). (ii), on model design, we present a novel architecture that enables to process arbitrary number of input scans, from various imaging modalities, which is trained with knowledge enhancement to leverage the rich domain knowledge; (iii), on evaluation, we initialize a new benchmark for multi-modal multi-anatomy long-tailed diagnosis. Our method shows superior results on it. Additionally, our final model serves as a pre-trained model, and can be finetuned to benefit diagnosis on various external datasets.","sentences":["In this study, we aim to investigate the problem of large-scale, large-vocabulary disease classification for radiologic images, which can be formulated as a multi-modal, multi-anatomy, multi-label, long-tailed classification.","Our main contributions are three folds: (i), on dataset construction, we build up an academically accessible, large-scale diagnostic dataset that encompasses 5568 disorders linked with 930 unique ICD-10-CM codes, containing 39,026 cases (192,675 scans).","(ii), on model design, we present a novel architecture that enables to process arbitrary number of input scans, from various imaging modalities, which is trained with knowledge enhancement to leverage the rich domain knowledge; (iii), on evaluation, we initialize a new benchmark for multi-modal multi-anatomy long-tailed diagnosis.","Our method shows superior results on it.","Additionally, our final model serves as a pre-trained model, and can be finetuned to benefit diagnosis on various external datasets."],"url":"http://arxiv.org/abs/2312.16151v1"}
{"created":"2023-12-26 18:18:04","title":"SoundCount: Sound Counting from Raw Audio with Dyadic Decomposition Neural Network","abstract":"In this paper, we study an underexplored, yet important and challenging problem: counting the number of distinct sounds in raw audio characterized by a high degree of polyphonicity. We do so by systematically proposing a novel end-to-end trainable neural network (which we call DyDecNet, consisting of a dyadic decomposition front-end and backbone network), and quantifying the difficulty level of counting depending on sound polyphonicity. The dyadic decomposition front-end progressively decomposes the raw waveform dyadically along the frequency axis to obtain time-frequency representation in multi-stage, coarse-to-fine manner. Each intermediate waveform convolved by a parent filter is further processed by a pair of child filters that evenly split the parent filter's carried frequency response, with the higher-half child filter encoding the detail and lower-half child filter encoding the approximation. We further introduce an energy gain normalization to normalize sound loudness variance and spectrum overlap, and apply it to each intermediate parent waveform before feeding it to the two child filters. To better quantify sound counting difficulty level, we further design three polyphony-aware metrics: polyphony ratio, max polyphony and mean polyphony. We test DyDecNet on various datasets to show its superiority, and we further show dyadic decomposition network can be used as a general front-end to tackle other acoustic tasks.","sentences":["In this paper, we study an underexplored, yet important and challenging problem: counting the number of distinct sounds in raw audio characterized by a high degree of polyphonicity.","We do so by systematically proposing a novel end-to-end trainable neural network (which we call DyDecNet, consisting of a dyadic decomposition front-end and backbone network), and quantifying the difficulty level of counting depending on sound polyphonicity.","The dyadic decomposition front-end progressively decomposes the raw waveform dyadically along the frequency axis to obtain time-frequency representation in multi-stage, coarse-to-fine manner.","Each intermediate waveform convolved by a parent filter is further processed by a pair of child filters that evenly split the parent filter's carried frequency response, with the higher-half child filter encoding the detail and lower-half child filter encoding the approximation.","We further introduce an energy gain normalization to normalize sound loudness variance and spectrum overlap, and apply it to each intermediate parent waveform before feeding it to the two child filters.","To better quantify sound counting difficulty level, we further design three polyphony-aware metrics: polyphony ratio, max polyphony and mean polyphony.","We test DyDecNet on various datasets to show its superiority, and we further show dyadic decomposition network can be used as a general front-end to tackle other acoustic tasks."],"url":"http://arxiv.org/abs/2312.16149v1"}
{"created":"2023-12-26 18:13:52","title":"The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias","abstract":"The way the media presents events can significantly affect public perception, which in turn can alter people's beliefs and views. Media bias describes a one-sided or polarizing perspective on a topic. This article summarizes the research on computational methods to detect media bias by systematically reviewing 3140 research papers published between 2019 and 2022. To structure our review and support a mutual understanding of bias across research domains, we introduce the Media Bias Taxonomy, which provides a coherent overview of the current state of research on media bias from different perspectives. We show that media bias detection is a highly active research field, in which transformer-based classification approaches have led to significant improvements in recent years. These improvements include higher classification accuracy and the ability to detect more fine-granular types of bias. However, we have identified a lack of interdisciplinarity in existing projects, and a need for more awareness of the various types of media bias to support methodologically thorough performance evaluations of media bias detection systems. Concluding from our analysis, we see the integration of recent machine learning advancements with reliable and diverse bias assessment strategies from other research areas as the most promising area for future research contributions in the field.","sentences":["The way the media presents events can significantly affect public perception, which in turn can alter people's beliefs and views.","Media bias describes a one-sided or polarizing perspective on a topic.","This article summarizes the research on computational methods to detect media bias by systematically reviewing 3140 research papers published between 2019 and 2022.","To structure our review and support a mutual understanding of bias across research domains, we introduce the Media Bias Taxonomy, which provides a coherent overview of the current state of research on media bias from different perspectives.","We show that media bias detection is a highly active research field, in which transformer-based classification approaches have led to significant improvements in recent years.","These improvements include higher classification accuracy and the ability to detect more fine-granular types of bias.","However, we have identified a lack of interdisciplinarity in existing projects, and a need for more awareness of the various types of media bias to support methodologically thorough performance evaluations of media bias detection systems.","Concluding from our analysis, we see the integration of recent machine learning advancements with reliable and diverse bias assessment strategies from other research areas as the most promising area for future research contributions in the field."],"url":"http://arxiv.org/abs/2312.16148v1"}
{"created":"2023-12-26 18:08:48","title":"One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications","abstract":"The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimensional adapters to erase multiple concepts from most DMs at once across versatile erasing applications. The concept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to learn targeted erasing, and meantime the alteration and erosion phenomenon is effectively mitigated via a novel Latent Anchoring fine-tuning strategy. Once obtained, SPMs can be flexibly combined and plug-and-play for other DMs without specific re-tuning, enabling timely and efficient adaptation to diverse scenarios. During generation, our Facilitated Transport mechanism dynamically regulates the permeability of each SPM to respond to different input prompts, further minimizing the impact on other concepts. Quantitative and qualitative results across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated the superior erasing of SPM. Our code and pre-tuned SPMs will be available on the project page https://lyumengyao.github.io/projects/spm.","sentences":["The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors.","Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability & deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase.","To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimensional adapters to erase multiple concepts from most DMs at once across versatile erasing applications.","The concept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to learn targeted erasing, and meantime the alteration and erosion phenomenon is effectively mitigated via a novel Latent Anchoring fine-tuning strategy.","Once obtained, SPMs can be flexibly combined and plug-and-play for other DMs without specific re-tuning, enabling timely and efficient adaptation to diverse scenarios.","During generation, our Facilitated Transport mechanism dynamically regulates the permeability of each SPM to respond to different input prompts, further minimizing the impact on other concepts.","Quantitative and qualitative results across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated the superior erasing of SPM.","Our code and pre-tuned SPMs will be available on the project page https://lyumengyao.github.io/projects/spm."],"url":"http://arxiv.org/abs/2312.16145v1"}
{"created":"2023-12-26 18:07:05","title":"JaColBERT and Hard Negatives, Towards Better Japanese-First Embeddings for Retrieval: Early Technical Report","abstract":"Document retrieval in many languages has been largely relying on multi-lingual models, and leveraging the vast wealth of English training data. In Japanese, the best performing deep-learning based retrieval approaches rely on multilingual dense embeddings. In this work, we introduce (1) a hard-negative augmented version of the Japanese MMARCO dataset and (2) JaColBERT, a document retrieval model built on the ColBERT model architecture, specifically for Japanese. JaColBERT vastly outperform all previous monolingual retrieval approaches and competes with the best multilingual methods, despite unfavourable evaluation settings (out-of-domain vs. in-domain for the multilingual models). JaColBERT reaches an average Recall@10 of 0.813, noticeably ahead of the previous monolingual best-performing model (0.716) and only slightly behind multilingual-e5-base (0.820), though more noticeably behind multilingual-e5-large (0.856). These results are achieved using only a limited, entirely Japanese, training set, more than two orders of magnitudes smaller than multilingual embedding models. We believe these results show great promise to support retrieval-enhanced application pipelines in a wide variety of domains.","sentences":["Document retrieval in many languages has been largely relying on multi-lingual models, and leveraging the vast wealth of English training data.","In Japanese, the best performing deep-learning based retrieval approaches rely on multilingual dense embeddings.","In this work, we introduce (1) a hard-negative augmented version of the Japanese MMARCO dataset and (2) JaColBERT, a document retrieval model built on the ColBERT model architecture, specifically for Japanese.","JaColBERT vastly outperform all previous monolingual retrieval approaches and competes with the best multilingual methods, despite unfavourable evaluation settings (out-of-domain vs. in-domain for the multilingual models).","JaColBERT reaches an average Recall@10 of 0.813, noticeably ahead of the previous monolingual best-performing model (0.716) and only slightly behind multilingual-e5-base (0.820), though more noticeably behind multilingual-e5-large (0.856).","These results are achieved using only a limited, entirely Japanese, training set, more than two orders of magnitudes smaller than multilingual embedding models.","We believe these results show great promise to support retrieval-enhanced application pipelines in a wide variety of domains."],"url":"http://arxiv.org/abs/2312.16144v1"}
{"created":"2023-12-26 18:06:48","title":"On the Trajectories of SGD Without Replacement","abstract":"This article examines the implicit regularization effect of Stochastic Gradient Descent (SGD). We consider the case of SGD without replacement, the variant typically used to optimize large-scale neural networks. We analyze this algorithm in a more realistic regime than typically considered in theoretical works on SGD, as, e.g., we allow the product of the learning rate and Hessian to be $O(1)$. Our core theoretical result is that optimizing with SGD without replacement is locally equivalent to making an additional step on a novel regularizer. This implies that the trajectory of SGD without replacement diverges from both noise-injected GD and SGD with replacement (in which batches are sampled i.i.d.). Indeed, the two SGDs travel flat regions of the loss landscape in distinct directions and at different speeds. In expectation, SGD without replacement may escape saddles significantly faster and present a smaller variance. Moreover, we find that SGD implicitly regularizes the trace of the noise covariance in the eigendirections of small and negative Hessian eigenvalues. This coincides with penalizing a weighted trace of the Fisher Matrix and the Hessian on several vision tasks, thus encouraging sparsity in the spectrum of the Hessian of the loss in line with empirical observations from prior work. We also propose an explanation for why SGD does not train at the edge of stability (as opposed to GD).","sentences":["This article examines the implicit regularization effect of Stochastic Gradient Descent (SGD).","We consider the case of SGD without replacement, the variant typically used to optimize large-scale neural networks.","We analyze this algorithm in a more realistic regime than typically considered in theoretical works on SGD, as, e.g., we allow the product of the learning rate and Hessian to be $O(1)$. Our core theoretical result is that optimizing with SGD without replacement is locally equivalent to making an additional step on a novel regularizer.","This implies that the trajectory of SGD without replacement diverges from both noise-injected GD and SGD with replacement (in which batches are sampled i.i.d.).","Indeed, the two SGDs travel flat regions of the loss landscape in distinct directions and at different speeds.","In expectation, SGD without replacement may escape saddles significantly faster and present a smaller variance.","Moreover, we find that SGD implicitly regularizes the trace of the noise covariance in the eigendirections of small and negative Hessian eigenvalues.","This coincides with penalizing a weighted trace of the Fisher Matrix and the Hessian on several vision tasks, thus encouraging sparsity in the spectrum of the Hessian of the loss in line with empirical observations from prior work.","We also propose an explanation for why SGD does not train at the edge of stability (as opposed to GD)."],"url":"http://arxiv.org/abs/2312.16143v1"}
{"created":"2023-12-26 18:04:49","title":"A Bayesian Framework of Deep Reinforcement Learning for Joint O-RAN/MEC Orchestration","abstract":"Multi-access Edge Computing (MEC) can be implemented together with Open Radio Access Network (O-RAN) over commodity platforms to offer low-cost deployment and bring the services closer to end-users. In this paper, a joint O-RAN/MEC orchestration using a Bayesian deep reinforcement learning (RL)-based framework is proposed that jointly controls the O-RAN functional splits, the allocated resources and hosting locations of the O-RAN/MEC services across geo-distributed platforms, and the routing for each O-RAN/MEC data flow. The goal is to minimize the long-term overall network operation cost and maximize the MEC performance criterion while adapting possibly time-varying O-RAN/MEC demands and resource availability. This orchestration problem is formulated as Markov decision process (MDP). However, the system consists of multiple BSs that share the same resources and serve heterogeneous demands, where their parameters have non-trivial relations. Consequently, finding the exact model of the underlying system is impractical, and the formulated MDP renders in a large state space with multi-dimensional discrete action. To address such modeling and dimensionality issues, a novel model-free RL agent is proposed for our solution framework. The agent is built from Double Deep Q-network (DDQN) that tackles the large state space and is then incorporated with action branching, an action decomposition method that effectively addresses the multi-dimensional discrete action with linear increase complexity. Further, an efficient exploration-exploitation strategy under a Bayesian framework using Thomson sampling is proposed to improve the learning performance and expedite its convergence. Trace-driven simulations are performed using an O-RAN-compliant model. The results show that our approach is data-efficient (i.e., converges faster) and increases the returned reward by 32\\% than its non-Bayesian version.","sentences":["Multi-access Edge Computing (MEC) can be implemented together with Open Radio Access Network (O-RAN) over commodity platforms to offer low-cost deployment and bring the services closer to end-users.","In this paper, a joint O-RAN/MEC orchestration using a Bayesian deep reinforcement learning (RL)-based framework is proposed that jointly controls the O-RAN functional splits, the allocated resources and hosting locations of the O-RAN/MEC services across geo-distributed platforms, and the routing for each O-RAN/MEC data flow.","The goal is to minimize the long-term overall network operation cost and maximize the MEC performance criterion while adapting possibly time-varying O-RAN/MEC demands and resource availability.","This orchestration problem is formulated as Markov decision process (MDP).","However, the system consists of multiple BSs that share the same resources and serve heterogeneous demands, where their parameters have non-trivial relations.","Consequently, finding the exact model of the underlying system is impractical, and the formulated MDP renders in a large state space with multi-dimensional discrete action.","To address such modeling and dimensionality issues, a novel model-free RL agent is proposed for our solution framework.","The agent is built from Double Deep Q-network (DDQN) that tackles the large state space and is then incorporated with action branching, an action decomposition method that effectively addresses the multi-dimensional discrete action with linear increase complexity.","Further, an efficient exploration-exploitation strategy under a Bayesian framework using Thomson sampling is proposed to improve the learning performance and expedite its convergence.","Trace-driven simulations are performed using an O-RAN-compliant model.","The results show that our approach is data-efficient (i.e., converges faster) and increases the returned reward by 32\\% than its non-Bayesian version."],"url":"http://arxiv.org/abs/2312.16142v1"}
{"created":"2023-12-26 18:03:05","title":"VirtualPainting: Addressing Sparsity with Virtual Points and Distance-Aware Data Augmentation for 3D Object Detection","abstract":"In recent times, there has been a notable surge in multimodal approaches that decorates raw LiDAR point clouds with camera-derived features to improve object detection performance. However, we found that these methods still grapple with the inherent sparsity of LiDAR point cloud data, primarily because fewer points are enriched with camera-derived features for sparsely distributed objects. We present an innovative approach that involves the generation of virtual LiDAR points using camera images and enhancing these virtual points with semantic labels obtained from image-based segmentation networks to tackle this issue and facilitate the detection of sparsely distributed objects, particularly those that are occluded or distant. Furthermore, we integrate a distance aware data augmentation (DADA) technique to enhance the models capability to recognize these sparsely distributed objects by generating specialized training samples. Our approach offers a versatile solution that can be seamlessly integrated into various 3D frameworks and 2D semantic segmentation methods, resulting in significantly improved overall detection accuracy. Evaluation on the KITTI and nuScenes datasets demonstrates substantial enhancements in both 3D and birds eye view (BEV) detection benchmarks","sentences":["In recent times, there has been a notable surge in multimodal approaches that decorates raw LiDAR point clouds with camera-derived features to improve object detection performance.","However, we found that these methods still grapple with the inherent sparsity of LiDAR point cloud data, primarily because fewer points are enriched with camera-derived features for sparsely distributed objects.","We present an innovative approach that involves the generation of virtual LiDAR points using camera images and enhancing these virtual points with semantic labels obtained from image-based segmentation networks to tackle this issue and facilitate the detection of sparsely distributed objects, particularly those that are occluded or distant.","Furthermore, we integrate a distance aware data augmentation (DADA) technique to enhance the models capability to recognize these sparsely distributed objects by generating specialized training samples.","Our approach offers a versatile solution that can be seamlessly integrated into various 3D frameworks and 2D semantic segmentation methods, resulting in significantly improved overall detection accuracy.","Evaluation on the KITTI and nuScenes datasets demonstrates substantial enhancements in both 3D and birds eye view (BEV) detection benchmarks"],"url":"http://arxiv.org/abs/2312.16141v1"}
{"created":"2023-12-26 17:40:55","title":"RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models","abstract":"The rapid evolution of large language models (LLMs) necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fiction. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters. To maintain high standards, we perform a hybrid quality check process combining automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative.   Our extensive evaluations of RoleEval across various open-source and proprietary large language models, under both the zero- and few-shot settings, reveal insightful findings. Notably, while GPT-4 outperforms other models on RoleEval-Global, Chinese LLMs excel on RoleEval-Chinese, highlighting significant knowledge distribution differences. We expect that RoleEval will highlight the significance of assessing role knowledge for foundation models across various languages and cultural settings.","sentences":["The rapid evolution of large language models (LLMs) necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions.","This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge.","RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fiction.","These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters.","To maintain high standards, we perform a hybrid quality check process combining automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative.   ","Our extensive evaluations of RoleEval across various open-source and proprietary large language models, under both the zero- and few-shot settings, reveal insightful findings.","Notably, while GPT-4 outperforms other models on RoleEval-Global, Chinese LLMs excel on RoleEval-Chinese, highlighting significant knowledge distribution differences.","We expect that RoleEval will highlight the significance of assessing role knowledge for foundation models across various languages and cultural settings."],"url":"http://arxiv.org/abs/2312.16132v1"}
{"created":"2023-12-26 17:24:15","title":"Interactive Shape Sonification for Breast Cancer Localization","abstract":"About 20 percent of patients undergoing breast-conserving surgery require reoperation due to cancerous tissue remaining inside the breast. Breast cancer localization systems utilize auditory feedback to convey the distance between a localization probe and a small marker (seed) implanted into the breast tumor prior to surgery. However, no information on the location of the tumor margin is provided. To reduce the reoperation rate by improving the usability and accuracy of the surgical task, we developed an auditory display using shape sonification to assist with tumor margin localization. Accuracy and usability of the interactive shape sonification were determined on models of the female breast in three user studies with both breast surgeons and non-clinical participants. The comparative studies showed a significant increase in usability (p<0.05) and localization accuracy (p<0.001) of the shape sonification over the auditory feedback currently used in surgery.","sentences":["About 20 percent of patients undergoing breast-conserving surgery require reoperation due to cancerous tissue remaining inside the breast.","Breast cancer localization systems utilize auditory feedback to convey the distance between a localization probe and a small marker (seed) implanted into the breast tumor prior to surgery.","However, no information on the location of the tumor margin is provided.","To reduce the reoperation rate by improving the usability and accuracy of the surgical task, we developed an auditory display using shape sonification to assist with tumor margin localization.","Accuracy and usability of the interactive shape sonification were determined on models of the female breast in three user studies with both breast surgeons and non-clinical participants.","The comparative studies showed a significant increase in usability (p<0.05) and localization accuracy (p<0.001) of the shape sonification over the auditory feedback currently used in surgery."],"url":"http://arxiv.org/abs/2312.16129v1"}
{"created":"2023-12-26 17:19:09","title":"Large Language Model Situational Awareness Based Planning","abstract":"This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning for situated agents.","sentences":["This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models.","We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks.","Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances.","Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning.","By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning for situated agents."],"url":"http://arxiv.org/abs/2312.16127v1"}
{"created":"2023-12-26 17:18:33","title":"The linear time encoding scheme fails to encode","abstract":"We point out an error in the paper \"Linear Time Encoding of LDPC Codes\" (by Jin Lu and Jos\\'e M. F. Moura, IEEE Trans). The paper claims to present a linear time encoding algorithm for every LDPC code. We present a family of counterexamples, and point out where the analysis fails. The algorithm in the aforementioned paper fails to encode our counterexample, let alone in linear time.","sentences":["We point out an error in the paper \"Linear Time Encoding of LDPC Codes\" (by Jin Lu and Jos\\'e M. F. Moura, IEEE Trans).","The paper claims to present a linear time encoding algorithm for every LDPC code.","We present a family of counterexamples, and point out where the analysis fails.","The algorithm in the aforementioned paper fails to encode our counterexample, let alone in linear time."],"url":"http://arxiv.org/abs/2312.16125v1"}
{"created":"2023-12-26 17:18:09","title":"Olfactory Label Prediction on aroma-chemical Pairs","abstract":"The application of deep learning techniques on aroma-chemicals has resulted in models more accurate than human experts at predicting olfactory qualities. However, public research in this domain has been limited to predicting the qualities of single molecules, whereas in industry applications, perfumers and food scientists are often concerned with blends of many odorants. In this paper, we apply both existing and novel approaches to a dataset we gathered consisting of labeled pairs of molecules. We present a publicly available model capable of generating accurate predictions for the non-linear qualities arising from blends of aroma-chemicals.","sentences":["The application of deep learning techniques on aroma-chemicals has resulted in models more accurate than human experts at predicting olfactory qualities.","However, public research in this domain has been limited to predicting the qualities of single molecules, whereas in industry applications, perfumers and food scientists are often concerned with blends of many odorants.","In this paper, we apply both existing and novel approaches to a dataset we gathered consisting of labeled pairs of molecules.","We present a publicly available model capable of generating accurate predictions for the non-linear qualities arising from blends of aroma-chemicals."],"url":"http://arxiv.org/abs/2312.16124v1"}
{"created":"2023-12-26 16:56:22","title":"A bi-objective $\u03b5$-constrained framework for quality-cost optimization in language model ensembles","abstract":"We propose an ensembling framework that uses diverse open-sourced Large Language Models (LLMs) to achieve high response quality while maintaining cost efficiency. We formulate a bi-objective optimization problem to represent the quality-cost tradeoff and then introduce an additional budget constraint that reduces the problem to a straightforward 0/1 knapsack problem. We empirically demonstrate that our framework outperforms the existing ensembling approaches in response quality while significantly reducing costs.","sentences":["We propose an ensembling framework that uses diverse open-sourced Large Language Models (LLMs) to achieve high response quality while maintaining cost efficiency.","We formulate a bi-objective optimization problem to represent the quality-cost tradeoff and then introduce an additional budget constraint that reduces the problem to a straightforward 0/1 knapsack problem.","We empirically demonstrate that our framework outperforms the existing ensembling approaches in response quality while significantly reducing costs."],"url":"http://arxiv.org/abs/2312.16119v1"}
{"created":"2023-12-26 16:53:21","title":"Quantum-Hybrid Stereo Matching With Nonlinear Regularization and Spatial Pyramids","abstract":"Quantum visual computing is advancing rapidly. This paper presents a new formulation for stereo matching with nonlinear regularizers and spatial pyramids on quantum annealers as a maximum a posteriori inference problem that minimizes the energy of a Markov Random Field. Our approach is hybrid (i.e., quantum-classical) and is compatible with modern D-Wave quantum annealers, i.e., it includes a quadratic unconstrained binary optimization (QUBO) objective. Previous quantum annealing techniques for stereo matching are limited to using linear regularizers, and thus, they do not exploit the fundamental advantages of the quantum computing paradigm in solving combinatorial optimization problems. In contrast, our method utilizes the full potential of quantum annealing for stereo matching, as nonlinear regularizers create optimization problems which are NP-hard. On the Middlebury benchmark, we achieve an improved root mean squared accuracy over the previous state of the art in quantum stereo matching of 2% and 22.5% when using different solvers.","sentences":["Quantum visual computing is advancing rapidly.","This paper presents a new formulation for stereo matching with nonlinear regularizers and spatial pyramids on quantum annealers as a maximum a posteriori inference problem that minimizes the energy of a Markov Random Field.","Our approach is hybrid (i.e., quantum-classical) and is compatible with modern D-Wave quantum annealers, i.e., it includes a quadratic unconstrained binary optimization (QUBO) objective.","Previous quantum annealing techniques for stereo matching are limited to using linear regularizers, and thus, they do not exploit the fundamental advantages of the quantum computing paradigm in solving combinatorial optimization problems.","In contrast, our method utilizes the full potential of quantum annealing for stereo matching, as nonlinear regularizers create optimization problems which are NP-hard.","On the Middlebury benchmark, we achieve an improved root mean squared accuracy over the previous state of the art in quantum stereo matching of 2% and 22.5% when using different solvers."],"url":"http://arxiv.org/abs/2312.16118v1"}
{"created":"2023-12-26 16:24:08","title":"fMPI: Fast Novel View Synthesis in the Wild with Layered Scene Representations","abstract":"In this study, we propose two novel input processing paradigms for novel view synthesis (NVS) methods based on layered scene representations that significantly improve their runtime without compromising quality. Our approach identifies and mitigates the two most time-consuming aspects of traditional pipelines: building and processing the so-called plane sweep volume (PSV), which is a high-dimensional tensor of planar re-projections of the input camera views. In particular, we propose processing this tensor in parallel groups for improved compute efficiency as well as super-sampling adjacent input planes to generate denser, and hence more accurate scene representation. The proposed enhancements offer significant flexibility, allowing for a balance between performance and speed, thus making substantial steps toward real-time applications. Furthermore, they are very general in the sense that any PSV-based method can make use of them, including methods that employ multiplane images, multisphere images, and layered depth images. In a comprehensive set of experiments, we demonstrate that our proposed paradigms enable the design of an NVS method that achieves state-of-the-art on public benchmarks while being up to $50x$ faster than existing state-of-the-art methods. It also beats the current forerunner in terms of speed by over $3x$, while achieving significantly better rendering quality.","sentences":["In this study, we propose two novel input processing paradigms for novel view synthesis (NVS) methods based on layered scene representations that significantly improve their runtime without compromising quality.","Our approach identifies and mitigates the two most time-consuming aspects of traditional pipelines: building and processing the so-called plane sweep volume (PSV), which is a high-dimensional tensor of planar re-projections of the input camera views.","In particular, we propose processing this tensor in parallel groups for improved compute efficiency as well as super-sampling adjacent input planes to generate denser, and hence more accurate scene representation.","The proposed enhancements offer significant flexibility, allowing for a balance between performance and speed, thus making substantial steps toward real-time applications.","Furthermore, they are very general in the sense that any PSV-based method can make use of them, including methods that employ multiplane images, multisphere images, and layered depth images.","In a comprehensive set of experiments, we demonstrate that our proposed paradigms enable the design of an NVS method that achieves state-of-the-art on public benchmarks while being up to $50x$ faster than existing state-of-the-art methods.","It also beats the current forerunner in terms of speed by over $3x$, while achieving significantly better rendering quality."],"url":"http://arxiv.org/abs/2312.16109v1"}
{"created":"2023-12-26 16:22:10","title":"LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving","abstract":"A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines. Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines. While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information. Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure. Our algorithm features two key modifications. One is a lane attention module to capture pivotal region details within the long-range feature space. Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, \\textit{i.e.}, map element detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the newly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains a real-time inference speed of 14.7 FPS. Code is accessible at https://github.com/OpenDriveLab/LaneSegNet.","sentences":["A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines.","However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines.","Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines.","While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information.","Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure.","Our algorithm features two key modifications.","One is a lane attention module to capture pivotal region details within the long-range feature space.","Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention.","On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, \\textit{i.e.}, map element detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the newly defined one, lane segment perception (+5.6 mAP).","Furthermore, it obtains a real-time inference speed of 14.7 FPS.","Code is accessible at https://github.com/OpenDriveLab/LaneSegNet."],"url":"http://arxiv.org/abs/2312.16108v1"}
{"created":"2023-12-26 16:21:15","title":"Clique Analysis and Bypassing in Continuous-Time Conflict-Based Search","abstract":"While the study of unit-cost Multi-Agent Pathfinding (MAPF) problems has been popular, many real-world problems require continuous time and costs due to various movement models. In this context, this paper studies symmetry-breaking enhancements for Continuous-Time Conflict-Based Search (CCBS), a solver for continuous-time MAPF. Resolving conflict symmetries in MAPF can require an exponential amount of work. We adapt known enhancements from unit-cost domains for CCBS: bypassing, which resolves cost symmetries and biclique constraints which resolve spatial conflict symmetries. We formulate a novel combination of biclique constraints with disjoint splitting for spatial conflict symmetries. Finally, we show empirically that these enhancements yield a statistically significant performance improvement versus previous state of the art, solving problems for up to 10% or 20% more agents in the same amount of time on dense graphs.","sentences":["While the study of unit-cost Multi-Agent Pathfinding (MAPF) problems has been popular, many real-world problems require continuous time and costs due to various movement models.","In this context, this paper studies symmetry-breaking enhancements for Continuous-Time Conflict-Based Search (CCBS), a solver for continuous-time MAPF.","Resolving conflict symmetries in MAPF can require an exponential amount of work.","We adapt known enhancements from unit-cost domains for CCBS: bypassing, which resolves cost symmetries and biclique constraints which resolve spatial conflict symmetries.","We formulate a novel combination of biclique constraints with disjoint splitting for spatial conflict symmetries.","Finally, we show empirically that these enhancements yield a statistically significant performance improvement versus previous state of the art, solving problems for up to 10% or 20% more agents in the same amount of time on dense graphs."],"url":"http://arxiv.org/abs/2312.16106v1"}
{"created":"2023-12-26 16:16:33","title":"Dotless Representation of Arabic Text: Analysis and Modeling","abstract":"This paper presents a novel dotless representation of Arabic text as an alternative to the standard Arabic text representation. We delve into its implications through comprehensive analysis across five diverse corpora and four different tokenization techniques. We explore the impact of dotless representation on the relationships between tokenization granularity and vocabulary size and compare them with standard text representation. Moreover, we analyze the information density of dotless versus standard text using text entropy calculations. To delve deeper into the implications of the dotless representation, statistical and neural language models are constructed using the various text corpora and tokenization techniques. A comparative assessment is then made against language models developed using the standard Arabic text representation. This multifaceted analysis provides valuable insights into the potential advantages and challenges associated with the dotless representation. Last but not the least, utilizing parallel corpora, we draw comparisons between the text analysis of Arabic and English to gain further insights. Our findings shed light on the potential benefits of dotless representation for various NLP tasks, paving the way for further exploration for Arabic natural language processing.","sentences":["This paper presents a novel dotless representation of Arabic text as an alternative to the standard Arabic text representation.","We delve into its implications through comprehensive analysis across five diverse corpora and four different tokenization techniques.","We explore the impact of dotless representation on the relationships between tokenization granularity and vocabulary size and compare them with standard text representation.","Moreover, we analyze the information density of dotless versus standard text using text entropy calculations.","To delve deeper into the implications of the dotless representation, statistical and neural language models are constructed using the various text corpora and tokenization techniques.","A comparative assessment is then made against language models developed using the standard Arabic text representation.","This multifaceted analysis provides valuable insights into the potential advantages and challenges associated with the dotless representation.","Last but not the least, utilizing parallel corpora, we draw comparisons between the text analysis of Arabic and English to gain further insights.","Our findings shed light on the potential benefits of dotless representation for various NLP tasks, paving the way for further exploration for Arabic natural language processing."],"url":"http://arxiv.org/abs/2312.16104v1"}
{"created":"2023-12-26 15:53:25","title":"Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models","abstract":"Recent work in zero-shot listwise reranking using LLMs has achieved state-of-the-art results. However, these methods are not without drawbacks. The proposed methods rely on large LLMs with billions of parameters and limited context sizes. This paper introduces LiT5-Distill and LiT5-Score, two methods for efficient zero-shot listwise reranking, leveraging T5 sequence-to-sequence encoder-decoder models. Our approaches demonstrate competitive reranking effectiveness compared to recent state-of-the-art LLM rerankers with substantially smaller models. Through LiT5-Score, we also explore the use of cross-attention to calculate relevance scores to perform reranking, eliminating the reliance on external passage relevance labels for training. We present a range of models from 220M parameters to 3B parameters, all with strong reranking results, challenging the necessity of large-scale models for effective zero-shot reranking and opening avenues for more efficient listwise reranking solutions. We provide code and scripts to reproduce our results at https://github.com/castorini/LiT5.","sentences":["Recent work in zero-shot listwise reranking using LLMs has achieved state-of-the-art results.","However, these methods are not without drawbacks.","The proposed methods rely on large LLMs with billions of parameters and limited context sizes.","This paper introduces LiT5-Distill and LiT5-Score, two methods for efficient zero-shot listwise reranking, leveraging T5 sequence-to-sequence encoder-decoder models.","Our approaches demonstrate competitive reranking effectiveness compared to recent state-of-the-art LLM rerankers with substantially smaller models.","Through LiT5-Score, we also explore the use of cross-attention to calculate relevance scores to perform reranking, eliminating the reliance on external passage relevance labels for training.","We present a range of models from 220M parameters to 3B parameters, all with strong reranking results, challenging the necessity of large-scale models for effective zero-shot reranking and opening avenues for more efficient listwise reranking solutions.","We provide code and scripts to reproduce our results at https://github.com/castorini/LiT5."],"url":"http://arxiv.org/abs/2312.16098v1"}
{"created":"2023-12-26 15:21:49","title":"Improved decoding of expander codes: fundamental trade-off between expansion ratio and minimum distance of inner code","abstract":"Tanner codes are graph-based linear codes whose parity-check matrices can be characterized by a bipartite graph $G$ together with an inner code $C_0$. Expander codes are Tanner codes whose defining bipartite graph $G$ has good expansion property. The landmark work of Sipser and Spielman showed that every bipartite expander $G$ with expansion ratio $\\delta>3/4$ together with a parity-check code defines an expander code which corrects $\\Omega(n)$ errors in $O(n)$ time, where $n$ is the code length. Viderman showed that $\\delta>2/3-\\Omega(1)$ is already sufficient. Our paper is motivated by the following natural and fundamental problem in decoding expander codes:   \\textbf{Question:} What are the sufficient and necessary conditions that $\\delta$ and $d_0$ should satisfy so that {\\it every} bipartite expander $G$ with expansion ratio $\\delta$ and {\\it every} inner code $C_0$ with minimum distance $d_0$ together define an expander code which corrects $\\Omega(n)$ errors in $O(n)$ time?   We give a near-optimal solution to the question above, showing that $\\delta d_0>3$ is sufficient and $\\delta d_0>1$ is necessary. Our result significantly improves the previously known result of Dowling and Gao, who showed that $d_0=\\Omega(c\\delta^{-2})$ is sufficient, where $c$ is the left-degree of $G$. We suspect that $\\delta d_0>1$ is also sufficient to solve the question above.","sentences":["Tanner codes are graph-based linear codes whose parity-check matrices can be characterized by a bipartite graph $G$ together with an inner code $C_0$. Expander codes are Tanner codes whose defining bipartite graph $G$ has good expansion property.","The landmark work of Sipser and Spielman showed that every bipartite expander $G$ with expansion ratio $\\delta>3/4$ together with a parity-check code defines an expander code which corrects $\\Omega(n)$ errors in $O(n)$ time, where $n$ is the code length.","Viderman showed that $\\delta>2/3-\\Omega(1)$ is already sufficient.","Our paper is motivated by the following natural and fundamental problem in decoding expander codes:   \\textbf{Question:} What are the sufficient and necessary conditions that $\\delta$ and $d_0$ should satisfy so that {\\it every} bipartite expander $G$ with expansion ratio $\\delta$ and {\\it every} inner code $C_0$ with minimum distance $d_0$ together define an expander code which corrects $\\Omega(n)$ errors in $O(n)$ time?   ","We give a near-optimal solution to the question above, showing that $\\delta d_0>3$ is sufficient and $\\delta d_0>1$ is necessary.","Our result significantly improves the previously known result of Dowling and Gao, who showed that $d_0=\\Omega(c\\delta^{-2})$ is sufficient, where $c$ is the left-degree of $G$. We suspect that $\\delta d_0>1$ is also sufficient to solve the question above."],"url":"http://arxiv.org/abs/2312.16087v1"}
{"created":"2023-12-26 15:14:37","title":"LangSplat: 3D Language Gaussian Splatting","abstract":"Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\\speed} $\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io","sentences":["Human lives in a 3D world and commonly uses natural language to interact with a 3D scene.","Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently.","This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces.","Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field.","By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF.","Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling.","Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects.","We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features.","Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin.","Notably, LangSplat is extremely efficient, achieving a {\\speed} $\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080.","We strongly recommend readers to check out our video results at https://langsplat.github.io"],"url":"http://arxiv.org/abs/2312.16084v1"}
{"created":"2023-12-26 15:11:55","title":"Dynamic Latent Graph-Guided Neural Temporal Point Processes","abstract":"Continuously-observed event occurrences, often exhibit self- and mutually-exciting effects, which can be well modeled using temporal point processes. Beyond that, these event dynamics may also change over time, with certain periodic trends. We propose a novel variational auto-encoder to capture such a mixture of temporal dynamics. More specifically, the whole time interval of the input sequence is partitioned into a set of sub-intervals. The event dynamics are assumed to be stationary within each sub-interval, but could be changing across those sub-intervals. In particular, we use a sequential latent variable model to learn a dependency graph between the observed dimensions, for each sub-interval. The model predicts the future event times, by using the learned dependency graph to remove the noncontributing influences of past events. By doing so, the proposed model demonstrates its higher accuracy in predicting inter-event times and event types for several real-world event sequences, compared with existing state of the art neural point processes.","sentences":["Continuously-observed event occurrences, often exhibit self- and mutually-exciting effects, which can be well modeled using temporal point processes.","Beyond that, these event dynamics may also change over time, with certain periodic trends.","We propose a novel variational auto-encoder to capture such a mixture of temporal dynamics.","More specifically, the whole time interval of the input sequence is partitioned into a set of sub-intervals.","The event dynamics are assumed to be stationary within each sub-interval, but could be changing across those sub-intervals.","In particular, we use a sequential latent variable model to learn a dependency graph between the observed dimensions, for each sub-interval.","The model predicts the future event times, by using the learned dependency graph to remove the noncontributing influences of past events.","By doing so, the proposed model demonstrates its higher accuracy in predicting inter-event times and event types for several real-world event sequences, compared with existing state of the art neural point processes."],"url":"http://arxiv.org/abs/2312.16083v1"}
{"created":"2023-12-26 14:58:57","title":"A Fractal-based Complex Belief Entropy for Uncertainty Measure in Complex Evidence Theory","abstract":"Complex evidence theory, as a generalized D-S evidence theory, has attracted academic attention because it can well express uncertainty by means of complex basic belief assignment (CBBA), and realize uncertainty reasoning by complex combination rule. However, the uncertainty measurement in complex evidence theory is still an open issue. In order to make better decisions, a complex pignistic belief transformation (CPBT) method has been proposed to assign CBBAs of multi-element focal elements to subsets. The essence of CPBT is the redistribution of complex mass function by means of the concept of fractal. In this paper, based on fractal theory, experimental simulation and analysis have been carried out on the generation process of CPBT in time dimension. Then, a new fractal-based complex belief (FCB) entropy is proposed to measure the uncertainty of CBBA. Finally, the properties of FCB entropy are analyzed, and several examples are used to verify its effectiveness.","sentences":["Complex evidence theory, as a generalized D-S evidence theory, has attracted academic attention because it can well express uncertainty by means of complex basic belief assignment (CBBA), and realize uncertainty reasoning by complex combination rule.","However, the uncertainty measurement in complex evidence theory is still an open issue.","In order to make better decisions, a complex pignistic belief transformation (CPBT) method has been proposed to assign CBBAs of multi-element focal elements to subsets.","The essence of CPBT is the redistribution of complex mass function by means of the concept of fractal.","In this paper, based on fractal theory, experimental simulation and analysis have been carried out on the generation process of CPBT in time dimension.","Then, a new fractal-based complex belief (FCB) entropy is proposed to measure the uncertainty of CBBA.","Finally, the properties of FCB entropy are analyzed, and several examples are used to verify its effectiveness."],"url":"http://arxiv.org/abs/2312.16080v1"}
{"created":"2023-12-26 14:58:24","title":"Coexistence assessment and interference mitigation for 5G and Fixed Satellite Stations in C-band in India","abstract":"In this paper, we present the findings of a study conducted to assess the coexistence of Fifth Generation (5G) wireless networks and Fixed Satellite Station (FSS) receivers in the C-Band (3300-4200 MHz) in India. Through simulations, we evaluate the coexistence feasibility and calculate the minimum separation distances required to mitigate interference, consider-ing factors such as 5G Base Station power, off-axis angle, clutter, filtering, and shielding. Next, we present various interference mitigation techniques, including distance, antenna tilt and height, power control, antenna design, coordination, filtering, and others, aiming for balanced coexistence. The simulation results corroborate the efficacy of these solutions in containing interference from 5G in the C-Band FSS receivers. The paper offers valuable insights into frequency allocation in India and considerations for 5G network design, including site selection and antenna orientation. The insights provided are relevant to other regions facing similar coexistence challenges. Overall, this paper offers a comprehensive overview of 5G and FSS coexistence in the C-band, emphasising the importance of addressing this issue during network design and deployment.","sentences":["In this paper, we present the findings of a study conducted to assess the coexistence of Fifth Generation (5G) wireless networks and Fixed Satellite Station (FSS) receivers in the C-Band (3300-4200 MHz) in India.","Through simulations, we evaluate the coexistence feasibility and calculate the minimum separation distances required to mitigate interference, consider-ing factors such as 5G Base Station power, off-axis angle, clutter, filtering, and shielding.","Next, we present various interference mitigation techniques, including distance, antenna tilt and height, power control, antenna design, coordination, filtering, and others, aiming for balanced coexistence.","The simulation results corroborate the efficacy of these solutions in containing interference from 5G in the C-Band FSS receivers.","The paper offers valuable insights into frequency allocation in India and considerations for 5G network design, including site selection and antenna orientation.","The insights provided are relevant to other regions facing similar coexistence challenges.","Overall, this paper offers a comprehensive overview of 5G and FSS coexistence in the C-band, emphasising the importance of addressing this issue during network design and deployment."],"url":"http://arxiv.org/abs/2312.16079v1"}
{"created":"2023-12-26 14:43:26","title":"Event-based Shape from Polarization with Spiking Neural Networks","abstract":"Recent advances in event-based shape determination from polarization offer a transformative approach that tackles the trade-off between speed and accuracy in capturing surface geometries. In this paper, we investigate event-based shape from polarization using Spiking Neural Networks (SNNs), introducing the Single-Timestep and Multi-Timestep Spiking UNets for effective and efficient surface normal estimation. Specificially, the Single-Timestep model processes event-based shape as a non-temporal task, updating the membrane potential of each spiking neuron only once, thereby reducing computational and energy demands. In contrast, the Multi-Timestep model exploits temporal dynamics for enhanced data extraction. Extensive evaluations on synthetic and real-world datasets demonstrate that our models match the performance of state-of-the-art Artifical Neural Networks (ANNs) in estimating surface normals, with the added advantage of superior energy efficiency. Our work not only contributes to the advancement of SNNs in event-based sensing but also sets the stage for future explorations in optimizing SNN architectures, integrating multi-modal data, and scaling for applications on neuromorphic hardware.","sentences":["Recent advances in event-based shape determination from polarization offer a transformative approach that tackles the trade-off between speed and accuracy in capturing surface geometries.","In this paper, we investigate event-based shape from polarization using Spiking Neural Networks (SNNs), introducing the Single-Timestep and Multi-Timestep Spiking UNets for effective and efficient surface normal estimation.","Specificially, the Single-Timestep model processes event-based shape as a non-temporal task, updating the membrane potential of each spiking neuron only once, thereby reducing computational and energy demands.","In contrast, the Multi-Timestep model exploits temporal dynamics for enhanced data extraction.","Extensive evaluations on synthetic and real-world datasets demonstrate that our models match the performance of state-of-the-art Artifical Neural Networks (ANNs) in estimating surface normals, with the added advantage of superior energy efficiency.","Our work not only contributes to the advancement of SNNs in event-based sensing but also sets the stage for future explorations in optimizing SNN architectures, integrating multi-modal data, and scaling for applications on neuromorphic hardware."],"url":"http://arxiv.org/abs/2312.16071v1"}
{"created":"2023-12-26 14:43:04","title":"Can ChatGPT Read Who You Are?","abstract":"The interplay between artificial intelligence (AI) and psychology, particularly in personality assessment, represents an important emerging area of research. Accurate personality trait estimation is crucial not only for enhancing personalization in human-computer interaction but also for a wide variety of applications ranging from mental health to education. This paper analyzes the capability of a generic chatbot, ChatGPT, to effectively infer personality traits from short texts. We report the results of a comprehensive user study featuring texts written in Czech by a representative population sample of 155 participants. Their self-assessments based on the Big Five Inventory (BFI) questionnaire serve as the ground truth. We compare the personality trait estimations made by ChatGPT against those by human raters and report ChatGPT's competitive performance in inferring personality traits from text. We also uncover a 'positivity bias' in ChatGPT's assessments across all personality dimensions and explore the impact of prompt composition on accuracy. This work contributes to the understanding of AI capabilities in psychological assessment, highlighting both the potential and limitations of using large language models for personality inference. Our research underscores the importance of responsible AI development, considering ethical implications such as privacy, consent, autonomy, and bias in AI applications.","sentences":["The interplay between artificial intelligence (AI) and psychology, particularly in personality assessment, represents an important emerging area of research.","Accurate personality trait estimation is crucial not only for enhancing personalization in human-computer interaction but also for a wide variety of applications ranging from mental health to education.","This paper analyzes the capability of a generic chatbot, ChatGPT, to effectively infer personality traits from short texts.","We report the results of a comprehensive user study featuring texts written in Czech by a representative population sample of 155 participants.","Their self-assessments based on the Big Five Inventory (BFI) questionnaire serve as the ground truth.","We compare the personality trait estimations made by ChatGPT against those by human raters and report ChatGPT's competitive performance in inferring personality traits from text.","We also uncover a 'positivity bias' in ChatGPT's assessments across all personality dimensions and explore the impact of prompt composition on accuracy.","This work contributes to the understanding of AI capabilities in psychological assessment, highlighting both the potential and limitations of using large language models for personality inference.","Our research underscores the importance of responsible AI development, considering ethical implications such as privacy, consent, autonomy, and bias in AI applications."],"url":"http://arxiv.org/abs/2312.16070v1"}
{"created":"2023-12-26 14:37:55","title":"A Prompt Learning Framework for Source Code Summarization","abstract":"(Source) code summarization is the task of automatically generating natural language summaries for given code snippets. Such summaries play a key role in helping developers understand and maintain source code. Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks. The main adaptation schemes include instruction prompting and task-oriented fine-tuning. However, instruction prompting involves designing crafted prompts for zero-shot learning or selecting appropriate samples for few-shot learning and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs. In this paper, we propose a novel prompt learning framework for code summarization called PromptCS. PromptCS trains a prompt agent that can generate continuous prompts to unleash the potential for LLMs in code summarization. Compared to the human-written discrete prompt, the continuous prompts are produced under the guidance of LLMs and are therefore easier to understand by LLMs. PromptCS freezes the parameters of LLMs when training the prompt agent, which can greatly reduce the requirements for training resources. We evaluate PromptCS on the CodeSearchNet dataset involving multiple programming languages. The results show that PromptCS significantly outperforms instruction prompting schemes on all four widely used metrics. In some base LLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS even outperforms the task-oriented fine-tuning scheme. More importantly, the training efficiency of PromptCS is faster than the task-oriented fine-tuning scheme, with a more pronounced advantage on larger LLMs. The results of the human evaluation demonstrate that PromptCS can generate more good summaries compared to baselines.","sentences":["(Source) code summarization is the task of automatically generating natural language summaries for given code snippets.","Such summaries play a key role in helping developers understand and maintain source code.","Recently, with the successful application of large language models (LLMs) in numerous fields, software engineering researchers have also attempted to adapt LLMs to solve code summarization tasks.","The main adaptation schemes include instruction prompting and task-oriented fine-tuning.","However, instruction prompting involves designing crafted prompts for zero-shot learning or selecting appropriate samples for few-shot learning and requires users to have professional domain knowledge, while task-oriented fine-tuning requires high training costs.","In this paper, we propose a novel prompt learning framework for code summarization called PromptCS.","PromptCS","trains a prompt agent that can generate continuous prompts to unleash the potential for LLMs in code summarization.","Compared to the human-written discrete prompt, the continuous prompts are produced under the guidance of LLMs and are therefore easier to understand by LLMs.","PromptCS","freezes the parameters of LLMs when training the prompt agent, which can greatly reduce the requirements for training resources.","We evaluate PromptCS","on the CodeSearchNet dataset involving multiple programming languages.","The results show that PromptCS","significantly outperforms instruction prompting schemes on all four widely used metrics.","In some base LLMs, e.g., CodeGen-Multi-2B and StarCoderBase-1B and -3B, PromptCS","even outperforms the task-oriented fine-tuning scheme.","More importantly, the training efficiency of PromptCS is faster than the task-oriented fine-tuning scheme, with a more pronounced advantage on larger LLMs.","The results of the human evaluation demonstrate that PromptCS can generate more good summaries compared to baselines."],"url":"http://arxiv.org/abs/2312.16066v1"}
{"created":"2023-12-26 14:30:52","title":"Goal-Oriented Integration of Sensing, Communication, Computing, and Control for Mission-Critical Internet-of-Things","abstract":"Driven by the development goal of network paradigm and demand for various functions in the sixth-generation (6G) mission-critical Internet-of-Things (MC-IoT), we foresee a goal-oriented integration of sensing, communication, computing, and control (GIS3C) in this paper. We first provide an overview of the tasks, requirements, and challenges of MC-IoT. Then we introduce an end-to-end GIS3C architecture, in which goal-oriented communication is leveraged to bridge and empower sensing, communication, control, and computing functionalities. By revealing the interplay among multiple subsystems in terms of key performance indicators and parameters, this paper introduces unified metrics, i.e., task completion effectiveness and cost, to facilitate S3C co-design in MC-IoT. The preliminary results demonstrate the benefits of GIS3C in improving task completion effectiveness while reducing costs. We also identify and highlight the gaps and challenges in applying GIS3C in the future 6G networks.","sentences":["Driven by the development goal of network paradigm and demand for various functions in the sixth-generation (6G) mission-critical Internet-of-Things (MC-IoT), we foresee a goal-oriented integration of sensing, communication, computing, and control (GIS3C) in this paper.","We first provide an overview of the tasks, requirements, and challenges of MC-IoT. Then we introduce an end-to-end GIS3C architecture, in which goal-oriented communication is leveraged to bridge and empower sensing, communication, control, and computing functionalities.","By revealing the interplay among multiple subsystems in terms of key performance indicators and parameters, this paper introduces unified metrics, i.e., task completion effectiveness and cost, to facilitate S3C co-design in MC-IoT.","The preliminary results demonstrate the benefits of GIS3C in improving task completion effectiveness while reducing costs.","We also identify and highlight the gaps and challenges in applying GIS3C in the future 6G networks."],"url":"http://arxiv.org/abs/2312.16064v1"}
{"created":"2023-12-26 14:20:36","title":"AutoTask: Executing Arbitrary Voice Commands by Exploring and Learning from Mobile GUI","abstract":"Voice command interfaces (VCIs) have gained increasing importance, enabling hands-free and eyes-free interaction with digital devices. However, the inherent complexity in constructing effective voice interfaces has limited the VCIs' functionalities to only a small fraction of GUI applications and tasks. This paper presents AutoTask, a VCI capable of automating any task in any mobile application without configuration or modification from developers or end users. The primary challenge for AutoTask is the lack of knowledge, as it needs to accomplish unknown tasks (e.g., user commands) within an unknown environment (e.g., GUI). To address this challenge, AutoTask employs two strategies: (1) trial and error: AutoTask explores the GUI, attempts potential operation sequences, and recovers from errors through backtracking; (2) learning from the environment: AutoTask accumulates experiences during exploration and summarizes correct knowledge from these experiences. We implemented AutoTask on Android devices and conducted an evaluation study, which proved the feasibility of AutoTask.","sentences":["Voice command interfaces (VCIs) have gained increasing importance, enabling hands-free and eyes-free interaction with digital devices.","However, the inherent complexity in constructing effective voice interfaces has limited the VCIs' functionalities to only a small fraction of GUI applications and tasks.","This paper presents AutoTask, a VCI capable of automating any task in any mobile application without configuration or modification from developers or end users.","The primary challenge for AutoTask is the lack of knowledge, as it needs to accomplish unknown tasks (e.g., user commands) within an unknown environment (e.g., GUI).","To address this challenge, AutoTask employs two strategies: (1) trial and error: AutoTask explores the GUI, attempts potential operation sequences, and recovers from errors through backtracking; (2) learning from the environment: AutoTask accumulates experiences during exploration and summarizes correct knowledge from these experiences.","We implemented AutoTask on Android devices and conducted an evaluation study, which proved the feasibility of AutoTask."],"url":"http://arxiv.org/abs/2312.16062v1"}
{"created":"2023-12-26 14:15:19","title":"Error-free Training for Artificial Neural Network","abstract":"Conventional training methods for artificial neural network (ANN) models never achieve zero error rate systematically for large data. A new training method consists of three steps: first create an auxiliary data from conventionally trained parameters which correspond exactly to a global minimum for the loss function of the cloned data; second create a one-parameter homotopy (hybrid) of the auxiliary data and the original data; and third train the model for the hybrid data iteratively from the auxiliary data end of the homotopy parameter to the original data end while maintaining the zero-error training rate at every iteration. This continuationmethod is guaranteed to converge numerically by a theorem which converts the ANN training problem into a continuation problem for fixed points of a parameterized transformation in the training parameter space to which the Uniform Contraction Mapping Theorem from dynamical systems applies.","sentences":["Conventional training methods for artificial neural network (ANN) models never achieve zero error rate systematically for large data.","A new training method consists of three steps: first create an auxiliary data from conventionally trained parameters which correspond exactly to a global minimum for the loss function of the cloned data; second create a one-parameter homotopy (hybrid) of the auxiliary data and the original data; and third train the model for the hybrid data iteratively from the auxiliary data end of the homotopy parameter to the original data end while maintaining the zero-error training rate at every iteration.","This continuationmethod is guaranteed to converge numerically by a theorem which converts the ANN training problem into a continuation problem for fixed points of a parameterized transformation in the training parameter space to which the Uniform Contraction Mapping Theorem from dynamical systems applies."],"url":"http://arxiv.org/abs/2312.16060v1"}
{"created":"2023-12-26 14:01:07","title":"Semantic Importance-Aware Based for Multi-User Communication Over MIMO Fading Channels","abstract":"Semantic communication, as a novel communication paradigm, has attracted the interest of many scholars, with multi-user, multi-input multi-output (MIMO) scenarios being one of the critical contexts. This paper presents a semantic importance-aware based communication system (SIA-SC) over MIMO Rayleigh fading channels. Combining the semantic symbols' inequality and the equivalent subchannels of MIMO channels based on Singular Value Decomposition (SVD) maximizes the end-to-end semantic performance through the new layer mapping method. For multi-user scenarios, a method of semantic interference cancellation is proposed. Furthermore, a new metric, namely semantic information distortion (SID), is established to unify the expressions of semantic performance, which is affected by channel bandwidth ratio (CBR) and signal-to-noise ratio (SNR). With the help of the proposed metric, we derived performance expressions and Semantic Outage Probability (SOP) of SIA-SC for Single-User Single-Input Single-Output (SU-SISO), Single-User MIMO (SU-MIMO), Multi-Users SISO (MU-MIMO) and Multi-Users MIMO (MU-MIMO) scenarios. Numerical experiments show that SIA-SC can significantly improve semantic performance across various scenarios.","sentences":["Semantic communication, as a novel communication paradigm, has attracted the interest of many scholars, with multi-user, multi-input multi-output (MIMO) scenarios being one of the critical contexts.","This paper presents a semantic importance-aware based communication system (SIA-SC) over MIMO Rayleigh fading channels.","Combining the semantic symbols' inequality and the equivalent subchannels of MIMO channels based on Singular Value Decomposition (SVD) maximizes the end-to-end semantic performance through the new layer mapping method.","For multi-user scenarios, a method of semantic interference cancellation is proposed.","Furthermore, a new metric, namely semantic information distortion (SID), is established to unify the expressions of semantic performance, which is affected by channel bandwidth ratio (CBR) and signal-to-noise ratio (SNR).","With the help of the proposed metric, we derived performance expressions and Semantic Outage Probability (SOP) of SIA-SC for Single-User Single-Input Single-Output (SU-SISO), Single-User MIMO (SU-MIMO), Multi-Users SISO (MU-MIMO) and Multi-Users MIMO (MU-MIMO) scenarios.","Numerical experiments show that SIA-SC can significantly improve semantic performance across various scenarios."],"url":"http://arxiv.org/abs/2312.16057v1"}
{"created":"2023-12-26 13:54:00","title":"A Logically Consistent Chain-of-Thought Approach for Stance Detection","abstract":"Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets. Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD. However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions. To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction. LC-CoT employs a three-step process. Initially, it assesses whether supplementary external knowledge is necessary. Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM. Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence. This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data.","sentences":["Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets.","Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD.","However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions.","To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction.","LC-CoT employs a three-step process.","Initially, it assesses whether supplementary external knowledge is necessary.","Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM.","Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence.","This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data."],"url":"http://arxiv.org/abs/2312.16054v1"}
{"created":"2023-12-26 13:36:05","title":"Inter-X: Towards Versatile Human-Human Interaction Analysis","abstract":"The analysis of the ubiquitous human-human interactions is pivotal for understanding humans as social beings. Existing human-human interaction datasets typically suffer from inaccurate body motions, lack of hand gestures and fine-grained textual descriptions. To better perceive and generate human-human interactions, we propose Inter-X, a currently largest human-human interaction dataset with accurate body movements and diverse interaction patterns, together with detailed hand gestures. The dataset includes ~11K interaction sequences and more than 8.1M frames. We also equip Inter-X with versatile annotations of more than 34K fine-grained human part-level textual descriptions, semantic interaction categories, interaction order, and the relationship and personality of the subjects. Based on the elaborate annotations, we propose a unified benchmark composed of 4 categories of downstream tasks from both the perceptual and generative directions. Extensive experiments and comprehensive analysis show that Inter-X serves as a testbed for promoting the development of versatile human-human interaction analysis. Our dataset and benchmark will be publicly available for research purposes.","sentences":["The analysis of the ubiquitous human-human interactions is pivotal for understanding humans as social beings.","Existing human-human interaction datasets typically suffer from inaccurate body motions, lack of hand gestures and fine-grained textual descriptions.","To better perceive and generate human-human interactions, we propose Inter-X, a currently largest human-human interaction dataset with accurate body movements and diverse interaction patterns, together with detailed hand gestures.","The dataset includes ~11K interaction sequences and more than 8.1M frames.","We also equip Inter-X with versatile annotations of more than 34K fine-grained human part-level textual descriptions, semantic interaction categories, interaction order, and the relationship and personality of the subjects.","Based on the elaborate annotations, we propose a unified benchmark composed of 4 categories of downstream tasks from both the perceptual and generative directions.","Extensive experiments and comprehensive analysis show that Inter-X serves as a testbed for promoting the development of versatile human-human interaction analysis.","Our dataset and benchmark will be publicly available for research purposes."],"url":"http://arxiv.org/abs/2312.16051v1"}
{"created":"2023-12-26 13:29:54","title":"Sliding Mode Control for 3-D Uncalibrated and Constrained Vision-based Shape Servoing within Input Saturation","abstract":"This paper designs a servo control system based on sliding mode control for the shape control of elastic objects. In order to solve the effect of non-smooth and asymmetric control saturation, a Gaussian-based continuous differentiable asymmetric saturation function is used for this goal. The proposed detection approach runs in a highly real-time manner. Meanwhile, this paper uses sliding mode control to prove that the estimation stability of the deformation Jacobian matrix and the system stability of the controller are combined, which verifies the control stability of the closed-loop system including estimation. Besides, an integral sliding mode function is designed to avoid the need for second-order derivatives of variables, which enhances the robustness of the system in actual situations. Finally, the Lyapunov theory is used to prove the consistent final boundedness of all variables of the system.","sentences":["This paper designs a servo control system based on sliding mode control for the shape control of elastic objects.","In order to solve the effect of non-smooth and asymmetric control saturation, a Gaussian-based continuous differentiable asymmetric saturation function is used for this goal.","The proposed detection approach runs in a highly real-time manner.","Meanwhile, this paper uses sliding mode control to prove that the estimation stability of the deformation Jacobian matrix and the system stability of the controller are combined, which verifies the control stability of the closed-loop system including estimation.","Besides, an integral sliding mode function is designed to avoid the need for second-order derivatives of variables, which enhances the robustness of the system in actual situations.","Finally, the Lyapunov theory is used to prove the consistent final boundedness of all variables of the system."],"url":"http://arxiv.org/abs/2312.16048v1"}
{"created":"2023-12-26 13:28:21","title":"2D-Guided 3D Gaussian Segmentation","abstract":"Recently, 3D Gaussian, as an explicit 3D representation method, has demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms of expressing complex scenes and training duration. These advantages signal a wide range of applications for 3D Gaussians in 3D understanding and editing. Meanwhile, the segmentation of 3D Gaussians is still in its infancy. The existing segmentation methods are not only cumbersome but also incapable of segmenting multiple objects simultaneously in a short amount of time. In response, this paper introduces a 3D Gaussian segmentation method implemented with 2D segmentation as supervision. This approach uses input 2D segmentation maps to guide the learning of the added 3D Gaussian semantic information, while nearest neighbor clustering and statistical filtering refine the segmentation results. Experiments show that our concise method can achieve comparable performances on mIOU and mAcc for multi-object segmentation as previous single-object segmentation methods.","sentences":["Recently, 3D Gaussian, as an explicit 3D representation method, has demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms of expressing complex scenes and training duration.","These advantages signal a wide range of applications for 3D Gaussians in 3D understanding and editing.","Meanwhile, the segmentation of 3D Gaussians is still in its infancy.","The existing segmentation methods are not only cumbersome but also incapable of segmenting multiple objects simultaneously in a short amount of time.","In response, this paper introduces a 3D Gaussian segmentation method implemented with 2D segmentation as supervision.","This approach uses input 2D segmentation maps to guide the learning of the added 3D Gaussian semantic information, while nearest neighbor clustering and statistical filtering refine the segmentation results.","Experiments show that our concise method can achieve comparable performances on mIOU and mAcc for multi-object segmentation as previous single-object segmentation methods."],"url":"http://arxiv.org/abs/2312.16047v1"}
{"created":"2023-12-26 13:23:03","title":"AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts","abstract":"Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \\emph{None}, \\emph{Light}, \\emph{Moderate}, \\emph{Heavy} and \\emph{Violent} on a large-scale precipitation benchmark named TIGGE. Finally, the average mean-absolute error (MAE) and average root-mean-square error (RMSE) of the proposed AdaNAS model are 0.98 and 2.04 mm/day, respectively. Additionally, the proposed AdaNAS model is compared with other neural architecture search methods and previous studies. Compared results reveal the satisfactory performance and superiority of the proposed AdaNAS model in terms of precipitation amount prediction and intensity classification. Concretely, the proposed AdaNAS model outperformed previous best-performing manual methods with MAE and RMSE improving by 80.5\\% and 80.3\\%, respectively.","sentences":["Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated.","Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor.","Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy.","In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas.","Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training.","Validation experiments have been performed under the cases of \\emph{None}, \\emph{Light}, \\emph{Moderate}, \\emph{Heavy} and \\emph{Violent} on a large-scale precipitation benchmark named TIGGE.","Finally, the average mean-absolute error (MAE) and average root-mean-square error (RMSE) of the proposed AdaNAS model are 0.98 and 2.04 mm/day, respectively.","Additionally, the proposed AdaNAS model is compared with other neural architecture search methods and previous studies.","Compared results reveal the satisfactory performance and superiority of the proposed AdaNAS model in terms of precipitation amount prediction and intensity classification.","Concretely, the proposed AdaNAS model outperformed previous best-performing manual methods with MAE and RMSE improving by 80.5\\% and 80.3\\%, respectively."],"url":"http://arxiv.org/abs/2312.16046v1"}
{"created":"2023-12-26 13:17:25","title":"Algebraic Positional Encodings","abstract":"We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often ad hoc, approaches. Our framework provides a flexible mapping from the algebraic specification of a domain to an interpretation as orthogonal operators. This design preserves the algebraic characteristics of the source domain, ensuring that the model upholds the desired structural properties. Our scheme can accommodate various structures, including sequences, grids and trees, as well as their compositions. We conduct a series of experiments to demonstrate the practical applicability of our approach. Results suggest performance on par with or surpassing the current state-of-the-art, without hyperparameter optimizations or ``task search'' of any kind. Code will be made available at \\url{github.com/konstantinosKokos/UnitaryPE}.","sentences":["We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often ad hoc, approaches.","Our framework provides a flexible mapping from the algebraic specification of a domain to an interpretation as orthogonal operators.","This design preserves the algebraic characteristics of the source domain, ensuring that the model upholds the desired structural properties.","Our scheme can accommodate various structures, including sequences, grids and trees, as well as their compositions.","We conduct a series of experiments to demonstrate the practical applicability of our approach.","Results suggest performance on par with or surpassing the current state-of-the-art, without hyperparameter optimizations or ``task search'' of any kind.","Code will be made available at \\url{github.com/konstantinosKokos/UnitaryPE}."],"url":"http://arxiv.org/abs/2312.16045v1"}
{"created":"2023-12-26 13:17:06","title":"Large Language Models as Traffic Signal Control Agents: Capacity and Opportunity","abstract":"Traffic signal control is crucial for optimizing the efficiency of road network by regulating traffic light phases. Existing research predominantly focuses on heuristic or reinforcement learning (RL)-based methods, which often lack transferability across diverse traffic scenarios and suffer from poor interpretability. This paper introduces a novel approach, LLMLight, utilizing large language models (LLMs) for traffic signal control tasks. By leveraging LLMs' impressive generalization and zero-shot reasoning capabilities, LLMLight executes a human-like decision-making process for efficient traffic management. Specifically, the framework begins by composing task descriptions, current traffic conditions, and prior knowledge into a prompt. Subsequently, we utilize LLM's chain-of-thought (CoT) reasoning ability to identify the next traffic signal phase, ensuring optimal efficiency in the road network. LLMLight achieves state-of-the-art (SOTA) or competitive results across five real-world traffic datasets. Notably, LLMLight showcases remarkable generalization, interpretability, and zero-shot reasoning abilities, even without any training for transportation management tasks. Our project is available at https://github.com/usail-hkust/LLMTSCS.","sentences":["Traffic signal control is crucial for optimizing the efficiency of road network by regulating traffic light phases.","Existing research predominantly focuses on heuristic or reinforcement learning (RL)-based methods, which often lack transferability across diverse traffic scenarios and suffer from poor interpretability.","This paper introduces a novel approach, LLMLight, utilizing large language models (LLMs) for traffic signal control tasks.","By leveraging LLMs' impressive generalization and zero-shot reasoning capabilities, LLMLight executes a human-like decision-making process for efficient traffic management.","Specifically, the framework begins by composing task descriptions, current traffic conditions, and prior knowledge into a prompt.","Subsequently, we utilize LLM's chain-of-thought (CoT) reasoning ability to identify the next traffic signal phase, ensuring optimal efficiency in the road network.","LLMLight achieves state-of-the-art (SOTA) or competitive results across five real-world traffic datasets.","Notably, LLMLight showcases remarkable generalization, interpretability, and zero-shot reasoning abilities, even without any training for transportation management tasks.","Our project is available at https://github.com/usail-hkust/LLMTSCS."],"url":"http://arxiv.org/abs/2312.16044v1"}
{"created":"2023-12-26 13:14:17","title":"An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification","abstract":"This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line search. Empirically, we have observed that the proposed approach outperforms $\\pi$-weighted convex focal loss and balanced classifier LIBLINEAR(logistic regression, SVM, and L2SVM) in terms of test classification accuracy with $51$ two-class and $67$ multi-class datasets. In binary classification problems, where the scale-class-imbalance ratio of the training dataset is not significant but the inconsistency exists, a group of SIC models with the best test accuracy for each dataset (TOP$1$) outperforms LIBSVM(C-SVC with RBF kernel), a well-known kernel-based classifier.","sentences":["This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function.","In contrast to the conventional $\\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function.","As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets.","This adaptation is achieved by creating a skewed hyperplane equation.","Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line search.","Empirically, we have observed that the proposed approach outperforms $\\pi$-weighted convex focal loss and balanced classifier LIBLINEAR(logistic regression, SVM, and L2SVM) in terms of test classification accuracy with $51$ two-class and $67$ multi-class datasets.","In binary classification problems, where the scale-class-imbalance ratio of the training dataset is not significant but the inconsistency exists, a group of SIC models with the best test accuracy for each dataset (TOP$1$) outperforms LIBSVM(C-SVC with RBF kernel), a well-known kernel-based classifier."],"url":"http://arxiv.org/abs/2312.16043v1"}
{"created":"2023-12-26 13:07:45","title":"Multi-scale Progressive Feature Embedding for Accurate NIR-to-RGB Spectral Domain Translation","abstract":"NIR-to-RGB spectral domain translation is a challenging task due to the mapping ambiguities, and existing methods show limited learning capacities. To address these challenges, we propose to colorize NIR images via a multi-scale progressive feature embedding network (MPFNet), with the guidance of grayscale image colorization. Specifically, we first introduce a domain translation module that translates NIR source images into the grayscale target domain. By incorporating a progressive training strategy, the statistical and semantic knowledge from both task domains are efficiently aligned with a series of pixel- and feature-level consistency constraints. Besides, a multi-scale progressive feature embedding network is designed to improve learning capabilities. Experiments show that our MPFNet outperforms state-of-the-art counterparts by 2.55 dB in the NIR-to-RGB spectral domain translation task in terms of PSNR.","sentences":["NIR-to-RGB spectral domain translation is a challenging task due to the mapping ambiguities, and existing methods show limited learning capacities.","To address these challenges, we propose to colorize NIR images via a multi-scale progressive feature embedding network (MPFNet), with the guidance of grayscale image colorization.","Specifically, we first introduce a domain translation module that translates NIR source images into the grayscale target domain.","By incorporating a progressive training strategy, the statistical and semantic knowledge from both task domains are efficiently aligned with a series of pixel- and feature-level consistency constraints.","Besides, a multi-scale progressive feature embedding network is designed to improve learning capabilities.","Experiments show that our MPFNet outperforms state-of-the-art counterparts by 2.55 dB in the NIR-to-RGB spectral domain translation task in terms of PSNR."],"url":"http://arxiv.org/abs/2312.16040v1"}
{"created":"2023-12-26 12:56:31","title":"Dual-scale Enhanced and Cross-generative Consistency Learning for Semi-supervised Polyp Segmentation","abstract":"Automatic polyp segmentation plays a crucial role in the early diagnosis and treatment of colorectal cancer (CRC). However, existing methods heavily rely on fully supervised training, which requires a large amount of labeled data with time-consuming pixel-wise annotations. Moreover, accurately segmenting polyps poses challenges due to variations in shape, size, and location. To address these issues, we propose a novel Dual-scale Enhanced and Cross-generative consistency learning framework for semi-supervised polyp Segmentation (DEC-Seg) from colonoscopy images. First, we propose a Cross-level Feature Aggregation (CFA) module that integrates cross-level adjacent layers to enhance the feature representation ability across different resolutions. To address scale variation, we present a scale-enhanced consistency constraint, which ensures consistency in the segmentation maps generated from the same input image at different scales. This constraint helps handle variations in polyp sizes and improves the robustness of the model. Additionally, we design a scale-aware perturbation consistency scheme to enhance the robustness of the mean teacher model. Furthermore, we propose a cross-generative consistency scheme, in which the original and perturbed images can be reconstructed using cross-segmentation maps. This consistency constraint allows us to mine effective feature representations and boost the segmentation performance. To produce more accurate segmentation maps, we propose a Dual-scale Complementary Fusion (DCF) module that integrates features from two scale-specific decoders operating at different scales. Extensive experimental results on five benchmark datasets demonstrate the effectiveness of our DEC-Seg against other state-of-the-art semi-supervised segmentation approaches. The implementation code will be released at https://github.com/taozh2017/DECSeg.","sentences":["Automatic polyp segmentation plays a crucial role in the early diagnosis and treatment of colorectal cancer (CRC).","However, existing methods heavily rely on fully supervised training, which requires a large amount of labeled data with time-consuming pixel-wise annotations.","Moreover, accurately segmenting polyps poses challenges due to variations in shape, size, and location.","To address these issues, we propose a novel Dual-scale Enhanced and Cross-generative consistency learning framework for semi-supervised polyp Segmentation (DEC-Seg) from colonoscopy images.","First, we propose a Cross-level Feature Aggregation (CFA) module that integrates cross-level adjacent layers to enhance the feature representation ability across different resolutions.","To address scale variation, we present a scale-enhanced consistency constraint, which ensures consistency in the segmentation maps generated from the same input image at different scales.","This constraint helps handle variations in polyp sizes and improves the robustness of the model.","Additionally, we design a scale-aware perturbation consistency scheme to enhance the robustness of the mean teacher model.","Furthermore, we propose a cross-generative consistency scheme, in which the original and perturbed images can be reconstructed using cross-segmentation maps.","This consistency constraint allows us to mine effective feature representations and boost the segmentation performance.","To produce more accurate segmentation maps, we propose a Dual-scale Complementary Fusion (DCF) module that integrates features from two scale-specific decoders operating at different scales.","Extensive experimental results on five benchmark datasets demonstrate the effectiveness of our DEC-Seg against other state-of-the-art semi-supervised segmentation approaches.","The implementation code will be released at https://github.com/taozh2017/DECSeg."],"url":"http://arxiv.org/abs/2312.16039v1"}
{"created":"2023-12-26 12:55:32","title":"Critical nonlinear aspects of hopping transport for reconfigurable logic in disordered dopant networks","abstract":"Nonlinear behavior in the hopping transport of interacting charges enables reconfigurable logic in disordered dopant network devices, where voltages applied at control electrodes tune the relation between voltages applied at input electrodes and the current measured at an output electrode. From kinetic Monte Carlo simulations we analyze the critical nonlinear aspects of variable-range hopping transport for realizing Boolean logic gates in these devices on three levels. First, we quantify the occurrence of individual gates for random choices of control voltages. We find that linearly inseparable gates such as the XOR gate are less likely to occur than linearly separable gates such as the AND gate, despite the fact that the number of different regions in the multidimensional control voltage space for which AND or XOR gates occur is comparable. Second, we use principal component analysis to characterize the distribution of the output current vectors for the (00,10,01,11) logic input combinations in terms of eigenvectors and eigenvalues of the output covariance matrix. This allows a simple and direct comparison of the behavior of different simulated devices and a comparison to experimental devices. Third, we quantify the nonlinearity in the distribution of the output current vectors necessary for realizing Boolean functionality by introducing three nonlinearity indicators. The analysis provides a physical interpretation of the effects of changing the hopping distance and temperature and is used in a comparison with data generated by a deep neural network trained on a physical device.","sentences":["Nonlinear behavior in the hopping transport of interacting charges enables reconfigurable logic in disordered dopant network devices, where voltages applied at control electrodes tune the relation between voltages applied at input electrodes and the current measured at an output electrode.","From kinetic Monte Carlo simulations we analyze the critical nonlinear aspects of variable-range hopping transport for realizing Boolean logic gates in these devices on three levels.","First, we quantify the occurrence of individual gates for random choices of control voltages.","We find that linearly inseparable gates such as the XOR gate are less likely to occur than linearly separable gates such as the AND gate, despite the fact that the number of different regions in the multidimensional control voltage space for which AND or XOR gates occur is comparable.","Second, we use principal component analysis to characterize the distribution of the output current vectors for the (00,10,01,11) logic input combinations in terms of eigenvectors and eigenvalues of the output covariance matrix.","This allows a simple and direct comparison of the behavior of different simulated devices and a comparison to experimental devices.","Third, we quantify the nonlinearity in the distribution of the output current vectors necessary for realizing Boolean functionality by introducing three nonlinearity indicators.","The analysis provides a physical interpretation of the effects of changing the hopping distance and temperature and is used in a comparison with data generated by a deep neural network trained on a physical device."],"url":"http://arxiv.org/abs/2312.16037v1"}
{"created":"2023-12-26 12:53:57","title":"Ensemble Learning to Assess Dynamics of Affective Experience Ratings and Physiological Change","abstract":"The congruence between affective experiences and physiological changes has been a debated topic for centuries. Recent technological advances in measurement and data analysis provide hope to solve this epic challenge. Open science and open data practices, together with data analysis challenges open to the academic community, are also promising tools for solving this problem. In this entry to the Emotion Physiology and Experience Collaboration (EPiC) challenge, we propose a data analysis solution that combines theoretical assumptions with data-driven methodologies. We used feature engineering and ensemble selection. Each predictor was trained on subsets of the training data that would maximize the information available for training. Late fusion was used with an averaging step. We chose to average considering a ``wisdom of crowds'' strategy. This strategy yielded an overall RMSE of 1.19 in the test set. Future work should carefully explore if our assumptions are correct and the potential of weighted fusion.","sentences":["The congruence between affective experiences and physiological changes has been a debated topic for centuries.","Recent technological advances in measurement and data analysis provide hope to solve this epic challenge.","Open science and open data practices, together with data analysis challenges open to the academic community, are also promising tools for solving this problem.","In this entry to the Emotion Physiology and Experience Collaboration (EPiC) challenge, we propose a data analysis solution that combines theoretical assumptions with data-driven methodologies.","We used feature engineering and ensemble selection.","Each predictor was trained on subsets of the training data that would maximize the information available for training.","Late fusion was used with an averaging step.","We chose to average considering a ``wisdom of crowds'' strategy.","This strategy yielded an overall RMSE of 1.19 in the test set.","Future work should carefully explore if our assumptions are correct and the potential of weighted fusion."],"url":"http://arxiv.org/abs/2312.16036v1"}
{"created":"2023-12-26 12:52:59","title":"Extended Ranking Mechanisms for the m-Capacitated Facility Location Problem in Bayesian Mechanism Design","abstract":"In this paper, we initiate the study of the $m$-Capacitated Facility Location Problem ($m$-CFLP) within a Bayesian Mechanism Design framework. We consider the case in which every agent's private information is their position on a line and assume that every agent's position is independently drawn from a common and known distribution $\\mu$. In this context, we propose the Extended Ranking Mechanisms (ERMs), a truthful generalization of the recently introduced Ranking Mechanisms, that allows to handle problems where the total facility capacity exceeds the number of agents. Our primary results pertain to the study of the efficiency guarantees of the ERMs. In particular, we demonstrate that the limit of the ratio between the expected Social Cost of an ERM and the expected optimal Social Cost is finite. En route to these results, we reveal that the optimal Social Cost and the Social Cost of any ERMs can be expressed as the objective value of a suitable norm minimization problem in the Wasserstein space. We then tackle the problem of determining an optimal ERM tailored to a $m$-CFLP and a distribution $\\mu$. Specifically, we aim to identify an ERM whose limit Bayesian approximation ratio is the lowest compared to all other ERMs. We detail how to retrieve an optimal ERM in two frameworks: (i) when the total facility capacity matches the number of agents and (ii) when $\\mu$ is the uniform distribution and we have two facilities to place. Lastly, we conduct extensive numerical experiments to compare the performance of the ERMs against other truthful mechanisms and to evaluate the convergence speed of the Bayesian approximation ratio. In summary, all our findings highlight that a well-tuned ERM consistently outperforms all other known mechanisms, making it a valid choice for solving the $m$-CFLP within a Bayesian framework.","sentences":["In this paper, we initiate the study of the $m$-Capacitated Facility Location Problem ($m$-CFLP) within a Bayesian Mechanism Design framework.","We consider the case in which every agent's private information is their position on a line and assume that every agent's position is independently drawn from a common and known distribution $\\mu$. In this context, we propose the Extended Ranking Mechanisms (ERMs), a truthful generalization of the recently introduced Ranking Mechanisms, that allows to handle problems where the total facility capacity exceeds the number of agents.","Our primary results pertain to the study of the efficiency guarantees of the ERMs.","In particular, we demonstrate that the limit of the ratio between the expected Social Cost of an ERM and the expected optimal Social Cost is finite.","En route to these results, we reveal that the optimal Social Cost and the Social Cost of any ERMs can be expressed as the objective value of a suitable norm minimization problem in the Wasserstein space.","We then tackle the problem of determining an optimal ERM tailored to a $m$-CFLP and a distribution $\\mu$. Specifically, we aim to identify an ERM whose limit Bayesian approximation ratio is the lowest compared to all other ERMs.","We detail how to retrieve an optimal ERM in two frameworks: (i) when the total facility capacity matches the number of agents and (ii) when $\\mu$ is the uniform distribution and we have two facilities to place.","Lastly, we conduct extensive numerical experiments to compare the performance of the ERMs against other truthful mechanisms and to evaluate the convergence speed of the Bayesian approximation ratio.","In summary, all our findings highlight that a well-tuned ERM consistently outperforms all other known mechanisms, making it a valid choice for solving the $m$-CFLP within a Bayesian framework."],"url":"http://arxiv.org/abs/2312.16034v1"}
{"created":"2023-12-26 12:49:25","title":"On the Hardness of Minimum Embedded Order Dependency Validation","abstract":"Order Dependencies (ODs) have many applications, such as query optimization, data integration, and data cleaning. Although many works addressed the problem of discovering OD (and its variants), they do not consider datasets with missing values, a standard observation in real-world datasets. This paper introduces the novel notion of Embedded ODs (eODs) to deal with missing values. The intuition of eODs is to confirm ODs only on tuples with no missing values on a given embedding (a set of attributes). In this paper, we address the problem of validating a given eOD. If the eOD holds, we return true. Otherwise, we search for an updated embedding such that the updated eOD holds. If such embedding does not exist, we return false. A trivial requirement is to consider an embedding such that the number of ignored tuples is minimized. We show that it is NP-complete to compute such embedding. We therefore propose an efficient heuristic algorithm for validating embedded ODs. We conduct experiments on real-world datasets, and the results confirm the efficiency of our algorithm.","sentences":["Order Dependencies (ODs) have many applications, such as query optimization, data integration, and data cleaning.","Although many works addressed the problem of discovering OD (and its variants), they do not consider datasets with missing values, a standard observation in real-world datasets.","This paper introduces the novel notion of Embedded ODs (eODs) to deal with missing values.","The intuition of eODs is to confirm ODs only on tuples with no missing values on a given embedding (a set of attributes).","In this paper, we address the problem of validating a given eOD.","If the eOD holds, we return true.","Otherwise, we search for an updated embedding such that the updated eOD holds.","If such embedding does not exist, we return false.","A trivial requirement is to consider an embedding such that the number of ignored tuples is minimized.","We show that it is NP-complete to compute such embedding.","We therefore propose an efficient heuristic algorithm for validating embedded ODs.","We conduct experiments on real-world datasets, and the results confirm the efficiency of our algorithm."],"url":"http://arxiv.org/abs/2312.16033v1"}
{"created":"2023-12-26 12:24:14","title":"DocMSU: A Comprehensive Benchmark for Document-level Multimodal Sarcasm Understanding","abstract":"Multimodal Sarcasm Understanding (MSU) has a wide range of applications in the news field such as public opinion analysis and forgery detection. However, existing MSU benchmarks and approaches usually focus on sentence-level MSU. In document-level news, sarcasm clues are sparse or small and are often concealed in long text. Moreover, compared to sentence-level comments like tweets, which mainly focus on only a few trends or hot topics (e.g., sports events), content in the news is considerably diverse. Models created for sentence-level MSU may fail to capture sarcasm clues in document-level news. To fill this gap, we present a comprehensive benchmark for Document-level Multimodal Sarcasm Understanding (DocMSU). Our dataset contains 102,588 pieces of news with text-image pairs, covering 9 diverse topics such as health, business, etc. The proposed large-scale and diverse DocMSU significantly facilitates the research of document-level MSU in real-world scenarios. To take on the new challenges posed by DocMSU, we introduce a fine-grained sarcasm comprehension method to properly align the pixel-level image features with word-level textual features in documents. Experiments demonstrate the effectiveness of our method, showing that it can serve as a baseline approach to the challenging DocMSU. Our code and dataset are available at https://github.com/Dulpy/DocMSU.","sentences":["Multimodal Sarcasm Understanding (MSU) has a wide range of applications in the news field such as public opinion analysis and forgery detection.","However, existing MSU benchmarks and approaches usually focus on sentence-level MSU.","In document-level news, sarcasm clues are sparse or small and are often concealed in long text.","Moreover, compared to sentence-level comments like tweets, which mainly focus on only a few trends or hot topics (e.g., sports events), content in the news is considerably diverse.","Models created for sentence-level MSU may fail to capture sarcasm clues in document-level news.","To fill this gap, we present a comprehensive benchmark for Document-level Multimodal Sarcasm Understanding (DocMSU).","Our dataset contains 102,588 pieces of news with text-image pairs, covering 9 diverse topics such as health, business, etc.","The proposed large-scale and diverse DocMSU significantly facilitates the research of document-level MSU in real-world scenarios.","To take on the new challenges posed by DocMSU, we introduce a fine-grained sarcasm comprehension method to properly align the pixel-level image features with word-level textual features in documents.","Experiments demonstrate the effectiveness of our method, showing that it can serve as a baseline approach to the challenging DocMSU.","Our code and dataset are available at https://github.com/Dulpy/DocMSU."],"url":"http://arxiv.org/abs/2312.16023v1"}
{"created":"2023-12-26 12:19:22","title":"Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks","abstract":"In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of models during pruning. Our results suggest a promising direction for creating efficient neural networks that do not compromise on accuracy, even in environments with constrained computational resources.","sentences":["In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process.","Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios.","Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods.","This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity.","We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness.","The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of models during pruning.","Our results suggest a promising direction for creating efficient neural networks that do not compromise on accuracy, even in environments with constrained computational resources."],"url":"http://arxiv.org/abs/2312.16020v1"}
{"created":"2023-12-26 12:12:58","title":"RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \\textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.   In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations. Specifically, we introduce importance-aware sampling, clustering-based sampling, and penalty for repetitive sampling for sampling high-quality, representative, and diverse training data. To enhance the prompt, we introduce position shifting strategy to mitigate position bias and augment the prompt with auxiliary information from conventional recommendation models, thereby enriching the contextual understanding of the LLM. Subsequently, we utilize the sampled data to assemble an instruction-tuning dataset with the augmented prompt comprising three distinct ranking tasks: pointwise, pairwise, and listwise rankings. We further propose a hybrid ranking method to enhance the model performance by ensembling these ranking tasks. Our empirical evaluations demonstrate the effectiveness of our proposed RecRanker in both direct and sequential recommendation scenarios.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems.","Numerous studies have employed specialized \\textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs.","For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation.","Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations.","Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance.","Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.   ","In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \\textbf{Ranker} for top-\\textit{k} \\textbf{Rec}ommendations.","Specifically, we introduce importance-aware sampling, clustering-based sampling, and penalty for repetitive sampling for sampling high-quality, representative, and diverse training data.","To enhance the prompt, we introduce position shifting strategy to mitigate position bias and augment the prompt with auxiliary information from conventional recommendation models, thereby enriching the contextual understanding of the LLM.","Subsequently, we utilize the sampled data to assemble an instruction-tuning dataset with the augmented prompt comprising three distinct ranking tasks: pointwise, pairwise, and listwise rankings.","We further propose a hybrid ranking method to enhance the model performance by ensembling these ranking tasks.","Our empirical evaluations demonstrate the effectiveness of our proposed RecRanker in both direct and sequential recommendation scenarios."],"url":"http://arxiv.org/abs/2312.16018v1"}
{"created":"2023-12-26 12:00:24","title":"V-STRONG: Visual Self-Supervised Traversability Learning for Off-road Navigation","abstract":"Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments. Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability. To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance. Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training. We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios. We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types. We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller. Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments. Videos and additional material can be found here: \\url{https://sites.google.com/view/visual-traversability-learning}.","sentences":["Reliable estimation of terrain traversability is critical for the successful deployment of autonomous systems in wild, outdoor environments.","Given the lack of large-scale annotated datasets for off-road navigation, strictly-supervised learning approaches remain limited in their generalization ability.","To this end, we introduce a novel, image-based self-supervised learning method for traversability prediction, leveraging a state-of-the-art vision foundation model for improved out-of-distribution performance.","Our method employs contrastive representation learning using both human driving data and instance-based segmentation masks during training.","We show that this simple, yet effective, technique drastically outperforms recent methods in predicting traversability for both on- and off-trail driving scenarios.","We compare our method with recent baselines on both a common benchmark as well as our own datasets, covering a diverse range of outdoor environments and varied terrain types.","We also demonstrate the compatibility of resulting costmap predictions with a model-predictive controller.","Finally, we evaluate our approach on zero- and few-shot tasks, demonstrating unprecedented performance for generalization to new environments.","Videos and additional material can be found here: \\url{https://sites.google.com/view/visual-traversability-learning}."],"url":"http://arxiv.org/abs/2312.16016v1"}
{"created":"2023-12-26 11:57:01","title":"A Comprehensive Survey of Evaluation Techniques for Recommendation Systems","abstract":"The effectiveness of recommendation systems is pivotal to user engagement and satisfaction in online platforms. As these recommendation systems increasingly influence user choices, their evaluation transcends mere technical performance and becomes central to business success. This paper addresses the multifaceted nature of recommendation system evaluation by introducing a comprehensive suite of metrics, each tailored to capture a distinct aspect of system performance. We discuss similarity metrics that quantify the precision of content-based and collaborative filtering mechanisms, along with candidate generation metrics which measure how well the system identifies a broad yet pertinent range of items. Following this, we delve into predictive metrics that assess the accuracy of forecasted preferences, ranking metrics that evaluate the order in which recommendations are presented, and business metrics that align system performance with economic objectives.   Our approach emphasizes the contextual application of these metrics and their interdependencies. In this paper, we identify the strengths and limitations of current evaluation practices and highlight the nuanced trade-offs that emerge when optimizing recommendation systems across different metrics. The paper concludes by proposing a framework for selecting and interpreting these metrics to not only improve system performance but also to advance business goals. This work is to aid researchers and practitioners in critically assessing recommendation systems and fosters the development of more nuanced, effective, and economically viable personalization strategies. Our code is available at GitHub - https://github.com/aryan-jadon/Evaluation-Metrics-for-Recommendation-Systems.","sentences":["The effectiveness of recommendation systems is pivotal to user engagement and satisfaction in online platforms.","As these recommendation systems increasingly influence user choices, their evaluation transcends mere technical performance and becomes central to business success.","This paper addresses the multifaceted nature of recommendation system evaluation by introducing a comprehensive suite of metrics, each tailored to capture a distinct aspect of system performance.","We discuss similarity metrics that quantify the precision of content-based and collaborative filtering mechanisms, along with candidate generation metrics which measure how well the system identifies a broad yet pertinent range of items.","Following this, we delve into predictive metrics that assess the accuracy of forecasted preferences, ranking metrics that evaluate the order in which recommendations are presented, and business metrics that align system performance with economic objectives.   ","Our approach emphasizes the contextual application of these metrics and their interdependencies.","In this paper, we identify the strengths and limitations of current evaluation practices and highlight the nuanced trade-offs that emerge when optimizing recommendation systems across different metrics.","The paper concludes by proposing a framework for selecting and interpreting these metrics to not only improve system performance but also to advance business goals.","This work is to aid researchers and practitioners in critically assessing recommendation systems and fosters the development of more nuanced, effective, and economically viable personalization strategies.","Our code is available at GitHub - https://github.com/aryan-jadon/Evaluation-Metrics-for-Recommendation-Systems."],"url":"http://arxiv.org/abs/2312.16015v1"}
{"created":"2023-12-26 11:49:23","title":"Passive Non-Line-of-Sight Imaging with Light Transport Modulation","abstract":"Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in recent years, due to its ability to image objects that are out of sight. The light transport condition plays an important role in this task since changing the conditions will lead to different imaging models. Existing learning-based NLOS methods usually train independent models for different light transport conditions, which is computationally inefficient and impairs the practicality of the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging method that effectively handles multiple light transport conditions with a single network. We achieve this by inferring a latent light transport representation from the projection image and using this representation to modulate the network that reconstructs the hidden image from the projection image. We train a light transport encoder together with a vector quantizer to obtain the light transport representation. To further regulate this representation, we jointly learn both the reconstruction network and the reprojection network during training. A set of light transport modulation blocks is used to modulate the two jointly trained networks in a multi-scale way. Extensive experiments on a large-scale passive NLOS dataset demonstrate the superiority of the proposed method. The code is available at https://github.com/JerryOctopus/NLOS-LTM.","sentences":["Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in recent years, due to its ability to image objects that are out of sight.","The light transport condition plays an important role in this task since changing the conditions will lead to different imaging models.","Existing learning-based NLOS methods usually train independent models for different light transport conditions, which is computationally inefficient and impairs the practicality of the models.","In this work, we propose NLOS-LTM, a novel passive NLOS imaging method that effectively handles multiple light transport conditions with a single network.","We achieve this by inferring a latent light transport representation from the projection image and using this representation to modulate the network that reconstructs the hidden image from the projection image.","We train a light transport encoder together with a vector quantizer to obtain the light transport representation.","To further regulate this representation, we jointly learn both the reconstruction network and the reprojection network during training.","A set of light transport modulation blocks is used to modulate the two jointly trained networks in a multi-scale way.","Extensive experiments on a large-scale passive NLOS dataset demonstrate the superiority of the proposed method.","The code is available at https://github.com/JerryOctopus/NLOS-LTM."],"url":"http://arxiv.org/abs/2312.16014v1"}
{"created":"2023-12-26 11:45:22","title":"Detection-based Intermediate Supervision for Visual Question Answering","abstract":"Recently, neural module networks (NMNs) have yielded ongoing success in answering compositional visual questions, especially those involving multi-hop visual and logical reasoning. NMNs decompose the complex question into several sub-tasks using instance-modules from the reasoning paths of that question and then exploit intermediate supervisions to guide answer prediction, thereby improving inference interpretability. However, their performance may be hindered due to sketchy modeling of intermediate supervisions. For instance, (1) a prior assumption that each instance-module refers to only one grounded object yet overlooks other potentially associated grounded objects, impeding full cross-modal alignment learning; (2) IoU-based intermediate supervisions may introduce noise signals as the bounding box overlap issue might guide the model's focus towards irrelevant objects. To address these issues, a novel method, \\textbf{\\underline{D}}etection-based \\textbf{\\underline{I}}ntermediate \\textbf{\\underline{S}}upervision (DIS), is proposed, which adopts a generative detection framework to facilitate multiple grounding supervisions via sequence generation. As such, DIS offers more comprehensive and accurate intermediate supervisions, thereby boosting answer prediction performance. Furthermore, by considering intermediate results, DIS enhances the consistency in answering compositional questions and their sub-questions.Extensive experiments demonstrate the superiority of our proposed DIS, showcasing both improved accuracy and state-of-the-art reasoning consistency compared to prior approaches.","sentences":["Recently, neural module networks (NMNs) have yielded ongoing success in answering compositional visual questions, especially those involving multi-hop visual and logical reasoning.","NMNs decompose the complex question into several sub-tasks using instance-modules from the reasoning paths of that question and then exploit intermediate supervisions to guide answer prediction, thereby improving inference interpretability.","However, their performance may be hindered due to sketchy modeling of intermediate supervisions.","For instance, (1) a prior assumption that each instance-module refers to only one grounded object yet overlooks other potentially associated grounded objects, impeding full cross-modal alignment learning; (2) IoU-based intermediate supervisions may introduce noise signals as the bounding box overlap issue might guide the model's focus towards irrelevant objects.","To address these issues, a novel method, \\textbf{\\underline{D}}etection-based \\textbf{\\underline{I}}ntermediate \\textbf{\\underline{S}}upervision (DIS), is proposed, which adopts a generative detection framework to facilitate multiple grounding supervisions via sequence generation.","As such, DIS offers more comprehensive and accurate intermediate supervisions, thereby boosting answer prediction performance.","Furthermore, by considering intermediate results, DIS enhances the consistency in answering compositional questions and their sub-questions.","Extensive experiments demonstrate the superiority of our proposed DIS, showcasing both improved accuracy and state-of-the-art reasoning consistency compared to prior approaches."],"url":"http://arxiv.org/abs/2312.16012v1"}
{"created":"2023-12-26 11:38:36","title":"Achieving Fairness in DareFightingICE Agents Evaluation Through a Delay Mechanism","abstract":"This paper proposes a delay mechanism to mitigate the impact of latency differences in the gRPC framework--a high-performance, open-source universal remote procedure call (RPC) framework--between different programming languages on the performance of agents in DareFightingICE, a fighting game research platform. The study finds that gRPC latency differences between Java and Python can significantly impact real-time decision-making. Without a delay mechanism, Java-based agents outperform Python-based ones due to lower gRPC latency on the Java platform. However, with the proposed delay mechanism, both Java-based and Python-based agents exhibit similar performance, leading to a fair comparison between agents developed using different programming languages. Thus, this work underscores the crucial importance of considering gRPC latency when developing and evaluating agents in DareFightingICE, and the insights gained could potentially extend to other gRPC-based applications.","sentences":["This paper proposes a delay mechanism to mitigate the impact of latency differences in the gRPC framework--a high-performance, open-source universal remote procedure call (RPC) framework--between different programming languages on the performance of agents in DareFightingICE, a fighting game research platform.","The study finds that gRPC latency differences between Java and Python can significantly impact real-time decision-making.","Without a delay mechanism, Java-based agents outperform Python-based ones due to lower gRPC latency on the Java platform.","However, with the proposed delay mechanism, both Java-based and Python-based agents exhibit similar performance, leading to a fair comparison between agents developed using different programming languages.","Thus, this work underscores the crucial importance of considering gRPC latency when developing and evaluating agents in DareFightingICE, and the insights gained could potentially extend to other gRPC-based applications."],"url":"http://arxiv.org/abs/2312.16010v1"}
{"created":"2023-12-26 11:26:44","title":"A fully decentralized auditing approach for edge computing: A Game-Theoretic Perspective","abstract":"Edge storage presents a viable data storage alternative for application vendors (AV), offering benefits such as reduced bandwidth overhead and latency compared to cloud storage. However, data cached in edge computing systems is susceptible to intentional or accidental disturbances. This paper proposes a decentralized integrity auditing scheme to safeguard data integrity and counter the traditional reliance on centralized third-party auditors (TPA), which are unfit for distributed systems. Our novel approach employs edge servers (ES) as mutual auditors, eliminating the need for a centralized entity. This decentralization minimizes potential collusion with malicious auditors and biases in audit outcomes. Using a strategic game model, we demonstrate that ESs are more motivated to audit each other than TPAs. The auditing process is addressed as a Nash Equilibrium problem, assuring accurate integrity proof through incentives for ESs. Our scheme's security and performance are rigorously assessed, showing it is secure within the random oracle model, offers improved speed, and is cost-effective compared to existing methods.","sentences":["Edge storage presents a viable data storage alternative for application vendors (AV), offering benefits such as reduced bandwidth overhead and latency compared to cloud storage.","However, data cached in edge computing systems is susceptible to intentional or accidental disturbances.","This paper proposes a decentralized integrity auditing scheme to safeguard data integrity and counter the traditional reliance on centralized third-party auditors (TPA), which are unfit for distributed systems.","Our novel approach employs edge servers (ES) as mutual auditors, eliminating the need for a centralized entity.","This decentralization minimizes potential collusion with malicious auditors and biases in audit outcomes.","Using a strategic game model, we demonstrate that ESs are more motivated to audit each other than TPAs.","The auditing process is addressed as a Nash Equilibrium problem, assuring accurate integrity proof through incentives for ESs.","Our scheme's security and performance are rigorously assessed, showing it is secure within the random oracle model, offers improved speed, and is cost-effective compared to existing methods."],"url":"http://arxiv.org/abs/2312.16007v1"}
{"created":"2023-12-26 11:07:37","title":"Pricing with Contextual Elasticity and Heteroscedastic Valuation","abstract":"We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality regarding $d$ and $T$ (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.","sentences":["We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price.","We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise.","To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences.","We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality regarding $d$ and $T$ (up to $\\log T$ factors).","Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies."],"url":"http://arxiv.org/abs/2312.15999v1"}
{"created":"2023-12-26 11:01:36","title":"Aligning Large Language Models with Human Preferences through Representation Engineering","abstract":"Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.","sentences":["Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness.","Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses.","Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation.","Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations.","This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement.","Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias).","RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance."],"url":"http://arxiv.org/abs/2312.15997v1"}
{"created":"2023-12-26 10:55:20","title":"Generalization in Kernel Regression Under Realistic Assumptions","abstract":"It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the kernel provides it with an implicit form of regularization, enabling good generalization. When applied to common kernels, our results imply benign overfitting in high input dimensions, nearly tempered overfitting in fixed dimensions, and explicit convergence rates for regularized regression. As a by-product, we obtain time-dependent bounds for neural networks trained in the kernel regime.","sentences":["It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise.","Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression.","However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup.","This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings.","Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples.","Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest.","These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the kernel provides it with an implicit form of regularization, enabling good generalization.","When applied to common kernels, our results imply benign overfitting in high input dimensions, nearly tempered overfitting in fixed dimensions, and explicit convergence rates for regularized regression.","As a by-product, we obtain time-dependent bounds for neural networks trained in the kernel regime."],"url":"http://arxiv.org/abs/2312.15995v1"}
{"created":"2023-12-26 10:54:15","title":"Practical Bias Mitigation through Proxy Sensitive Attribute Label Generation","abstract":"Addressing bias in the trained machine learning system often requires access to sensitive attributes. In practice, these attributes are not available either due to legal and policy regulations or data unavailability for a given demographic. Existing bias mitigation algorithms are limited in their applicability to real-world scenarios as they require access to sensitive attributes to achieve fairness. In this research work, we aim to address this bottleneck through our proposed unsupervised proxy-sensitive attribute label generation technique. Towards this end, we propose a two-stage approach of unsupervised embedding generation followed by clustering to obtain proxy-sensitive labels. The efficacy of our work relies on the assumption that bias propagates through non-sensitive attributes that are correlated to the sensitive attributes and, when mapped to the high dimensional latent space, produces clusters of different demographic groups that exist in the data. Experimental results demonstrate that bias mitigation using existing algorithms such as Fair Mixup and Adversarial Debiasing yields comparable results on derived proxy labels when compared against using true sensitive attributes.","sentences":["Addressing bias in the trained machine learning system often requires access to sensitive attributes.","In practice, these attributes are not available either due to legal and policy regulations or data unavailability for a given demographic.","Existing bias mitigation algorithms are limited in their applicability to real-world scenarios as they require access to sensitive attributes to achieve fairness.","In this research work, we aim to address this bottleneck through our proposed unsupervised proxy-sensitive attribute label generation technique.","Towards this end, we propose a two-stage approach of unsupervised embedding generation followed by clustering to obtain proxy-sensitive labels.","The efficacy of our work relies on the assumption that bias propagates through non-sensitive attributes that are correlated to the sensitive attributes and, when mapped to the high dimensional latent space, produces clusters of different demographic groups that exist in the data.","Experimental results demonstrate that bias mitigation using existing algorithms such as Fair Mixup and Adversarial Debiasing yields comparable results on derived proxy labels when compared against using true sensitive attributes."],"url":"http://arxiv.org/abs/2312.15994v1"}
{"created":"2023-12-26 10:51:46","title":"Adaptive Kalman-based hybrid car following strategy using TD3 and CACC","abstract":"In autonomous driving, the hybrid strategy of deep reinforcement learning and cooperative adaptive cruise control (CACC) can fully utilize the advantages of the two algorithms and significantly improve the performance of car following. However, it is challenging for the traditional hybrid strategy based on fixed coefficients to adapt to mixed traffic flow scenarios, which may decrease the performance and even lead to accidents. To address the above problems, a hybrid car following strategy based on an adaptive Kalman Filter is proposed by regarding CACC and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Different from traditional hybrid strategy based on fixed coefficients, the Kalman gain H, using as an adaptive coefficient, is derived from multi-timestep predictions and Monte Carlo Tree Search. At the end of study, simulation results with 4157745 timesteps indicate that, compared with the TD3 and HCFS algorithms, the proposed algorithm in this study can substantially enhance the safety of car following in mixed traffic flow without compromising the comfort and efficiency.","sentences":["In autonomous driving, the hybrid strategy of deep reinforcement learning and cooperative adaptive cruise control (CACC) can fully utilize the advantages of the two algorithms and significantly improve the performance of car following.","However, it is challenging for the traditional hybrid strategy based on fixed coefficients to adapt to mixed traffic flow scenarios, which may decrease the performance and even lead to accidents.","To address the above problems, a hybrid car following strategy based on an adaptive Kalman Filter is proposed by regarding CACC and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms.","Different from traditional hybrid strategy based on fixed coefficients, the Kalman gain H, using as an adaptive coefficient, is derived from multi-timestep predictions and Monte Carlo Tree Search.","At the end of study, simulation results with 4157745 timesteps indicate that, compared with the TD3 and HCFS algorithms, the proposed algorithm in this study can substantially enhance the safety of car following in mixed traffic flow without compromising the comfort and efficiency."],"url":"http://arxiv.org/abs/2312.15993v1"}
{"created":"2023-12-26 10:34:02","title":"Full-Stack End-to-End Sub-THz Simulations at 140 GHz using NYUSIM Channel Model in ns-3","abstract":"The next generation of wireless communication, is expected to harness the potential of the sub-THz bands to achieve exceptional performance and ubiquitous connectivity. However, network simulators such as ns-3 currently lack support for channel models above 100 GHz. This limits the ability of researchers to study, design, and evaluate systems operating above 100 GHz. Here, we show that drop based NYUSIM channel model can be used to simulate channels above 100 GHz in all 3GPP scenarios including urban microcell (UMi), urban macrocell (UMa), rural macrocell (RMa), indoor hotspot (InH), and indoor factory (InF). We evaluate the full stack downlink end-to-end performance (throughput, latency and packet drop) experienced by single user equipment (UE) connected to a Next Generation Node B (gNB) operating in the sub-THz bands for three gNB-UE antenna configurations: 8x8-4x4, 16x16-4x4, and 64x64-8x8 by using NYUSIM channel model at 140 GHz in the ns-3 mmWave module. Additionally, it is found that determining the exact number of realizations required to obtain statistically significant results using simulation platforms like ns-3 remains challenging, as end-to-end performance metrics vary strongly with the number of realizations. Hence, we show the variation in throughput vs number of realizations and find the optimal number of realizations required to obtain statistically significant results. We strongly encourage researchers worldwide to adopt a similar approach, as it enables the readers to assess the accuracy and reliability of the reported results and enhance the finding's overall interpretability.","sentences":["The next generation of wireless communication, is expected to harness the potential of the sub-THz bands to achieve exceptional performance and ubiquitous connectivity.","However, network simulators such as ns-3 currently lack support for channel models above 100 GHz.","This limits the ability of researchers to study, design, and evaluate systems operating above 100 GHz.","Here, we show that drop based NYUSIM channel model can be used to simulate channels above 100 GHz in all 3GPP scenarios including urban microcell (UMi), urban macrocell (UMa), rural macrocell (RMa), indoor hotspot (InH), and indoor factory (InF).","We evaluate the full stack downlink end-to-end performance (throughput, latency and packet drop) experienced by single user equipment (UE) connected to a Next Generation Node B (gNB) operating in the sub-THz bands for three gNB-UE antenna configurations: 8x8-4x4, 16x16-4x4, and 64x64-8x8 by using NYUSIM channel model at 140 GHz in the ns-3 mmWave module.","Additionally, it is found that determining the exact number of realizations required to obtain statistically significant results using simulation platforms like ns-3 remains challenging, as end-to-end performance metrics vary strongly with the number of realizations.","Hence, we show the variation in throughput vs number of realizations and find the optimal number of realizations required to obtain statistically significant results.","We strongly encourage researchers worldwide to adopt a similar approach, as it enables the readers to assess the accuracy and reliability of the reported results and enhance the finding's overall interpretability."],"url":"http://arxiv.org/abs/2312.15987v1"}
{"created":"2023-12-26 10:30:05","title":"Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents","abstract":"Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages. This study aims to explore the efficiency of language as a communication medium. We put forth two specific hypotheses: First, discrete messages are more effective than continuous ones when agents have diverse personal experiences. Second, communications using multiple discrete tokens are more advantageous than those using a single token. To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners. Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency. The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies.","sentences":["Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages.","This study aims to explore the efficiency of language as a communication medium.","We put forth two specific hypotheses:","First, discrete messages are more effective than continuous ones when agents have diverse personal experiences.","Second, communications using multiple discrete tokens are more advantageous than those using a single token.","To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners.","Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency.","The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies."],"url":"http://arxiv.org/abs/2312.15985v1"}
{"created":"2023-12-26 10:15:28","title":"HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D","abstract":"Recent progress in single-image 3D generation highlights the importance of multi-view coherency, leveraging 3D priors from large-scale diffusion models pretrained on Internet-scale images. However, the aspect of novel-view diversity remains underexplored within the research landscape due to the ambiguity in converting a 2D image into 3D content, where numerous potential shapes can emerge. Here, we aim to address this research gap by simultaneously addressing both consistency and diversity. Yet, striking a balance between these two aspects poses a considerable challenge due to their inherent trade-offs. This work introduces HarmonyView, a simple yet effective diffusion sampling technique adept at decomposing two intricate aspects in single-image 3D generation: consistency and diversity. This approach paves the way for a more nuanced exploration of the two critical dimensions within the sampling process. Moreover, we propose a new evaluation metric based on CLIP image and text encoders to comprehensively assess the diversity of the generated views, which closely aligns with human evaluators' judgments. In experiments, HarmonyView achieves a harmonious balance, demonstrating a win-win scenario in both consistency and diversity.","sentences":["Recent progress in single-image 3D generation highlights the importance of multi-view coherency, leveraging 3D priors from large-scale diffusion models pretrained on Internet-scale images.","However, the aspect of novel-view diversity remains underexplored within the research landscape due to the ambiguity in converting a 2D image into 3D content, where numerous potential shapes can emerge.","Here, we aim to address this research gap by simultaneously addressing both consistency and diversity.","Yet, striking a balance between these two aspects poses a considerable challenge due to their inherent trade-offs.","This work introduces HarmonyView, a simple yet effective diffusion sampling technique adept at decomposing two intricate aspects in single-image 3D generation: consistency and diversity.","This approach paves the way for a more nuanced exploration of the two critical dimensions within the sampling process.","Moreover, we propose a new evaluation metric based on CLIP image and text encoders to comprehensively assess the diversity of the generated views, which closely aligns with human evaluators' judgments.","In experiments, HarmonyView achieves a harmonious balance, demonstrating a win-win scenario in both consistency and diversity."],"url":"http://arxiv.org/abs/2312.15980v1"}
{"created":"2023-12-26 10:00:33","title":"Considerations about temporal rescaling, discretization, and linearization of RNNs","abstract":"We explore the mathematical foundations of Recurrent Neural Networks (RNNs) and three fundamental procedures: temporal rescaling, discretization, and linearization. These techniques provide essential tools for characterizing RNN behaviour, enabling insights into temporal dynamics, practical computational implementation, and linear approximations for analysis. We discuss the flexible order of application of these procedures, emphasizing their significance in modelling and analyzing RNNs for computational neuroscience and machine learning applications. We explicitly describe here under what conditions these procedures can be interchangeable.","sentences":["We explore the mathematical foundations of Recurrent Neural Networks (RNNs) and three fundamental procedures: temporal rescaling, discretization, and linearization.","These techniques provide essential tools for characterizing RNN behaviour, enabling insights into temporal dynamics, practical computational implementation, and linear approximations for analysis.","We discuss the flexible order of application of these procedures, emphasizing their significance in modelling and analyzing RNNs for computational neuroscience and machine learning applications.","We explicitly describe here under what conditions these procedures can be interchangeable."],"url":"http://arxiv.org/abs/2312.15974v1"}
{"created":"2023-12-26 09:43:30","title":"Graph Context Transformation Learning for Progressive Correspondence Pruning","abstract":"Most of existing correspondence pruning methods only concentrate on gathering the context information as much as possible while neglecting effective ways to utilize such information. In order to tackle this dilemma, in this paper we propose Graph Context Transformation Network (GCT-Net) enhancing context information to conduct consensus guidance for progressive correspondence pruning. Specifically, we design the Graph Context Enhance Transformer which first generates the graph network and then transforms it into multi-branch graph contexts. Moreover, it employs self-attention and cross-attention to magnify characteristics of each graph context for emphasizing the unique as well as shared essential information. To further apply the recalibrated graph contexts to the global domain, we propose the Graph Context Guidance Transformer. This module adopts a confident-based sampling strategy to temporarily screen high-confidence vertices for guiding accurate classification by searching global consensus between screened vertices and remaining ones. The extensive experimental results on outlier removal and relative pose estimation clearly demonstrate the superior performance of GCT-Net compared to state-of-the-art methods across outdoor and indoor datasets. The source code will be available at: https://github.com/guobaoxiao/GCT-Net/.","sentences":["Most of existing correspondence pruning methods only concentrate on gathering the context information as much as possible while neglecting effective ways to utilize such information.","In order to tackle this dilemma, in this paper we propose Graph Context Transformation Network (GCT-Net) enhancing context information to conduct consensus guidance for progressive correspondence pruning.","Specifically, we design the Graph Context Enhance Transformer which first generates the graph network and then transforms it into multi-branch graph contexts.","Moreover, it employs self-attention and cross-attention to magnify characteristics of each graph context for emphasizing the unique as well as shared essential information.","To further apply the recalibrated graph contexts to the global domain, we propose the Graph Context Guidance Transformer.","This module adopts a confident-based sampling strategy to temporarily screen high-confidence vertices for guiding accurate classification by searching global consensus between screened vertices and remaining ones.","The extensive experimental results on outlier removal and relative pose estimation clearly demonstrate the superior performance of GCT-Net compared to state-of-the-art methods across outdoor and indoor datasets.","The source code will be available at: https://github.com/guobaoxiao/GCT-Net/."],"url":"http://arxiv.org/abs/2312.15971v1"}
{"created":"2023-12-26 09:36:21","title":"Learning Deformable Hypothesis Sampling for Accurate PatchMatch Multi-View Stereo","abstract":"This paper introduces a learnable Deformable Hypothesis Sampler (DeformSampler) to address the challenging issue of noisy depth estimation for accurate PatchMatch Multi-View Stereo (MVS). We observe that the heuristic depth hypothesis sampling modes employed by PatchMatch MVS solvers are insensitive to (i) the piece-wise smooth distribution of depths across the object surface, and (ii) the implicit multi-modal distribution of depth prediction probabilities along the ray direction on the surface points. Accordingly, we develop DeformSampler to learn distribution-sensitive sample spaces to (i) propagate depths consistent with the scene's geometry across the object surface, and (ii) fit a Laplace Mixture model that approaches the point-wise probabilities distribution of the actual depths along the ray direction. We integrate DeformSampler into a learnable PatchMatch MVS system to enhance depth estimation in challenging areas, such as piece-wise discontinuous surface boundaries and weakly-textured regions. Experimental results on DTU and Tanks \\& Temples datasets demonstrate its superior performance and generalization capabilities compared to state-of-the-art competitors. Code is available at https://github.com/Geo-Tell/DS-PMNet.","sentences":["This paper introduces a learnable Deformable Hypothesis Sampler (DeformSampler) to address the challenging issue of noisy depth estimation for accurate PatchMatch Multi-View Stereo (MVS).","We observe that the heuristic depth hypothesis sampling modes employed by PatchMatch MVS solvers are insensitive to (i) the piece-wise smooth distribution of depths across the object surface, and (ii) the implicit multi-modal distribution of depth prediction probabilities along the ray direction on the surface points.","Accordingly, we develop DeformSampler to learn distribution-sensitive sample spaces to (i) propagate depths consistent with the scene's geometry across the object surface, and (ii) fit a Laplace Mixture model that approaches the point-wise probabilities distribution of the actual depths along the ray direction.","We integrate DeformSampler into a learnable PatchMatch MVS system to enhance depth estimation in challenging areas, such as piece-wise discontinuous surface boundaries and weakly-textured regions.","Experimental results on DTU and Tanks \\& Temples datasets demonstrate its superior performance and generalization capabilities compared to state-of-the-art competitors.","Code is available at https://github.com/Geo-Tell/DS-PMNet."],"url":"http://arxiv.org/abs/2312.15970v1"}
{"created":"2023-12-26 09:32:42","title":"Exploiting the capacity of deep networks only at training stage for nonlinear black-box system identification","abstract":"To benefit from the modeling capacity of deep models in system identification, without worrying about inference time, this study presents a novel training strategy that uses deep models only at the training stage. For this purpose two separate models with different structures and goals are employed. The first one is a deep generative model aiming at modeling the distribution of system output(s), called the teacher model, and the second one is a shallow basis function model, named the student model, fed by system input(s) to predict the system output(s). That means these isolated paths must reach the same ultimate target. As deep models show a great performance in modeling of highly nonlinear systems, aligning the representation space learned by these two models make the student model to inherit the approximation power of the teacher model. The proposed objective function consists of the objective of each student and teacher model adding up with a distance penalty between the learned latent representations. The simulation results on three nonlinear benchmarks show a comparative performance with examined deep architectures applied on the same benchmarks. Algorithmic transparency and structure efficiency are also achieved as byproducts.","sentences":["To benefit from the modeling capacity of deep models in system identification, without worrying about inference time, this study presents a novel training strategy that uses deep models only at the training stage.","For this purpose two separate models with different structures and goals are employed.","The first one is a deep generative model aiming at modeling the distribution of system output(s), called the teacher model, and the second one is a shallow basis function model, named the student model, fed by system input(s) to predict the system output(s).","That means these isolated paths must reach the same ultimate target.","As deep models show a great performance in modeling of highly nonlinear systems, aligning the representation space learned by these two models make the student model to inherit the approximation power of the teacher model.","The proposed objective function consists of the objective of each student and teacher model adding up with a distance penalty between the learned latent representations.","The simulation results on three nonlinear benchmarks show a comparative performance with examined deep architectures applied on the same benchmarks.","Algorithmic transparency and structure efficiency are also achieved as byproducts."],"url":"http://arxiv.org/abs/2312.15969v1"}
