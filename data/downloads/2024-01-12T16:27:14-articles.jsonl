{"created":"2024-01-10 18:37:59","title":"Leveraging Print Debugging to Improve Code Generation in Large Language Models","abstract":"Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a \"print debugging\" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.","sentences":["Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal.","To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a \"print debugging\" method, which involves inserting print statements to trace and analysing logs for fixing the bug.","We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system.","Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%."],"url":"http://arxiv.org/abs/2401.05319v1"}
{"created":"2024-01-10 18:09:36","title":"Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?","abstract":"Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot's behavior X, would the human observer find it explicable?\". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test. We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack.","sentences":["Large Language Models have shown exceptional generative abilities in various natural language and generation tasks.","However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models.","While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction.","In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer.","We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors.","The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot's behavior X, would the human observer find it explicable?\".","We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains.","A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities.","We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test.","We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack."],"url":"http://arxiv.org/abs/2401.05302v1"}
{"created":"2024-01-10 18:06:27","title":"I am a Strange Dataset: Metalinguistic Tests for Language Models","abstract":"Statements involving metalinguistic self-reference (\"This paper has six sections.\") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present \"I am a Strange Dataset\", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like \"The penultimate word in this sentence is\" (where a correct continuation is \"is\"). In verification, models judge the truth of statements like \"The penultimate word in this sentence is sentence.\" (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range. The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset.","sentences":["Statements involving metalinguistic self-reference (\"This paper has six sections.\") are prevalent in many domains.","Can large language models (LLMs) handle such language?","In this paper, we present \"I am a Strange Dataset\", a new dataset for addressing this question.","There are two subtasks: generation and verification.","In generation, models continue statements like \"The penultimate word in this sentence is\" (where a correct continuation is \"is\").","In verification, models judge the truth of statements like \"The penultimate word in this sentence is sentence.\"","(false).","We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all.","The dataset is hand-crafted by experts and validated by non-expert annotators.","We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs.","All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale.","GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range.","The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset."],"url":"http://arxiv.org/abs/2401.05300v1"}
{"created":"2024-01-10 17:13:28","title":"INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges","abstract":"This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The paper also discusses potential enhancements and future applications, positioning INACIA as a model for worldwide AI integration in legal domains.","sentences":["This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU).","The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation.","Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations.","Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment.","The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems.","The paper also discusses potential enhancements and future applications, positioning INACIA as a model for worldwide AI integration in legal domains."],"url":"http://arxiv.org/abs/2401.05273v1"}
{"created":"2024-01-10 16:57:24","title":"AUTOACT: Automatic Agent Learning from Scratch via Self-Planning","abstract":"Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.","sentences":["Language agents have achieved considerable performance on various complex tasks.","Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions.","To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4).","Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models.","Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task.","We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines.","We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent.","Code will be available at https://github.com/zjunlp/AutoAct."],"url":"http://arxiv.org/abs/2401.05268v1"}
{"created":"2024-01-10 16:21:18","title":"CASA: Causality-driven Argument Sufficiency Assessment","abstract":"The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.","sentences":["The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.","To tackle this task, existing works often train a classifier on data annotated by humans.","However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria.","Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework.","PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent.","To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event.","Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments.","We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments.","Code and data are available at https://github.com/xxxiaol/CASA."],"url":"http://arxiv.org/abs/2401.05249v1"}
{"created":"2024-01-10 15:27:41","title":"Pre-trained Large Language Models for Financial Sentiment Analysis","abstract":"Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral). In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples. In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4]. Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms.","sentences":["Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral).","In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples.","To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs)","[1, 2, 3] to solve this problem.","The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples.","In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4].","Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2401.05215v1"}
{"created":"2024-01-10 14:53:18","title":"Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking","abstract":"Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBeluga2 trailing closely, presenting an attractive option given its data privacy and customization benefits. Overall, this work offers preliminary insights for factories considering using LLM-tools for knowledge management.","sentences":["Managing knowledge efficiently is crucial for organizational success.","In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators.","In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation.","The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge.","To assess its effectiveness, we conducted an evaluation in a factory setting.","The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues.","However, the study also highlighted a preference for learning from a human expert when such an option is available.","Furthermore, we benchmarked several closed and open-sourced LLMs for this system.","GPT-4 consistently outperformed its counterparts, with open-source models like StableBeluga2 trailing closely, presenting an attractive option given its data privacy and customization benefits.","Overall, this work offers preliminary insights for factories considering using LLM-tools for knowledge management."],"url":"http://arxiv.org/abs/2401.05200v1"}
{"created":"2024-01-10 14:50:46","title":"Monte Carlo Tree Search for Recipe Generation using GPT-2","abstract":"Automatic food recipe generation methods provide a creative tool for chefs to explore and to create new, and interesting culinary delights. Given the recent success of large language models (LLMs), they have the potential to create new recipes that can meet individual preferences, dietary constraints, and adapt to what is in your refrigerator. Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes. However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes. In this paper, we propose RecipeMC, a text generation method using GPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to define reward functions to put soft constraints on text generation and thus improve the credibility of the generated recipes. Our results show that human evaluators prefer recipes generated with RecipeMC more often than recipes generated with other baseline methods when compared with real recipes.","sentences":["Automatic food recipe generation methods provide a creative tool for chefs to explore and to create new, and interesting culinary delights.","Given the recent success of large language models (LLMs), they have the potential to create new recipes that can meet individual preferences, dietary constraints, and adapt to what is in your refrigerator.","Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes.","However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes.","In this paper, we propose RecipeMC, a text generation method using GPT-2 that relies on Monte Carlo Tree Search (MCTS).","RecipeMC allows us to define reward functions to put soft constraints on text generation and thus improve the credibility of the generated recipes.","Our results show that human evaluators prefer recipes generated with RecipeMC more often than recipes generated with other baseline methods when compared with real recipes."],"url":"http://arxiv.org/abs/2401.05199v1"}
{"created":"2024-01-10 14:38:46","title":"Divide and Conquer for Large Language Models Reasoning","abstract":"Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed strategy significantly boosts the models' reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks. For instance, compared to baseline, we make a striking improvement on low confidence subsets of 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense. In addition, through extensive analysis on length of rationale and number of options, we verify that longer reasoning paths in PKR could prevent models from referring infer-harmful shortcuts, and also find that removing irrelevant choices in FCR would substantially avoid models' confusion. The code is at \\url{https://github.com/AiMijie/Divide-and-Conquer}","sentences":["Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs).","However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones.","To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning.","First, we divide questions into different subsets based on the statistical confidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants.","Our experiments demonstrate that this proposed strategy significantly boosts the models' reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks.","For instance, compared to baseline, we make a striking improvement on low confidence subsets of 8.72\\% for AQuA, 15.07\\% for ARC Challenge and 7.71\\% for RiddleSense.","In addition, through extensive analysis on length of rationale and number of options, we verify that longer reasoning paths in PKR could prevent models from referring infer-harmful shortcuts, and also find that removing irrelevant choices in FCR would substantially avoid models' confusion.","The code is at \\url{https://github.com/AiMijie/Divide-and-Conquer}"],"url":"http://arxiv.org/abs/2401.05190v1"}
{"created":"2024-01-10 14:20:33","title":"Can ChatGPT Rival Neural Machine Translation? A Comparative Study","abstract":"Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings provide valuable insights into the potential of ChatGPT as a capable machine translator, and the influence of prompt engineering on its performance.","sentences":["Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English.","Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics.","Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task.","Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment.","These findings provide valuable insights into the potential of ChatGPT as a capable machine translator, and the influence of prompt engineering on its performance."],"url":"http://arxiv.org/abs/2401.05176v1"}
{"created":"2024-01-10 13:56:40","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA","abstract":"Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP. Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models. The code and model weights will be released upon the paper's acceptance.","sentences":["Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance.","However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios.","Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking.","In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks.","Unlike existing methods, we treat medical VQA as a generative task.","We unify the text encoder and multimodal encoder and align image-text features through multi-task learning.","Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP.","Experiments show that our method achieves excellent results with fewer multimodal datasets and demonstrates the advantages of generative VQA models.","The code and model weights will be released upon the paper's acceptance."],"url":"http://arxiv.org/abs/2401.05163v1"}
{"created":"2024-01-10 11:03:53","title":"Aligning Translation-Specific Understanding to General Understanding in Large Language Models","abstract":"Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation. One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs. To align the translation-specific understanding to the general one, we propose a novel translation process xIoD (Cross-Lingual Interpretation of Difficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation. Specifically, xIoD performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools of QE to tackle the challenges of xIoD in the detection of difficult words and the generation of helpful interpretations. We conduct experiments on the self-constructed benchmark ChallengeMT, which includes cases in which multiple SOTA translation systems consistently underperform. Experimental results show the effectiveness of our xIoD, which improves up to +3.85 COMET.","sentences":["Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation.","One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs.","To align the translation-specific understanding to the general one, we propose a novel translation process xIoD (Cross-Lingual Interpretation of Difficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation.","Specifically, xIoD performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations.","Furthermore, we reframe the external tools of QE to tackle the challenges of xIoD in the detection of difficult words and the generation of helpful interpretations.","We conduct experiments on the self-constructed benchmark ChallengeMT, which includes cases in which multiple SOTA translation systems consistently underperform.","Experimental results show the effectiveness of our xIoD, which improves up to +3.85 COMET."],"url":"http://arxiv.org/abs/2401.05072v1"}
{"created":"2024-01-10 09:49:10","title":"Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk","abstract":"Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via \"self-talk\" of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.","sentences":["Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging.","Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate.","Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions.","Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles.","This approach generates a training data via \"self-talk\" of LLMs that can be refined and utilized for supervised fine-tuning.","We introduce an automated way to measure the (partial) success of a dialogue.","This metric is used to filter the generated conversational data that is fed back in LLM for training.","Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results.","In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data."],"url":"http://arxiv.org/abs/2401.05033v1"}
{"created":"2024-01-10 08:28:56","title":"Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis","abstract":"Recently, large language models such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, \\ie task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by experiments to systematically analyze the impact of different factors on two public datasets. Finally, we summarize promising directions to shed lights on future research.","sentences":["Recently, large language models such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems.","To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering.","We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders.","To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios.","As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs.","As for prompt engineering, we further analyze the impact of four important components of prompts, \\ie task descriptions, user interest modeling, candidate items construction and prompting strategies.","In each section, we first define and categorize concepts in line with the existing literature.","Then, we propose inspiring research questions followed by experiments to systematically analyze the impact of different factors on two public datasets.","Finally, we summarize promising directions to shed lights on future research."],"url":"http://arxiv.org/abs/2401.04997v1"}
{"created":"2024-01-10 06:21:47","title":"Can AI Write Classical Chinese Poetry like Humans? An Empirical Study Inspired by Turing Test","abstract":"Some argue that the essence of humanity, such as creativity and sentiment, can never be mimicked by machines. This paper casts doubt on this belief by studying a vital question: Can AI compose poetry as well as humans? To answer the question, we propose ProFTAP, a novel evaluation framework inspired by Turing test to assess AI's poetry writing capability. We apply it on current large language models (LLMs) and find that recent LLMs do indeed possess the ability to write classical Chinese poems nearly indistinguishable from those of humans. We also reveal that various open-source LLMs can outperform GPT-4 on this task.","sentences":["Some argue that the essence of humanity, such as creativity and sentiment, can never be mimicked by machines.","This paper casts doubt on this belief by studying a vital question: Can AI compose poetry as well as humans?","To answer the question, we propose ProFTAP, a novel evaluation framework inspired by Turing test to assess AI's poetry writing capability.","We apply it on current large language models (LLMs) and find that recent LLMs do indeed possess the ability to write classical Chinese poems nearly indistinguishable from those of humans.","We also reveal that various open-source LLMs can outperform GPT-4 on this task."],"url":"http://arxiv.org/abs/2401.04952v1"}
{"created":"2024-01-10 04:37:38","title":"The Impact of Reasoning Step Length on Large Language Models","abstract":"Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.","sentences":["Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs).","However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown.","To shed light on this, we have conducted several empirical experiments to explore the relations.","Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant.","We have the following key findings.","First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets.","Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models.","This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios.","Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations.","Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference.","Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences."],"url":"http://arxiv.org/abs/2401.04925v1"}
