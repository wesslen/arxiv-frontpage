{"text":"However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation.","cats":{"robustness":0,"programming":0}}
{"text":"These findings offer instructive insights aimed at advancing the field of tool learning.","cats":{"security":0,"social-sciences":0,"programming":0}}
{"text":"We provide extensive evaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval, revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs.","cats":{"security":0,"production":1,"programming":0}}
{"text":"Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP).","cats":{"security":0,"programming":0}}
{"text":"Different from existing studies on controllable text generation, CoDI-Eval extends the scope to the prevalent instruction-following paradigm for the first time.","cats":{"hci":0,"programming":0}}
{"text":"We highlight three advantages of the low-code LLM: controllable generation results, user-friendly human-LLM interaction, and broadly applicable scenarios.","cats":{"hci":1,"programming":1}}
{"text":"We impose this constraint by finetuning the generative model with a set of anchor images that capture the characteristics of the input image.","cats":{"social-sciences":0,"education":0,"programming":0}}
{"text":"As such, there is a need for tools and technologies which may assist teachers and other evaluators in identifying whether any portion of a source code is LLM generated.   ","cats":{"education":1,"programming":1}}
{"text":"However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing.","cats":{"programming":0}}
{"text":"Specifically, DSR generates SAR images at arbitrary view angles in real-time.","cats":{"programming":0}}
{"text":"In this technical report, we will illustrate in detail all aspects of GeoGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation.","cats":{"programming":0}}
{"text":"Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers.","cats":{"programming":0}}
{"text":"This paper proposes a method for collision avoidance in an autonomous fast moving dynamic quadrotor UAV tracking and following another target UAV.","cats":{"programming":0}}
{"text":"We implement our solution on AirSim simulator over PX4 flight controller and with numerical results, we validate our approach through several simulation experiments with multiple scenarios and trajectories.","cats":{"programming":0}}
{"text":"Extensive experiments demonstrate that InsActor achieves state-of-the-art results on various tasks, including instruction-driven motion generation and instruction-driven waypoint heading.","cats":{"programming":0}}
{"text":"This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks.","cats":{"programming":1}}
{"text":"The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow.","cats":{"programming":1}}
{"text":"After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements.","cats":{"programming":1}}
{"text":"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects.","cats":{"programming":0}}
{"text":"Finally, by using the previous result, we show that the class of languages described by linear GF(2)-grammars is not closed under GF(2)-concatenation","cats":{"programming":0}}
{"text":"We propose a zero-shot LLM-based method enriched by retrieval-augmented generation and MapReduce, which pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis.","cats":{"programming":0}}
{"text":"Additionally, through an empirical analysis of existing works, we demonstrate that Mukhyansh outperforms all other models, achieving an impressive average ROUGE-L score of 31.43 across all 8 languages.","cats":{"programming":0}}
{"text":"Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID","cats":{"programming":0}}
{"text":"The LLM will make use of graph-based code representations for advanced semantic comprehension.","cats":{"programming":1}}
{"text":"We present LaMPilot, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives.","cats":{"programming":1}}
{"text":"We then evaluate a wide range of state-of-the-art code generation language models on tasks from the LaMPilot Benchmark.","cats":{"programming":1}}
{"text":"Next, we study a more naturalistic task: predicting the next character in a dataset of computer code.","cats":{"programming":1}}
{"text":"We find similar generalization gaps between the original model and simplified proxies, and conduct further analysis to investigate which aspects of the code completion task are associated with the largest gaps.","cats":{"programming":1}}
{"text":"Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated remarkable accuracy in a wide range of tasks.","cats":{"programming":0}}
{"text":"Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code.","cats":{"programming":1}}
{"text":"The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion.","cats":{"programming":1}}
{"text":"We outline how such models could be integrated into computing education tools, and discuss their potential for supporting students when learning programming.","cats":{"programming":1}}
{"text":"In this paper, we present a comprehensive study on multilingual code embeddings, focusing on the cross-lingual capabilities of these embeddings across different programming languages.","cats":{"programming":1}}
{"text":"The large language based-model chatbot ChatGPT gained a lot of popularity since its launch and has been used in a wide range of situations.","cats":{"programming":0}}
